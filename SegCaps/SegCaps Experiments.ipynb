{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/computation/__init__.py:18: UserWarning: The installed version of numexpr 2.4.3 is not supported in pandas and will be not be used\n",
      "The minimum supported version is 2.4.6\n",
      "\n",
      "  ver=ver, min_ver=_MIN_NUMEXPR_VERSION), UserWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "import numpy as np\n",
    "import capsnet\n",
    "import capsule_layers\n",
    "import dataset\n",
    "import model_functions as mf\n",
    "import audio_functions as af\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Set other variables\n",
    "sample_rate=16384\n",
    "n_fft=1024\n",
    "fft_hop=256\n",
    "patch_window=256\n",
    "patch_hop=128\n",
    "n_parallel_readers=2\n",
    "normalise=True\n",
    "batch_size = 5\n",
    "shuffle=False\n",
    "n_shuffle = 10\n",
    "\n",
    "#directory_a = 'C:/Users/Toby/MSc_Project/Test_Audio/CHiME/test/Mixed'\n",
    "#directory_b = 'C:/Users/Toby/MSc_Project/Test_Audio/CHiME/test/Voice'\n",
    "\n",
    "directory_a = '/home/enterprise.internal.city.ac.uk/acvn728/LibriSpeechMini/Voice/train-clean-100/19/198/'\n",
    "directory_b = '/home/enterprise.internal.city.ac.uk/acvn728/LibriSpeechMini/Mixed/train-clean-100/19/198/'\n",
    "\n",
    "#  Create the pipeline\n",
    "tf.reset_default_graph()\n",
    "data = dataset.zip_files(directory_a, directory_b)\n",
    "data = dataset.get_paired_dataset(data,\n",
    "                                  sample_rate,\n",
    "                                  n_fft,\n",
    "                                  fft_hop,\n",
    "                                  patch_window,\n",
    "                                  patch_hop,\n",
    "                                  n_parallel_readers,\n",
    "                                  batch_size,\n",
    "                                  n_shuffle,\n",
    "                                  normalise)\n",
    "\n",
    "#  Create the iterator\n",
    "mixed_spec, voice_spec, mixed_audio, voice_audio = data.make_one_shot_iterator().get_next()\n",
    "\n",
    "#  Create variable placeholders\n",
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_mag = tf.expand_dims(mixed_spec[:, :, :-1, 0], 3)\n",
    "mixed_phase = tf.expand_dims(mixed_spec[:, :, :-1, 1], 3)\n",
    "voice_mag = tf.expand_dims(voice_spec[:, :, :-1, 0], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(256), Dimension(512), Dimension(1)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_mag.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SegCaps_CapsNetBasic(object):\n",
    "    \n",
    "    def __init__(self, mixed_mag, mixed_phase, voice_mag, is_training, reuse=True, name='SegCaps_CapsNetBasic'):\n",
    "        \"\"\"\n",
    "        input_tensor: Tensor with shape [batch_size, height, width, channels]\n",
    "        is_training:  Boolean - should the model be trained on the current input or not\n",
    "        name:         Model instance name\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            self.mixed_mag = mixed_mag\n",
    "            self.mixed_phase = mixed_phase\n",
    "            self.voice_mag = voice_mag\n",
    "            \n",
    "            with tf.variable_scope('Convolution'):\n",
    "                conv1 = mf.conv(mixed_mag, filters=128, kernel_size=5, stride=(1, 1))\n",
    "                \n",
    "                # Reshape layer to be 1 capsule x [filters] atoms\n",
    "                _, H, W, C = conv1.get_shape()\n",
    "                conv1 = layers.Reshape((H.value, W.value, 1, C.value))(conv1)\n",
    "                #conv1 = tf.expand_dims(conv1, 2)\n",
    "                self.conv1 = conv1\n",
    "            \n",
    "            with tf.variable_scope('Primary_Caps'):\n",
    "                primary_caps = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=8, num_atoms=16, strides=1, padding='same',\n",
    "                                                               routings=1, name='primarycaps')(conv1)\n",
    "                self.primary_caps = primary_caps\n",
    "                \n",
    "            with tf.variable_scope('Seg_Caps'):\n",
    "                seg_caps = capsule_layers.ConvCapsuleLayer(kernel_size=1, num_capsule=1, num_atoms=16, strides=1, padding='same',\n",
    "                                                           routings=3, name='seg_caps')(primary_caps)\n",
    "                self.seg_caps = seg_caps\n",
    "            \n",
    "            with tf.variable_scope('Reconstruction'):\n",
    "                reconstruction = capsule_layers.ConvCapsuleLayer(kernel_size=1, num_capsule=1, num_atoms=1, strides=1, padding='same',\n",
    "                                                           routings=3, name='seg_caps')(seg_caps)\n",
    "                reconstruction = tf.squeeze(reconstruction,-1)\n",
    "                self.reconstruction = reconstruction\n",
    "            \n",
    "            self.cost = mf.l1_loss(self.reconstruction, voice_mag)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=0.0002,\n",
    "                beta1=0.5,\n",
    "            )\n",
    "            self.train_op = self.optimizer.minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UNetModel(object):\n",
    "    \"\"\"\n",
    "    Top level U-Net object.\n",
    "    Attributes:\n",
    "        mixed_mag: Input placeholder for magnitude spectrogram of mixed signals (voice plus background noise) - X\n",
    "        voice_mag: Input placeholder for magnitude spectrogram of isolated voice signal - Y\n",
    "        mixed_phase: Input placeholder for phase spectrogram of mixed signals (voice plus background noise)\n",
    "        mixed_audio: Input placeholder for waveform audio of mixed signals (voice plus background noise)\n",
    "        voice_audio: Input placeholder for waveform audio of isolated voice signal\n",
    "        variant: The type of U-Net model (Normal convolutional or capsule based)\n",
    "        is_training: Boolean - should the model be trained on the current input or not\n",
    "        name: Model instance name\n",
    "    \"\"\"\n",
    "    def __init__(self, mixed_mag, voice_mag, mixed_phase, mixed_audio, voice_audio, variant, is_training, name):\n",
    "        with tf.variable_scope(name):\n",
    "            self.mixed_mag = mixed_mag\n",
    "            self.voice_mag = voice_mag\n",
    "            self.mixed_phase = mixed_phase\n",
    "            self.mixed_audio = mixed_audio\n",
    "            self.voice_audio = voice_audio\n",
    "            self.variant = variant\n",
    "            self.is_training = is_training\n",
    "\n",
    "            self.voice_mask_unet = UNet(mixed_mag, variant, is_training=is_training, reuse=False, name='voice-mask-unet')\n",
    "\n",
    "            self.voice_mask = self.voice_mask_unet.output\n",
    "\n",
    "            self.gen_voice = self.voice_mask * mixed_mag\n",
    "\n",
    "            self.pw_cost = mf.pw_l1_loss(self.gen_voice, voice_mag)\n",
    "            self.cost = tf.reduce_mean(self.pw_cost)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=0.0002,\n",
    "                beta1=0.5,\n",
    "            )\n",
    "            self.train_op = self.optimizer.minimize(self.pw_cost)\n",
    "\n",
    "\n",
    "class UNet(object):\n",
    "\n",
    "    def __init__(self, input_tensor, variant, is_training, reuse, name):\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "            self.variant = variant\n",
    "\n",
    "            if self.variant == 'unet':\n",
    "                self.encoder = UNetEncoder(input_tensor, is_training, reuse)\n",
    "                self.decoder = UNetDecoder(self.encoder.output, self.encoder, is_training, reuse)\n",
    "            elif self.variant == 'capsunet':\n",
    "                self.encoder = CapsUNetEncoder(input_tensor, is_training, reuse)\n",
    "                self.decoder = CapsUNetDecoder(self.encoder.output, self.encoder, is_training, reuse)\n",
    "\n",
    "            self.output = mf.tanh(self.decoder.output) / 2 + .5\n",
    "\n",
    "class UNetEncoder(object):\n",
    "    \"\"\"\n",
    "    The down-convolution side of a convoltional U-Net model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse):\n",
    "        net = input_tensor\n",
    "        with tf.variable_scope('encoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = mf.conv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                self.l1 = net\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = mf.lrelu(net)\n",
    "                net = mf.conv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = mf.batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l2 = net\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = mf.lrelu(net)\n",
    "                net = mf.conv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = mf.batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l3 = net\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = mf.lrelu(net)\n",
    "                net = mf.conv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = mf.batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l4 = net\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = mf.lrelu(net)\n",
    "                net = mf.conv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = mf.batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l5 = net\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = mf.lrelu(net)\n",
    "                net = mf.conv(net, filters=512, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n",
    "\n",
    "\n",
    "class UNetDecoder(object):\n",
    "    \"\"\"\n",
    "    The up-convolution side of a convolutional U-Net model\n",
    "    \"\"\"\n",
    "    def __init__(self, input_tensor, encoder, is_training, reuse):\n",
    "        net = input_tensor\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = mf.relu(net)\n",
    "                net = mf.deconv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = mf.batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = mf.dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = mf.relu(mf.concat(net, encoder.l5))\n",
    "                net = mf.deconv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = mf.batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = mf.dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = mf.relu(mf.concat(net, encoder.l4))\n",
    "                net = mf.deconv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = mf.batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = mf.dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = mf.relu(mf.concat(net, encoder.l3))\n",
    "                net = mf.deconv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = mf.batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = mf.relu(mf.concat(net, encoder.l2))\n",
    "                net = mf.deconv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                net = mf.batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = mf.relu(mf.concat(net, encoder.l1))\n",
    "                net = mf.deconv(net, filters=1, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n",
    "\n",
    "\n",
    "class CapsUNetEncoder(object):\n",
    "    \"\"\"\n",
    "    The down-convolutional side of a capsule based U-Net model (based on SegCaps R3 model).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_tensor, is_training, reuse):\n",
    "        #net = layers.Input(shape=input_tensor)\n",
    "        net = input_tensor\n",
    "        with tf.variable_scope('Encoder'):\n",
    "            with tf.variable_scope('Convolution'):\n",
    "                # Layer 1: Just a conventional Conv2D layer\n",
    "                net = layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='same', activation='relu', name='conv1')(net)\n",
    "\n",
    "                # Reshape layer to be 1 capsule x [filters] atoms\n",
    "                _, H, W, C = net.get_shape()\n",
    "                net = layers.Reshape((H.value, W.value, 1, C.value))(net)\n",
    "                self.conv1 = net\n",
    "\n",
    "            with tf.variable_scope('Primary_Caps'):\n",
    "                # Layer 1: Primary Capsule: Conv cap with routing 1\n",
    "                net = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=2, num_atoms=16, strides=2, padding='same',\n",
    "                                                    routings=1, name='primarycaps')(net)\n",
    "                self.primary_caps = net\n",
    "\n",
    "            with tf.variable_scope('Conv_caps_2'):\n",
    "                # Layer 2: Convolutional Capsule\n",
    "                net = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=4, num_atoms=16, strides=1, padding='same',\n",
    "                                                      routings=3, name='conv_cap_2_1')(net)\n",
    "                self.conv_cap_2_1 = net\n",
    "\n",
    "                # Layer 2: Convolutional Capsule\n",
    "                net = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=4, num_atoms=32, strides=2, padding='same',\n",
    "                                                      routings=3, name='conv_cap_2_2')(net)\n",
    "                self.conv_cap_2_2 = net\n",
    "\n",
    "            with tf.variable_scope('Conv_caps_3'):\n",
    "                # Layer 3: Convolutional Capsule\n",
    "                net = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=8, num_atoms=32, strides=1, padding='same',\n",
    "                                                      routings=3, name='conv_cap_3_1')(net)\n",
    "                self.conv_cap_3_1 = net\n",
    "\n",
    "                # Layer 3: Convolutional Capsule\n",
    "                net = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=8, num_atoms=64, strides=2, padding='same',\n",
    "                                                    routings=3, name='conv_cap_3_2')(net)\n",
    "                self.conv_cap_3_2 = net\n",
    "\n",
    "            with tf.variable_scope('Conv_caps_4'):\n",
    "                # Layer 4: Convolutional Capsule\n",
    "                net = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=8, num_atoms=32, strides=1, padding='same',\n",
    "                                                      routings=3, name='conv_cap_4_1')(net)\n",
    "\n",
    "            self.output = net\n",
    "\n",
    "\n",
    "class CapsUNetDecoder(object):\n",
    "    \"\"\"\n",
    "    The up-convolutional side of a capsule based U-Net model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_tensor, encoder, is_training, reuse):\n",
    "        net = input_tensor\n",
    "        with tf.variable_scope('Decoder'):\n",
    "            with tf.variable_scope('UpCaps_1'):\n",
    "                    # Layer 1 Up: Deconvolutional Capsule\n",
    "                net = capsule_layers.DeconvCapsuleLayer(kernel_size=4, num_capsule=8, num_atoms=32, upsamp_type='deconv',\n",
    "                                                        scaling=2, padding='same', routings=3, name='deconv_cap_1_1')(net)\n",
    "                self.upcap_1_1 = net\n",
    "\n",
    "                # Skip connection\n",
    "                net = layers.Concatenate(axis=-2, name='up_1')([net, encoder.conv_cap_3_1])\n",
    "\n",
    "                # Layer 1 Up: Deconvolutional Capsule\n",
    "                net = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=4, num_atoms=32, strides=1,\n",
    "                                                      padding='same', routings=3, name='deconv_cap_1_2')(net)\n",
    "                self.upcap_1_2 = net\n",
    "\n",
    "            with tf.variable_scope('UpCaps_2'):\n",
    "                # Layer 2 Up: Deconvolutional Capsule\n",
    "                net = capsule_layers.DeconvCapsuleLayer(kernel_size=4, num_capsule=4, num_atoms=16, upsamp_type='deconv',\n",
    "                                                        scaling=2, padding='same', routings=3, name='deconv_cap_2_1')(net)\n",
    "                self.upcap_2_1 = net\n",
    "\n",
    "                # Skip connection\n",
    "                net = layers.Concatenate(axis=-2, name='up_2')([net, encoder.conv_cap_2_1])\n",
    "\n",
    "                # Layer 2 Up: Deconvolutional Capsule\n",
    "                net = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=4, num_atoms=16, strides=1,\n",
    "                                                      padding='same', routings=3, name='deconv_cap_2_2')(net)\n",
    "                self.upcap_2_2 = net\n",
    "\n",
    "            with tf.variable_scope('UpCaps_3'):\n",
    "                # Layer 3 Up: Deconvolutional Capsule\n",
    "                net = capsule_layers.DeconvCapsuleLayer(kernel_size=4, num_capsule=2, num_atoms=16, upsamp_type='deconv',\n",
    "                                                        scaling=2, padding='same', routings=3, name='deconv_cap_3_1')(net)\n",
    "                self.upcap_3_1 = net\n",
    "\n",
    "                # Skip connection\n",
    "                net = layers.Concatenate(axis=-2, name='up_3')([net, encoder.conv1])\n",
    "            with tf.variable_scope('Reconstruction'):\n",
    "                # Layer 4: Convolutional Capsule: 1x1\n",
    "                net = capsule_layers.ConvCapsuleLayer(kernel_size=1, num_capsule=1, num_atoms=16, strides=1, padding='same',\n",
    "                                                      routings=3, name='seg_caps')(net)\n",
    "                \n",
    "                _, H, W, C, D = net.get_shape()\n",
    "                \n",
    "                net = layers.Reshape((H.value, W.value, D.value))(net)\n",
    "\n",
    "                net = layers.Conv2D(filters=64, kernel_size=1, padding='same', kernel_initializer='he_normal',\n",
    "                                    activation='relu', name='recon_1')(net)\n",
    "\n",
    "                net = layers.Conv2D(filters=128, kernel_size=1, padding='same', kernel_initializer='he_normal',\n",
    "                                    activation='relu', name='recon_2')(net)\n",
    "\n",
    "                net = layers.Conv2D(filters=1, kernel_size=1, padding='same', kernel_initializer='he_normal',\n",
    "                                    activation='sigmoid', name='out_recon')(net)\n",
    "\n",
    "            self.output = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#model = SegCaps_CapsNetBasic(mixed_mag, mixed_phase, voice_mag, is_training=False)\n",
    "model = UNetModel(mixed_mag, voice_mag, mixed_phase, mixed_audio, voice_audio, 'capsunet', is_training=True, name='R3_CapsNet')\n",
    "#model = UNetModel(mixed_mag, voice_mag, mixed_phase, mixed_audio, voice_audio, 'unet', is_training=True, name='R3_CapsNet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Caps Net - Layer Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers' Shapes:\n",
      "\n",
      "Input:  [None, 256, 512, 1] \n",
      "Convolution:  [None, 256, 512, 1, 128] \n",
      "Primary Caps:  [None, 256, 512, 8, 16] \n",
      "Seg Caps:  [None, 256, 512, 1, 16] \n",
      "Recontruction:  [None, 256, 512, 1]\n"
     ]
    }
   ],
   "source": [
    "print('Layers\\' Shapes:\\n'\n",
    "      '\\nInput: ', mixed_mag.get_shape().as_list(),\n",
    "      '\\nConvolution: ',model.conv1.get_shape().as_list(),\n",
    "      '\\nPrimary Caps: ',model.primary_caps.get_shape().as_list(),\n",
    "      '\\nSeg Caps: ',model.seg_caps.get_shape().as_list(),\n",
    "      '\\nRecontruction: ',model.reconstruction.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R3 Caps Net - Layer Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers' Shapes:\n",
      "\n",
      "Input:  [None, 256, 512, 1] \n",
      "Convolution:  [None, 256, 512, 1, 16] \n",
      "Primary Caps:  [None, 128, 256, 2, 16] \n",
      "Conv Caps 2.1:  [None, 128, 256, 4, 16] \n",
      "Conv Caps 2.2:  [None, 64, 128, 4, 32] \n",
      "Conv Caps 3.1:  [None, 64, 128, 8, 32] \n",
      "Conv Caps 3.2:  [None, 32, 64, 8, 64] \n",
      "Conv Caps 4 (output):  [None, 32, 64, 8, 32] \n",
      "\n",
      "Up Caps 1.1:  [None, 64, 128, 8, 32] \n",
      "Up Caps 1.2:  [None, 64, 128, 4, 32] \n",
      "Up Caps 2.1:  [None, 128, 256, 4, 16] \n",
      "Up Caps 2.2:  [None, 128, 256, 4, 16] \n",
      "Up Caps 3.1:  [None, 256, 512, 2, 16] \n",
      "Up Caps 4 (output):  [None, 256, 512, 1]\n"
     ]
    }
   ],
   "source": [
    "print('Layers\\' Shapes:\\n'\n",
    "      '\\nInput: ', mixed_mag.get_shape().as_list(),\n",
    "      '\\nConvolution: ',model.voice_mask_unet.encoder.conv1.get_shape().as_list(),\n",
    "      '\\nPrimary Caps: ',model.voice_mask_unet.encoder.primary_caps.get_shape().as_list(),\n",
    "      '\\nConv Caps 2.1: ',model.voice_mask_unet.encoder.conv_cap_2_1.get_shape().as_list(),\n",
    "      '\\nConv Caps 2.2: ',model.voice_mask_unet.encoder.conv_cap_2_2.get_shape().as_list(),\n",
    "      '\\nConv Caps 3.1: ',model.voice_mask_unet.encoder.conv_cap_3_1.get_shape().as_list(),\n",
    "      '\\nConv Caps 3.2: ',model.voice_mask_unet.encoder.conv_cap_3_2.get_shape().as_list(),\n",
    "      '\\nConv Caps 4 (output): ',model.voice_mask_unet.encoder.output.get_shape().as_list(),\n",
    "      \n",
    "      '\\n\\nUp Caps 1.1: ',model.voice_mask_unet.decoder.upcap_1_1.get_shape().as_list(),\n",
    "      '\\nUp Caps 1.2: ',model.voice_mask_unet.decoder.upcap_1_2.get_shape().as_list(),\n",
    "      '\\nUp Caps 2.1: ',model.voice_mask_unet.decoder.upcap_2_1.get_shape().as_list(),\n",
    "      '\\nUp Caps 2.2: ',model.voice_mask_unet.decoder.upcap_2_2.get_shape().as_list(),\n",
    "      '\\nUp Caps 3.1: ',model.voice_mask_unet.decoder.upcap_3_1.get_shape().as_list(),\n",
    "      '\\nUp Caps 4 (output): ',model.voice_mask_unet.decoder.output.get_shape().as_list()\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mix, phase, voice, mask = sess.run([model.mixed_mag, model.mixed_phase, model.voice_mag, model.reconstruction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['image.cmap'] = 'hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(20,10))\n",
    "\n",
    "ax[0,0].imshow(mix[0,:,:,0])\n",
    "ax[0,0].set_title('mixed input')\n",
    "ax[0,1].imshow(voice[0,:,:,0])\n",
    "ax[0,1].set_title('isolated voice')\n",
    "ax[1,0].imshow(mask[0,:,:,0])\n",
    "ax[1,0].set_title('mask')\n",
    "ax[1,1].imshow(mix[0,:,:,0] * mask[0,:,:,0])\n",
    "ax[1,1].set_title('generated voice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    _, mix, phase, voice, mask, cost = sess.run([model.train_op, model.mixed_mag, model.mixed_phase, \n",
    "                                                 model.voice_mag, model.voice_mask, model.cost])\n",
    "    \n",
    "    print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(20,10))\n",
    "\n",
    "ax[0,0].imshow(mix[0,:,:,0])\n",
    "ax[0,0].set_title('mixed input')\n",
    "ax[0,1].imshow(voice[0,:,:,0])\n",
    "ax[0,1].set_title('isolated voice')\n",
    "ax[1,0].imshow(mask[0,:,:,0])\n",
    "ax[1,0].set_title('mask')\n",
    "ax[1,1].imshow(mix[0,:,:,0] * mask[0,:,:,0])\n",
    "ax[1,1].set_title('generated voice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mix_wave = []\n",
    "voice_wave = []\n",
    "gen_wave = []\n",
    "\n",
    "for i in range(mix.shape[0]):\n",
    "    mix_wave.append(af.spectrogramToAudioFile(np.squeeze(mix[i, :, :, :]).T, n_fft,\n",
    "                                                    fft_hop, phase=np.squeeze(phase[i, :, :, :]).T))\n",
    "    voice_wave.append(af.spectrogramToAudioFile(np.squeeze(voice[i, :, :, :]).T, n_fft,\n",
    "                                                    fft_hop, phase=np.squeeze(phase[i, :, :, :]).T))\n",
    "    gen_wave.append(af.spectrogramToAudioFile(np.squeeze(mix[i, :, :, :] * mask[i, :, :, :]).T, n_fft,\n",
    "                                                    fft_hop, phase=np.squeeze(phase[i, :, :, :]).T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The original mixture\n",
    "ipd.Audio(mix_wave[0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The isolated voice\n",
    "ipd.Audio(voice_wave[0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  The model output\n",
    "ipd.Audio(gen_wave[0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask[4,:,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix[0,:,:,0] * mask[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice[0,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
