{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating NaN Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Basic Capsnet model is supposed to act as a proof of concept audio capsule model, but training of these models has repeatedly resulted in the cost function returning nan after less than one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import a bunch of stuff\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.cmap'] = 'hot'\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "import mir_eval\n",
    "from SegCaps import capsule_layers\n",
    "from keras import layers\n",
    "\n",
    "import audio_functions as af\n",
    "import model_functions as mf\n",
    "import audio_models\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'audio_models' from '/home/enterprise.internal.city.ac.uk/acvn728/MScFinalProject/audio_models.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(audio_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the variables and data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((?, 128, 513, 2), (?, 128, 513, 2), (?, 32512, 1), (?, 32512, 1)), types: (tf.float32, tf.float32, tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Set variables\n",
    "sample_rate=8192\n",
    "n_fft=1024\n",
    "fft_hop=256\n",
    "patch_window=128\n",
    "patch_hop=64\n",
    "n_parallel_readers=4\n",
    "normalise=True\n",
    "batch_size = 5\n",
    "shuffle=False\n",
    "n_shuffle = 1\n",
    "mag_phase = True\n",
    "learning_rate = 0.0002\n",
    "\n",
    "#checkpoint = '52/52-10' #  Best U-net\n",
    "checkpoint = '88/88-1'\n",
    "model_base_dir = '/home/enterprise.internal.city.ac.uk/acvn728/checkpoints'\n",
    "\n",
    "#directory_a = 'C:/Users/Toby/MSc_Project/Test_Audio/CHiME/test/Mixed'\n",
    "#directory_b = 'C:/Users/Toby/MSc_Project/Test_Audio/CHiME/test/Voice'\n",
    "\n",
    "#directory_a = 'C:/Users/Toby/Speech_Data/LibriSpeechMini/Mixed/train-clean-100/19/198/'\n",
    "#directory_b = 'C:/Users/Toby/Speech_Data/LibriSpeechMini/Voice/train-clean-100/19/198/'\n",
    "\n",
    "directory_a = '/home/enterprise.internal.city.ac.uk/acvn728/LibriSpeechMini/Mixed/train-clean-100/19/198/'\n",
    "directory_b = '/home/enterprise.internal.city.ac.uk/acvn728/LibriSpeechMini/Voice/train-clean-100/19/198/'\n",
    "\n",
    "#directory_a = '/data/Speech_Data/LibriSpeech/Mixed/test-clean/1580/141083/'\n",
    "#directory_b = '/data/Speech_Data/LibriSpeech/Voice/test-clean/1580/141083/'\n",
    "\n",
    "\n",
    "#  Create the pipeline\n",
    "tf.reset_default_graph()\n",
    "data = dataset.zip_files(directory_a, directory_b)\n",
    "data = dataset.get_paired_dataset(data,\n",
    "                                  sample_rate,\n",
    "                                  n_fft,\n",
    "                                  fft_hop,\n",
    "                                  patch_window,\n",
    "                                  patch_hop,\n",
    "                                  n_parallel_readers,\n",
    "                                  batch_size,\n",
    "                                  n_shuffle,\n",
    "                                  normalise,\n",
    "                                  mag_phase)\n",
    "\n",
    "#  Create the iterator\n",
    "pipeline = data.make_initializable_iterator()\n",
    "mixed_spec, voice_spec, mixed_audio, voice_audio = pipeline.get_next()\n",
    "\n",
    "#  Create variable placeholders\n",
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_mag = tf.expand_dims(mixed_spec[:, :, :-1, 0], 3)\n",
    "mixed_phase = tf.expand_dims(mixed_spec[:, :, :-1, 1], 3)\n",
    "voice_mag = tf.expand_dims(voice_spec[:, :, :-1, 0], 3)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model\n"
     ]
    }
   ],
   "source": [
    "# Build U-Net model\n",
    "print('Creating model')\n",
    "model = audio_models.MagnitudeModel(mixed_mag, voice_mag, mixed_phase, mixed_audio, \n",
    "                                    voice_audio, 'basic_capsnet', is_training, learning_rate, name='U_Net_Model')\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved weights - Skip this cell to hear results from a randomly initialised network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n",
      "INFO:tensorflow:Restoring parameters from /home/enterprise.internal.city.ac.uk/acvn728/checkpoints/88/88-1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Loading checkpoint')\n",
    "checkpoint_path = os.path.join(model_base_dir, checkpoint)\n",
    "restorer = tf.train.Saver()\n",
    "restorer.restore(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mix_mag, mix_phase, voice_mag, voice_est_mag, mask, voice_wave, mix_wave = sess.run([model.mixed_mag, \n",
    "                                                                                     model.mixed_phase, \n",
    "                                                                                     model.voice_mag,\n",
    "                                                                                     model.gen_voice, \n",
    "                                                                                     model.voice_mask, \n",
    "                                                                                     model.voice_audio, \n",
    "                                                                                     model.mixed_audio],\n",
    "                                                                                    {model.is_training:False})\n",
    "voice_est_wave = list()\n",
    "\n",
    "for i in range(voice_mag.shape[0]):\n",
    "    voice_est_wave.append(af.spectrogramToAudioFile(np.squeeze(voice_est_mag[i, :, :, :]).T, n_fft,\n",
    "                                                    fft_hop, phaseIterations=0, phase=np.squeeze(mix_phase[i, :, :, :]).T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/Convolution/conv2d/kernel:0' shape=(5, 5, 1, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/Convolution/conv2d/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/Primary_Caps/primarycaps/W:0' shape=(5, 5, 128, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/Primary_Caps/primarycaps/b:0' shape=(1, 1, 8, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/Seg_Caps/seg_caps/W:0' shape=(1, 1, 8, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/Seg_Caps/seg_caps/b:0' shape=(1, 1, 1, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/Reconstruction/reconstruction/W:0' shape=(1, 1, 8, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/Reconstruction/reconstruction/b:0' shape=(1, 1, 1, 1) dtype=float32_ref>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MagnitudeModel(object):\n",
    "    \"\"\"\n",
    "    Top level U-Net object.\n",
    "    Attributes:\n",
    "        mixed_mag: Input placeholder for magnitude spectrogram of mixed signals (voice plus background noise) - X\n",
    "        voice_mag: Input placeholder for magnitude spectrogram of isolated voice signal - Y\n",
    "        mixed_phase: Input placeholder for phase spectrogram of mixed signals (voice plus background noise)\n",
    "        mixed_audio: Input placeholder for waveform audio of mixed signals (voice plus background noise)\n",
    "        voice_audio: Input placeholder for waveform audio of isolated voice signal\n",
    "        variant: The type of U-Net model (Normal convolutional or capsule based)\n",
    "        is_training: Boolean - should the model be trained on the current input or not\n",
    "        name: Model instance name\n",
    "    \"\"\"\n",
    "    def __init__(self, mixed_mag, voice_mag, mixed_phase, mixed_audio, voice_audio, variant, is_training, learning_rate,\n",
    "                 name):\n",
    "        with tf.variable_scope(name):\n",
    "            self.mixed_mag = mixed_mag\n",
    "            self.voice_mag = voice_mag\n",
    "            self.mixed_phase = mixed_phase\n",
    "            self.mixed_audio = mixed_audio\n",
    "            self.voice_audio = voice_audio\n",
    "            self.variant = variant\n",
    "            self.is_training = is_training\n",
    "\n",
    "            if self.variant in ['unet', 'capsunet']:\n",
    "                self.voice_mask_network = UNet(mixed_mag, variant, is_training=is_training, reuse=False, name='voice-mask-unet')\n",
    "            elif self.variant == 'basic_capsnet':\n",
    "                self.voice_mask_network = BasicCapsnet(mixed_mag, name='CapsNetBasic')\n",
    "\n",
    "            self.voice_mask = self.voice_mask_network.output\n",
    "\n",
    "            self.gen_voice = self.voice_mask * mixed_mag\n",
    "\n",
    "            self.cost = mf.l1_loss(self.gen_voice, voice_mag)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(\n",
    "                learning_rate=learning_rate,\n",
    "                beta1=0.5,\n",
    "            )\n",
    "            self.train_op = self.optimizer.minimize(self.cost)\n",
    "\n",
    "class BasicCapsnet(object):\n",
    "\n",
    "    def __init__(self, mixed_mag, name):\n",
    "        \"\"\"\n",
    "        input_tensor: Tensor with shape [batch_size, height, width, channels]\n",
    "        is_training:  Boolean - should the model be trained on the current input or not\n",
    "        name:         Model instance name\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            self.mixed_mag = mixed_mag\n",
    "\n",
    "            net = mf.conv(mixed_mag, filters=128, kernel_size=5, stride=(1, 1))\n",
    "\n",
    "            # Reshape layer to be 1 capsule x [filters] atoms\n",
    "            _, H, W, C = net.get_shape()\n",
    "            net = layers.Reshape((H.value, W.value, 1, C.value))(net)\n",
    "            self.conv1 = net\n",
    "            \n",
    "            net = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=8, num_atoms=8, strides=1,\n",
    "                                                  padding='same',\n",
    "                                                  routings=1, name='primarycaps')(net)\n",
    "            self.primary_caps = net\n",
    "\n",
    "            net = capsule_layers.ConvCapsuleLayer(kernel_size=1, num_capsule=1, num_atoms=8, strides=1,\n",
    "                                                  padding='same',\n",
    "                                                  routings=3, name='seg_caps')(net)\n",
    "            self.seg_caps = net\n",
    "            \n",
    "            net = capsule_layers.ConvCapsuleLayer(kernel_size=1, num_capsule=1, num_atoms=1, strides=1,\n",
    "                                                  padding='same',\n",
    "                                                  routings=3, name='reconstruction')(net)\n",
    "            net = tf.squeeze(net, -1)\n",
    "\n",
    "            self.output = net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MagnitudeModel(mixed_mag, voice_mag, mixed_phase, mixed_audio, \n",
    "                       voice_audio, 'basic_capsnet', is_training, learning_rate, name='U_Net_Model')\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/conv2d/kernel:0' shape=(5, 5, 1, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/conv2d/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/primarycaps/W:0' shape=(5, 5, 128, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/primarycaps/b:0' shape=(1, 1, 8, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/seg_caps/W:0' shape=(1, 1, 8, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/seg_caps/b:0' shape=(1, 1, 1, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/reconstruction/W:0' shape=(1, 1, 8, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/reconstruction/b:0' shape=(1, 1, 1, 1) dtype=float32_ref>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = 0\n",
    "i = 0\n",
    "sess.run(pipeline.initializer)\n",
    "weights = []\n",
    "biases = []\n",
    "while i < 200 and not np.isnan(cost):\n",
    "    try:\n",
    "        _, cost, conv_w, conv_b, prim_w, prim_b, \\\n",
    "        seg_w, seg_b, reco_w, reco_b = sess.run([model.train_op, model.cost,\n",
    "                                                 'U_Net_Model/SegCaps_CapsNetBasic/conv2d/kernel:0',\n",
    "                                                 'U_Net_Model/SegCaps_CapsNetBasic/conv2d/bias:0',\n",
    "                                                 'U_Net_Model/SegCaps_CapsNetBasic/primarycaps/W:0',\n",
    "                                                 'U_Net_Model/SegCaps_CapsNetBasic/primarycaps/b:0',\n",
    "                                                 'U_Net_Model/SegCaps_CapsNetBasic/seg_caps/W:0',\n",
    "                                                 'U_Net_Model/SegCaps_CapsNetBasic/seg_caps/b:0',\n",
    "                                                 'U_Net_Model/SegCaps_CapsNetBasic/reconstruction/W:0',\n",
    "                                                 'U_Net_Model/SegCaps_CapsNetBasic/reconstruction/b:0'])\n",
    "        print('Iteration {i}: cost = {c}'.format(i=i, c=cost))\n",
    "        #print('\\t', conv_w.mean(), prim_w.mean(), seg_w.mean(), reco_w.mean(),'\\n\\t',\n",
    "        #      conv_b.mean(), prim_b.mean(), seg_b.mean(), reco_b.mean())\n",
    "        weights.extend([conv_w.mean(), prim_w.mean(), seg_w.mean(), reco_w.mean()])\n",
    "        biases.extend([conv_b.mean(), prim_b.mean(), seg_b.mean(), reco_b.mean()])\n",
    "        i += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Epoch complete')\n",
    "        sess.run(pipeline.initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = int(len(weights)/4)\n",
    "weights = np.array(weights).reshape((d2,4))\n",
    "biases = np.array(biases).reshape((d2,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = range(weights.shape[0])\n",
    "for i in range(4):\n",
    "    plt.plot(x, weights[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, conv, prim, seg = sess.run([model.voice_mask_network.mixed_mag, model.voice_mask_network.conv1, model.voice_mask_network.primary_caps, model.voice_mask_network.seg_caps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/conv2d/kernel:0' shape=(5, 5, 1, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/conv2d/bias:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/primarycaps/W:0' shape=(5, 5, 128, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/primarycaps/b:0' shape=(1, 1, 8, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/seg_caps/W:0' shape=(1, 1, 8, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/seg_caps/b:0' shape=(1, 1, 1, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/reconstruction/W:0' shape=(1, 1, 8, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'U_Net_Model/SegCaps_CapsNetBasic/reconstruction/b:0' shape=(1, 1, 1, 1) dtype=float32_ref>]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan, nan, nan, nan, nan]]]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run('U_Net_Model/SegCaps_CapsNetBasic/seg_caps/W:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
