{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mir_eval\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Set variables\n",
    "sample_rate=16384\n",
    "n_fft=1024\n",
    "fft_hop=256\n",
    "patch_window=256\n",
    "patch_hop=128\n",
    "n_parallel_readers=4\n",
    "normalise=True\n",
    "batch_size = 5\n",
    "shuffle=False\n",
    "n_shuffle = 1\n",
    "\n",
    "root = 'C:/Users/Toby/Speech_Data/BG_test/'\n",
    "#root = '/home/enterprise.internal.city.ac.uk/acvn728/NewCHiME/'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  Create the pipeline\n",
    "tf.reset_default_graph()\n",
    "data = np.empty((0, 3))\n",
    "for env in ['bus']:#, 'caf', 'ped', 'str']:\n",
    "    directory_a = root + 'et05_' + env + '_simu'\n",
    "    directory_b = root + 'et05_bth'\n",
    "    directory_c = root + 'et05_' + env + '_bg'\n",
    "\n",
    "    file_list = dataset.zip_files(directory_a, directory_b, directory_c)\n",
    "    data = np.concatenate((data, file_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 3), dtype=float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.zip_files(directory_a, directory_b, directory_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((?, 256, 513, 4), (?, 256, 513, 4), (?, 256, 513, 4), (?, 65280, 1), (?, 65280, 1), (?, 65280, 1)), types: (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset.get_paired_dataset(data,\n",
    "                                  sample_rate,\n",
    "                                  n_fft,\n",
    "                                  fft_hop,\n",
    "                                  patch_window,\n",
    "                                  patch_hop,\n",
    "                                  n_parallel_readers,\n",
    "                                  batch_size,\n",
    "                                  n_shuffle,\n",
    "                                  normalise)\n",
    "\n",
    "#  Create the iterator\n",
    "pipe = data.make_initializable_iterator()\n",
    "_, _, _, mixed_audio, voice_audio, background_audio = pipe.get_next()\n",
    "\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Data and Colect Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2e38a898ddca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtf_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallow_growth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtf_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpu_options\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisible_device_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.visible_device_list = str(0)\n",
    "sess = tf.Session(config=tf_config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_costs = []\n",
    "sdrs = np.empty((0, 2))\n",
    "sirs = np.empty((0, 2))\n",
    "sars = np.empty((0, 2))\n",
    "nsdrs = np.empty((0, 2))\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        mix, voice, background = sess.run([mixed_audio, voice_audio, background_audio])\n",
    "\n",
    "        # Reshape for mir_eval\n",
    "        mixed = np.transpose(mixed, (0, 2, 1))\n",
    "        voice = np.transpose(voice, (0, 2, 1))\n",
    "        background = np.transpose(background, (0, 2, 1))\n",
    "\n",
    "        for i in range(voice.shape[0]):\n",
    "            ref_sources = np.concatenate((voice[i, :, :], background[i, :, :]), axis=0)\n",
    "            est_sources = np.concatenate((voice[i, :, :], background[i, :, :]), axis=0)\n",
    "            mixed_sources = np.concatenate((mixed[i, :, :], mixed[i, :, :]), axis=0)\n",
    "\n",
    "            # Calculate audio quality statistics\n",
    "            sdr, sir, sar, _ = mir_eval.separation.bss_eval_sources(ref_sources, est_sources, compute_permutation=False)\n",
    "            sdr_mr, _, _, _ = mir_eval.separation.bss_eval_sources(ref_sources, mixed_sources, compute_permutation=False)\n",
    "            nsdr = sdr - sdr_mr\n",
    "            sdrs = np.concatenate((sdrs, np.expand_dims(sdr, 1).T), axis=0)\n",
    "            sirs = np.concatenate((sirs, np.expand_dims(sir, 1).T), axis=0)\n",
    "            sars = np.concatenate((sars, np.expand_dims(sar, 1).T), axis=0)\n",
    "            nsdrs = np.concatenate((nsdrs, np.expand_dims(nsdr, 1).T), axis=0)\n",
    "        print('{ts}:\\t{f} processed.'.format(ts=datetime.datetime.now(), f=file))\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        mean_cost = sum(test_costs) / len(test_costs)\n",
    "        mean_sdr = np.mean(sdrs, axis=0)\n",
    "        mean_sir = np.mean(sirs, axis=0)\n",
    "        mean_sar = np.mean(sars, axis=0)\n",
    "        mean_nsdr = sum(nsdrs) / len(nsdrs)\n",
    "        for (k, v) in (('voice', 0), ('background', 1)):\n",
    "            metrics.append({'test': str(test) + '_' + k, 'mean_cost': mean_cost, 'mean_sdr': mean_sdr[v],\n",
    "                            'mean_sir': mean_sir[v], 'mean_sar': mean_sar[v], 'mean_nsdr': mean_nsdr[v]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir('test_metrics'):\n",
    "    os.mkdir('test_metrics')\n",
    "file_name = 'test_metrics/NewCHiMEDatasetBaselineMetrics.csv'\n",
    "with open(file_name, 'w') as csvfile:\n",
    "    fieldnames = ['test', 'mean_cost', 'mean_sdr', 'mean_sir', 'mean_sar', 'mean_nsdr']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, lineterminator='\\n')\n",
    "    writer.writeheader()\n",
    "    for test in metrics:\n",
    "        writer.writerow(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
