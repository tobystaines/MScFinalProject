{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import a bunch of stuff\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.cmap'] = 'hot'\n",
    "import IPython.display as ipd\n",
    "\n",
    "import audio_functions as af\n",
    "import model_functions as mf\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from SegCaps import capsule_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Set some variables\n",
    "sample_rate=16384\n",
    "n_fft=1024\n",
    "fft_hop=256\n",
    "patch_window=256\n",
    "patch_hop=128\n",
    "n_parallel_readers=4\n",
    "normalise=True\n",
    "batch_size = 5\n",
    "shuffle=False\n",
    "n_shuffle = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pair of files and convert to spectrogram\n",
    "mix_file = 'C:/Users/Toby/MSc_Project/Test_Audio/CHiME/test/Mixed/F01_423C020U_BUS.CH1.wav'\n",
    "voice_file = 'C:/Users/Toby/MSc_Project/Test_Audio/CHiME/test/Voice/F01_423C020U_BTH.CH1.wav'\n",
    "\n",
    "mix_audio, _ = librosa.core.load(mix_file, sample_rate)\n",
    "voice_audio, _ = librosa.core.load(voice_file, sample_rate)\n",
    "\n",
    "mix_spec = librosa.stft(mix_audio, n_fft=n_fft, hop_length=fft_hop, window='hann')\n",
    "voice_spec = librosa.stft(voice_audio, n_fft=n_fft, hop_length=fft_hop, window='hann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split real and imaginary parts into separate channels\n",
    "mixed_spec_channels = np.concatenate((np.expand_dims(mix_spec.real, axis=2), np.expand_dims(mix_spec.imag, axis=2)), axis=2)\n",
    "voice_spec_channels = np.concatenate((np.expand_dims(voice_spec.real, axis=2), np.expand_dims(voice_spec.imag, axis=2)), axis=2)\n",
    "\n",
    "mixed_spec_tensor = tf.convert_to_tensor(mixed_spec)\n",
    "voice_spec_tensor = tf.convert_to_tensor(voice_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(513), Dimension(492), Dimension(2), Dimension(2)])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_spec_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ComplexNumberCapsNet(object):\n",
    "    \n",
    "    def __init__(self, mixed_spec, voice_spec, is_training, reuse=True, name='complex_number_capsnet'):\n",
    "        \"\"\"\n",
    "        input_tensor: Tensor with shape [batch_size, height, width, 2], where the two channels are the real \n",
    "                      and imaginary parts of the spectrogram\n",
    "        is_training:  Boolean - should the model be trained on the current input or not\n",
    "        name:         Model instance name\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            self.mixed_spec = mixed_spec\n",
    "            self.voice_spec = voice_spec\n",
    "            \n",
    "            with tf.variable_scope('Primary_Caps'):\n",
    "                \n",
    "                # Reshape layer to be 1 capsule x [filters] atoms\n",
    "                _, H, W, C = mixed_spec.get_shape()\n",
    "                input_caps = layers.Reshape((H.value, W.value, 1, C.value))(mixed_spec)\n",
    "                self.input_caps = input_caps\n",
    "            \n",
    "            with tf.variable_scope('Conv_Caps'):\n",
    "                conv_caps = capsule_layers.ConvCapsuleLayer(kernel_size=5, num_capsule=8, num_atoms=32, strides=1, padding='same',\n",
    "                                                               routings=1, name='primarycaps')(input_caps)\n",
    "                self.conv_caps = conv_caps\n",
    "                \n",
    "            with tf.variable_scope('Seg_Caps'):\n",
    "                seg_caps = capsule_layers.ConvCapsuleLayer(kernel_size=1, num_capsule=1, num_atoms=16, strides=1, padding='same',\n",
    "                                                           routings=3, name='seg_caps')(conv_caps)\n",
    "                self.seg_caps = seg_caps\n",
    "            \n",
    "#            with tf.variable_scope('Reconstruction'):\n",
    "#                reconstruction = capsule_layers.ConvCapsuleLayer(kernel_size=1, num_capsule=1, num_atoms=1, strides=1, padding='same',\n",
    "#                                                           routings=3, name='seg_caps')(seg_caps)\n",
    "#                reconstruction = tf.squeeze(reconstruction,-1)\n",
    "#                self.reconstruction = reconstruction\n",
    "            \n",
    "#            self.cost = mf.l1_loss(self.reconstruction, voice_spec)\n",
    "\n",
    "#            self.optimizer = tf.train.AdamOptimizer(\n",
    "#                learning_rate=0.0002,\n",
    "#                beta1=0.5,\n",
    "#            )\n",
    "#            self.train_op = self.optimizer.minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComplexNumberCapsNet(mixed_spec, voice_spec, is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers' Shapes:\n",
      "\n",
      "Input:  [513, 492, 2, 2] \n",
      "Primary Caps:  [513, 492, 2, 1, 2] \n",
      "Conv Caps:  [None, 492, 2, 8, 32] \n",
      "Seg Caps:  [None, 492, 2, 1, 16]\n"
     ]
    }
   ],
   "source": [
    "print('Layers\\' Shapes:\\n'\n",
    "      '\\nInput: ', mixed_spec.get_shape().as_list(),\n",
    "      '\\nPrimary Caps: ',model.input_caps.get_shape().as_list(),\n",
    "      '\\nConv Caps: ',model.conv_caps.get_shape().as_list(),\n",
    "      '\\nSeg Caps: ',model.seg_caps.get_shape().as_list(),\n",
    "#      '\\nRecontruction: ',model.reconstruction.get_shape().as_list()\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
