{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Self-Contained Source Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "First let's import libraries and create some convenience functions for our neural network operations with sane default values.\n",
    "\n",
    "Unfortunately there isn't an easy way to hide code blocks in jupyter, so just skip over this :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "plt.rcParams['image.cmap'] = 'hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def concat(x, y):\n",
    "    return tf.concat([x, y], axis=3)\n",
    "\n",
    "\n",
    "def conv(inputs, filters, kernel_size, stride):\n",
    "    out = tf.layers.conv2d(\n",
    "        inputs, filters=filters, kernel_size=kernel_size,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        strides=stride, padding='SAME')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def deconv(inputs, filters, kernel_size, stride):\n",
    "    out = tf.layers.conv2d_transpose(\n",
    "        inputs, filters=filters, kernel_size=kernel_size,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        strides=stride, padding='SAME')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def batch_norm(inputs, is_training, reuse):\n",
    "    return tf.contrib.layers.batch_norm(\n",
    "        inputs,\n",
    "        decay=0.9,\n",
    "        updates_collections=None,\n",
    "        epsilon=1e-5,\n",
    "        scale=True,\n",
    "        is_training=is_training,\n",
    "        reuse=reuse)\n",
    "\n",
    "\n",
    "def dropout(inputs, rate):\n",
    "    return tf.nn.dropout(inputs, keep_prob=1 - rate)\n",
    "\n",
    "\n",
    "def relu(inputs):\n",
    "    return tf.nn.relu(inputs)\n",
    "\n",
    "\n",
    "def tanh(inputs):\n",
    "    return tf.nn.tanh(inputs)\n",
    "\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    with tf.variable_scope('lrelu'):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "\n",
    "def read_audio(path, sample_rate, n_channels):\n",
    "\n",
    "    def read_audio_py(py_path):\n",
    "        #if n_channels == 1:\n",
    "            mono, _ = librosa.load(py_path, sr=sample_rate, mono=True)\n",
    "            return np.expand_dims(mono, 1)\n",
    "        #elif n_channels == 2:\n",
    "            #stereo, _ = librosa.load(py_path, sr=sample_rate, mono=False)\n",
    "            #return stereo.T\n",
    "        #else:\n",
    "            #raise ValueError('Invalid channels: %d' % n_channels)\n",
    "\n",
    "    return tf.py_func(read_audio_py, [path], tf.float32, stateful=False)\n",
    "\n",
    "\n",
    "def fake_stereo(audio):\n",
    "\n",
    "    def fake_stereo(x):\n",
    "        return tf.stack([x, x], 1)\n",
    "    \n",
    "    voice = audio[:, 0]\n",
    "    mixed = voice * 2\n",
    "    return fake_stereo(mixed), fake_stereo(voice)\n",
    "\n",
    "\n",
    "def compute_spectrogram(audio, n_fft, fft_hop, n_channels):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio : single to dual channel audio shaped (n_samples, n_channels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n_frames, 1 + n_fft / 2, n_channels * 2), where the\n",
    "        last dimension is (left_mag, right_mag, left_phase, right_phase)\n",
    "    '''\n",
    "\n",
    "    def stft(x):\n",
    "        spec = librosa.stft(\n",
    "            x, n_fft=n_fft, hop_length=fft_hop, window='hann')\n",
    "        # TODO: normalize?\n",
    "        #mag = np.abs(spec)\n",
    "        #temp = mag - mag.min()\n",
    "        #mag_norm = temp / temp.max()\n",
    "        return np.abs(spec), np.angle(spec)\n",
    "\n",
    "    #def stereo_func(py_audio):\n",
    "    #    left_mag, left_phase = stft(py_audio[:, 0])\n",
    "    #    right_mag, right_phase = stft(py_audio[:, 1])\n",
    "    #    ret = np.array([left_mag, right_mag, left_phase, right_phase]).T\n",
    "    #    return ret.astype(np.float32)\n",
    "\n",
    "    def mono_func(py_audio):\n",
    "        mag, phase = stft(py_audio[:, 0])\n",
    "        ret = np.array([mag, phase]).T\n",
    "        return ret.astype(np.float32)\n",
    "\n",
    "    #if n_channels == 2:\n",
    "    #    func = stereo_func\n",
    "    #elif n_channels == 1:\n",
    "    #    func = mono_func\n",
    "    #else:\n",
    "    #    raise ValueError('Invalid channels: %d' % n_channels)\n",
    "\n",
    "    with tf.name_scope('read_spectrogram'):\n",
    "        ret = tf.py_func(mono_func, [audio], tf.float32, stateful=False)\n",
    "        ret.set_shape([None, 1 + n_fft / 2, 2])   # n_channels * 2])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def extract_spectrogram_patches(\n",
    "        spec, n_fft, n_channels, patch_window, patch_hop):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    spec : Spectrogram of shape (n_frames, 1 + n_fft / 2, n_channels * 2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n_patches, patch_window, 1 + n_fft / 2, n_channels * 2)\n",
    "        containing patches from spec.\n",
    "    '''\n",
    "    with tf.name_scope('extract_spectrogram_patches'):\n",
    "        spec4d = tf.expand_dims(spec, 0)\n",
    "\n",
    "        patches = tf.extract_image_patches(\n",
    "            spec4d, ksizes=[1, patch_window, 1 + n_fft / 2, 1],\n",
    "            strides=[1, patch_hop, 1 + n_fft / 2, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "\n",
    "        num_patches = tf.shape(patches)[1]\n",
    "\n",
    "        return tf.reshape(patches, [num_patches, patch_window,\n",
    "                                    int(1 + n_fft / 2), 2])   # int(n_channels * 2)]) #here was '1 + n_fft / 2, n_channels * 2', \n",
    "                                                                              #it was causing an error in 6th code block\n",
    "\n",
    "def replace_spectrogram_patches(patches):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Tensor of shape (n_patches, patch_window, 1 + n_fft / 2, n_channels * 2)\n",
    "    containing patches from spec.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spec : Spectrogram of shape (n_frames, 1 + n_fft / 2, n_channels * 2)\n",
    "    '''\n",
    "    # Need to account for patch overlap\n",
    "    # Is it possible to account for no set num patches per whole spectrogram?\n",
    "    pass\n",
    "\n",
    "def invert_spectrogram(mag, phase, n_fft, fft_hop):\n",
    "\n",
    "    # Could be a problem if the spec has been normalised\n",
    "    spec = np.array([mag.T, phase.T]).T\n",
    "    wave = librosa.istft(spec, win_length=n_fft, hop_length=fft_hop, window='hann')\n",
    "\n",
    "    return wave\n",
    "\n",
    "def compute_cochleagram(audio, SAMPLE_RATE, low_lim, hi_lim):\n",
    "    coch = cgram.human_cochleagram(audio, SAMPLE_RATE, low_lim, hi_lim, strict=False)\n",
    "    \n",
    "        \n",
    "def hwr_tf(x):\n",
    "    return x * tf.cast(x > 0.0, tf.float32)\n",
    "\n",
    "\n",
    "def compute_acapella_diff(mixed, noise):\n",
    "    mixed_mag = mixed[:, :, 0:, :2]\n",
    "    mixed_phase = mixed[:, :, 0:, 2:]\n",
    "    noise_mag = noise[:, :, 0:, :2]\n",
    "    voice_mag = hwr_tf(mixed_mag - noise_mag) # TODO: normalize?\n",
    "    voice_phase = mixed_phase\n",
    "    return mixed, noise, tf.concat((voice_mag, voice_phase), axis=3)\n",
    "\n",
    "\n",
    "def partial_argv(func, *args, **kwargs):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    func    : A function that takes scalar argument and returns scalar value\n",
    "    *args   : Args to partially apply to func\n",
    "    *kwargs : Keyword args to partially apply to func\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    func(*args) : A function that maps func over *args and returns a tuple\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    func = partial_argv(abs)\n",
    "    func(-1, 2, -3, 4)\n",
    "\n",
    "    # returns (1, 2, 3, 4)\n",
    "    '''\n",
    "    return lambda *other_args: tuple(map(partial(func, *args, **kwargs), other_args))\n",
    "\n",
    "\n",
    "def zip_tensor_slices(*args):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    *args : list of _n_ _k_-dimensional tensors, where _k_ >= 2\n",
    "        The first dimension has _m_ elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : Dataset of _m_ examples, where each example has _n_\n",
    "        records of _k_ - 1 dimensions.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ds = (\n",
    "        tf.data.Dataset.zip((\n",
    "            tf.data.Dataset.from_tensors([[1,2], [3,4], [5, 6]]),\n",
    "            tf.data.Dataset.from_tensors([[10, 20], [30, 40], [50, 60]])\n",
    "        ))\n",
    "        .flat_map(zip_tensor_slices)  # <--- *HERE*\n",
    "    )\n",
    "    el = ds.make_one_shot_iterator().get_next()\n",
    "    print sess.run(el)\n",
    "    print sess.run(el)\n",
    "\n",
    "    # Output:\n",
    "    # (array([1, 2], dtype=int32), array([10, 20], dtype=int32))\n",
    "    # (array([3, 4], dtype=int32), array([30, 40], dtype=int32))\n",
    "    '''\n",
    "    return tf.data.Dataset.zip(tuple([\n",
    "        tf.data.Dataset.from_tensor_slices(arg)\n",
    "        for arg in args\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's define a few constants we'll use throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100\n",
    "N_FFT = 1024\n",
    "FFT_HOP = 256\n",
    "N_CHANNELS = 1   #was N_CHANNELS = 2\n",
    "N_PARALLEL_READERS = 4\n",
    "PATCH_WINDOW = 256\n",
    "PATCH_HOP = 128\n",
    "BATCH_SIZE = 8\n",
    "N_SHUFFLE = 20\n",
    "#low_lim = 50\n",
    "#hi_lim = 5000\n",
    "\n",
    "# With default params, each spectrum represents 0.02s, and hop length is 0.005s (so ~200frames/s)\n",
    "# Each patch is roughly 1.25s long - this is a lot shorter than those used in the music paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## U-Net Model\n",
    "\n",
    "Next, we create our U-Nets, one for noise and one for voice. Each `UNetModel` has a `UNetEncoder` and a `UNetDecoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class UNetModel(object):\n",
    "\n",
    "    def __init__(self, mixed, voice, mixed_phase, is_training):\n",
    "        self.mixed = mixed\n",
    "        self.voice = voice\n",
    "        self.mixed_phase = mixed_phase\n",
    "        self.is_training = is_training\n",
    "\n",
    "        self.voice_mask_unet = UNet(mixed, is_training=is_training, reuse=False, name='voice-mask-unet')\n",
    "\n",
    "        self.voice_mask = self.voice_mask_unet.output\n",
    "\n",
    "        self.gen_voice = self.voice_mask * mixed\n",
    "\n",
    "        self.cost = l1_loss(self.gen_voice, voice)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=0.0002,\n",
    "            beta1=0.5,\n",
    "        )\n",
    "        self.train_op = self.optimizer.minimize(self.cost)\n",
    "\n",
    "\n",
    "class UNet(object):\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse, name):\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "\n",
    "            self.encoder = UNetEncoder(input_tensor, is_training, reuse)\n",
    "            self.decoder = UNetDecoder(self.encoder.output, self.encoder, is_training, reuse)\n",
    "\n",
    "            self.output = tanh(self.decoder.output) / 2 + .5\n",
    "\n",
    "\n",
    "class UNetEncoder(object):\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse):\n",
    "        net = input_tensor\n",
    "        with tf.variable_scope('encoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = conv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                self.l1 = net\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l2 = net\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l3 = net\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l4 = net\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l5 = net\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=512, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n",
    "\n",
    "\n",
    "class UNetDecoder(object):\n",
    "\n",
    "    def __init__(self, input_tensor, encoder, is_training, reuse):\n",
    "        net = input_tensor\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = relu(net)\n",
    "                net = deconv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = relu(concat(net, encoder.l5))\n",
    "                net = deconv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = relu(concat(net, encoder.l4))\n",
    "                net = deconv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = relu(concat(net, encoder.l3))\n",
    "                net = deconv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = relu(concat(net, encoder.l2))\n",
    "                net = deconv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = relu(concat(net, encoder.l1))\n",
    "                net = deconv(net, filters=1, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Data Pipelines\n",
    "\n",
    "Next we define our data reading functions. We make use of [the Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def training_dataset(\n",
    "        data_folder,\n",
    "        sample_rate,\n",
    "        n_fft,\n",
    "        fft_hop,\n",
    "        n_channels,\n",
    "        patch_window,\n",
    "        patch_hop,\n",
    "        #batch_size,\n",
    "        #n_shuffle,\n",
    "        n_parallel_readers\n",
    "):\n",
    "    \"\"\"Still need to fix this to stop it producing a tuple\"\"\"\n",
    "    return (\n",
    "        tf.data.Dataset.list_files(data_folder + '/*.wav', shuffle=False)\n",
    "        .map(partial(\n",
    "            read_audio,\n",
    "            sample_rate=sample_rate,\n",
    "            n_channels=n_channels\n",
    "        ), num_parallel_calls=n_parallel_readers)\n",
    "        #.map(fake_stereo, num_parallel_calls=n_parallel_readers)\n",
    "        .map(partial_argv(\n",
    "            compute_spectrogram,\n",
    "            n_fft=n_fft,\n",
    "            fft_hop=fft_hop,\n",
    "            n_channels=n_channels,\n",
    "        ), num_parallel_calls=n_parallel_readers)\n",
    "        .map(partial_argv(\n",
    "            extract_spectrogram_patches,\n",
    "            n_fft=n_fft,\n",
    "            n_channels=n_channels,\n",
    "            patch_window=patch_window,\n",
    "            patch_hop=patch_hop,\n",
    "        ))\n",
    "        .flat_map(zip_tensor_slices)\n",
    "        #.batch(batch_size)\n",
    "        #.shuffle(n_shuffle)\n",
    "        #.repeat()\n",
    "    )\n",
    "\n",
    "def zip_datasets(dataset_a, dataset_b, n_shuffle, batch_size):\n",
    "    return tf.data.Dataset.zip((dataset_a, dataset_b)).batch(batch_size).shuffle(n_shuffle).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next let's create the Tensorflow [Session](https://www.tensorflow.org/programmers_guide/graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_folder = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Mixed'\n",
    "clean_folder = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Voice'\n",
    "mds = training_dataset(mixed_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP,  N_PARALLEL_READERS)\n",
    "cds = training_dataset(clean_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP, N_PARALLEL_READERS)\n",
    "ds = zip_datasets(mds, cds, BATCH_SIZE, N_SHUFFLE)\n",
    "mixed, voice = ds.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have both the model and the data pipeline we can actually train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_mag = mixed[0][:, :, 1:, :2] # Yet more hacking to get around this tuple problem\n",
    "mixed_phase = mixed[0][:, :, 1:, 2:]\n",
    "voice_mag = voice[0][:, :, 1:, :2]\n",
    "\n",
    "model = UNetModel(\n",
    "    mixed_mag,\n",
    "    voice_mag,\n",
    "    mixed_phase,\n",
    "    is_training\n",
    ")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's train a small number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            , 0, 0.907840371131897\n",
      "            , 1, 0.9170125722885132\n",
      "            , 2, 0.8991769552230835\n",
      "            , 3, 0.9209613800048828\n",
      "            , 4, 0.906827449798584\n",
      "            , 5, 0.9208844900131226\n",
      "            , 6, 0.9021686315536499\n",
      "            , 7, 0.9201065301895142\n",
      "            , 8, 0.9121226072311401\n",
      "            , 9, 0.8954215049743652\n",
      "            , 10, 0.9003520011901855\n",
      "            , 11, 0.9106419682502747\n",
      "            , 12, 0.9132102131843567\n",
      "            , 13, 0.905126690864563\n",
      "            , 14, 0.9038329124450684\n",
      "            , 15, 0.8804886937141418\n",
      "            , 16, 0.8894723653793335\n",
      "            , 17, 0.8998304605484009\n",
      "            , 18, 0.8833758234977722\n",
      "            , 19, 0.8846715688705444\n",
      "            , 20, 0.8755289912223816\n",
      "            , 21, 0.8850868940353394\n",
      "            , 22, 0.8955572247505188\n",
      "            , 23, 0.8848239779472351\n",
      "            , 24, 0.8835453987121582\n",
      "            , 25, 0.8751486539840698\n",
      "            , 26, 0.8770691156387329\n",
      "            , 27, 0.8756806254386902\n",
      "            , 28, 0.894859790802002\n",
      "            , 29, 0.8870751261711121\n",
      "            , 30, 0.8823572993278503\n",
      "            , 31, 0.8629516363143921\n",
      "            , 32, 0.8615306615829468\n",
      "            , 33, 0.8840175867080688\n",
      "            , 34, 0.8806822896003723\n",
      "            , 35, 0.8705093264579773\n",
      "            , 36, 0.8693695068359375\n",
      "            , 37, 0.8674087524414062\n",
      "            , 38, 0.8702729344367981\n",
      "            , 39, 0.8851152658462524\n",
      "            , 40, 0.8779091835021973\n",
      "            , 41, 0.8548315763473511\n",
      "            , 42, 0.8702119588851929\n",
      "            , 43, 0.860885500907898\n",
      "            , 44, 0.8516002893447876\n",
      "            , 45, 0.8577026128768921\n",
      "            , 46, 0.8707376718521118\n",
      "            , 47, 0.8710077404975891\n",
      "            , 48, 0.8690856695175171\n",
      "            , 49, 0.8558803796768188\n",
      "            , 50, 0.8584663271903992\n",
      "            , 51, 0.857995867729187\n",
      "            , 52, 0.8621516227722168\n",
      "            , 53, 0.8597005605697632\n",
      "            , 54, 0.8659877777099609\n",
      "            , 55, 0.8680620193481445\n",
      "            , 56, 0.8734785318374634\n",
      "            , 57, 0.8501221537590027\n",
      "            , 58, 0.844446063041687\n",
      "            , 59, 0.8423554301261902\n",
      "            , 60, 0.8656076192855835\n",
      "            , 61, 0.8525217771530151\n",
      "            , 62, 0.8421548008918762\n",
      "            , 63, 0.8516786694526672\n",
      "            , 64, 0.8466536402702332\n",
      "            , 65, 0.859467625617981\n",
      "            , 66, 0.8593704104423523\n",
      "            , 67, 0.8458582162857056\n",
      "            , 68, 0.8675767183303833\n",
      "            , 69, 0.8521162271499634\n",
      "            , 70, 0.8377114534378052\n",
      "            , 71, 0.8517705798149109\n",
      "            , 72, 0.8434969782829285\n",
      "            , 73, 0.8502300381660461\n",
      "            , 74, 0.8377708196640015\n",
      "            , 75, 0.8460003137588501\n",
      "            , 76, 0.8488289713859558\n",
      "            , 77, 0.8465425372123718\n",
      "            , 78, 0.8542211651802063\n",
      "            , 79, 0.8580621480941772\n",
      "            , 80, 0.8406857252120972\n",
      "            , 81, 0.8623892068862915\n",
      "            , 82, 0.8529963493347168\n",
      "            , 83, 0.8335664868354797\n",
      "            , 84, 0.8393054008483887\n",
      "            , 85, 0.8423493504524231\n",
      "            , 86, 0.8609949350357056\n",
      "            , 87, 0.8463804125785828\n",
      "            , 88, 0.8549501299858093\n",
      "            , 89, 0.8348577618598938\n",
      "            , 90, 0.8310977220535278\n",
      "            , 91, 0.8373761177062988\n",
      "            , 92, 0.843562126159668\n",
      "            , 93, 0.8496030569076538\n",
      "            , 94, 0.8419876098632812\n",
      "            , 95, 0.8492233157157898\n",
      "            , 96, 0.8391779065132141\n",
      "            , 97, 0.8357559442520142\n",
      "            , 98, 0.8318322896957397\n",
      "            , 99, 0.8287391662597656\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    _, cost = sess.run([model.train_op, model.cost], {model.is_training: True})\n",
    "    print(\"            , {0}, {1}\".format(i, cost)) #here we had code 'print \"            \", i, cost', it was causing an error\n",
    "                                                    #most likely related to python syntax changes in newer version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's try to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/Checkpoint_test/Checkpoint_test-99'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2)\n",
    "checkpoint_folder = 'C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/Checkpoint_test/Checkpoint_test'\n",
    "saver.save(sess, checkpoint_folder, global_step=int(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the contenst of the saved checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta1_power (DT_FLOAT) []\n",
      "beta2_power (DT_FLOAT) []\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/beta (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/beta/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/beta/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/gamma (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/gamma/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/gamma/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/moving_mean (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/moving_variance (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/bias (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/bias/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/beta (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/beta/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/beta/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/gamma (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/gamma/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/gamma/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/moving_mean (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/moving_variance (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/bias (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/bias/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel (DT_FLOAT) [5,5,128,512]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,128,512]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,128,512]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/beta (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/beta/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/beta/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/gamma (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/gamma/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/gamma/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/moving_mean (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/moving_variance (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/bias (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/bias/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel (DT_FLOAT) [5,5,64,256]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,64,256]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,64,256]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/beta (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/beta/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/beta/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/gamma (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/gamma/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/gamma/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/moving_mean (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/moving_variance (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/bias (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/bias/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel (DT_FLOAT) [5,5,32,128]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,32,128]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,32,128]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/beta (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/beta/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/beta/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/gamma (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/gamma/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/gamma/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/moving_mean (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/moving_variance (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/bias (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/bias/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel (DT_FLOAT) [5,5,16,64]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,16,64]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,16,64]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/bias (DT_FLOAT) [1]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/bias/Adam (DT_FLOAT) [1]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [1]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel (DT_FLOAT) [5,5,1,32]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,1,32]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,1,32]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/bias (DT_FLOAT) [16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/bias/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/bias/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/kernel (DT_FLOAT) [5,5,2,16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/kernel/Adam (DT_FLOAT) [5,5,2,16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,2,16]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/beta (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/beta/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/beta/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/gamma (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/gamma/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/gamma/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/moving_mean (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/moving_variance (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/bias (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/bias/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/bias/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/kernel (DT_FLOAT) [5,5,16,32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/kernel/Adam (DT_FLOAT) [5,5,16,32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,16,32]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/beta (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/beta/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/beta/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/gamma (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/gamma/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/gamma/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/moving_mean (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/moving_variance (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/bias (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/bias/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/bias/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/kernel (DT_FLOAT) [5,5,32,64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/kernel/Adam (DT_FLOAT) [5,5,32,64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,32,64]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/beta (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/beta/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/beta/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/gamma (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/gamma/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/gamma/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/moving_mean (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/moving_variance (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/bias (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/bias/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/bias/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/kernel (DT_FLOAT) [5,5,64,128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/kernel/Adam (DT_FLOAT) [5,5,64,128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,64,128]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/beta (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/beta/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/beta/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/gamma (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/gamma/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/gamma/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/moving_mean (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/moving_variance (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/bias (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/bias/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/bias/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/kernel (DT_FLOAT) [5,5,128,256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/kernel/Adam (DT_FLOAT) [5,5,128,256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,128,256]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/bias (DT_FLOAT) [512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/bias/Adam (DT_FLOAT) [512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/bias/Adam_1 (DT_FLOAT) [512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/kernel (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/kernel/Adam (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,256,512]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "#i = 99\n",
    "checkpoint_folder = 'C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/Checkpoint_test/Checkpoint_test'\n",
    "checkpoint = checkpoint_folder + '-' + str(i)\n",
    "# List ALL tensors.\n",
    "print_tensors_in_checkpoint_file(file_name=checkpoint, tensor_name='', all_tensors='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now delete the variable 'model', close the session and reset the graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... start a new session and recreate our variables and model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "mixed_folder = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Mixed'\n",
    "clean_folder = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Voice'\n",
    "mds = training_dataset(mixed_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP,  N_PARALLEL_READERS)\n",
    "cds = training_dataset(clean_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP, N_PARALLEL_READERS)\n",
    "ds = zip_datasets(mds, cds, BATCH_SIZE, N_SHUFFLE)\n",
    "mixed, voice = ds.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_mag = mixed[0][:, :, 1:, :2] # Yet more hacking to get around this tuple problem\n",
    "mixed_phase = mixed[0][:, :, 1:, 2:]\n",
    "voice_mag = voice[0][:, :, 1:, :2]\n",
    "\n",
    "model = UNetModel(\n",
    "    mixed_mag,\n",
    "    voice_mag,\n",
    "    mixed_phase,\n",
    "    is_training\n",
    ")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put some data through before restoring ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9039811]\n",
      "[[-0.03537172  0.01470058 -0.00822074]\n",
      " [ 0.00713794 -0.00845241  0.018083  ]\n",
      " [ 0.02894733 -0.00187844 -0.01820544]]\n"
     ]
    }
   ],
   "source": [
    "cost = sess.run([model.cost], {model.is_training: False})\n",
    "print(cost)\n",
    "#Show a small section of the weights:\n",
    "var = [v for v in tf.trainable_variables() if v.name == \"voice-mask-unet/encoder/layer-5/conv2d/kernel:0\"][0]\n",
    "var = sess.run(var)\n",
    "print(var[0, 0, 0:3, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then restore the checkpoint and put some more data through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/Checkpoint_test/Checkpoint_test-99\n"
     ]
    }
   ],
   "source": [
    "restorer = tf.train.Saver()\n",
    "restorer.restore(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8456491]\n",
      "[[-0.02912672 -0.01266418  0.01293571]\n",
      " [-0.00605743 -0.01404153 -0.00759478]\n",
      " [-0.01654004 -0.03171291 -0.0079945 ]]\n"
     ]
    }
   ],
   "source": [
    "cost = sess.run([model.cost], {model.is_training: False})\n",
    "print(cost)\n",
    "#Show a small section of the weights:\n",
    "var = [v for v in tf.trainable_variables() if v.name == \"voice-mask-unet/encoder/layer-5/conv2d/kernel:0\"][0]\n",
    "var = sess.run(var)\n",
    "print(var[0, 0, 0:3, 0:3]) # Have confirmed that this produces the same numbers every time (when loading the same checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Hopefully the cost comes out closer to that seen at the end of training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "self_contained_ikala.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
