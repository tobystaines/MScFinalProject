{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import capslayer as cl\n",
    "import tensorflow as tf\n",
    "import Model_functions as mf\n",
    "import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Set other variables\n",
    "sample_rate=16384\n",
    "n_fft=1024\n",
    "fft_hop=256\n",
    "patch_window=256\n",
    "patch_hop=128\n",
    "n_parallel_readers=4\n",
    "normalise=True\n",
    "batch_size = 5\n",
    "shuffle=False\n",
    "n_shuffle = 10\n",
    "\n",
    "directory_a = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/test/Mixed'\n",
    "directory_b = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/test/Voice'\n",
    "\n",
    "#  Create the pipeline\n",
    "tf.reset_default_graph()\n",
    "data = Dataset.zip_files(directory_a, directory_b)\n",
    "data = Dataset.get_paired_dataset(data,\n",
    "                                  sample_rate,\n",
    "                                  n_fft,\n",
    "                                  fft_hop,\n",
    "                                  patch_window,\n",
    "                                  patch_hop,\n",
    "                                  n_parallel_readers,\n",
    "                                  batch_size,\n",
    "                                  n_shuffle,\n",
    "                                  normalise)\n",
    "\n",
    "#  Create the iterator\n",
    "mixed_spec, voice_spec, mixed_audio, voice_audio = data.make_one_shot_iterator().get_next()\n",
    "\n",
    "#  Create variable placeholders\n",
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_mag = tf.expand_dims(mixed_spec[:, :, 1:, 0], 3)\n",
    "mixed_phase = tf.expand_dims(mixed_spec[:, :, 1:, 1], 3)\n",
    "voice_mag = tf.expand_dims(voice_spec[:, :, 1:, 0], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CapsNet Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class basicCapsNet(object):\n",
    "    \n",
    "    def __init__(self, mixed_mag, voice_mag, is_training, reuse=True, name='basic_caps_net'):\n",
    "        \"\"\"\n",
    "        input_tensor: Tensor with shape [batch_size, height, width, channels]\n",
    "        is_training:  Boolean - should the model be trained on the current input or not\n",
    "        name:         Model instance name\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            self.mixed_mag = mixed_mag\n",
    "            self.voice_mag = voice_mag\n",
    "            self.is_training = is_training\n",
    "            \n",
    "            with tf.variable_scope('Convolution'):\n",
    "                net = mf.conv(mixed_mag, filters=128, kernel_size=5, stride=(1, 1))\n",
    "                self.conv1 = net\n",
    "                \n",
    "            with tf.variable_scope('Primary_Caps'):\n",
    "                # poses is the output of each capsule in the layer. Represents the state of the feature which the capsule detects\n",
    "                # probs is the detection probability for each capsule, corresponding to the magnitude of poses\n",
    "                poses, probs = cl.layers.primaryCaps(inputs=net, \n",
    "                                                     filters=16, \n",
    "                                                     kernel_size=5, \n",
    "                                                     strides=(1,1),\n",
    "                                                     out_caps_dims=[8,1], \n",
    "                                                     method='norm')\n",
    "                self.primary_caps = (poses, probs)\n",
    "            \n",
    "            with tf.variable_scope('Conv_Caps'):\n",
    "                poses, probs = cl.layers.conv2d(inputs=poses,\n",
    "                                                activation=probs,\n",
    "                                                filters=1,\n",
    "                                                out_caps_dims=[16,1],\n",
    "                                                kernel_size=1,\n",
    "                                                strides=(1,1),\n",
    "                                                padding=\"valid\",\n",
    "                                                routing_method=\"DynamicRouting\",\n",
    "                                                reuse=None)\n",
    "                self.conv_caps = (poses, probs)\n",
    "            \"\"\"\"\"\"\n",
    "            #with tf.variable_scope('Reconstruction_Conv'): # Output should be (?, 256, 512, 128)\n",
    "            #    net = \n",
    "                \n",
    "            #with tf.vriable_scope('Mask_Constructor'): # Output.shape == mixed_mag.shape ((?, 256, 512, 1))\n",
    "            # Output of caps layers needs to be?\n",
    "            #           Mask of shape [input_tensor]\n",
    "            self.voice_mask = net\n",
    "            #self.gen_voice = self.voice_mask * mixed_mag\n",
    "            \n",
    "            #self.cost = ####\n",
    "            #self.optimizer = ####\n",
    "            #self.training_op = ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = basicCapsNet(mixed_mag, voice_mag, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'basic_caps_net/Conv_Caps/conv2d/routing/Squeeze:0' shape=(?, 252, 508, 1, 16, 1) dtype=float32>,\n",
       " <tf.Tensor 'basic_caps_net/Conv_Caps/conv2d/clip_by_value:0' shape=(?, 252, 508, 1) dtype=float32>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv_caps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network so far:\n",
    "- Input: (?, 256, 512, 1)\n",
    "- Convolution: (?, 252, 508, 128)\n",
    "- Primary_Caps: (?, 252, 508, 16, 8, 1), (?, 252, 508, 16)\n",
    "- Conv_Caps: (?, 252, 508, 16, 1), (?, 252, 508, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up-ConvCaps layer needs to:\n",
    "- take - \n",
    "    - input: [batch size, height, width, channels (capsule layer count), caps dims 1, caps dim 2]\n",
    "    - activtion: [batch size, height, width, channels (capsule layer count)]\n",
    "    - output dims: [channels (capsule layer count), caps dims 1, caps dim 2]\n",
    "    - strides\n",
    "    - padding\n",
    "- return - \n",
    "    - output: [batch size, height, width, channels (capsule layer count), caps dims 1, caps dim 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv_caps to Conv layer needs to:\n",
    "- take - \n",
    "    - input: 6D Tensor, [batch size, height, width, channels (capsule layer count), caps dims 1, caps dim 2]\n",
    "    - activtion: 4D Tensor, [batch size, height, width, channels (capsule layer count)]\n",
    "    - output filters: int, channels (convolutional filter count)\n",
    "    - strides\n",
    "    - padding\n",
    "- return - \n",
    "    - output: 4D Tensor, [batch size, height, width, channels (convolutional filter count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask reconstruction layer needs to:\n",
    "- take - \n",
    "    - input: 4D Tensor, [batch size, height, width, channels (convolutional filter count)]\n",
    "    - activtion: [batch size, height, width, channels (capsule layer count)]\n",
    "    - output filters: int, channels (depth of original input, generally one, but flexibility would be good)\n",
    "    - strides\n",
    "    - padding\n",
    "- return - \n",
    "    - output: 4D Tensor, [batch size, height, width, channels (depth of original input, generally one, but flexibility would be good)]\n",
    "\n",
    "Can this be a normal tf.conv2d_transpose?\n",
    "- Args -\n",
    "    - value: 4D Tensor, [batch, height, width, in_channels]\n",
    "    - filter: 4D Tensor, [height, width, output_channels, in_channels]\n",
    "    - output_shape: 1-D Tensor representing the output shape of the deconvolution op\n",
    "    - strides\n",
    "    - padding\n",
    "- Returns -\n",
    "    - A Tensor with the same type as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ExpandDims:0' shape=(?, 256, 512, 1) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = tf.reshape(mixed_mag, shape=[-1, mixed_mag.shape[1], mixed_mag.shape[2], mixed_mag.shape[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.primary_caps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = transforming(model.voice_mask,1, [16,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transforming/Sum:0' shape=(?, 252, 508, 16, 1, 16, 1) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'conv2d/routing/Squeeze:0' shape=(?, 252, 508, 1, 16, 1) dtype=float32>,\n",
       " <tf.Tensor 'conv2d/clip_by_value:0' shape=(?, 252, 508, 1) dtype=float32>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = cl.layers.conv2d(inputs=model.primary_caps[0],\n",
    "                         activation=model.primary_caps[1],\n",
    "                         filters=1,\n",
    "                         out_caps_dims=[16,1],\n",
    "                         kernel_size=1,\n",
    "                         strides=(1,1),\n",
    "                         padding=\"valid\",\n",
    "                         routing_method=\"DynamicRouting\",\n",
    "                         name=None,\n",
    "                         reuse=None)\n",
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spatial_shape = [1, 2, 3, 4]\n",
    "input_shape = [5,5,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, 1, 2, 3, 4, 5, 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[-1] + spatial_shape + input_shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def space_to_batch_nd_v1(inputs, kernel_size, strides, name=None):\n",
    "    \"\"\" for convCapsNet model: memory 4719M, speed 0.169 sec/step\n",
    "    \"\"\"\n",
    "    name = \"space_to_batch_nd\" if name is None else name\n",
    "    with tf.name_scope(name):\n",
    "        height, width, depth = cl.shape(inputs)[1:4]\n",
    "        h_offsets = [[(h + k) for k in range(0, kernel_size[0])] for h in range(0, height + 1 - kernel_size[0], strides[0])]\n",
    "        w_offsets = [[(w + k) for k in range(0, kernel_size[1])] for w in range(0, width + 1 - kernel_size[1], strides[1])]\n",
    "        d_offsets = [[(d + k) for k in range(0, kernel_size[2])] for d in range(0, depth + 1 - kernel_size[2], strides[2])]\n",
    "        patched = tf.gather(inputs, h_offsets, axis=1)\n",
    "        patched = tf.gather(patched, w_offsets, axis=3)\n",
    "        patched = tf.gather(patched, d_offsets, axis=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "inputs = model.primary_caps[0]\n",
    "height, width, depth = cl.shape(inputs)[1:4]\n",
    "kernel_size = [5,5,1]\n",
    "strides = [2,2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_offsets = [[(h + k) for k in range(0, kernel_size[0])] for h in range(0, height + 1 - kernel_size[0], strides[0])]\n",
    "w_offsets = [[(w + k) for k in range(0, kernel_size[1])] for w in range(0, width + 1 - kernel_size[1], strides[1])]\n",
    "d_offsets = [[(d + k) for k in range(0, kernel_size[2])] for d in range(0, depth + 1 - kernel_size[2], strides[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'GatherV2:0' shape=(?, 124, 5, 508, 16, 8, 1) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched = tf.gather(inputs, h_offsets, axis=1)\n",
    "patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'GatherV2_1:0' shape=(?, 124, 5, 252, 5, 16, 8, 1) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched = tf.gather(patched, w_offsets, axis=3)\n",
    "patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'GatherV2_2:0' shape=(?, 124, 5, 252, 5, 16, 1, 8, 1) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched = tf.gather(patched, d_offsets, axis=5)\n",
    "patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'shape_1/strided_slice:0' shape=() dtype=int32>,\n",
       " 124,\n",
       " 252,\n",
       " 16,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 8,\n",
       " 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(patched.shape) == 7:\n",
    "    perm = [0, 1, 3, 5, 2, 4, 6]\n",
    "else:\n",
    "    perm = [0, 1, 3, 5, 2, 4, 6, 7, 8]\n",
    "patched = tf.transpose(patched, perm=perm)\n",
    "shape = cl.shape(patched)\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'shape_2/strided_slice:0' shape=() dtype=int32>,\n",
       " 124,\n",
       " 252,\n",
       " 400,\n",
       " 8,\n",
       " 1]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "shape = shape[:3] + [np.prod(shape[3:-2])] + shape[-2:] if len(patched.shape) == 9 else shape[:3] + [np.prod(shape[3:])]\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(?, 124, 252, 400, 8, 1) dtype=float32>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patched = tf.reshape(patched, shape=shape)\n",
    "patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4],\n",
       " [2, 3, 4, 5, 6],\n",
       " [4, 5, 6, 7, 8],\n",
       " [6, 7, 8, 9, 10],\n",
       " [8, 9, 10, 11, 12],\n",
       " [10, 11, 12, 13, 14],\n",
       " [12, 13, 14, 15, 16],\n",
       " [14, 15, 16, 17, 18],\n",
       " [16, 17, 18, 19, 20],\n",
       " [18, 19, 20, 21, 22],\n",
       " [20, 21, 22, 23, 24],\n",
       " [22, 23, 24, 25, 26],\n",
       " [24, 25, 26, 27, 28],\n",
       " [26, 27, 28, 29, 30],\n",
       " [28, 29, 30, 31, 32],\n",
       " [30, 31, 32, 33, 34],\n",
       " [32, 33, 34, 35, 36],\n",
       " [34, 35, 36, 37, 38],\n",
       " [36, 37, 38, 39, 40],\n",
       " [38, 39, 40, 41, 42],\n",
       " [40, 41, 42, 43, 44],\n",
       " [42, 43, 44, 45, 46],\n",
       " [44, 45, 46, 47, 48],\n",
       " [46, 47, 48, 49, 50],\n",
       " [48, 49, 50, 51, 52],\n",
       " [50, 51, 52, 53, 54],\n",
       " [52, 53, 54, 55, 56],\n",
       " [54, 55, 56, 57, 58],\n",
       " [56, 57, 58, 59, 60],\n",
       " [58, 59, 60, 61, 62],\n",
       " [60, 61, 62, 63, 64],\n",
       " [62, 63, 64, 65, 66],\n",
       " [64, 65, 66, 67, 68],\n",
       " [66, 67, 68, 69, 70],\n",
       " [68, 69, 70, 71, 72],\n",
       " [70, 71, 72, 73, 74],\n",
       " [72, 73, 74, 75, 76],\n",
       " [74, 75, 76, 77, 78],\n",
       " [76, 77, 78, 79, 80],\n",
       " [78, 79, 80, 81, 82],\n",
       " [80, 81, 82, 83, 84],\n",
       " [82, 83, 84, 85, 86],\n",
       " [84, 85, 86, 87, 88],\n",
       " [86, 87, 88, 89, 90],\n",
       " [88, 89, 90, 91, 92],\n",
       " [90, 91, 92, 93, 94],\n",
       " [92, 93, 94, 95, 96],\n",
       " [94, 95, 96, 97, 98],\n",
       " [96, 97, 98, 99, 100],\n",
       " [98, 99, 100, 101, 102],\n",
       " [100, 101, 102, 103, 104],\n",
       " [102, 103, 104, 105, 106],\n",
       " [104, 105, 106, 107, 108],\n",
       " [106, 107, 108, 109, 110],\n",
       " [108, 109, 110, 111, 112],\n",
       " [110, 111, 112, 113, 114],\n",
       " [112, 113, 114, 115, 116],\n",
       " [114, 115, 116, 117, 118],\n",
       " [116, 117, 118, 119, 120],\n",
       " [118, 119, 120, 121, 122],\n",
       " [120, 121, 122, 123, 124],\n",
       " [122, 123, 124, 125, 126],\n",
       " [124, 125, 126, 127, 128],\n",
       " [126, 127, 128, 129, 130],\n",
       " [128, 129, 130, 131, 132],\n",
       " [130, 131, 132, 133, 134],\n",
       " [132, 133, 134, 135, 136],\n",
       " [134, 135, 136, 137, 138],\n",
       " [136, 137, 138, 139, 140],\n",
       " [138, 139, 140, 141, 142],\n",
       " [140, 141, 142, 143, 144],\n",
       " [142, 143, 144, 145, 146],\n",
       " [144, 145, 146, 147, 148],\n",
       " [146, 147, 148, 149, 150],\n",
       " [148, 149, 150, 151, 152],\n",
       " [150, 151, 152, 153, 154],\n",
       " [152, 153, 154, 155, 156],\n",
       " [154, 155, 156, 157, 158],\n",
       " [156, 157, 158, 159, 160],\n",
       " [158, 159, 160, 161, 162],\n",
       " [160, 161, 162, 163, 164],\n",
       " [162, 163, 164, 165, 166],\n",
       " [164, 165, 166, 167, 168],\n",
       " [166, 167, 168, 169, 170],\n",
       " [168, 169, 170, 171, 172],\n",
       " [170, 171, 172, 173, 174],\n",
       " [172, 173, 174, 175, 176],\n",
       " [174, 175, 176, 177, 178],\n",
       " [176, 177, 178, 179, 180],\n",
       " [178, 179, 180, 181, 182],\n",
       " [180, 181, 182, 183, 184],\n",
       " [182, 183, 184, 185, 186],\n",
       " [184, 185, 186, 187, 188],\n",
       " [186, 187, 188, 189, 190],\n",
       " [188, 189, 190, 191, 192],\n",
       " [190, 191, 192, 193, 194],\n",
       " [192, 193, 194, 195, 196],\n",
       " [194, 195, 196, 197, 198],\n",
       " [196, 197, 198, 199, 200],\n",
       " [198, 199, 200, 201, 202],\n",
       " [200, 201, 202, 203, 204],\n",
       " [202, 203, 204, 205, 206],\n",
       " [204, 205, 206, 207, 208],\n",
       " [206, 207, 208, 209, 210],\n",
       " [208, 209, 210, 211, 212],\n",
       " [210, 211, 212, 213, 214],\n",
       " [212, 213, 214, 215, 216],\n",
       " [214, 215, 216, 217, 218],\n",
       " [216, 217, 218, 219, 220],\n",
       " [218, 219, 220, 221, 222],\n",
       " [220, 221, 222, 223, 224],\n",
       " [222, 223, 224, 225, 226],\n",
       " [224, 225, 226, 227, 228],\n",
       " [226, 227, 228, 229, 230],\n",
       " [228, 229, 230, 231, 232],\n",
       " [230, 231, 232, 233, 234],\n",
       " [232, 233, 234, 235, 236],\n",
       " [234, 235, 236, 237, 238],\n",
       " [236, 237, 238, 239, 240],\n",
       " [238, 239, 240, 241, 242],\n",
       " [240, 241, 242, 243, 244],\n",
       " [242, 243, 244, 245, 246],\n",
       " [244, 245, 246, 247, 248],\n",
       " [246, 247, 248, 249, 250]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import capslayer as cl\n",
    "import tensorflow as tf\n",
    "\n",
    "from capslayer.core import routing\n",
    "from capslayer.core import transforming\n",
    "\n",
    "\n",
    "def conv2d_transpose(inputs,\n",
    "                     activation,\n",
    "                     filters,\n",
    "                     out_caps_dims,\n",
    "                     kernel_size,\n",
    "                     strides,\n",
    "                     padding=\"valid\",\n",
    "                     routing_method=\"EMRouting\",\n",
    "                     name=None,\n",
    "                     reuse=None):\n",
    "    \"\"\"A 2D transposed convolutional (sometimes called deconvolutional) capsule layer.\n",
    "    Args:\n",
    "        inputs: A 6-D tensor with shape [batch_size, in_height, in_width, in_channels] + in_caps_dims.\n",
    "        activation: A 4-D tensor with shape [batch_size, in_height, in_width, in_channels].\n",
    "        filters: Integer, the dimensionality of the output space (i.e. the number of filters in the convolution).\n",
    "        out_caps_dims: A tuple/list of 2 integers, specifying the dimensions of output capsule, e.g. out_caps_dims=[4, 4] representing that each output capsule has shape [4, 4].\n",
    "        kernel_size:  An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "        strides: An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions.\n",
    "        padding: One of \"valid\" or \"same\" (case-insensitive), now only support \"valid\".\n",
    "        routing_method: One of \"EMRouting\" or \"DynamicRouting\", the method of routing-by-agreement algorithm.\n",
    "        name: A string, the name of the layer.\n",
    "        reuse: Boolean, whether to reuse the weights of a previous layer by the same name.\n",
    "    Returns:\n",
    "        pose: A 6-D tensor with shape [batch_size, out_height, out_width, out_channesl] + out_caps_dims.\n",
    "        activation: A 4-D tensor with shape [batch_size, out_height, out_width, out_channels].\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"conv2d\" if name is None else name\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "\n",
    "        input_shape = cl.shape(inputs)\n",
    "        input_rank = len(input_shape)\n",
    "        activation_rank = len(activation.shape)\n",
    "        if not input_rank == 6:\n",
    "            raise ValueError('Inputs to `conv2d` should have rank 6. Received inputs rank:', str(input_rank))\n",
    "        if not activation_rank == 4:\n",
    "            raise ValueError('Activation to `conv2d` should have rank 4. Received activation rank:', str(activation_rank))\n",
    "\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = [kernel_size, kernel_size, input_shape[3]]\n",
    "        elif isinstance(kernel_size, (list, tuple)) and len(kernel_size) == 2:\n",
    "            kernel_size = [kernel_size[0], kernel_size[1], input_shape[3]]\n",
    "        else:\n",
    "            raise ValueError('\"kernel_size\" should be an integer or tuple/list of 2 integers. Received:', str(kernel_size))\n",
    "\n",
    "        if isinstance(strides, int):\n",
    "            strides = [strides, strides, 1]\n",
    "        elif isinstance(strides, (list, tuple)) and len(strides) == 2:\n",
    "            strides = [strides[0], strides[1], 1]\n",
    "        else:\n",
    "            raise ValueError('\"strides\" should be an integer or tuple/list of 2 integers. Received:', str(kernel_size))\n",
    "\n",
    "        if not isinstance(out_caps_dims, (list, tuple)) or len(out_caps_dims) != 2:\n",
    "            raise ValueError('\"out_caps_dims\" should be a tuple/list of 2 integers. Received:', str(out_caps_dims))\n",
    "        elif isinstance(out_caps_dims, tuple):\n",
    "            out_caps_dims = list(out_caps_dims)\n",
    "\n",
    "        # 1. space to batch\n",
    "        # patching everything into [batch_size, out_height, out_width, in_channels] + in_caps_dims (batched)\n",
    "        # and [batch_size, out_height, out_width, in_channels] (activation).\n",
    "        batched = cl.space_to_batch_nd(inputs, kernel_size, strides)\n",
    "        activation = cl.space_to_batch_nd(activation, kernel_size, strides)\n",
    "\n",
    "        # 2. transforming\n",
    "        # transforming to [batch_size, out_height, out_width, in_channels, out_channels/filters] + out_caps_dims\n",
    "        vote = transforming(batched,\n",
    "                            num_outputs=filters,\n",
    "                            out_caps_dims=out_caps_dims)\n",
    "\n",
    "        # 3. routing\n",
    "        pose, activation = routing(vote, activation, method=routing_method)\n",
    "\n",
    "        return pose, activation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
