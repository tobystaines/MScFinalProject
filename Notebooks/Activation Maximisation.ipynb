{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "import numpy as np\n",
    "import datetime\n",
    "#import capsnet\n",
    "from SegCaps import capsule_layers\n",
    "import dataset\n",
    "import model_functions as mf\n",
    "import audio_functions as af\n",
    "import audio_models\n",
    "\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.cmap'] = 'hot'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((?, 256, 513, 4), (?, 256, 513, 4), (?, 256, 513, 4), (?, 65280, 1), (?, 65280, 1), (?, 65280, 1)), types: (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Set variables\n",
    "sample_rate=16384\n",
    "n_fft=1024\n",
    "fft_hop=256\n",
    "patch_window=256\n",
    "patch_hop=128\n",
    "n_parallel_readers=16\n",
    "normalise=True\n",
    "batch_size = 1\n",
    "shuffle=False\n",
    "n_shuffle = 1\n",
    "mag_phase = True\n",
    "learning_rate = 0.0002\n",
    "model_variant = 'unet'\n",
    "data_type = 'mag'\n",
    "phase_weight = 0.0005\n",
    "\n",
    "checkpoint = '234/234-8'\n",
    "model_base_dir = '/home/enterprise.internal.city.ac.uk/acvn728/checkpoints'\n",
    "\n",
    "directory_a = '/home/enterprise.internal.city.ac.uk/acvn728/miniCHiME/Mixed'\n",
    "directory_b = '/home/enterprise.internal.city.ac.uk/acvn728/miniCHiME/Voice'\n",
    "directory_c = '/home/enterprise.internal.city.ac.uk/acvn728/miniCHiME/Background'\n",
    "\n",
    "#directory_a = 'C:/Users/Toby/MSc_Project/Test_Audio/miniCHiME/Mixed'\n",
    "#directory_b = 'C:/Users/Toby/MSc_Project/Test_Audio/miniCHiME/Voice'\n",
    "#directory_c = 'C:/Users/Toby/MSc_Project/Test_Audio/miniCHiME/Background'\n",
    "\n",
    "\n",
    "#  Create the pipeline\n",
    "tf.reset_default_graph()\n",
    "data = dataset.zip_files(directory_a, directory_b, directory_c)\n",
    "data = dataset.get_paired_dataset(data,\n",
    "                                  sample_rate,\n",
    "                                  n_fft,\n",
    "                                  fft_hop,\n",
    "                                  patch_window,\n",
    "                                  patch_hop,\n",
    "                                  n_parallel_readers,\n",
    "                                  batch_size,\n",
    "                                  n_shuffle,\n",
    "                                  normalise)\n",
    "\n",
    "#  Create the iterator\n",
    "pipe = data.make_initializable_iterator()\n",
    "mixed_spec, voice_spec, background_spec, mixed_audio, voice_audio, background_audio = pipe.get_next()\n",
    "\n",
    "#  Create variable placeholders\n",
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_phase = tf.expand_dims(mixed_spec[:, :, :-1, 3], 3)\n",
    "if data_type == 'mag':\n",
    "    mixed_input = tf.expand_dims(mixed_spec[:, :, :-1, 2], 3)\n",
    "    voice_input = tf.expand_dims(voice_spec[:, :, :-1, 2], 3)\n",
    "elif data_type in ['mag_phase', 'mag_phase_diff']:\n",
    "    mixed_input = mixed_spec[:, :, :-1, 2:4]\n",
    "    voice_input = voice_spec[:, :, :-1, 2:4]\n",
    "elif data_type == 'real_imag':\n",
    "    mixed_input = mixed_spec[:, :, :-1, 0:2]\n",
    "    voice_input = voice_spec[:, :, :-1, 0:2]\n",
    "elif data_type == 'mag_real_imag':\n",
    "    mixed_input = tf.concat([tf.expand_dims(mixed_spec[:, :, :-1, 2], 3), mixed_spec[:, :, :-1, 0:2]], 3)\n",
    "    voice_input = tf.concat([tf.expand_dims(voice_spec[:, :, :-1, 2], 3), voice_spec[:, :, :-1, 0:2]], 3)\n",
    "elif data_type == 'mag_phase_real_imag':\n",
    "    mixed_input = mixed_spec[:, :, :-1, :]\n",
    "    voice_input = voice_spec[:, :, :-1, :]\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build U-Net model\n",
    "print('Creating model')\n",
    "model = audio_models.MagnitudeModel(mixed_input, voice_input, mixed_phase, mixed_audio, \n",
    "                                    voice_audio, background_audio, model_variant, is_training, learning_rate, \n",
    "                                    data_type, phase_weight, name='Magnitude_Model')\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.visible_device_list = str(1)\n",
    "sess = tf.Session(config=tf_config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a saved checkpoint\n",
    "print('Loading checkpoint')\n",
    "checkpoint_path = os.path.join(model_base_dir, checkpoint)\n",
    "restorer = tf.train.Saver()\n",
    "restorer.restore(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for getting the names of all the convolutional layers in the neural network. We could have made this list manually, but for larger neural networks it is easier to do this with a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_conv_layer_names():\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    # Create a list of names for the operations in the graph\n",
    "    # for the Inception model where the operator-type is 'Conv2D'.\n",
    "    names = [op.name for op in graph.get_operations() if op.type=='Conv2D']\n",
    "\n",
    "    return names\n",
    "\n",
    "conv_names = get_conv_layer_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This function finds the input image that maximizes a given feature in the network. It essentially just performs optimization with gradient ascent. The image is initialized with small random values and is then iteratively updated using the gradient for the given feature with regard to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_image(conv_id=0, feature=0,\n",
    "                   num_iterations=30, show_progress=True):\n",
    "    \"\"\"\n",
    "    Find an image that maximizes the feature\n",
    "    given by the conv_id and feature number.\n",
    "\n",
    "    Parameters:\n",
    "    conv_id: Integer identifying the convolutional layer to\n",
    "             maximize. It is an index into conv_names.\n",
    "             If None then use the last fully-connected layer\n",
    "             before the softmax output.\n",
    "    feature: Index into the layer for the feature to maximize.\n",
    "    num_iteration: Number of optimization iterations to perform.\n",
    "    show_progress: Boolean whether to show the progress.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the loss-function that must be maximized.\n",
    "\n",
    "    # Get the name of the convolutional operator.\n",
    "    conv_name = conv_names[conv_id]\n",
    "\n",
    "    # Get the default TensorFlow graph.\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "    # Get a reference to the tensor that is output by the\n",
    "    # operator. Note that \":0\" is added to the name for this.\n",
    "    tensor = graph.get_tensor_by_name(conv_name + \":0\")\n",
    "\n",
    "    # The loss-function is the average of all the\n",
    "    # tensor-values for the given feature. This\n",
    "    # ensures that we generate the whole input image.\n",
    "    # You can try and modify this so it only uses\n",
    "    # a part of the tensor.\n",
    "    loss = tf.reduce_mean(tensor[:,:,:,feature])\n",
    "\n",
    "    # Get the gradient for the loss-function with regard to\n",
    "    # the input image. This creates a mathematical\n",
    "    # function for calculating the gradient.\n",
    "    gradient = tf.gradients(loss, x_image)\n",
    "\n",
    "    # Generate a random image of the same size as the raw input.\n",
    "    # Each pixel is a small random value between 0.45 and 0.55,\n",
    "    # which is the middle of the valid range between 0 and 1.\n",
    "    img_shape = tuple(mixed_input.shape.as_list()[1:3])\n",
    "    image = 0.1 * np.random.uniform(size=img_shape) + 0.45\n",
    "\n",
    "    # Perform a number of optimization iterations to find\n",
    "    # the image that maximizes the loss-function.\n",
    "    for i in range(num_iterations):\n",
    "        # Reshape the array so it is a 4-rank tensor.\n",
    "        img_reshaped = image[np.newaxis,:,:,np.newaxis]\n",
    "\n",
    "        # Create a feed-dict for inputting the image to the graph.\n",
    "        feed_dict = {model.mixed_input: img_reshaped}\n",
    "\n",
    "        # Calculate the predicted class-scores,\n",
    "        # as well as the gradient and the loss-value.\n",
    "        grad, loss_value = sess.run([gradient, loss],\n",
    "                                     feed_dict=feed_dict)\n",
    "        \n",
    "        # Squeeze the dimensionality for the gradient-array.\n",
    "        grad = np.array(grad).squeeze()\n",
    "\n",
    "        # The gradient now tells us how much we need to change the\n",
    "        # input image in order to maximize the given feature.\n",
    "\n",
    "        # Calculate the step-size for updating the image.\n",
    "        # This step-size was found to give fast convergence.\n",
    "        # The addition of 1e-8 is to protect from div-by-zero.\n",
    "        step_size = 1.0 / (grad.std() + 1e-8)\n",
    "\n",
    "        # Update the image by adding the scaled gradient\n",
    "        # This is called gradient ascent.\n",
    "        image += step_size * grad\n",
    "\n",
    "        # Ensure all pixel-values in the image are between 0 and 1.\n",
    "        image = np.clip(image, 0.0, 1.0)\n",
    "\n",
    "        if show_progress:\n",
    "            print(\"Iteration:\", i)\n",
    "\n",
    "            # Convert the predicted class-scores to a one-dim array.\n",
    "            #pred = np.squeeze(pred)\n",
    "\n",
    "            # The predicted class for the Inception model.\n",
    "            #pred_cls = np.argmax(pred)\n",
    "\n",
    "            # The score (probability) for the predicted class.\n",
    "            #cls_score = pred[pred_cls]\n",
    "\n",
    "            # Print the predicted score etc.\n",
    "            #msg = \"Predicted class: {0}, score: {1:>7.2%}\"\n",
    "            #print(msg.format(pred_cls, cls_score))\n",
    "\n",
    "            # Print statistics for the gradient.\n",
    "            msg = \"Gradient min: {0:>9.6f}, max: {1:>9.6f}, stepsize: {2:>9.2f}\"\n",
    "            print(msg.format(grad.min(), grad.max(), step_size))\n",
    "\n",
    "            # Print the loss-value.\n",
    "            print(\"Loss:\", loss_value)\n",
    "\n",
    "            # Newline.\n",
    "            print()\n",
    "\n",
    "    return image.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = optimize_image()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
