{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Import a bunch of stuff\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['image.cmap'] = 'hot'\n",
    "import IPython.display as ipd\n",
    "import tensorflow as tf\n",
    "import mir_eval\n",
    "import importlib\n",
    "from datetime import datetime\n",
    "import soundfile as sf\n",
    "\n",
    "import audio_functions as af\n",
    "import audio_models\n",
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Set variables\n",
    "sample_rate=16384\n",
    "n_fft=1024\n",
    "fft_hop=256\n",
    "patch_window=256\n",
    "patch_hop=128\n",
    "n_parallel_readers=16\n",
    "normalise=True\n",
    "batch_size = 25\n",
    "shuffle=False\n",
    "n_shuffle = 1\n",
    "mag_phase = True\n",
    "learning_rate = 0.0002\n",
    "data_type = 'mag_phase'\n",
    "\n",
    "checkpoint = '174/174-8'\n",
    "model_base_dir = '/home/enterprise.internal.city.ac.uk/acvn728/checkpoints'\n",
    "\n",
    "directory_a = '/home/enterprise.internal.city.ac.uk/acvn728/NewCHiME/et05_bus_simu'\n",
    "directory_b = '/home/enterprise.internal.city.ac.uk/acvn728/NewCHiME/et05_bth'\n",
    "directory_c = '/home/enterprise.internal.city.ac.uk/acvn728/NewCHiME/et05_bus_bg'\n",
    "\n",
    "\n",
    "#  Create the pipeline\n",
    "tf.reset_default_graph()\n",
    "data = dataset.zip_files(directory_a, directory_b, directory_c)\n",
    "data = dataset.get_paired_dataset(data,\n",
    "                                  sample_rate,\n",
    "                                  n_fft,\n",
    "                                  fft_hop,\n",
    "                                  patch_window,\n",
    "                                  patch_hop,\n",
    "                                  n_parallel_readers,\n",
    "                                  batch_size,\n",
    "                                  n_shuffle,\n",
    "                                  normalise)\n",
    "\n",
    "#  Create the iterator\n",
    "pipe = data.make_initializable_iterator()\n",
    "mixed_spec, voice_spec, background_spec, mixed_audio, voice_audio, background_audio = pipe.get_next()\n",
    "\n",
    "#  Create variable placeholders\n",
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_phase = tf.expand_dims(mixed_spec[:, :, :-1, 3], 3)\n",
    "if data_type == 'mag':\n",
    "    mixed_input = tf.expand_dims(mixed_spec[:, :, :-1, 2], 3)\n",
    "    voice_input = tf.expand_dims(voice_spec[:, :, :-1, 2], 3)\n",
    "elif data_type in ['mag_phase', 'mag_phase_diff']:\n",
    "    mixed_input = mixed_spec[:, :, :-1, 2:4]\n",
    "    voice_input = voice_spec[:, :, :-1, 2:4]\n",
    "elif data_type == 'real_imag':\n",
    "    mixed_input = mixed_spec[:, :, :-1, 0:2]\n",
    "    voice_input = voice_spec[:, :, :-1, 0:2]\n",
    "elif data_type == 'mag_real_imag':\n",
    "    mixed_input = tf.concat([tf.expand_dims(mixed_spec[:, :, :-1, 2], 3), mixed_spec[:, :, :-1, 0:2]], 3)\n",
    "    voice_input = tf.concat([tf.expand_dims(voice_spec[:, :, :-1, 2], 3), voice_spec[:, :, :-1, 0:2]], 3)\n",
    "elif data_type == 'mag_phase_real_imag':\n",
    "    mixed_input = mixed_spec[:, :, :-1, :]\n",
    "    voice_input = voice_spec[:, :, :-1, :]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build U-Net model\n",
    "print('Creating model')\n",
    "model = audio_models.MagnitudeModel(mixed_input, voice_input, mixed_phase, mixed_audio, \n",
    "                                    voice_audio, background_audio, 'unet', is_training, learning_rate, \n",
    "                                    data_type, name='Magnitude_Model')\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.gpu_options.visible_device_list = str(1)\n",
    "sess = tf.Session(config=tf_config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
