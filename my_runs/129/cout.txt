INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "129"
Experiment ID: 129
Preparing dataset
Dataset ready
2018-10-15 10:46:27.803155: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-10-15 10:46:35.120584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-15 10:46:35.121140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:26:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-10-15 10:46:35.121157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Loading checkpoint
INFO:tensorflow:Restoring parameters from /home/enterprise.internal.city.ac.uk/acvn728/checkpoints/115/115-473
INFO - tensorflow - Restoring parameters from /home/enterprise.internal.city.ac.uk/acvn728/checkpoints/115/115-473
Starting training
2018-10-15 10:47:39.855757:	Training iteration: 200, Loss: 0.005181710235774517
2018-10-15 10:48:32.531863:	Training iteration: 400, Loss: 0.005094350781291723
2018-10-15 10:49:22.176304:	Training iteration: 600, Loss: 0.007037562318146229
2018-10-15 10:50:12.041121:	Training iteration: 800, Loss: 0.0068571073934435844
2018-10-15 10:51:02.035464:	Training iteration: 1000, Loss: 0.006442829966545105
Checkpoint
2018-10-15 10:51:53.042023:	Training iteration: 1200, Loss: 0.00769659411162138
2018-10-15 10:52:46.253229:	Training iteration: 1400, Loss: 0.00478314608335495
2018-10-15 10:53:39.865245:	Training iteration: 1600, Loss: 0.004838868975639343
2018-10-15 10:54:30.537412:	Training iteration: 1800, Loss: 0.005199121776968241
2018-10-15 10:55:21.277114:	Training iteration: 2000, Loss: 0.005490062292665243
Checkpoint
2018-10-15 10:56:13.395010:	Training iteration: 2200, Loss: 0.004789064638316631
2018-10-15 10:57:04.169389:	Training iteration: 2400, Loss: 0.004293359816074371
2018-10-15 10:57:55.324090:	Training iteration: 2600, Loss: 0.00480634206905961
2018-10-15 10:58:46.107417:	Training iteration: 2800, Loss: 0.006414490751922131
2018-10-15 10:59:37.174039:	Training iteration: 3000, Loss: 0.005518424324691296
Checkpoint
2018-10-15 11:00:28.900396:	Training iteration: 3200, Loss: 0.005086397286504507
2018-10-15 11:01:19.880802:	Training iteration: 3400, Loss: 0.006101300939917564
2018-10-15 11:02:14.124315:	Training iteration: 3600, Loss: 0.0070597222074866295
2018-10-15 11:03:05.155185:	Training iteration: 3800, Loss: 0.006752412766218185
2018-10-15 11:03:56.298989:	Training iteration: 4000, Loss: 0.004941178020089865
Checkpoint
2018-10-15 11:04:48.213329:	Training iteration: 4200, Loss: 0.0053605930879712105
2018-10-15 11:05:39.876090:	Training iteration: 4400, Loss: 0.005052993074059486
2018-10-15 11:06:33.877067:	Training iteration: 4600, Loss: 0.006887695286422968
2018-10-15 11:07:24.820204:	Training iteration: 4800, Loss: 0.005784842185676098
2018-10-15 11:08:25.608952:	Training iteration: 5000, Loss: 0.006982931401580572
Checkpoint
2018-10-15 11:09:20.493885:	Training iteration: 5200, Loss: 0.006555821746587753
2018-10-15 11:10:11.626605:	Training iteration: 5400, Loss: 0.006580234970897436
2018-10-15 11:11:02.909184:	Training iteration: 5600, Loss: 0.005993980448693037
2018-10-15 11:11:53.981417:	Training iteration: 5800, Loss: 0.006260675843805075
2018-10-15 11:12:45.145180:	Training iteration: 6000, Loss: 0.006691686809062958
Checkpoint
2018-10-15 11:13:37.585989:	Training iteration: 6200, Loss: 0.005899165757000446
2018-10-15 11:14:29.052620:	Training iteration: 6400, Loss: 0.005384589079767466
2018-10-15 11:15:20.230874:	Training iteration: 6600, Loss: 0.0064637684263288975
2018-10-15 11:16:11.717444:	Training iteration: 6800, Loss: 0.006957676261663437
2018-10-15 11:17:02.829484:	Training iteration: 7000, Loss: 0.007944891229271889
Checkpoint
2018-10-15 11:17:55.306434:	Training iteration: 7200, Loss: 0.006850732956081629
2018-10-15 11:18:49.839096:	Training iteration: 7400, Loss: 0.006693041417747736
2018-10-15 11:19:41.595804:	Training iteration: 7600, Loss: 0.00808973889797926
2018-10-15 11:20:32.884348:	Training iteration: 7800, Loss: 0.006466703023761511
2018-10-15 11:21:24.211933:	Training iteration: 8000, Loss: 0.008502611890435219
Checkpoint
2018-10-15 11:22:19.872924:	Training iteration: 8200, Loss: 0.006246727425605059
2018-10-15 11:23:14.305382:	Training iteration: 8400, Loss: 0.006587223615497351
2018-10-15 11:24:08.775659:	Training iteration: 8600, Loss: 0.0068148402497172356
2018-10-15 11:25:00.119417:	Training iteration: 8800, Loss: 0.006655181758105755
2018-10-15 11:25:51.698189:	Training iteration: 9000, Loss: 0.008172394707798958
Checkpoint
2018-10-15 11:26:44.069444:	Training iteration: 9200, Loss: 0.008005281910300255
2018-10-15 11:27:35.890025:	Training iteration: 9400, Loss: 0.008670009672641754
2018-10-15 11:28:27.352304:	Training iteration: 9600, Loss: 0.006941778119653463
2018-10-15 11:29:19.421190:	Training iteration: 9800, Loss: 0.008646033704280853
2018-10-15 11:30:13.817222:	Training iteration: 10000, Loss: 0.00703869154676795
Checkpoint
2018-10-15 11:30:33.342604: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 625 of 1000
2018-10-15 11:30:36.742879: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-15 11:31:19.638694:	Training iteration: 10200, Loss: 0.005272095091640949
2018-10-15 11:32:10.992198:	Training iteration: 10400, Loss: 0.00900021381676197
2018-10-15 11:33:02.196739:	Training iteration: 10600, Loss: 0.009120729751884937
2018-10-15 11:33:53.499739:	Training iteration: 10800, Loss: 0.007349119987338781
2018-10-15 11:34:44.959991:	Training iteration: 11000, Loss: 0.007824709638953209
Checkpoint
2018-10-15 11:35:37.216592:	Training iteration: 11200, Loss: 0.008347614668309689
2018-10-15 11:36:29.021134:	Training iteration: 11400, Loss: 0.008741484954953194
2018-10-15 11:37:20.564236:	Training iteration: 11600, Loss: 0.006412245333194733
2018-10-15 11:38:12.594289:	Training iteration: 11800, Loss: 0.008542299270629883
2018-10-15 11:39:07.215602:	Training iteration: 12000, Loss: 0.006650847382843494
Checkpoint
2018-10-15 11:40:02.640108:	Training iteration: 12200, Loss: 0.008000663481652737
2018-10-15 11:40:54.330253:	Training iteration: 12400, Loss: 0.006994097027927637
2018-10-15 11:41:45.627552:	Training iteration: 12600, Loss: 0.007684745825827122
2018-10-15 11:42:37.066911:	Training iteration: 12800, Loss: 0.007191734854131937
2018-10-15 11:43:29.177045:	Training iteration: 13000, Loss: 0.008316121995449066
Checkpoint
2018-10-15 11:44:21.761022:	Training iteration: 13200, Loss: 0.008120901882648468
2018-10-15 11:45:13.326189:	Training iteration: 13400, Loss: 0.007801821921020746
2018-10-15 11:46:05.079478:	Training iteration: 13600, Loss: 0.008864264003932476
2018-10-15 11:46:56.876757:	Training iteration: 13800, Loss: 0.006746350321918726
2018-10-15 11:47:48.260134:	Training iteration: 14000, Loss: 0.0060690040700137615
Checkpoint
2018-10-15 11:48:40.763707:	Training iteration: 14200, Loss: 0.007900052703917027
2018-10-15 11:49:32.259160:	Training iteration: 14400, Loss: 0.009307915344834328
2018-10-15 11:50:23.846796:	Training iteration: 14600, Loss: 0.008995412848889828
2018-10-15 11:51:15.544804:	Training iteration: 14800, Loss: 0.007245910819619894
2018-10-15 11:52:06.849448:	Training iteration: 15000, Loss: 0.006650879979133606
Checkpoint
2018-10-15 11:53:14.466210:	Training iteration: 15200, Loss: 0.004794321022927761
2018-10-15 11:54:05.618396:	Training iteration: 15400, Loss: 0.004921055398881435
2018-10-15 11:55:00.375713:	Training iteration: 15600, Loss: 0.004973595961928368
2018-10-15 11:55:51.201355:	Training iteration: 15800, Loss: 0.006293162703514099
2018-10-15 11:56:45.918833:	Training iteration: 16000, Loss: 0.006739744450896978
Checkpoint
2018-10-15 11:57:37.802864:	Training iteration: 16200, Loss: 0.005946780554950237
2018-10-15 11:58:29.377616:	Training iteration: 16400, Loss: 0.006380090024322271
2018-10-15 11:59:20.728127:	Training iteration: 16600, Loss: 0.004898355342447758
2018-10-15 12:00:18.660759:	Training iteration: 16800, Loss: 0.005939771421253681
2018-10-15 12:01:09.917870:	Training iteration: 17000, Loss: 0.006232056766748428
Checkpoint
2018-10-15 12:02:02.350668:	Training iteration: 17200, Loss: 0.007293886039406061
2018-10-15 12:02:53.817687:	Training iteration: 17400, Loss: 0.00530861085280776
2018-10-15 12:03:45.389264:	Training iteration: 17600, Loss: 0.004798448644578457
2018-10-15 12:04:40.269422:	Training iteration: 17800, Loss: 0.008471569046378136
2018-10-15 12:05:31.348848:	Training iteration: 18000, Loss: 0.006473046727478504
Checkpoint
2018-10-15 12:06:23.849499:	Training iteration: 18200, Loss: 0.0058011203072965145
2018-10-15 12:07:18.610132:	Training iteration: 18400, Loss: 0.007695142179727554
2018-10-15 12:08:09.673101:	Training iteration: 18600, Loss: 0.0046620965003967285
2018-10-15 12:09:01.090823:	Training iteration: 18800, Loss: 0.004666087683290243
2018-10-15 12:09:52.324227:	Training iteration: 19000, Loss: 0.006037463434040546
Checkpoint
2018-10-15 12:11:07.863681:	Training iteration: 19200, Loss: 0.006874757818877697
2018-10-15 12:11:59.205544:	Training iteration: 19400, Loss: 0.006422147154808044
2018-10-15 12:12:50.445982:	Training iteration: 19600, Loss: 0.005422515328973532
2018-10-15 12:13:41.969115:	Training iteration: 19800, Loss: 0.006465258542448282
2018-10-15 12:14:33.098679:	Training iteration: 20000, Loss: 0.007185976952314377
Checkpoint
2018-10-15 12:15:25.413331:	Training iteration: 20200, Loss: 0.0050443559885025024
2018-10-15 12:16:24.895279:	Training iteration: 20400, Loss: 0.010004160925745964
2018-10-15 12:17:15.999264:	Training iteration: 20600, Loss: 0.009592962451279163
2018-10-15 12:18:06.881239:	Training iteration: 20800, Loss: 0.00853070430457592
2018-10-15 12:18:57.845217:	Training iteration: 21000, Loss: 0.007062702439725399
Checkpoint
2018-10-15 12:19:49.667093:	Training iteration: 21200, Loss: 0.008343015797436237
2018-10-15 12:20:40.480920:	Training iteration: 21400, Loss: 0.007201853208243847
2018-10-15 12:21:31.327969:	Training iteration: 21600, Loss: 0.003409643890336156
2018-10-15 12:22:22.092202:	Training iteration: 21800, Loss: 0.008495311252772808
2018-10-15 12:23:12.908124:	Training iteration: 22000, Loss: 0.007628507912158966
Checkpoint
2018-10-15 12:24:04.669870:	Training iteration: 22200, Loss: 0.00628440547734499
2018-10-15 12:24:55.656057:	Training iteration: 22400, Loss: 0.00643434002995491
2018-10-15 12:26:10.035068:	Training iteration: 22600, Loss: 0.007381802890449762
2018-10-15 12:27:00.750772:	Training iteration: 22800, Loss: 0.00704317819327116
2018-10-15 12:27:51.532273:	Training iteration: 23000, Loss: 0.006048757582902908
Checkpoint
2018-10-15 12:28:43.406278:	Training iteration: 23200, Loss: 0.006054223980754614
2018-10-15 12:29:34.438117:	Training iteration: 23400, Loss: 0.005809408612549305
2018-10-15 12:30:25.379235:	Training iteration: 23600, Loss: 0.005798663012683392
2018-10-15 12:31:16.258517:	Training iteration: 23800, Loss: 0.0091912392526865
2018-10-15 12:32:07.432716:	Training iteration: 24000, Loss: 0.007984178140759468
Checkpoint
2018-10-15 12:32:59.207768:	Training iteration: 24200, Loss: 0.006208562292158604
2018-10-15 12:34:10.605542:	Training iteration: 24400, Loss: 0.006523449905216694
2018-10-15 12:35:05.297553:	Training iteration: 24600, Loss: 0.00825152825564146
2018-10-15 12:35:56.145288:	Training iteration: 24800, Loss: 0.006211348809301853
2018-10-15 12:36:47.164164:	Training iteration: 25000, Loss: 0.004778729751706123
Checkpoint
2018-10-15 12:37:38.988887:	Training iteration: 25200, Loss: 0.005845847073942423
2018-10-15 12:38:29.932744:	Training iteration: 25400, Loss: 0.006322352681308985
2018-10-15 12:39:20.870269:	Training iteration: 25600, Loss: 0.007033194415271282
2018-10-15 12:40:35.562976:	Training iteration: 25800, Loss: 0.0063147591426968575
2018-10-15 12:41:26.075215:	Training iteration: 26000, Loss: 0.007428330834954977
Checkpoint
2018-10-15 12:42:17.901279:	Training iteration: 26200, Loss: 0.007169954478740692
2018-10-15 12:43:08.866116:	Training iteration: 26400, Loss: 0.006788541562855244
2018-10-15 12:43:59.675201:	Training iteration: 26600, Loss: 0.005598513409495354
2018-10-15 12:44:50.785411:	Training iteration: 26800, Loss: 0.012181035242974758
2018-10-15 12:46:05.550894:	Training iteration: 27000, Loss: 0.005453275050967932
Checkpoint
2018-10-15 12:47:20.767928:	Training iteration: 27200, Loss: 0.0047693876549601555
2018-10-15 12:48:11.387768:	Training iteration: 27400, Loss: 0.00690620020031929
2018-10-15 12:49:02.247623:	Training iteration: 27600, Loss: 0.008224505931138992
2018-10-15 12:49:53.167126:	Training iteration: 27800, Loss: 0.005651070736348629
2018-10-15 12:50:44.203951:	Training iteration: 28000, Loss: 0.005012333393096924
Checkpoint
2018-10-15 12:51:36.008324:	Training iteration: 28200, Loss: 0.008449790067970753
2018-10-15 12:52:50.503086:	Training iteration: 28400, Loss: 0.008297624066472054
2018-10-15 12:53:41.635991:	Training iteration: 28600, Loss: 0.006950882729142904
2018-10-15 12:54:33.474191:	Training iteration: 28800, Loss: 0.005990796722471714
2018-10-15 12:55:25.125909:	Training iteration: 29000, Loss: 0.005228398833423853
Checkpoint
2018-10-15 12:56:17.382049:	Training iteration: 29200, Loss: 0.011050338856875896
2018-10-15 12:57:09.136966:	Training iteration: 29400, Loss: 0.00831926055252552
2018-10-15 12:58:00.549164:	Training iteration: 29600, Loss: 0.00670685013756156
2018-10-15 12:59:15.637438:	Training iteration: 29800, Loss: 0.006539690308272839
2018-10-15 13:00:06.367590:	Training iteration: 30000, Loss: 0.007936077192425728
Checkpoint
2018-10-15 13:00:58.269685:	Training iteration: 30200, Loss: 0.005403510294854641
2018-10-15 13:01:49.140649:	Training iteration: 30400, Loss: 0.00979322288185358
2018-10-15 13:02:40.444562:	Training iteration: 30600, Loss: 0.006009425036609173
2018-10-15 13:03:31.532489:	Training iteration: 30800, Loss: 0.004323230590671301
2018-10-15 13:04:23.227544:	Training iteration: 31000, Loss: 0.005796988494694233
Checkpoint
2018-10-15 13:05:15.365109:	Training iteration: 31200, Loss: 0.004346042405813932
2018-10-15 13:06:06.576482:	Training iteration: 31400, Loss: 0.006245849188417196
2018-10-15 13:06:58.245216:	Training iteration: 31600, Loss: 0.00612290482968092
2018-10-15 13:07:49.818478:	Training iteration: 31800, Loss: 0.007820234633982182
2018-10-15 13:08:41.205892:	Training iteration: 32000, Loss: 0.005461022257804871
Checkpoint
2018-10-15 13:09:33.626026:	Training iteration: 32200, Loss: 0.012505295686423779
2018-10-15 13:10:25.379368:	Training iteration: 32400, Loss: 0.006105898879468441
2018-10-15 13:11:16.596738:	Training iteration: 32600, Loss: 0.004810611717402935
2018-10-15 13:12:08.089048:	Training iteration: 32800, Loss: 0.010738085955381393
2018-10-15 13:12:59.724973:	Training iteration: 33000, Loss: 0.008049341849982738
Checkpoint
2018-10-15 13:13:51.934490:	Training iteration: 33200, Loss: 0.006393448915332556
2018-10-15 13:14:43.686472:	Training iteration: 33400, Loss: 0.009722594171762466
2018-10-15 13:15:35.207392:	Training iteration: 33600, Loss: 0.011017844080924988
2018-10-15 13:16:26.643804:	Training iteration: 33800, Loss: 0.009402642026543617
2018-10-15 13:17:18.377379:	Training iteration: 34000, Loss: 0.006483943667262793
Checkpoint
2018-10-15 13:18:10.764390:	Training iteration: 34200, Loss: 0.005509619601070881
2018-10-15 13:19:25.352917:	Training iteration: 34400, Loss: 0.007641486823558807
2018-10-15 13:20:16.305778:	Training iteration: 34600, Loss: 0.009952748194336891
2018-10-15 13:21:07.189530:	Training iteration: 34800, Loss: 0.0065724486485123634
2018-10-15 13:21:58.064311:	Training iteration: 35000, Loss: 0.012591036967933178
Checkpoint
2018-10-15 13:22:50.038472:	Training iteration: 35200, Loss: 0.007479967083781958
2018-10-15 13:23:40.997109:	Training iteration: 35400, Loss: 0.00471950089558959
2018-10-15 13:24:32.573516:	Training iteration: 35600, Loss: 0.005186308175325394
2018-10-15 13:25:23.896923:	Training iteration: 35800, Loss: 0.0043588378466665745
2018-10-15 13:26:15.211268:	Training iteration: 36000, Loss: 0.007958467118442059
Checkpoint
2018-10-15 13:27:07.291173:	Training iteration: 36200, Loss: 0.006909660063683987
2018-10-15 13:27:58.471842:	Training iteration: 36400, Loss: 0.006042188499122858
2018-10-15 13:28:49.802369:	Training iteration: 36600, Loss: 0.01068846695125103
2018-10-15 13:29:40.952303:	Training iteration: 36800, Loss: 0.007774765603244305
2018-10-15 13:30:32.357987:	Training iteration: 37000, Loss: 0.00655582407489419
Checkpoint
2018-10-15 13:31:24.582862:	Training iteration: 37200, Loss: 0.00790862925350666
2018-10-15 13:32:16.150456:	Training iteration: 37400, Loss: 0.005110211204737425
2018-10-15 13:33:07.655758:	Training iteration: 37600, Loss: 0.005899312440305948
2018-10-15 13:33:59.316618:	Training iteration: 37800, Loss: 0.013731276616454124
2018-10-15 13:34:50.957145:	Training iteration: 38000, Loss: 0.007668233010917902
Checkpoint
2018-10-15 13:35:43.661669:	Training iteration: 38200, Loss: 0.00580038595944643
2018-10-15 13:36:58.477286:	Training iteration: 38400, Loss: 0.0036270476412028074
2018-10-15 13:38:13.137477:	Training iteration: 38600, Loss: 0.008297547698020935
2018-10-15 13:39:03.721711:	Training iteration: 38800, Loss: 0.008672195486724377
2018-10-15 13:39:54.550029:	Training iteration: 39000, Loss: 0.010301882401108742
Checkpoint
2018-10-15 13:40:46.207055:	Training iteration: 39200, Loss: 0.008398559875786304
2018-10-15 13:41:37.139383:	Training iteration: 39400, Loss: 0.007587994448840618
2018-10-15 13:42:28.309485:	Training iteration: 39600, Loss: 0.011586925014853477
2018-10-15 13:43:19.689898:	Training iteration: 39800, Loss: 0.010054592974483967
2018-10-15 13:44:11.027969:	Training iteration: 40000, Loss: 0.00858321227133274
Checkpoint
2018-10-15 13:45:03.076689:	Training iteration: 40200, Loss: 0.005775829311460257
2018-10-15 13:45:54.490469:	Training iteration: 40400, Loss: 0.006888291332870722
2018-10-15 13:46:45.607343:	Training iteration: 40600, Loss: 0.00875912420451641
2018-10-15 13:47:37.179831:	Training iteration: 40800, Loss: 0.010215884074568748
2018-10-15 13:48:28.415063:	Training iteration: 41000, Loss: 0.007718185894191265
Checkpoint
2018-10-15 13:49:20.848558:	Training iteration: 41200, Loss: 0.004806354641914368
2018-10-15 13:50:12.286326:	Training iteration: 41400, Loss: 0.006151099689304829
2018-10-15 13:51:03.557369:	Training iteration: 41600, Loss: 0.006707201711833477
2018-10-15 13:51:55.318497:	Training iteration: 41800, Loss: 0.008547641336917877
2018-10-15 13:52:46.939322:	Training iteration: 42000, Loss: 0.0060986243188381195
Checkpoint
2018-10-15 13:53:39.090366:	Training iteration: 42200, Loss: 0.006510857492685318
2018-10-15 13:54:30.933613:	Training iteration: 42400, Loss: 0.012091112323105335
2018-10-15 13:55:22.434145:	Training iteration: 42600, Loss: 0.0054390751756727695
2018-10-15 13:56:14.247014:	Training iteration: 42800, Loss: 0.007358282804489136
2018-10-15 13:57:05.606928:	Training iteration: 43000, Loss: 0.008732767775654793
Checkpoint
2018-10-15 13:57:57.919942:	Training iteration: 43200, Loss: 0.007424597628414631
2018-10-15 13:58:49.763608:	Training iteration: 43400, Loss: 0.007072232663631439
2018-10-15 13:59:41.216678:	Training iteration: 43600, Loss: 0.00706879049539566
2018-10-15 14:00:32.884392:	Training iteration: 43800, Loss: 0.00883754063397646
2018-10-15 14:01:24.432377:	Training iteration: 44000, Loss: 0.005739066284149885
Checkpoint
2018-10-15 14:02:39.887866:	Training iteration: 44200, Loss: 0.007075123488903046
2018-10-15 14:03:30.642271:	Training iteration: 44400, Loss: 0.007308593951165676
2018-10-15 14:04:21.377730:	Training iteration: 44600, Loss: 0.005370981991291046
2018-10-15 14:05:35.607123:	Training iteration: 44800, Loss: 0.007628273218870163
2018-10-15 14:06:26.472033:	Training iteration: 45000, Loss: 0.007638984825462103
Checkpoint
2018-10-15 14:07:18.591931:	Training iteration: 45200, Loss: 0.01097810734063387
2018-10-15 14:08:09.390852:	Training iteration: 45400, Loss: 0.006970914546400309
2018-10-15 14:09:00.326435:	Training iteration: 45600, Loss: 0.005444027949124575
2018-10-15 14:09:51.499833:	Training iteration: 45800, Loss: 0.003988287877291441
2018-10-15 14:10:42.498496:	Training iteration: 46000, Loss: 0.007731020450592041
Checkpoint
2018-10-15 14:11:34.641634:	Training iteration: 46200, Loss: 0.0063882432878017426
2018-10-15 14:12:49.305971:	Training iteration: 46400, Loss: 0.005010370165109634
2018-10-15 14:13:40.032802:	Training iteration: 46600, Loss: 0.010906954295933247
2018-10-15 14:14:30.852325:	Training iteration: 46800, Loss: 0.006934165023267269
2018-10-15 14:15:21.659012:	Training iteration: 47000, Loss: 0.009692187421023846
Checkpoint
2018-10-15 14:16:13.416678:	Training iteration: 47200, Loss: 0.0058023566380143166
2018-10-15 14:17:04.426483:	Training iteration: 47400, Loss: 0.00426244642585516
2018-10-15 14:17:55.385496:	Training iteration: 47600, Loss: 0.005563758779317141
2018-10-15 14:18:46.650716:	Training iteration: 47800, Loss: 0.006330064032226801
2018-10-15 14:19:37.669378:	Training iteration: 48000, Loss: 0.007931089028716087
Checkpoint
2018-10-15 14:20:29.458268:	Training iteration: 48200, Loss: 0.00554972980171442
2018-10-15 14:21:43.868143:	Training iteration: 48400, Loss: 0.005741031374782324
2018-10-15 14:22:34.696489:	Training iteration: 48600, Loss: 0.007103970739990473
2018-10-15 14:23:25.619718:	Training iteration: 48800, Loss: 0.010400146245956421
2018-10-15 14:24:16.418097:	Training iteration: 49000, Loss: 0.004924170672893524
Checkpoint
2018-10-15 14:25:32.131340:	Training iteration: 49200, Loss: 0.003971774131059647
2018-10-15 14:26:22.870691:	Training iteration: 49400, Loss: 0.0068013593554496765
2018-10-15 14:27:13.620597:	Training iteration: 49600, Loss: 0.009913088753819466
2018-10-15 14:28:05.045670:	Training iteration: 49800, Loss: 0.013226041570305824
2018-10-15 14:28:55.773861:	Training iteration: 50000, Loss: 0.006770764477550983
Checkpoint
2018-10-15 14:29:47.680968:	Training iteration: 50200, Loss: 0.010193319991230965
2018-10-15 14:30:38.486203:	Training iteration: 50400, Loss: 0.005654425360262394
2018-10-15 14:31:29.434171:	Training iteration: 50600, Loss: 0.010538307949900627
2018-10-15 14:32:20.271450:	Training iteration: 50800, Loss: 0.00830849725753069
2018-10-15 14:33:11.311102:	Training iteration: 51000, Loss: 0.005405779927968979
Checkpoint
2018-10-15 14:34:03.150256:	Training iteration: 51200, Loss: 0.0063505759462714195
2018-10-15 14:34:54.134790:	Training iteration: 51400, Loss: 0.006352752447128296
2018-10-15 14:36:08.719527:	Training iteration: 51600, Loss: 0.005331674590706825
2018-10-15 14:36:59.432635:	Training iteration: 51800, Loss: 0.006986064370721579
2018-10-15 14:37:50.204591:	Training iteration: 52000, Loss: 0.006698197219520807
Checkpoint
2018-10-15 14:38:41.994170:	Training iteration: 52200, Loss: 0.006891939789056778
2018-10-15 14:39:32.754215:	Training iteration: 52400, Loss: 0.007109798491001129
2018-10-15 14:40:41.537408:	Training iteration: 52600, Loss: 0.006501343101263046
2018-10-15 14:41:38.125151:	Training iteration: 52800, Loss: 0.008536532521247864
2018-10-15 14:42:28.887242:	Training iteration: 53000, Loss: 0.004459901247173548
Checkpoint
2018-10-15 14:43:20.301520:	Training iteration: 53200, Loss: 0.006538575980812311
2018-10-15 14:44:34.643865:	Training iteration: 53400, Loss: 0.00765236746519804
2018-10-15 14:45:25.622597:	Training iteration: 53600, Loss: 0.005600175820291042
2018-10-15 14:46:16.696867:	Training iteration: 53800, Loss: 0.00785548985004425
2018-10-15 14:47:07.720638:	Training iteration: 54000, Loss: 0.010313448496162891
Checkpoint
2018-10-15 14:47:59.986018:	Training iteration: 54200, Loss: 0.009284181520342827
2018-10-15 14:48:51.623187:	Training iteration: 54400, Loss: 0.007392046041786671
2018-10-15 14:49:42.991205:	Training iteration: 54600, Loss: 0.0066995942033827305
