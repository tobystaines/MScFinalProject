INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "114"
Experiment ID: 114
Preparing dataset
Dataset ready
2018-10-11 20:25:11.251627: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-10-11 20:25:11.437248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-11 20:25:11.437747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:23:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-10-11 20:25:11.437765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:23:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Loading checkpoint
INFO:tensorflow:Restoring parameters from /home/enterprise.internal.city.ac.uk/acvn728/checkpoints/113/113-123
INFO - tensorflow - Restoring parameters from /home/enterprise.internal.city.ac.uk/acvn728/checkpoints/113/113-123
Starting training
2018-10-11 20:25:29.998278: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 982 of 1000
2018-10-11 20:25:30.197764: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-11 20:25:32.246998:	Training iteration: 1, Loss: 0.005318013485521078
Checkpoint
2018-10-11 20:25:33.480062:	Training iteration: 2, Loss: 0.0054129199124872684
Checkpoint
2018-10-11 20:25:34.621229:	Training iteration: 3, Loss: 0.006070257164537907
Checkpoint
2018-10-11 20:25:35.761249:	Training iteration: 4, Loss: 0.0047760773450136185
Checkpoint
2018-10-11 20:25:36.903674:	Training iteration: 5, Loss: 0.005378770176321268
Checkpoint
2018-10-11 20:25:38.045067:	Training iteration: 6, Loss: 0.005228845868259668
Checkpoint
2018-10-11 20:25:39.219113:	Training iteration: 7, Loss: 0.005420624278485775
Checkpoint
2018-10-11 20:25:40.379824:	Training iteration: 8, Loss: 0.004328487906605005
Checkpoint
2018-10-11 20:25:41.549399:	Training iteration: 9, Loss: 0.00558537757024169
Checkpoint
2018-10-11 20:25:42.714230:	Training iteration: 10, Loss: 0.005124955903738737
Checkpoint
2018-10-11 20:25:43.878265:	Training iteration: 11, Loss: 0.004461376927793026
Checkpoint
2018-10-11 20:25:45.091073:	Training iteration: 12, Loss: 0.003829019609838724
Checkpoint
2018-10-11 20:25:46.348597:	Training iteration: 13, Loss: 0.005769441835582256
Checkpoint
2018-10-11 20:25:47.506335:	Training iteration: 14, Loss: 0.00583225954324007
Checkpoint
2018-10-11 20:25:48.675210:	Training iteration: 15, Loss: 0.00421514455229044
Checkpoint
2018-10-11 20:25:49.823881:	Training iteration: 16, Loss: 0.005622141063213348
Checkpoint
2018-10-11 20:25:50.975612:	Training iteration: 17, Loss: 0.00568736344575882
Checkpoint
2018-10-11 20:25:52.164870:	Training iteration: 18, Loss: 0.006077730096876621
Checkpoint
2018-10-11 20:25:53.497310:	Training iteration: 19, Loss: 0.005182773806154728
Checkpoint
2018-10-11 20:25:54.729303:	Training iteration: 20, Loss: 0.005221827886998653
Checkpoint
2018-10-11 20:25:55.886610:	Training iteration: 21, Loss: 0.005841029807925224
Checkpoint
2018-10-11 20:25:57.051912:	Training iteration: 22, Loss: 0.004987969994544983
Checkpoint
2018-10-11 20:25:58.205584:	Training iteration: 23, Loss: 0.0049656652845442295
Checkpoint
2018-10-11 20:25:59.377253:	Training iteration: 24, Loss: 0.004436163697391748
Checkpoint
2018-10-11 20:26:00.535201:	Training iteration: 25, Loss: 0.005121238529682159
Checkpoint
2018-10-11 20:26:01.708196:	Training iteration: 26, Loss: 0.004339833743870258
Checkpoint
2018-10-11 20:26:02.875457:	Training iteration: 27, Loss: 0.005357668735086918
Checkpoint
2018-10-11 20:26:04.146541:	Training iteration: 28, Loss: 0.005449160933494568
Checkpoint
2018-10-11 20:26:05.289619:	Training iteration: 29, Loss: 0.006692203693091869
Checkpoint
2018-10-11 20:26:06.450146:	Training iteration: 30, Loss: 0.008206024765968323
Checkpoint
2018-10-11 20:26:07.672178:	Training iteration: 31, Loss: 0.006476497743278742
Checkpoint
2018-10-11 20:26:08.830758:	Training iteration: 32, Loss: 0.007665394339710474
Checkpoint
2018-10-11 20:26:09.992878:	Training iteration: 33, Loss: 0.004477692302316427
Checkpoint
2018-10-11 20:26:11.201723:	Training iteration: 34, Loss: 0.006125561892986298
Checkpoint
2018-10-11 20:26:12.512310:	Training iteration: 35, Loss: 0.006907048635184765
Checkpoint
2018-10-11 20:26:13.850601:	Training iteration: 36, Loss: 0.005226896144449711
Checkpoint
2018-10-11 20:26:15.183603:	Training iteration: 37, Loss: 0.0056841326877474785
Checkpoint
2018-10-11 20:26:16.494173:	Training iteration: 38, Loss: 0.007139884866774082
Checkpoint
2018-10-11 20:26:17.629679:	Training iteration: 39, Loss: 0.00541271548718214
Checkpoint
2018-10-11 20:26:18.860846:	Training iteration: 40, Loss: 0.005145466420799494
Checkpoint
2018-10-11 20:26:20.176187:	Training iteration: 41, Loss: 0.005268406122922897
Checkpoint
2018-10-11 20:26:21.436878:	Training iteration: 42, Loss: 0.005181431770324707
Checkpoint
2018-10-11 20:26:22.593743:	Training iteration: 43, Loss: 0.0054434118792414665
Checkpoint
2018-10-11 20:26:23.753177:	Training iteration: 44, Loss: 0.005503824446350336
Checkpoint
2018-10-11 20:26:25.170718:	Training iteration: 45, Loss: 0.006161756347864866
Checkpoint
2018-10-11 20:26:26.386927:	Training iteration: 46, Loss: 0.004447001963853836
Checkpoint
2018-10-11 20:26:27.560685:	Training iteration: 47, Loss: 0.004556775093078613
Checkpoint
2018-10-11 20:26:28.708059:	Training iteration: 48, Loss: 0.0066755167208611965
Checkpoint
2018-10-11 20:26:30.084109:	Training iteration: 49, Loss: 0.005827628076076508
Checkpoint
2018-10-11 20:26:31.299171:	Training iteration: 50, Loss: 0.005951221100986004
Checkpoint
2018-10-11 20:26:32.479539:	Training iteration: 51, Loss: 0.0045713214203715324
Checkpoint
2018-10-11 20:26:33.746042:	Training iteration: 52, Loss: 0.006330651231110096
Checkpoint
2018-10-11 20:26:34.930342:	Training iteration: 53, Loss: 0.005669572856277227
Checkpoint
2018-10-11 20:26:36.093988:	Training iteration: 54, Loss: 0.0037552607245743275
Checkpoint
2018-10-11 20:26:37.239099:	Training iteration: 55, Loss: 0.005499404855072498
Checkpoint
2018-10-11 20:26:38.377027:	Training iteration: 56, Loss: 0.005809417925775051
Checkpoint
2018-10-11 20:26:39.532667:	Training iteration: 57, Loss: 0.0050270832143723965
Checkpoint
2018-10-11 20:26:40.693661:	Training iteration: 58, Loss: 0.004762175027281046
Checkpoint
2018-10-11 20:26:41.872050:	Training iteration: 59, Loss: 0.006624248810112476
Checkpoint
2018-10-11 20:26:43.067955:	Training iteration: 60, Loss: 0.004690753761678934
Checkpoint
2018-10-11 20:26:44.227323:	Training iteration: 61, Loss: 0.005718725733458996
Checkpoint
2018-10-11 20:26:45.635746:	Training iteration: 62, Loss: 0.00625273585319519
Checkpoint
2018-10-11 20:26:46.783936:	Training iteration: 63, Loss: 0.004519079811871052
Checkpoint
2018-10-11 20:26:47.977937:	Training iteration: 64, Loss: 0.005443591624498367
Checkpoint
2018-10-11 20:26:49.116813:	Training iteration: 65, Loss: 0.005433320999145508
Checkpoint
2018-10-11 20:26:50.257489:	Training iteration: 66, Loss: 0.004961628466844559
Checkpoint
2018-10-11 20:26:51.426873:	Training iteration: 67, Loss: 0.005844669882208109
Checkpoint
2018-10-11 20:26:52.583374:	Training iteration: 68, Loss: 0.005599503871053457
Checkpoint
2018-10-11 20:26:53.749956:	Training iteration: 69, Loss: 0.005534446332603693
Checkpoint
2018-10-11 20:26:54.917868:	Training iteration: 70, Loss: 0.0049566528759896755
Checkpoint
2018-10-11 20:26:56.082876:	Training iteration: 71, Loss: 0.005908317863941193
Checkpoint
2018-10-11 20:26:57.241809:	Training iteration: 72, Loss: 0.0056650699116289616
Checkpoint
2018-10-11 20:26:58.401494:	Training iteration: 73, Loss: 0.004982121754437685
Checkpoint
2018-10-11 20:26:59.551277:	Training iteration: 74, Loss: 0.004941198974847794
Checkpoint
2018-10-11 20:27:00.710740:	Training iteration: 75, Loss: 0.0044922176748514175
Checkpoint
2018-10-11 20:27:01.863084:	Training iteration: 76, Loss: 0.00514368899166584
Checkpoint
2018-10-11 20:27:03.053969:	Training iteration: 77, Loss: 0.004267099779099226
Checkpoint
2018-10-11 20:27:04.205497:	Training iteration: 78, Loss: 0.005383051000535488
Checkpoint
2018-10-11 20:27:05.337811:	Training iteration: 79, Loss: 0.005092041101306677
Checkpoint
2018-10-11 20:27:06.512900:	Training iteration: 80, Loss: 0.00425706198439002
Checkpoint
2018-10-11 20:27:07.677016:	Training iteration: 81, Loss: 0.005224595312029123
Checkpoint
2018-10-11 20:27:08.840079:	Training iteration: 82, Loss: 0.005521432962268591
Checkpoint
2018-10-11 20:27:10.006871:	Training iteration: 83, Loss: 0.0057877227663993835
Checkpoint
2018-10-11 20:27:11.165486:	Training iteration: 84, Loss: 0.004996792413294315
Checkpoint
2018-10-11 20:27:12.355231:	Training iteration: 85, Loss: 0.007161698304116726
Checkpoint
2018-10-11 20:27:13.520061:	Training iteration: 86, Loss: 0.005571006331592798
Checkpoint
2018-10-11 20:27:14.676894:	Training iteration: 87, Loss: 0.006095956079661846
Checkpoint
2018-10-11 20:27:15.839777:	Training iteration: 88, Loss: 0.003658737987279892
Checkpoint
2018-10-11 20:27:16.993599:	Training iteration: 89, Loss: 0.00547602865844965
Checkpoint
2018-10-11 20:27:18.168075:	Training iteration: 90, Loss: 0.0058723813854157925
Checkpoint
2018-10-11 20:27:19.336834:	Training iteration: 91, Loss: 0.0063790613785386086
Checkpoint
2018-10-11 20:27:20.512435:	Training iteration: 92, Loss: 0.006372171454131603
Checkpoint
2018-10-11 20:27:21.726322:	Training iteration: 93, Loss: 0.005779746919870377
Checkpoint
2018-10-11 20:27:22.904492:	Training iteration: 94, Loss: 0.004855784587562084
Checkpoint
2018-10-11 20:27:24.075472:	Training iteration: 95, Loss: 0.004711294546723366
Checkpoint
2018-10-11 20:27:25.238025:	Training iteration: 96, Loss: 0.006058094557374716
Checkpoint
2018-10-11 20:27:26.467028:	Training iteration: 97, Loss: 0.004880019463598728
Checkpoint
2018-10-11 20:27:27.624592:	Training iteration: 98, Loss: 0.0039321063086390495
Checkpoint
2018-10-11 20:27:28.785687:	Training iteration: 99, Loss: 0.00466886954382062
Checkpoint
2018-10-11 20:27:29.939224:	Training iteration: 100, Loss: 0.005477302707731724
Checkpoint
2018-10-11 20:27:31.078754:	Training iteration: 101, Loss: 0.00408755149692297
Checkpoint
2018-10-11 20:27:32.260798:	Training iteration: 102, Loss: 0.004349205642938614
Checkpoint
2018-10-11 20:27:33.412261:	Training iteration: 103, Loss: 0.0062921494245529175
Checkpoint
2018-10-11 20:27:34.584812:	Training iteration: 104, Loss: 0.007639238145202398
Checkpoint
2018-10-11 20:27:35.816175:	Training iteration: 105, Loss: 0.006264690309762955
Checkpoint
2018-10-11 20:27:37.003034:	Training iteration: 106, Loss: 0.005699436645954847
Checkpoint
2018-10-11 20:27:38.190886:	Training iteration: 107, Loss: 0.004096810705959797
Checkpoint
2018-10-11 20:27:39.347841:	Training iteration: 108, Loss: 0.006030024029314518
Checkpoint
2018-10-11 20:27:40.505124:	Training iteration: 109, Loss: 0.004651947878301144
Checkpoint
2018-10-11 20:27:41.662371:	Training iteration: 110, Loss: 0.0037905797362327576
Checkpoint
2018-10-11 20:27:42.827358:	Training iteration: 111, Loss: 0.004401115700602531
Checkpoint
2018-10-11 20:27:44.000235:	Training iteration: 112, Loss: 0.006351678166538477
Checkpoint
2018-10-11 20:27:45.158143:	Training iteration: 113, Loss: 0.005095919128507376
Checkpoint
2018-10-11 20:27:46.326296:	Training iteration: 114, Loss: 0.005668187979608774
Checkpoint
2018-10-11 20:27:47.498668:	Training iteration: 115, Loss: 0.0058168829418718815
Checkpoint
2018-10-11 20:27:48.676226:	Training iteration: 116, Loss: 0.005714526865631342
Checkpoint
2018-10-11 20:27:49.856409:	Training iteration: 117, Loss: 0.004913931246846914
Checkpoint
2018-10-11 20:27:51.098627:	Training iteration: 118, Loss: 0.004445727914571762
Checkpoint
2018-10-11 20:27:52.256203:	Training iteration: 119, Loss: 0.003984494600445032
Checkpoint
2018-10-11 20:27:53.419669:	Training iteration: 120, Loss: 0.004577376879751682
Checkpoint
2018-10-11 20:27:54.573332:	Training iteration: 121, Loss: 0.005119404289871454
Checkpoint
2018-10-11 20:27:55.775246:	Training iteration: 122, Loss: 0.004855944775044918
Checkpoint
2018-10-11 20:27:57.008001:	Training iteration: 123, Loss: 0.0060944342985749245
Checkpoint
2018-10-11 20:27:58.200264:	Training iteration: 124, Loss: 0.004661331418901682
Checkpoint
2018-10-11 20:27:59.391114:	Training iteration: 125, Loss: 0.005182222463190556
Checkpoint
2018-10-11 20:28:00.563434:	Training iteration: 126, Loss: 0.006165600381791592
Checkpoint
2018-10-11 20:28:01.730462:	Training iteration: 127, Loss: 0.005209809634834528
Checkpoint
2018-10-11 20:28:02.893129:	Training iteration: 128, Loss: 0.007907840423285961
Checkpoint
2018-10-11 20:28:04.054501:	Training iteration: 129, Loss: 0.005253949202597141
Checkpoint
2018-10-11 20:28:05.207250:	Training iteration: 130, Loss: 0.005665299948304892
Checkpoint
2018-10-11 20:28:06.363480:	Training iteration: 131, Loss: 0.005836444906890392
Checkpoint
2018-10-11 20:28:07.528183:	Training iteration: 132, Loss: 0.005574207287281752
Checkpoint
2018-10-11 20:28:08.779218:	Training iteration: 133, Loss: 0.005257742945104837
Checkpoint
2018-10-11 20:28:09.938504:	Training iteration: 134, Loss: 0.005289403256028891
Checkpoint
2018-10-11 20:28:11.109732:	Training iteration: 135, Loss: 0.003973857033997774
Checkpoint
2018-10-11 20:28:12.289947:	Training iteration: 136, Loss: 0.004908491857349873
Checkpoint
2018-10-11 20:28:13.463271:	Training iteration: 137, Loss: 0.005250851158052683
Checkpoint
2018-10-11 20:28:14.622803:	Training iteration: 138, Loss: 0.005055252928286791
Checkpoint
2018-10-11 20:28:15.782299:	Training iteration: 139, Loss: 0.006097402423620224
Checkpoint
2018-10-11 20:28:16.947337:	Training iteration: 140, Loss: 0.004748134408146143
Checkpoint
2018-10-11 20:28:18.184721:	Training iteration: 141, Loss: 0.004144492093473673
Checkpoint
2018-10-11 20:28:19.331073:	Training iteration: 142, Loss: 0.005681424401700497
Checkpoint
2018-10-11 20:28:20.477073:	Training iteration: 143, Loss: 0.004254327621310949
Checkpoint
2018-10-11 20:28:21.636553:	Training iteration: 144, Loss: 0.005126963369548321
Checkpoint
2018-10-11 20:28:22.901495:	Training iteration: 145, Loss: 0.005362282507121563
Checkpoint
2018-10-11 20:28:24.128205:	Training iteration: 146, Loss: 0.006204905454069376
Checkpoint
2018-10-11 20:28:25.509133:	Training iteration: 147, Loss: 0.004996507428586483
Checkpoint
2018-10-11 20:28:26.832072:	Training iteration: 148, Loss: 0.005169976036995649
Checkpoint
2018-10-11 20:28:28.082682:	Training iteration: 149, Loss: 0.005964805372059345
Checkpoint
2018-10-11 20:28:29.302868:	Training iteration: 150, Loss: 0.004826799035072327
Checkpoint
2018-10-11 20:28:30.537231:	Training iteration: 151, Loss: 0.00533575052395463
Checkpoint
2018-10-11 20:28:31.660418:	Training iteration: 152, Loss: 0.005078043323010206
Checkpoint
2018-10-11 20:28:32.796287:	Training iteration: 153, Loss: 0.006423535756766796
Checkpoint
2018-10-11 20:28:33.923415:	Training iteration: 154, Loss: 0.004335611127316952
Checkpoint
2018-10-11 20:28:35.173472:	Training iteration: 155, Loss: 0.004714975133538246
Checkpoint
2018-10-11 20:28:36.435949:	Training iteration: 156, Loss: 0.005940546747297049
Checkpoint
2018-10-11 20:28:37.569601:	Training iteration: 157, Loss: 0.004441648256033659
Checkpoint
2018-10-11 20:28:38.737978:	Training iteration: 158, Loss: 0.007056155242025852
Checkpoint
2018-10-11 20:28:40.072799:	Training iteration: 159, Loss: 0.005765560083091259
Checkpoint
2018-10-11 20:28:41.236808:	Training iteration: 160, Loss: 0.005629873368889093
Checkpoint
2018-10-11 20:28:42.408239:	Training iteration: 161, Loss: 0.0054777683690190315
Checkpoint
2018-10-11 20:28:43.752505:	Training iteration: 162, Loss: 0.0054923295974731445
Checkpoint
2018-10-11 20:28:44.926141:	Training iteration: 163, Loss: 0.004268682096153498
Checkpoint
2018-10-11 20:28:46.095230:	Training iteration: 164, Loss: 0.005525178741663694
Checkpoint
2018-10-11 20:28:47.298771:	Training iteration: 165, Loss: 0.005565189756453037
Checkpoint
2018-10-11 20:28:48.457781:	Training iteration: 166, Loss: 0.006376819219440222
Checkpoint
2018-10-11 20:28:49.628265:	Training iteration: 167, Loss: 0.004708840511739254
Checkpoint
2018-10-11 20:28:50.892893:	Training iteration: 168, Loss: 0.006284420844167471
Checkpoint
2018-10-11 20:28:52.121827:	Training iteration: 169, Loss: 0.006055270787328482
Checkpoint
2018-10-11 20:28:53.339360:	Training iteration: 170, Loss: 0.0052991993725299835
Checkpoint
2018-10-11 20:28:54.525420:	Training iteration: 171, Loss: 0.005341391544789076
Checkpoint
2018-10-11 20:28:55.824507:	Training iteration: 172, Loss: 0.006005754228681326
Checkpoint
2018-10-11 20:28:57.004291:	Training iteration: 173, Loss: 0.004321274347603321
Checkpoint
2018-10-11 20:28:58.176380:	Training iteration: 174, Loss: 0.005888757761567831
Checkpoint
2018-10-11 20:28:59.331532:	Training iteration: 175, Loss: 0.004530787467956543
Checkpoint
2018-10-11 20:29:00.467638:	Training iteration: 176, Loss: 0.005228959023952484
Checkpoint
2018-10-11 20:29:01.608536:	Training iteration: 177, Loss: 0.005580783821642399
Checkpoint
2018-10-11 20:29:02.749345:	Training iteration: 178, Loss: 0.004785140044987202
Checkpoint
2018-10-11 20:29:03.909733:	Training iteration: 179, Loss: 0.00414982158690691
Checkpoint
2018-10-11 20:29:05.073986:	Training iteration: 180, Loss: 0.00442060362547636
Checkpoint
2018-10-11 20:29:06.245768:	Training iteration: 181, Loss: 0.005812883377075195
Checkpoint
2018-10-11 20:29:07.427240:	Training iteration: 182, Loss: 0.007607275154441595
Checkpoint
2018-10-11 20:29:08.610742:	Training iteration: 183, Loss: 0.004607439506798983
Checkpoint
2018-10-11 20:29:09.795940:	Training iteration: 184, Loss: 0.007234255783259869
Checkpoint
2018-10-11 20:29:11.014229:	Training iteration: 185, Loss: 0.004880818072706461
Checkpoint
2018-10-11 20:29:12.184512:	Training iteration: 186, Loss: 0.004673859570175409
Checkpoint
2018-10-11 20:29:13.347057:	Training iteration: 187, Loss: 0.005008618347346783
Checkpoint
2018-10-11 20:29:14.512425:	Training iteration: 188, Loss: 0.004222050774842501
Checkpoint
2018-10-11 20:29:15.779581:	Training iteration: 189, Loss: 0.005681258626282215
Checkpoint
2018-10-11 20:29:17.003825:	Training iteration: 190, Loss: 0.003552644280716777
Checkpoint
2018-10-11 20:29:18.149622:	Training iteration: 191, Loss: 0.004979986697435379
Checkpoint
2018-10-11 20:29:19.321399:	Training iteration: 192, Loss: 0.004510978236794472
Checkpoint
2018-10-11 20:29:20.486807:	Training iteration: 193, Loss: 0.005025510676205158
Checkpoint
2018-10-11 20:29:21.669200:	Training iteration: 194, Loss: 0.004617135040462017
Checkpoint
2018-10-11 20:29:22.824603:	Training iteration: 195, Loss: 0.005440686829388142
Checkpoint
2018-10-11 20:29:23.993378:	Training iteration: 196, Loss: 0.004864586982876062
Checkpoint
2018-10-11 20:29:25.185413:	Training iteration: 197, Loss: 0.006429028697311878
Checkpoint
2018-10-11 20:29:26.342803:	Training iteration: 198, Loss: 0.005972203332930803
Checkpoint
2018-10-11 20:29:27.497681:	Training iteration: 199, Loss: 0.005204288754612207
Checkpoint
2018-10-11 20:29:28.699113:	Training iteration: 200, Loss: 0.006561717484146357
Checkpoint
2018-10-11 20:29:29.859858:	Training iteration: 201, Loss: 0.003951133694499731
Checkpoint
2018-10-11 20:29:31.036478:	Training iteration: 202, Loss: 0.0057204123586416245
Checkpoint
2018-10-11 20:29:32.192439:	Training iteration: 203, Loss: 0.004926024470478296
Checkpoint
2018-10-11 20:29:33.449336:	Training iteration: 204, Loss: 0.005013208370655775
Checkpoint
2018-10-11 20:29:34.600383:	Training iteration: 205, Loss: 0.004300788976252079
Checkpoint
2018-10-11 20:29:35.766356:	Training iteration: 206, Loss: 0.003972061909735203
Checkpoint
2018-10-11 20:29:36.930315:	Training iteration: 207, Loss: 0.006224700249731541
Checkpoint
2018-10-11 20:29:38.183686:	Training iteration: 208, Loss: 0.004464565310627222
Checkpoint
2018-10-11 20:29:39.333067:	Training iteration: 209, Loss: 0.004938976373523474
Checkpoint
2018-10-11 20:29:40.487629:	Training iteration: 210, Loss: 0.004388410598039627
Checkpoint
2018-10-11 20:29:41.666044:	Training iteration: 211, Loss: 0.006884363479912281
Checkpoint
2018-10-11 20:29:42.883056:	Training iteration: 212, Loss: 0.0047743492759764194
Checkpoint
2018-10-11 20:29:44.041547:	Training iteration: 213, Loss: 0.004490998573601246
Checkpoint
2018-10-11 20:29:45.199967:	Training iteration: 214, Loss: 0.005066893063485622
Checkpoint
2018-10-11 20:29:46.361360:	Training iteration: 215, Loss: 0.005753147881478071
Checkpoint
2018-10-11 20:29:47.531656:	Training iteration: 216, Loss: 0.005986203905194998
Checkpoint
2018-10-11 20:29:48.705654:	Training iteration: 217, Loss: 0.006258570589125156
Checkpoint
2018-10-11 20:29:49.884471:	Training iteration: 218, Loss: 0.004353218246251345
Checkpoint
2018-10-11 20:29:51.061119:	Training iteration: 219, Loss: 0.003403619397431612
Checkpoint
2018-10-11 20:29:52.226166:	Training iteration: 220, Loss: 0.0053732628002762794
Checkpoint
2018-10-11 20:29:53.407919:	Training iteration: 221, Loss: 0.005440021865069866
Checkpoint
2018-10-11 20:29:54.596181:	Training iteration: 222, Loss: 0.006860307417809963
Checkpoint
2018-10-11 20:29:55.757150:	Training iteration: 223, Loss: 0.006642578635364771
Checkpoint
2018-10-11 20:29:56.986670:	Training iteration: 224, Loss: 0.006090350914746523
Checkpoint
2018-10-11 20:29:58.154579:	Training iteration: 225, Loss: 0.0040989890694618225
Checkpoint
2018-10-11 20:29:59.412223:	Training iteration: 226, Loss: 0.004993146751075983
Checkpoint
2018-10-11 20:30:00.635504:	Training iteration: 227, Loss: 0.005863734520971775
Checkpoint
2018-10-11 20:30:01.792959:	Training iteration: 228, Loss: 0.003749492345377803
Checkpoint
2018-10-11 20:30:02.969569:	Training iteration: 229, Loss: 0.0058259498327970505
Checkpoint
2018-10-11 20:30:04.143712:	Training iteration: 230, Loss: 0.004952640272676945
Checkpoint
2018-10-11 20:30:05.315675:	Training iteration: 231, Loss: 0.00434830691665411
Checkpoint
2018-10-11 20:30:06.491687:	Training iteration: 232, Loss: 0.004905439913272858
Checkpoint
2018-10-11 20:30:07.666393:	Training iteration: 233, Loss: 0.004821505397558212
Checkpoint
2018-10-11 20:30:08.845488:	Training iteration: 234, Loss: 0.005748790688812733
Checkpoint
2018-10-11 20:30:10.022582:	Training iteration: 235, Loss: 0.004483765456825495
Checkpoint
2018-10-11 20:30:11.192444:	Training iteration: 236, Loss: 0.006370894610881805
Checkpoint
2018-10-11 20:30:12.348312:	Training iteration: 237, Loss: 0.005221597850322723
Checkpoint
2018-10-11 20:30:13.527125:	Training iteration: 238, Loss: 0.006929571274667978
Checkpoint
2018-10-11 20:30:14.703826:	Training iteration: 239, Loss: 0.006076109129935503
Checkpoint
2018-10-11 20:30:15.979528:	Training iteration: 240, Loss: 0.0046685365960001945
Checkpoint
2018-10-11 20:30:17.206210:	Training iteration: 241, Loss: 0.005732482764869928
Checkpoint
2018-10-11 20:30:18.380271:	Training iteration: 242, Loss: 0.004489903803914785
Checkpoint
2018-10-11 20:30:19.554478:	Training iteration: 243, Loss: 0.005759809166193008
Checkpoint
2018-10-11 20:30:20.720900:	Training iteration: 244, Loss: 0.004391660448163748
Checkpoint
2018-10-11 20:30:21.884919:	Training iteration: 245, Loss: 0.004890700336545706
Checkpoint
2018-10-11 20:30:23.051617:	Training iteration: 246, Loss: 0.007863312028348446
Checkpoint
2018-10-11 20:30:24.208284:	Training iteration: 247, Loss: 0.007424375507980585
Checkpoint
2018-10-11 20:30:25.379503:	Training iteration: 248, Loss: 0.0066582076251506805
Checkpoint
2018-10-11 20:30:26.533358:	Training iteration: 249, Loss: 0.004861233755946159
Checkpoint
2018-10-11 20:30:27.690967:	Training iteration: 250, Loss: 0.0051037040539085865
Checkpoint
2018-10-11 20:30:28.849275:	Training iteration: 251, Loss: 0.005210849456489086
Checkpoint
2018-10-11 20:30:30.011083:	Training iteration: 252, Loss: 0.008297404274344444
Checkpoint
2018-10-11 20:30:31.147690:	Training iteration: 253, Loss: 0.003969239536672831
Checkpoint
2018-10-11 20:30:32.356234:	Training iteration: 254, Loss: 0.004582744091749191
Checkpoint
2018-10-11 20:30:33.535026:	Training iteration: 255, Loss: 0.005024718586355448
Checkpoint
2018-10-11 20:30:34.796925:	Training iteration: 256, Loss: 0.0032270285300910473
Checkpoint
2018-10-11 20:30:35.951778:	Training iteration: 257, Loss: 0.006001134868711233
Checkpoint
2018-10-11 20:30:37.296718:	Training iteration: 258, Loss: 0.004992065951228142
Checkpoint
2018-10-11 20:30:38.589622:	Training iteration: 259, Loss: 0.0039551216177642345
Checkpoint
2018-10-11 20:30:39.898811:	Training iteration: 260, Loss: 0.004460749216377735
Checkpoint
2018-10-11 20:30:41.143296:	Training iteration: 261, Loss: 0.0055731115862727165
Checkpoint
2018-10-11 20:30:42.352273:	Training iteration: 262, Loss: 0.006017791572958231
Checkpoint
2018-10-11 20:30:43.682389:	Training iteration: 263, Loss: 0.005423238966614008
Checkpoint
2018-10-11 20:30:44.825895:	Training iteration: 264, Loss: 0.005519629921764135
Checkpoint
2018-10-11 20:30:46.043045:	Training iteration: 265, Loss: 0.005079392809420824
Checkpoint
2018-10-11 20:30:47.252584:	Training iteration: 266, Loss: 0.0052582016214728355
Checkpoint
2018-10-11 20:30:48.449134:	Training iteration: 267, Loss: 0.007269692607223988
Checkpoint
2018-10-11 20:30:49.867253:	Training iteration: 268, Loss: 0.005509502254426479
Checkpoint
2018-10-11 20:30:51.016445:	Training iteration: 269, Loss: 0.00666375458240509
Checkpoint
2018-10-11 20:30:52.183935:	Training iteration: 270, Loss: 0.004911580588668585
Checkpoint
2018-10-11 20:30:53.362177:	Training iteration: 271, Loss: 0.006847107317298651
Checkpoint
2018-10-11 20:30:54.646286:	Training iteration: 272, Loss: 0.0052150944247841835
Checkpoint
2018-10-11 20:30:55.889676:	Training iteration: 273, Loss: 0.003966550808399916
Checkpoint
2018-10-11 20:30:57.043287:	Training iteration: 274, Loss: 0.003940863069146872
Checkpoint
2018-10-11 20:30:58.520714:	Training iteration: 275, Loss: 0.004105295054614544
Checkpoint
2018-10-11 20:30:59.672767:	Training iteration: 276, Loss: 0.00689653167501092
Checkpoint
2018-10-11 20:31:00.839488:	Training iteration: 277, Loss: 0.004158170893788338
Checkpoint
2018-10-11 20:31:02.036678:	Training iteration: 278, Loss: 0.004191452171653509
Checkpoint
2018-10-11 20:31:03.259149:	Training iteration: 279, Loss: 0.005464409478008747
Checkpoint
2018-10-11 20:31:04.472816:	Training iteration: 280, Loss: 0.0045684888027608395
Checkpoint
2018-10-11 20:31:05.665838:	Training iteration: 281, Loss: 0.005242484621703625
Checkpoint
2018-10-11 20:31:06.815880:	Training iteration: 282, Loss: 0.005363927688449621
Checkpoint
2018-10-11 20:31:07.963486:	Training iteration: 283, Loss: 0.005303273443132639
Checkpoint
2018-10-11 20:31:09.100302:	Training iteration: 284, Loss: 0.0060128262266516685
Checkpoint
2018-10-11 20:31:10.340427:	Training iteration: 285, Loss: 0.004329153336584568
Checkpoint
2018-10-11 20:31:11.517256:	Training iteration: 286, Loss: 0.004137441515922546
Checkpoint
2018-10-11 20:31:12.688736:	Training iteration: 287, Loss: 0.0050571998581290245
Checkpoint
2018-10-11 20:31:13.827320:	Training iteration: 288, Loss: 0.006524334661662579
Checkpoint
2018-10-11 20:31:14.980303:	Training iteration: 289, Loss: 0.0047487663105130196
Checkpoint
2018-10-11 20:31:16.112308:	Training iteration: 290, Loss: 0.005429678596556187
Checkpoint
2018-10-11 20:31:17.281179:	Training iteration: 291, Loss: 0.0069473134353756905
Checkpoint
2018-10-11 20:31:18.441270:	Training iteration: 292, Loss: 0.004611654672771692
Checkpoint
2018-10-11 20:31:19.622634:	Training iteration: 293, Loss: 0.005026963539421558
Checkpoint
2018-10-11 20:31:20.783019:	Training iteration: 294, Loss: 0.006533416919410229
Checkpoint
2018-10-11 20:31:21.942760:	Training iteration: 295, Loss: 0.005569694563746452
Checkpoint
2018-10-11 20:31:23.102787:	Training iteration: 296, Loss: 0.00533819105476141
Checkpoint
2018-10-11 20:31:24.262402:	Training iteration: 297, Loss: 0.004910503048449755
Checkpoint
2018-10-11 20:31:25.429440:	Training iteration: 298, Loss: 0.007860277779400349
Checkpoint
2018-10-11 20:31:26.666642:	Training iteration: 299, Loss: 0.005935228429734707
Checkpoint
2018-10-11 20:31:27.806625:	Training iteration: 300, Loss: 0.0042636035941541195
Checkpoint
2018-10-11 20:31:28.937087:	Training iteration: 301, Loss: 0.004842524882405996
Checkpoint
2018-10-11 20:31:30.117492:	Training iteration: 302, Loss: 0.004467296879738569
Checkpoint
2018-10-11 20:31:31.287282:	Training iteration: 303, Loss: 0.005126303993165493
Checkpoint
2018-10-11 20:31:32.467091:	Training iteration: 304, Loss: 0.005479062907397747
Checkpoint
2018-10-11 20:31:33.685730:	Training iteration: 305, Loss: 0.004783953540027142
Checkpoint
2018-10-11 20:31:34.841273:	Training iteration: 306, Loss: 0.004676259588450193
Checkpoint
2018-10-11 20:31:35.995620:	Training iteration: 307, Loss: 0.006274516694247723
Checkpoint
2018-10-11 20:31:37.166916:	Training iteration: 308, Loss: 0.004152807407081127
Checkpoint
2018-10-11 20:31:38.326090:	Training iteration: 309, Loss: 0.006370054092258215
Checkpoint
2018-10-11 20:31:39.701554:	Training iteration: 310, Loss: 0.005174432881176472
Checkpoint
2018-10-11 20:31:40.865213:	Training iteration: 311, Loss: 0.0040171947330236435
Checkpoint
2018-10-11 20:31:42.026836:	Training iteration: 312, Loss: 0.005036016460508108
Checkpoint
2018-10-11 20:31:43.193643:	Training iteration: 313, Loss: 0.005268564913421869
Checkpoint
2018-10-11 20:31:44.367543:	Training iteration: 314, Loss: 0.0057748109102249146
Checkpoint
2018-10-11 20:31:45.532650:	Training iteration: 315, Loss: 0.004694617819041014
Checkpoint
2018-10-11 20:31:46.695571:	Training iteration: 316, Loss: 0.006771458778530359
Checkpoint
2018-10-11 20:31:47.856677:	Training iteration: 317, Loss: 0.005806808825582266
Checkpoint
2018-10-11 20:31:49.029876:	Training iteration: 318, Loss: 0.006240198388695717
Checkpoint
2018-10-11 20:31:50.195856:	Training iteration: 319, Loss: 0.006249182391911745
Checkpoint
2018-10-11 20:31:51.391749:	Training iteration: 320, Loss: 0.00812247209250927
Checkpoint
2018-10-11 20:31:52.555956:	Training iteration: 321, Loss: 0.0048838406801223755
Checkpoint
2018-10-11 20:31:53.712190:	Training iteration: 322, Loss: 0.003895034547895193
Checkpoint
2018-10-11 20:31:54.884409:	Training iteration: 323, Loss: 0.005035488400608301
Checkpoint
2018-10-11 20:31:56.031377:	Training iteration: 324, Loss: 0.00614860188215971
Checkpoint
2018-10-11 20:31:57.183206:	Training iteration: 325, Loss: 0.004091853741556406
Checkpoint
2018-10-11 20:31:58.346343:	Training iteration: 326, Loss: 0.004413057118654251
Checkpoint
2018-10-11 20:31:59.512109:	Training iteration: 327, Loss: 0.006326156202703714
Checkpoint
2018-10-11 20:32:00.675998:	Training iteration: 328, Loss: 0.007166015915572643
Checkpoint
2018-10-11 20:32:01.843613:	Training iteration: 329, Loss: 0.007036297116428614
Checkpoint
2018-10-11 20:32:03.009728:	Training iteration: 330, Loss: 0.00615251949056983
Checkpoint
2018-10-11 20:32:04.169987:	Training iteration: 331, Loss: 0.005479746498167515
Checkpoint
2018-10-11 20:32:05.330741:	Training iteration: 332, Loss: 0.005994142033159733
Checkpoint
2018-10-11 20:32:06.500532:	Training iteration: 333, Loss: 0.006272292695939541
Checkpoint
2018-10-11 20:32:07.683183:	Training iteration: 334, Loss: 0.004995377734303474
Checkpoint
2018-10-11 20:32:08.845764:	Training iteration: 335, Loss: 0.003963478375226259
Checkpoint
2018-10-11 20:32:10.099809:	Training iteration: 336, Loss: 0.005272407550364733
Checkpoint
2018-10-11 20:32:11.281781:	Training iteration: 337, Loss: 0.007196201477199793
Checkpoint
2018-10-11 20:32:12.494913:	Training iteration: 338, Loss: 0.004897945560514927
Checkpoint
2018-10-11 20:32:13.652634:	Training iteration: 339, Loss: 0.004815900232642889
Checkpoint
2018-10-11 20:32:14.817274:	Training iteration: 340, Loss: 0.0060760932974517345
Checkpoint
2018-10-11 20:32:15.977124:	Training iteration: 341, Loss: 0.005375891923904419
Checkpoint
2018-10-11 20:32:17.147930:	Training iteration: 342, Loss: 0.004741213284432888
Checkpoint
2018-10-11 20:32:18.347215:	Training iteration: 343, Loss: 0.006464748177677393
Checkpoint
2018-10-11 20:32:19.513133:	Training iteration: 344, Loss: 0.005539064761251211
Checkpoint
2018-10-11 20:32:20.749698:	Training iteration: 345, Loss: 0.005549161229282618
Checkpoint
2018-10-11 20:32:22.000772:	Training iteration: 346, Loss: 0.004825299140065908
Checkpoint
2018-10-11 20:32:23.174753:	Training iteration: 347, Loss: 0.0039259567856788635
Checkpoint
2018-10-11 20:32:24.345154:	Training iteration: 348, Loss: 0.004351832903921604
Checkpoint
2018-10-11 20:32:25.511458:	Training iteration: 349, Loss: 0.004515735432505608
Checkpoint
2018-10-11 20:32:26.671437:	Training iteration: 350, Loss: 0.006690724287182093
Checkpoint
2018-10-11 20:32:27.835068:	Training iteration: 351, Loss: 0.005628560204058886
Checkpoint
2018-10-11 20:32:29.014515:	Training iteration: 352, Loss: 0.005005537997931242
Checkpoint
2018-10-11 20:32:30.191110:	Training iteration: 353, Loss: 0.0051133474335074425
Checkpoint
2018-10-11 20:32:31.363418:	Training iteration: 354, Loss: 0.005036941729485989
Checkpoint
2018-10-11 20:32:32.565412:	Training iteration: 355, Loss: 0.006314382888376713
Checkpoint
2018-10-11 20:32:33.738231:	Training iteration: 356, Loss: 0.0050340997986495495
Checkpoint
2018-10-11 20:32:34.900599:	Training iteration: 357, Loss: 0.0045857965014874935
Checkpoint
2018-10-11 20:32:36.074042:	Training iteration: 358, Loss: 0.0047675310634076595
Checkpoint
2018-10-11 20:32:37.232554:	Training iteration: 359, Loss: 0.006340458989143372
Checkpoint
2018-10-11 20:32:38.409195:	Training iteration: 360, Loss: 0.006237241439521313
Checkpoint
2018-10-11 20:32:39.579245:	Training iteration: 361, Loss: 0.005336465779691935
Checkpoint
2018-10-11 20:32:40.742356:	Training iteration: 362, Loss: 0.0046961866319179535
Checkpoint
2018-10-11 20:32:41.903429:	Training iteration: 363, Loss: 0.004695808049291372
Checkpoint
2018-10-11 20:32:43.062348:	Training iteration: 364, Loss: 0.0038908247370272875
Checkpoint
2018-10-11 20:32:44.210184:	Training iteration: 365, Loss: 0.0047444491647183895
Checkpoint
2018-10-11 20:32:45.341757:	Training iteration: 366, Loss: 0.008000599220395088
Checkpoint
2018-10-11 20:32:46.518777:	Training iteration: 367, Loss: 0.004351907409727573
Checkpoint
2018-10-11 20:32:47.738203:	Training iteration: 368, Loss: 0.005171407479792833
Checkpoint
2018-10-11 20:32:48.957381:	Training iteration: 369, Loss: 0.004466019105166197
Checkpoint
2018-10-11 20:32:50.106200:	Training iteration: 370, Loss: 0.004939271602779627
Checkpoint
2018-10-11 20:32:51.584878:	Training iteration: 371, Loss: 0.00522999931126833
Checkpoint
2018-10-11 20:32:52.896079:	Training iteration: 372, Loss: 0.005801052320748568
Checkpoint
2018-10-11 20:32:54.169986:	Training iteration: 373, Loss: 0.004352407529950142
Checkpoint
2018-10-11 20:32:55.392451:	Training iteration: 374, Loss: 0.007102037779986858
Checkpoint
2018-10-11 20:32:56.674373:	Training iteration: 375, Loss: 0.004434204660356045
Checkpoint
2018-10-11 20:32:57.815611:	Training iteration: 376, Loss: 0.004647449124604464
Checkpoint
2018-10-11 20:32:58.994481:	Training iteration: 377, Loss: 0.005332839209586382
Checkpoint
2018-10-11 20:33:00.162120:	Training iteration: 378, Loss: 0.004500431474298239
Checkpoint
2018-10-11 20:33:01.587977:	Training iteration: 379, Loss: 0.005374856758862734
Checkpoint
2018-10-11 20:33:02.776008:	Training iteration: 380, Loss: 0.005015035159885883
Checkpoint
2018-10-11 20:33:04.016614:	Training iteration: 381, Loss: 0.004750379361212254
Checkpoint
2018-10-11 20:33:05.209477:	Training iteration: 382, Loss: 0.0054319254122674465
Checkpoint
2018-10-11 20:33:06.376750:	Training iteration: 383, Loss: 0.005572908092290163
Checkpoint
2018-10-11 20:33:07.539015:	Training iteration: 384, Loss: 0.0050920057110488415
Checkpoint
2018-10-11 20:33:08.818407:	Training iteration: 385, Loss: 0.005836397409439087
Checkpoint
2018-10-11 20:33:10.025248:	Training iteration: 386, Loss: 0.004502432886511087
Checkpoint
2018-10-11 20:33:11.161668:	Training iteration: 387, Loss: 0.006053687073290348
Checkpoint
2018-10-11 20:33:12.341633:	Training iteration: 388, Loss: 0.004783426411449909
Checkpoint
2018-10-11 20:33:13.645474:	Training iteration: 389, Loss: 0.004296041093766689
Checkpoint
2018-10-11 20:33:14.800051:	Training iteration: 390, Loss: 0.004468136467039585
Checkpoint
2018-10-11 20:33:15.946299:	Training iteration: 391, Loss: 0.008487449958920479
Checkpoint
2018-10-11 20:33:17.143412:	Training iteration: 392, Loss: 0.005403950810432434
Checkpoint
2018-10-11 20:33:18.327030:	Training iteration: 393, Loss: 0.003974781837314367
Checkpoint
2018-10-11 20:33:19.493435:	Training iteration: 394, Loss: 0.005486411042511463
Checkpoint
2018-10-11 20:33:20.656731:	Training iteration: 395, Loss: 0.005083348136395216
Checkpoint
2018-10-11 20:33:21.928153:	Training iteration: 396, Loss: 0.005652266088873148
Checkpoint
2018-10-11 20:33:23.055904:	Training iteration: 397, Loss: 0.005976702552288771
Checkpoint
2018-10-11 20:33:24.283919:	Training iteration: 398, Loss: 0.005296774208545685
Checkpoint
2018-10-11 20:33:25.441266:	Training iteration: 399, Loss: 0.004967131651937962
Checkpoint
2018-10-11 20:33:26.604686:	Training iteration: 400, Loss: 0.005345662124454975
Checkpoint
2018-10-11 20:33:27.745031:	Training iteration: 401, Loss: 0.004397546872496605
Checkpoint
2018-10-11 20:33:28.894951:	Training iteration: 402, Loss: 0.007369029335677624
Checkpoint
2018-10-11 20:33:30.133734:	Training iteration: 403, Loss: 0.005824006628245115
Checkpoint
2018-10-11 20:33:31.267862:	Training iteration: 404, Loss: 0.007325713522732258
Checkpoint
2018-10-11 20:33:32.431484:	Training iteration: 405, Loss: 0.004471109248697758
Checkpoint
2018-10-11 20:33:33.601418:	Training iteration: 406, Loss: 0.0056872861459851265
Checkpoint
2018-10-11 20:33:34.847910:	Training iteration: 407, Loss: 0.0053209043107926846
Checkpoint
2018-10-11 20:33:36.028084:	Training iteration: 408, Loss: 0.006347984075546265
Checkpoint
2018-10-11 20:33:37.259334:	Training iteration: 409, Loss: 0.005481931380927563
Checkpoint
2018-10-11 20:33:38.426954:	Training iteration: 410, Loss: 0.006194987799972296
Checkpoint
2018-10-11 20:33:39.600669:	Training iteration: 411, Loss: 0.006005416624248028
Checkpoint
2018-10-11 20:33:40.787829:	Training iteration: 412, Loss: 0.00521583016961813
Checkpoint
2018-10-11 20:33:41.949761:	Training iteration: 413, Loss: 0.005851262249052525
Checkpoint
2018-10-11 20:33:43.126928:	Training iteration: 414, Loss: 0.00652466481551528
Checkpoint
2018-10-11 20:33:44.293593:	Training iteration: 415, Loss: 0.005202201195061207
Checkpoint
2018-10-11 20:33:45.463125:	Training iteration: 416, Loss: 0.005171237979084253
Checkpoint
2018-10-11 20:33:46.676562:	Training iteration: 417, Loss: 0.005648783408105373
Checkpoint
2018-10-11 20:33:47.835324:	Training iteration: 418, Loss: 0.006341172847896814
Checkpoint
2018-10-11 20:33:48.995187:	Training iteration: 419, Loss: 0.005948349833488464
Checkpoint
2018-10-11 20:33:50.187659:	Training iteration: 420, Loss: 0.003932782914489508
Checkpoint
2018-10-11 20:33:51.376682:	Training iteration: 421, Loss: 0.005820261314511299
Checkpoint
2018-10-11 20:33:52.541346:	Training iteration: 422, Loss: 0.004136941395699978
Checkpoint
2018-10-11 20:33:53.703751:	Training iteration: 423, Loss: 0.00432042870670557
Checkpoint
2018-10-11 20:33:54.873495:	Training iteration: 424, Loss: 0.005549674853682518
Checkpoint
2018-10-11 20:33:56.118285:	Training iteration: 425, Loss: 0.006143741309642792
Checkpoint
2018-10-11 20:33:57.288035:	Training iteration: 426, Loss: 0.00592656247317791
Checkpoint
2018-10-11 20:33:58.456942:	Training iteration: 427, Loss: 0.005207080394029617
Checkpoint
2018-10-11 20:33:59.680472:	Training iteration: 428, Loss: 0.0038976226933300495
Checkpoint
2018-10-11 20:34:00.872154:	Training iteration: 429, Loss: 0.005522554274648428
Checkpoint
2018-10-11 20:34:02.049195:	Training iteration: 430, Loss: 0.004393214825540781
Checkpoint
2018-10-11 20:34:03.273856:	Training iteration: 431, Loss: 0.006640066392719746
Checkpoint
2018-10-11 20:34:04.449509:	Training iteration: 432, Loss: 0.005628117825835943
Checkpoint
2018-10-11 20:34:05.604656:	Training iteration: 433, Loss: 0.006967446301132441
Checkpoint
2018-10-11 20:34:06.779850:	Training iteration: 434, Loss: 0.00544500770047307
Checkpoint
2018-10-11 20:34:07.941263:	Training iteration: 435, Loss: 0.005809118039906025
Checkpoint
2018-10-11 20:34:09.097144:	Training iteration: 436, Loss: 0.004548725672066212
Checkpoint
