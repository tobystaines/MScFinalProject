INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "139"
Experiment ID: 139
Preparing dataset
Dataset ready
2018-10-18 09:02:13.676501: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-10-18 09:02:14.651165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-18 09:02:14.651693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:26:00.0
totalMemory: 10.91GiB freeMemory: 10.76GiB
2018-10-18 09:02:14.651710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Starting training
2018-10-18 09:02:33.203809: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 759 of 1000
2018-10-18 09:02:36.129598: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-18 09:02:39.057893: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.64GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-10-18 09:02:39.262790: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-10-18 09:06:55.712333:	Training iteration: 200, Loss: 0.005651277024298906
2018-10-18 09:11:09.809745:	Training iteration: 400, Loss: 0.002717929892241955
2018-10-18 09:15:20.520186:	Training iteration: 600, Loss: 0.005823893938213587
2018-10-18 09:19:31.235324:	Training iteration: 800, Loss: 0.0029023578390479088
2018-10-18 09:24:12.533698:	Training iteration: 1000, Loss: 0.003473109332844615
2018-10-18 09:28:27.457126:	Training iteration: 1200, Loss: 0.0037366782780736685
2018-10-18 09:32:44.431196:	Training iteration: 1400, Loss: 0.005286340601742268
2018-10-18 09:37:28.766721:	Training iteration: 1600, Loss: 0.0035922920797020197
2018-10-18 09:41:45.873142:	Training iteration: 1800, Loss: 0.0029599610716104507
2018-10-18 09:46:03.584706:	Training iteration: 2000, Loss: 0.003605253528803587
2018-10-18 09:50:20.933594:	Training iteration: 2200, Loss: 0.0049856239929795265
2018-10-18 09:54:38.066218:	Training iteration: 2400, Loss: 0.0036431048065423965
2018-10-18 09:58:55.377125:	Training iteration: 2600, Loss: 0.004250655882060528
2018-10-18 10:03:12.800688:	Training iteration: 2800, Loss: 0.004542827606201172
2018-10-18 10:07:29.157927:	Training iteration: 3000, Loss: 0.004434010945260525
2018-10-18 10:12:13.337187:	Training iteration: 3200, Loss: 0.004567063879221678
2018-10-18 10:16:57.321022:	Training iteration: 3400, Loss: 0.005399910733103752
2018-10-18 10:21:14.293348:	Training iteration: 3600, Loss: 0.007044448517262936
2018-10-18 10:25:31.596569:	Training iteration: 3800, Loss: 0.004900743253529072
2018-10-18 10:30:42.938573:	Training iteration: 4000, Loss: 0.0038359721656888723
2018-10-18 10:35:25.145687:	Training iteration: 4200, Loss: 0.006241503171622753
2018-10-18 10:40:36.158258:	Training iteration: 4400, Loss: 0.005033128894865513
2018-10-18 10:45:16.359605:	Training iteration: 4600, Loss: 0.0055164676159620285
2018-10-18 10:49:58.777254:	Training iteration: 4800, Loss: 0.0039470382034778595
2018-10-18 10:54:40.429859:	Training iteration: 5000, Loss: 0.005934172309935093
2018-10-18 10:58:56.481953:	Training iteration: 5200, Loss: 0.003791265422478318
2018-10-18 11:03:13.784329:	Training iteration: 5400, Loss: 0.0022922297939658165
2018-10-18 11:07:30.529899:	Training iteration: 5600, Loss: 0.0048691276460886
2018-10-18 11:11:46.870457:	Training iteration: 5800, Loss: 0.0032253095414489508
