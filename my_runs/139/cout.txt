INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "139"
Experiment ID: 139
Preparing dataset
Dataset ready
2018-10-18 09:02:13.676501: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-10-18 09:02:14.651165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-18 09:02:14.651693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:26:00.0
totalMemory: 10.91GiB freeMemory: 10.76GiB
2018-10-18 09:02:14.651710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Starting training
2018-10-18 09:02:33.203809: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 759 of 1000
2018-10-18 09:02:36.129598: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-18 09:02:39.057893: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.64GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-10-18 09:02:39.262790: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-10-18 09:06:55.712333:	Training iteration: 200, Loss: 0.005651277024298906
2018-10-18 09:11:09.809745:	Training iteration: 400, Loss: 0.002717929892241955
2018-10-18 09:15:20.520186:	Training iteration: 600, Loss: 0.005823893938213587
2018-10-18 09:19:31.235324:	Training iteration: 800, Loss: 0.0029023578390479088
2018-10-18 09:24:12.533698:	Training iteration: 1000, Loss: 0.003473109332844615
2018-10-18 09:28:27.457126:	Training iteration: 1200, Loss: 0.0037366782780736685
2018-10-18 09:32:44.431196:	Training iteration: 1400, Loss: 0.005286340601742268
2018-10-18 09:37:28.766721:	Training iteration: 1600, Loss: 0.0035922920797020197
2018-10-18 09:41:45.873142:	Training iteration: 1800, Loss: 0.0029599610716104507
2018-10-18 09:46:03.584706:	Training iteration: 2000, Loss: 0.003605253528803587
2018-10-18 09:50:20.933594:	Training iteration: 2200, Loss: 0.0049856239929795265
2018-10-18 09:54:38.066218:	Training iteration: 2400, Loss: 0.0036431048065423965
2018-10-18 09:58:55.377125:	Training iteration: 2600, Loss: 0.004250655882060528
2018-10-18 10:03:12.800688:	Training iteration: 2800, Loss: 0.004542827606201172
2018-10-18 10:07:29.157927:	Training iteration: 3000, Loss: 0.004434010945260525
2018-10-18 10:12:13.337187:	Training iteration: 3200, Loss: 0.004567063879221678
2018-10-18 10:16:57.321022:	Training iteration: 3400, Loss: 0.005399910733103752
2018-10-18 10:21:14.293348:	Training iteration: 3600, Loss: 0.007044448517262936
2018-10-18 10:25:31.596569:	Training iteration: 3800, Loss: 0.004900743253529072
2018-10-18 10:30:42.938573:	Training iteration: 4000, Loss: 0.0038359721656888723
2018-10-18 10:35:25.145687:	Training iteration: 4200, Loss: 0.006241503171622753
2018-10-18 10:40:36.158258:	Training iteration: 4400, Loss: 0.005033128894865513
2018-10-18 10:45:16.359605:	Training iteration: 4600, Loss: 0.0055164676159620285
2018-10-18 10:49:58.777254:	Training iteration: 4800, Loss: 0.0039470382034778595
2018-10-18 10:54:40.429859:	Training iteration: 5000, Loss: 0.005934172309935093
2018-10-18 10:58:56.481953:	Training iteration: 5200, Loss: 0.003791265422478318
2018-10-18 11:03:13.784329:	Training iteration: 5400, Loss: 0.0022922297939658165
2018-10-18 11:07:30.529899:	Training iteration: 5600, Loss: 0.0048691276460886
2018-10-18 11:11:46.870457:	Training iteration: 5800, Loss: 0.0032253095414489508
2018-10-18 11:16:05.507198:	Training iteration: 6000, Loss: 0.0033722971566021442
2018-10-18 11:20:25.638948:	Training iteration: 6200, Loss: 0.005270035937428474
2018-10-18 11:24:44.799781:	Training iteration: 6400, Loss: 0.006461350247263908
2018-10-18 11:29:03.945427:	Training iteration: 6600, Loss: 0.00451799388974905
2018-10-18 11:33:23.873540:	Training iteration: 6800, Loss: 0.00482991524040699
2018-10-18 11:37:42.869429:	Training iteration: 7000, Loss: 0.00410604290664196
2018-10-18 11:42:03.442621:	Training iteration: 7200, Loss: 0.004788249731063843
2018-10-18 11:46:23.478574:	Training iteration: 7400, Loss: 0.004940916784107685
2018-10-18 11:50:42.704743:	Training iteration: 7600, Loss: 0.005114959552884102
2018-10-18 11:55:02.090198:	Training iteration: 7800, Loss: 0.005300954449921846
2018-10-18 11:59:20.430374:	Training iteration: 8000, Loss: 0.004032742232084274
2018-10-18 12:03:38.932588:	Training iteration: 8200, Loss: 0.005583358928561211
2018-10-18 12:07:57.351259:	Training iteration: 8400, Loss: 0.0043824901804327965
2018-10-18 12:12:17.002474:	Training iteration: 8600, Loss: 0.008556606248021126
2018-10-18 12:16:35.835036:	Training iteration: 8800, Loss: 0.0040414780378341675
2018-10-18 12:20:55.011621:	Training iteration: 9000, Loss: 0.004770408850163221
2018-10-18 12:25:14.174982:	Training iteration: 9200, Loss: 0.004151086322963238
2018-10-18 12:29:33.629922:	Training iteration: 9400, Loss: 0.004582107532769442
2018-10-18 12:33:52.852783:	Training iteration: 9600, Loss: 0.00248205685056746
2018-10-18 12:38:13.025548:	Training iteration: 9800, Loss: 0.003227971028536558
2018-10-18 12:42:32.366843:	Training iteration: 10000, Loss: 0.00435757776722312
2018-10-18 12:46:51.866130:	Training iteration: 10200, Loss: 0.004286298528313637
2018-10-18 12:51:11.552834:	Training iteration: 10400, Loss: 0.004211798310279846
2018-10-18 12:55:31.212327:	Training iteration: 10600, Loss: 0.003624554257839918
2018-10-18 12:59:50.936860:	Training iteration: 10800, Loss: 0.0032422435469925404
2018-10-18 13:04:09.796350:	Training iteration: 11000, Loss: 0.004767233040183783
2018-10-18 13:08:28.302387:	Training iteration: 11200, Loss: 0.003971525467932224
2018-10-18 13:12:47.256035:	Training iteration: 11400, Loss: 0.002941640093922615
2018-10-18 13:17:05.149174:	Training iteration: 11600, Loss: 0.003581801662221551
2018-10-18 13:21:22.822034:	Training iteration: 11800, Loss: 0.005580353084951639
2018-10-18 13:25:40.401431:	Training iteration: 12000, Loss: 0.00398925831541419
2018-10-18 13:29:58.383313:	Training iteration: 12200, Loss: 0.007080487906932831
2018-10-18 13:34:22.120115: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 761 of 1000
2018-10-18 13:34:25.078689: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-18 13:34:30.619413:	Training iteration: 12400, Loss: 0.004660874139517546
2018-10-18 13:38:47.974407:	Training iteration: 12600, Loss: 0.004173354245722294
2018-10-18 13:43:07.432697:	Training iteration: 12800, Loss: 0.00788787379860878
2018-10-18 13:47:26.444175:	Training iteration: 13000, Loss: 0.005321497097611427
2018-10-18 13:51:46.939526:	Training iteration: 13200, Loss: 0.006251918151974678
2018-10-18 13:56:05.774031:	Training iteration: 13400, Loss: 0.004921752959489822
2018-10-18 14:00:25.041315:	Training iteration: 13600, Loss: 0.0067186513915658
2018-10-18 14:04:46.071171:	Training iteration: 13800, Loss: 0.003944919444620609
2018-10-18 14:09:05.097520:	Training iteration: 14000, Loss: 0.004040825646370649
2018-10-18 14:13:24.459998:	Training iteration: 14200, Loss: 0.004874728620052338
2018-10-18 14:17:43.827046:	Training iteration: 14400, Loss: 0.004630059003829956
2018-10-18 14:22:03.520767:	Training iteration: 14600, Loss: 0.004438254050910473
2018-10-18 14:26:22.215999:	Training iteration: 14800, Loss: 0.004038691520690918
2018-10-18 14:30:40.895798:	Training iteration: 15000, Loss: 0.004833373241126537
2018-10-18 14:35:00.165461:	Training iteration: 15200, Loss: 0.0062685776501894
2018-10-18 14:39:19.514942:	Training iteration: 15400, Loss: 0.004960079677402973
2018-10-18 14:43:38.458020:	Training iteration: 15600, Loss: 0.00420848373323679
2018-10-18 14:47:57.317066:	Training iteration: 15800, Loss: 0.0052813030779361725
2018-10-18 14:52:16.583522:	Training iteration: 16000, Loss: 0.009430661797523499
2018-10-18 14:56:37.438299:	Training iteration: 16200, Loss: 0.004803787916898727
2018-10-18 15:00:57.472514:	Training iteration: 16400, Loss: 0.004688624292612076
2018-10-18 15:05:16.010645:	Training iteration: 16600, Loss: 0.004519916605204344
2018-10-18 15:09:35.438214:	Training iteration: 16800, Loss: 0.006307016126811504
2018-10-18 15:13:54.716064:	Training iteration: 17000, Loss: 0.006204165983945131
2018-10-18 15:18:13.272061:	Training iteration: 17200, Loss: 0.00449935719370842
2018-10-18 15:22:32.941593:	Training iteration: 17400, Loss: 0.0037384191527962685
2018-10-18 15:26:52.384320:	Training iteration: 17600, Loss: 0.0050970022566616535
2018-10-18 15:31:11.094207:	Training iteration: 17800, Loss: 0.005892097018659115
2018-10-18 15:35:30.652658:	Training iteration: 18000, Loss: 0.00524654146283865
2018-10-18 15:39:50.985484:	Training iteration: 18200, Loss: 0.004903692286461592
2018-10-18 15:44:09.798951:	Training iteration: 18400, Loss: 0.006031026132404804
2018-10-18 15:48:29.699248:	Training iteration: 18600, Loss: 0.007512613665312529
2018-10-18 15:52:48.478575:	Training iteration: 18800, Loss: 0.004319711122661829
2018-10-18 15:57:07.710386:	Training iteration: 19000, Loss: 0.003497688565403223
2018-10-18 16:01:26.821972:	Training iteration: 19200, Loss: 0.005208148621022701
2018-10-18 16:05:46.378142:	Training iteration: 19400, Loss: 0.003682671347633004
2018-10-18 16:10:06.161595:	Training iteration: 19600, Loss: 0.005686817690730095
2018-10-18 16:14:27.117554:	Training iteration: 19800, Loss: 0.003774599637836218
2018-10-18 16:18:45.758079:	Training iteration: 20000, Loss: 0.004434049129486084
2018-10-18 16:23:05.194447:	Training iteration: 20200, Loss: 0.004722115583717823
2018-10-18 16:27:25.321362:	Training iteration: 20400, Loss: 0.0029157623648643494
2018-10-18 16:31:45.384033:	Training iteration: 20600, Loss: 0.0032329647801816463
2018-10-18 16:36:04.932296:	Training iteration: 20800, Loss: 0.0028237085789442062
2018-10-18 16:40:24.669122:	Training iteration: 21000, Loss: 0.004966599866747856
2018-10-18 16:44:44.973348:	Training iteration: 21200, Loss: 0.004192378371953964
2018-10-18 16:49:04.147807:	Training iteration: 21400, Loss: 0.00399862602353096
2018-10-18 16:53:23.353150:	Training iteration: 21600, Loss: 0.004290735349059105
2018-10-18 16:57:41.824228:	Training iteration: 21800, Loss: 0.0034230491146445274
2018-10-18 17:02:01.201719:	Training iteration: 22000, Loss: 0.004036762751638889
2018-10-18 17:06:19.802821:	Training iteration: 22200, Loss: 0.005057195201516151
2018-10-18 17:10:38.247922:	Training iteration: 22400, Loss: 0.007963001728057861
2018-10-18 17:14:57.282817:	Training iteration: 22600, Loss: 0.004984842147678137
2018-10-18 17:19:16.614240:	Training iteration: 22800, Loss: 0.004787059035152197
2018-10-18 17:23:36.639404:	Training iteration: 23000, Loss: 0.003944168798625469
2018-10-18 17:27:57.179129:	Training iteration: 23200, Loss: 0.005815078504383564
2018-10-18 17:32:17.726864:	Training iteration: 23400, Loss: 0.004109143745154142
2018-10-18 17:36:37.076737:	Training iteration: 23600, Loss: 0.003808170324191451
2018-10-18 17:40:56.546072:	Training iteration: 23800, Loss: 0.005862067453563213
2018-10-18 17:45:15.342190:	Training iteration: 24000, Loss: 0.0051648905500769615
2018-10-18 17:49:35.963266:	Training iteration: 24200, Loss: 0.005102220922708511
2018-10-18 17:53:56.017257:	Training iteration: 24400, Loss: 0.0035916564520448446
2018-10-18 17:58:15.507386:	Training iteration: 24600, Loss: 0.004299509339034557
2018-10-18 18:02:34.813979:	Training iteration: 24800, Loss: 0.003796882461756468
2018-10-18 18:04:07.845343: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 756 of 1000
2018-10-18 18:04:10.747161: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-18 18:07:06.017341:	Training iteration: 25000, Loss: 0.005193105898797512
2018-10-18 18:11:24.959099:	Training iteration: 25200, Loss: 0.005220578983426094
2018-10-18 18:15:43.190764:	Training iteration: 25400, Loss: 0.004158210940659046
2018-10-18 18:20:01.370422:	Training iteration: 25600, Loss: 0.0056322477757930756
2018-10-18 18:24:19.810700:	Training iteration: 25800, Loss: 0.004854008089751005
2018-10-18 18:28:38.876737:	Training iteration: 26000, Loss: 0.00911029614508152
2018-10-18 18:32:58.514833:	Training iteration: 26200, Loss: 0.0060190255753695965
2018-10-18 18:37:18.176185:	Training iteration: 26400, Loss: 0.004076952580362558
2018-10-18 18:41:37.675891:	Training iteration: 26600, Loss: 0.0056983474642038345
2018-10-18 18:45:56.669700:	Training iteration: 26800, Loss: 0.007498213090002537
2018-10-18 18:50:16.984077:	Training iteration: 27000, Loss: 0.0077623832039535046
2018-10-18 18:54:36.136423:	Training iteration: 27200, Loss: 0.0032970979809761047
2018-10-18 18:58:55.811030:	Training iteration: 27400, Loss: 0.006621871143579483
2018-10-18 19:03:15.838639:	Training iteration: 27600, Loss: 0.00606817752122879
2018-10-18 19:07:35.306505:	Training iteration: 27800, Loss: 0.004179922863841057
2018-10-18 19:11:55.459808:	Training iteration: 28000, Loss: 0.00539826275780797
2018-10-18 19:16:14.297063:	Training iteration: 28200, Loss: 0.004325437359511852
2018-10-18 19:20:33.346723:	Training iteration: 28400, Loss: 0.005623379722237587
2018-10-18 19:24:52.519119:	Training iteration: 28600, Loss: 0.006401743274182081
2018-10-18 19:29:11.791536:	Training iteration: 28800, Loss: 0.005400230176746845
2018-10-18 19:33:30.188994:	Training iteration: 29000, Loss: 0.0048718759790062904
2018-10-18 19:37:48.951285:	Training iteration: 29200, Loss: 0.006270034238696098
2018-10-18 19:42:09.102987:	Training iteration: 29400, Loss: 0.005098086781799793
2018-10-18 19:46:28.203972:	Training iteration: 29600, Loss: 0.00497802160680294
2018-10-18 19:50:48.160174:	Training iteration: 29800, Loss: 0.004995574243366718
2018-10-18 19:55:07.601608:	Training iteration: 30000, Loss: 0.005033151246607304
2018-10-18 19:59:27.491183:	Training iteration: 30200, Loss: 0.008364992216229439
2018-10-18 20:03:46.808715:	Training iteration: 30400, Loss: 0.005843024700880051
2018-10-18 20:08:06.345928:	Training iteration: 30600, Loss: 0.0046348110772669315
2018-10-18 20:12:26.247795:	Training iteration: 30800, Loss: 0.005354857072234154
2018-10-18 20:16:47.078591:	Training iteration: 31000, Loss: 0.008275561034679413
2018-10-18 20:21:06.572377:	Training iteration: 31200, Loss: 0.005478333216160536
2018-10-18 20:25:27.256090:	Training iteration: 31400, Loss: 0.005167343653738499
2018-10-18 20:29:45.941696:	Training iteration: 31600, Loss: 0.004097188822925091
2018-10-18 20:34:04.931025:	Training iteration: 31800, Loss: 0.003788992762565613
2018-10-18 20:38:24.224026:	Training iteration: 32000, Loss: 0.0055520241148769855
2018-10-18 20:42:43.367361:	Training iteration: 32200, Loss: 0.0037400731816887856
2018-10-18 20:47:01.959066:	Training iteration: 32400, Loss: 0.006691342685371637
2018-10-18 20:51:21.113550:	Training iteration: 32600, Loss: 0.004113531671464443
2018-10-18 20:55:40.861202:	Training iteration: 32800, Loss: 0.005444868467748165
2018-10-18 20:59:59.620859:	Training iteration: 33000, Loss: 0.003230602480471134
2018-10-18 21:04:19.199571:	Training iteration: 33200, Loss: 0.0054622916504740715
2018-10-18 21:08:38.257785:	Training iteration: 33400, Loss: 0.004071604926139116
2018-10-18 21:12:58.002745:	Training iteration: 33600, Loss: 0.00589875690639019
2018-10-18 21:17:16.789634:	Training iteration: 33800, Loss: 0.007268314715474844
2018-10-18 21:21:35.906157:	Training iteration: 34000, Loss: 0.004664951004087925
2018-10-18 21:25:55.590585:	Training iteration: 34200, Loss: 0.00426213163882494
2018-10-18 21:30:15.099934:	Training iteration: 34400, Loss: 0.004743166733533144
2018-10-18 21:34:35.804990:	Training iteration: 34600, Loss: 0.005251693539321423
2018-10-18 21:38:54.981666:	Training iteration: 34800, Loss: 0.004843884147703648
2018-10-18 21:43:14.343218:	Training iteration: 35000, Loss: 0.004175590351223946
2018-10-18 21:47:34.245923:	Training iteration: 35200, Loss: 0.005638273898512125
2018-10-18 21:51:52.686759:	Training iteration: 35400, Loss: 0.00391750643029809
2018-10-18 21:56:11.351810:	Training iteration: 35600, Loss: 0.005384594202041626
2018-10-18 22:00:30.005736:	Training iteration: 35800, Loss: 0.005978616885840893
2018-10-18 22:04:48.227118:	Training iteration: 36000, Loss: 0.0052216509357094765
2018-10-18 22:09:07.086271:	Training iteration: 36200, Loss: 0.006066260859370232
2018-10-18 22:13:26.767295:	Training iteration: 36400, Loss: 0.010411364957690239
2018-10-18 22:17:45.229544:	Training iteration: 36600, Loss: 0.0049800993874669075
2018-10-18 22:22:04.860886:	Training iteration: 36800, Loss: 0.004761985503137112
2018-10-18 22:26:24.359362:	Training iteration: 37000, Loss: 0.008390390314161777
2018-10-18 22:30:43.208273:	Training iteration: 37200, Loss: 0.006360030733048916
2018-10-18 22:33:32.705103: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 774 of 1000
2018-10-18 22:33:35.429671: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-18 22:35:15.154887:	Training iteration: 37400, Loss: 0.004021625034511089
2018-10-18 22:39:34.202617:	Training iteration: 37600, Loss: 0.004127829801291227
2018-10-18 22:43:53.992726:	Training iteration: 37800, Loss: 0.004956656135618687
2018-10-18 22:48:13.740660:	Training iteration: 38000, Loss: 0.005080367438495159
2018-10-18 22:52:32.910931:	Training iteration: 38200, Loss: 0.004540249705314636
2018-10-18 22:56:52.204785:	Training iteration: 38400, Loss: 0.003491877345368266
2018-10-18 23:01:10.462985:	Training iteration: 38600, Loss: 0.004283543676137924
2018-10-18 23:05:30.081496:	Training iteration: 38800, Loss: 0.004635822959244251
2018-10-18 23:09:49.170050:	Training iteration: 39000, Loss: 0.0039643957279622555
2018-10-18 23:14:07.150994:	Training iteration: 39200, Loss: 0.0023901937529444695
2018-10-18 23:18:26.729036:	Training iteration: 39400, Loss: 0.004374775104224682
2018-10-18 23:22:46.067240:	Training iteration: 39600, Loss: 0.0036847374867647886
2018-10-18 23:27:05.989805:	Training iteration: 39800, Loss: 0.005641454365104437
2018-10-18 23:31:25.402949:	Training iteration: 40000, Loss: 0.005561003461480141
2018-10-18 23:35:45.190719:	Training iteration: 40200, Loss: 0.004269883967936039
2018-10-18 23:40:04.542642:	Training iteration: 40400, Loss: 0.004284962546080351
2018-10-18 23:44:24.042573:	Training iteration: 40600, Loss: 0.0025373317766934633
2018-10-18 23:48:43.686393:	Training iteration: 40800, Loss: 0.003950661979615688
2018-10-18 23:53:03.921713:	Training iteration: 41000, Loss: 0.004483656957745552
2018-10-18 23:57:23.470200:	Training iteration: 41200, Loss: 0.003438780549913645
2018-10-19 00:01:43.266295:	Training iteration: 41400, Loss: 0.006056612357497215
2018-10-19 00:06:03.616369:	Training iteration: 41600, Loss: 0.003963846247643232
2018-10-19 00:10:23.651424:	Training iteration: 41800, Loss: 0.005279109813272953
2018-10-19 00:14:42.124165:	Training iteration: 42000, Loss: 0.004723208956420422
2018-10-19 00:19:01.248517:	Training iteration: 42200, Loss: 0.004507229197770357
2018-10-19 00:23:19.700513:	Training iteration: 42400, Loss: 0.003894046414643526
2018-10-19 00:27:37.709080:	Training iteration: 42600, Loss: 0.0031249751336872578
2018-10-19 00:31:57.204257:	Training iteration: 42800, Loss: 0.003994828090071678
2018-10-19 00:36:16.082656:	Training iteration: 43000, Loss: 0.005160864908248186
2018-10-19 00:40:37.169264:	Training iteration: 43200, Loss: 0.005769059062004089
2018-10-19 00:44:55.739302:	Training iteration: 43400, Loss: 0.0034790602512657642
2018-10-19 00:49:15.273637:	Training iteration: 43600, Loss: 0.004751395434141159
2018-10-19 00:53:34.281018:	Training iteration: 43800, Loss: 0.004475724883377552
2018-10-19 00:57:54.101193:	Training iteration: 44000, Loss: 0.0021737669594585896
2018-10-19 01:02:13.517445:	Training iteration: 44200, Loss: 0.002455280162394047
2018-10-19 01:06:32.085180:	Training iteration: 44400, Loss: 0.005284909624606371
2018-10-19 01:10:52.570056:	Training iteration: 44600, Loss: 0.004293346777558327
2018-10-19 01:15:12.060888:	Training iteration: 44800, Loss: 0.0031107505783438683
2018-10-19 01:19:31.622307:	Training iteration: 45000, Loss: 0.004961702041327953
2018-10-19 01:23:50.133187:	Training iteration: 45200, Loss: 0.004361210390925407
2018-10-19 01:28:09.193797:	Training iteration: 45400, Loss: 0.0058721015229821205
2018-10-19 01:32:28.282652:	Training iteration: 45600, Loss: 0.00617343420162797
2018-10-19 01:36:46.749941:	Training iteration: 45800, Loss: 0.0028461110778152943
2018-10-19 01:41:06.492354:	Training iteration: 46000, Loss: 0.004542706534266472
2018-10-19 01:45:26.624129:	Training iteration: 46200, Loss: 0.003744380548596382
2018-10-19 01:49:45.298089:	Training iteration: 46400, Loss: 0.004194940440356731
2018-10-19 01:54:04.848577:	Training iteration: 46600, Loss: 0.005308180116117001
2018-10-19 01:58:24.910545:	Training iteration: 46800, Loss: 0.0032418896444141865
2018-10-19 02:02:43.773434:	Training iteration: 47000, Loss: 0.004023120738565922
2018-10-19 02:07:02.919272:	Training iteration: 47200, Loss: 0.004760316573083401
2018-10-19 02:11:22.178049:	Training iteration: 47400, Loss: 0.00310450978577137
2018-10-19 02:15:41.074027:	Training iteration: 47600, Loss: 0.0036066079046577215
2018-10-19 02:20:00.581206:	Training iteration: 47800, Loss: 0.0032275307457894087
2018-10-19 02:24:21.009042:	Training iteration: 48000, Loss: 0.0026833803858608007
2018-10-19 02:28:40.627387:	Training iteration: 48200, Loss: 0.004540877416729927
2018-10-19 02:33:00.942514:	Training iteration: 48400, Loss: 0.0035694981925189495
2018-10-19 02:37:20.664992:	Training iteration: 48600, Loss: 0.0028279274702072144
2018-10-19 02:41:39.445753:	Training iteration: 48800, Loss: 0.005353198386728764
2018-10-19 02:45:58.422905:	Training iteration: 49000, Loss: 0.003976297564804554
2018-10-19 02:50:16.709759:	Training iteration: 49200, Loss: 0.0028043841011822224
2018-10-19 02:54:36.310492:	Training iteration: 49400, Loss: 0.005022446159273386
2018-10-19 02:58:55.360176:	Training iteration: 49600, Loss: 0.0037801708094775677
2018-10-19 03:03:14.975247:	Training iteration: 49800, Loss: 0.0032158279791474342
2018-10-19 03:07:34.214758:	Training iteration: 50000, Loss: 0.0030819482635706663
2018-10-19 03:11:54.065498:	Training iteration: 50200, Loss: 0.00555455032736063
2018-10-19 03:15:09.296916: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 896 of 1000
2018-10-19 03:15:10.476312: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-19 03:16:24.008472:	Training iteration: 50400, Loss: 0.006385046988725662
2018-10-19 03:20:43.741461:	Training iteration: 50600, Loss: 0.005261644255369902
2018-10-19 03:25:02.634598:	Training iteration: 50800, Loss: 0.006432780064642429
2018-10-19 03:29:22.709266:	Training iteration: 51000, Loss: 0.005270084366202354
2018-10-19 03:33:41.066522:	Training iteration: 51200, Loss: 0.007108220364898443
2018-10-19 03:38:00.742784:	Training iteration: 51400, Loss: 0.005626711994409561
2018-10-19 03:42:20.631076:	Training iteration: 51600, Loss: 0.004668037407100201
2018-10-19 03:46:38.948613:	Training iteration: 51800, Loss: 0.0038999903481453657
2018-10-19 03:50:57.581872:	Training iteration: 52000, Loss: 0.005167075432837009
2018-10-19 03:55:16.024405:	Training iteration: 52200, Loss: 0.002909563947468996
2018-10-19 03:59:35.542090:	Training iteration: 52400, Loss: 0.006297115236520767
2018-10-19 04:03:53.995850:	Training iteration: 52600, Loss: 0.004171178676187992
2018-10-19 04:08:12.443757:	Training iteration: 52800, Loss: 0.00722942128777504
2018-10-19 04:12:31.045415:	Training iteration: 53000, Loss: 0.0049016885459423065
2018-10-19 04:16:50.249485:	Training iteration: 53200, Loss: 0.011607479304075241
2018-10-19 04:21:09.197450:	Training iteration: 53400, Loss: 0.010524973273277283
2018-10-19 04:25:28.874281:	Training iteration: 53600, Loss: 0.006636199541389942
2018-10-19 04:29:48.001640:	Training iteration: 53800, Loss: 0.00806390680372715
2018-10-19 04:34:07.475632:	Training iteration: 54000, Loss: 0.006807014811784029
2018-10-19 04:38:25.915087:	Training iteration: 54200, Loss: 0.004124424420297146
2018-10-19 04:42:44.734134:	Training iteration: 54400, Loss: 0.004773612134158611
2018-10-19 04:47:04.584305:	Training iteration: 54600, Loss: 0.0029187307227402925
2018-10-19 04:51:24.278683:	Training iteration: 54800, Loss: 0.004954047966748476
2018-10-19 04:55:43.910320:	Training iteration: 55000, Loss: 0.0034061786718666553
2018-10-19 05:00:02.878994:	Training iteration: 55200, Loss: 0.005867106840014458
2018-10-19 05:04:21.211725:	Training iteration: 55400, Loss: 0.004216664470732212
2018-10-19 05:08:39.435277:	Training iteration: 55600, Loss: 0.005931606516242027
2018-10-19 05:12:58.170366:	Training iteration: 55800, Loss: 0.007766510359942913
2018-10-19 05:17:16.766778:	Training iteration: 56000, Loss: 0.0025526839308440685
2018-10-19 05:21:35.590049:	Training iteration: 56200, Loss: 0.007681822404265404
2018-10-19 05:25:54.064014:	Training iteration: 56400, Loss: 0.0056443302892148495
2018-10-19 05:30:13.535413:	Training iteration: 56600, Loss: 0.0036684852093458176
2018-10-19 05:34:32.642311:	Training iteration: 56800, Loss: 0.004434441216289997
2018-10-19 05:38:51.296970:	Training iteration: 57000, Loss: 0.00504449475556612
2018-10-19 05:43:10.114622:	Training iteration: 57200, Loss: 0.004331403877586126
2018-10-19 05:47:29.462997:	Training iteration: 57400, Loss: 0.004078123718500137
2018-10-19 05:51:49.357690:	Training iteration: 57600, Loss: 0.006897091865539551
2018-10-19 05:56:08.357187:	Training iteration: 57800, Loss: 0.006353732198476791
2018-10-19 06:00:26.802705:	Training iteration: 58000, Loss: 0.0036115003749728203
2018-10-19 06:04:46.871873:	Training iteration: 58200, Loss: 0.008799374103546143
2018-10-19 06:09:05.804614:	Training iteration: 58400, Loss: 0.004635038319975138
2018-10-19 06:13:24.620427:	Training iteration: 58600, Loss: 0.0032515940256416798
2018-10-19 06:17:43.065111:	Training iteration: 58800, Loss: 0.0035038620699197054
2018-10-19 06:22:01.172607:	Training iteration: 59000, Loss: 0.0035661859437823296
2018-10-19 06:26:19.773452:	Training iteration: 59200, Loss: 0.002542733447626233
2018-10-19 06:30:38.368090:	Training iteration: 59400, Loss: 0.004902658052742481
2018-10-19 06:34:57.603866:	Training iteration: 59600, Loss: 0.0048222048208117485
2018-10-19 06:39:16.156777:	Training iteration: 59800, Loss: 0.005896174348890781
2018-10-19 06:43:35.756299:	Training iteration: 60000, Loss: 0.009467534720897675
2018-10-19 06:47:54.520675:	Training iteration: 60200, Loss: 0.008043893612921238
2018-10-19 06:52:13.822365:	Training iteration: 60400, Loss: 0.005785830784589052
2018-10-19 06:56:33.895137:	Training iteration: 60600, Loss: 0.005488790571689606
2018-10-19 07:00:52.992785:	Training iteration: 60800, Loss: 0.003712018486112356
2018-10-19 07:05:13.240271:	Training iteration: 61000, Loss: 0.003240058431401849
2018-10-19 07:09:32.330719:	Training iteration: 61200, Loss: 0.0050165955908596516
2018-10-19 07:13:51.420440:	Training iteration: 61400, Loss: 0.006812142673879862
2018-10-19 07:18:10.910170:	Training iteration: 61600, Loss: 0.00363431335426867
2018-10-19 07:22:30.447333:	Training iteration: 61800, Loss: 0.005468766205012798
2018-10-19 07:26:49.314824:	Training iteration: 62000, Loss: 0.003700645174831152
2018-10-19 07:31:08.990757:	Training iteration: 62200, Loss: 0.005094484891742468
2018-10-19 07:35:27.043969:	Training iteration: 62400, Loss: 0.005250090733170509
2018-10-19 07:39:45.364182:	Training iteration: 62600, Loss: 0.0043382178992033005
2018-10-19 07:44:04.538779:	Training iteration: 62800, Loss: 0.004159874282777309
2018-10-19 07:48:23.769569:	Training iteration: 63000, Loss: 0.004114840179681778
2018-10-19 07:52:43.230580:	Training iteration: 63200, Loss: 0.004730981774628162
2018-10-19 07:57:01.754428:	Training iteration: 63400, Loss: 0.004961859900504351
2018-10-19 08:01:21.386676:	Training iteration: 63600, Loss: 0.005798162426799536
2018-10-19 08:05:40.090059:	Training iteration: 63800, Loss: 0.005527751054614782
2018-10-19 08:10:00.060696:	Training iteration: 64000, Loss: 0.007191309705376625
2018-10-19 08:14:19.894334:	Training iteration: 64200, Loss: 0.007400427013635635
2018-10-19 08:18:39.139436:	Training iteration: 64400, Loss: 0.0071143051609396935
2018-10-19 08:22:59.730667:	Training iteration: 64600, Loss: 0.007914131507277489
2018-10-19 08:27:18.924762:	Training iteration: 64800, Loss: 0.005723755806684494
2018-10-19 08:31:37.888689:	Training iteration: 65000, Loss: 0.004457096569240093
2018-10-19 08:35:56.946987:	Training iteration: 65200, Loss: 0.004226483404636383
2018-10-19 08:40:16.116269:	Training iteration: 65400, Loss: 0.005497242324054241
2018-10-19 08:44:35.135100:	Training iteration: 65600, Loss: 0.0054306816309690475
2018-10-19 08:48:52.864648:	Training iteration: 65800, Loss: 0.0026000565849244595
2018-10-19 08:53:11.722375:	Training iteration: 66000, Loss: 0.00585817638784647
2018-10-19 08:57:30.310920:	Training iteration: 66200, Loss: 0.008384685963392258
2018-10-19 09:01:48.378496:	Training iteration: 66400, Loss: 0.006604715250432491
2018-10-19 09:06:07.085564:	Training iteration: 66600, Loss: 0.006451444700360298
2018-10-19 09:10:25.994746:	Training iteration: 66800, Loss: 0.005462914239615202
2018-10-19 09:14:44.794780:	Training iteration: 67000, Loss: 0.00783558003604412
2018-10-19 09:19:03.257766:	Training iteration: 67200, Loss: 0.007319825701415539
2018-10-19 09:23:22.133577:	Training iteration: 67400, Loss: 0.008722502738237381
2018-10-19 09:27:40.986699:	Training iteration: 67600, Loss: 0.006769342347979546
2018-10-19 09:32:00.420057:	Training iteration: 67800, Loss: 0.006343998946249485
2018-10-19 09:36:19.654678:	Training iteration: 68000, Loss: 0.010848218575119972
2018-10-19 09:40:39.731746:	Training iteration: 68200, Loss: 0.003954799845814705
2018-10-19 09:44:59.433293:	Training iteration: 68400, Loss: 0.0055013722740113735
2018-10-19 09:49:18.470789:	Training iteration: 68600, Loss: 0.0060328831896185875
2018-10-19 09:53:36.984770:	Training iteration: 68800, Loss: 0.005746384616941214
2018-10-19 09:57:55.436964:	Training iteration: 69000, Loss: 0.00259612244553864
2018-10-19 10:02:13.450328:	Training iteration: 69200, Loss: 0.006306913215667009
2018-10-19 10:06:31.725234:	Training iteration: 69400, Loss: 0.003813659306615591
2018-10-19 10:10:50.331481:	Training iteration: 69600, Loss: 0.006357154808938503
2018-10-19 10:15:09.322172:	Training iteration: 69800, Loss: 0.00790337286889553
2018-10-19 10:19:28.620650:	Training iteration: 70000, Loss: 0.003861659672111273
2018-10-19 10:23:47.949431:	Training iteration: 70200, Loss: 0.006633106619119644
2018-10-19 10:28:06.304747:	Training iteration: 70400, Loss: 0.004909593611955643
2018-10-19 10:32:26.297550:	Training iteration: 70600, Loss: 0.0031257434748113155
2018-10-19 10:36:45.604927:	Training iteration: 70800, Loss: 0.005619109608232975
2018-10-19 10:41:04.193442:	Training iteration: 71000, Loss: 0.007304646540433168
2018-10-19 10:45:23.088340:	Training iteration: 71200, Loss: 0.005250820890069008
2018-10-19 10:49:42.060296:	Training iteration: 71400, Loss: 0.004728361964225769
2018-10-19 10:54:00.614884:	Training iteration: 71600, Loss: 0.007526696659624577
2018-10-19 10:58:20.726566:	Training iteration: 71800, Loss: 0.004329180344939232
2018-10-19 11:02:39.890355:	Training iteration: 72000, Loss: 0.0058305831626057625
2018-10-19 11:06:58.921730:	Training iteration: 72200, Loss: 0.006520875729620457
2018-10-19 11:11:18.461159:	Training iteration: 72400, Loss: 0.006499654613435268
2018-10-19 11:15:37.388324:	Training iteration: 72600, Loss: 0.0029280667658895254
2018-10-19 11:19:55.209869:	Training iteration: 72800, Loss: 0.003909536171704531
2018-10-19 11:24:14.069982:	Training iteration: 73000, Loss: 0.007309138774871826
2018-10-19 11:28:32.643786:	Training iteration: 73200, Loss: 0.004619138315320015
2018-10-19 11:32:52.466344:	Training iteration: 73400, Loss: 0.00733194500207901
2018-10-19 11:37:10.863965:	Training iteration: 73600, Loss: 0.007876325398683548
2018-10-19 11:41:30.750969:	Training iteration: 73800, Loss: 0.007312457077205181
2018-10-19 11:45:49.781948:	Training iteration: 74000, Loss: 0.005656348541378975
2018-10-19 11:50:09.227957:	Training iteration: 74200, Loss: 0.006576803978532553
2018-10-19 11:54:28.524757:	Training iteration: 74400, Loss: 0.010288352146744728
2018-10-19 11:58:48.026742:	Training iteration: 74600, Loss: 0.006145781837403774
2018-10-19 12:03:07.590399:	Training iteration: 74800, Loss: 0.004611399956047535
2018-10-19 12:07:27.030192:	Training iteration: 75000, Loss: 0.008262638002634048
2018-10-19 12:11:45.767830:	Training iteration: 75200, Loss: 0.00617759907618165
2018-10-19 12:16:04.550371:	Training iteration: 75400, Loss: 0.005231266841292381
2018-10-19 12:20:22.080846:	Training iteration: 75600, Loss: 0.007411319762468338
2018-10-19 12:24:40.047050:	Training iteration: 75800, Loss: 0.006003004498779774
2018-10-19 12:28:58.728693:	Training iteration: 76000, Loss: 0.004484853707253933
2018-10-19 12:33:16.609893:	Training iteration: 76200, Loss: 0.003560300450772047
2018-10-19 12:37:35.731881:	Training iteration: 76400, Loss: 0.008009228855371475
2018-10-19 12:41:54.789267:	Training iteration: 76600, Loss: 0.009836820885539055
2018-10-19 12:46:14.621381:	Training iteration: 76800, Loss: 0.005622514057904482
2018-10-19 12:50:33.683580:	Training iteration: 77000, Loss: 0.005652497988194227
2018-10-19 12:54:53.268066:	Training iteration: 77200, Loss: 0.004843838978558779
2018-10-19 12:59:11.701403:	Training iteration: 77400, Loss: 0.004903518129140139
2018-10-19 13:03:30.588935:	Training iteration: 77600, Loss: 0.007128958590328693
2018-10-19 13:07:49.048773:	Training iteration: 77800, Loss: 0.006959025748074055
2018-10-19 13:12:08.687841:	Training iteration: 78000, Loss: 0.008348757401108742
2018-10-19 13:16:28.366918:	Training iteration: 78200, Loss: 0.006630077492445707
2018-10-19 13:20:47.646884:	Training iteration: 78400, Loss: 0.0052876947447657585
2018-10-19 13:25:06.832522:	Training iteration: 78600, Loss: 0.012272408232092857
2018-10-19 13:29:26.472115:	Training iteration: 78800, Loss: 0.004823203664273024
2018-10-19 13:33:45.537839:	Training iteration: 79000, Loss: 0.010130425915122032
2018-10-19 13:38:04.590644:	Training iteration: 79200, Loss: 0.005509756505489349
2018-10-19 13:42:22.805238:	Training iteration: 79400, Loss: 0.005245998501777649
2018-10-19 13:46:41.311693:	Training iteration: 79600, Loss: 0.0025894346181303263
2018-10-19 13:51:00.202790:	Training iteration: 79800, Loss: 0.00576967466622591
2018-10-19 13:55:19.890819:	Training iteration: 80000, Loss: 0.002776683308184147
2018-10-19 13:59:39.482491:	Training iteration: 80200, Loss: 0.004095647484064102
2018-10-19 14:03:58.047769:	Training iteration: 80400, Loss: 0.004897936247289181
2018-10-19 14:08:18.227379:	Training iteration: 80600, Loss: 0.004660159349441528
2018-10-19 14:12:36.921637:	Training iteration: 80800, Loss: 0.00621136836707592
2018-10-19 14:16:55.865841:	Training iteration: 81000, Loss: 0.005378125235438347
2018-10-19 14:21:14.969659:	Training iteration: 81200, Loss: 0.0029586004093289375
2018-10-19 14:25:34.232772:	Training iteration: 81400, Loss: 0.005544295534491539
2018-10-19 14:29:54.013402:	Training iteration: 81600, Loss: 0.004737402778118849
2018-10-19 14:34:12.800965:	Training iteration: 81800, Loss: 0.0031466386280953884
2018-10-19 14:38:31.454321:	Training iteration: 82000, Loss: 0.00594283314421773
2018-10-19 14:42:50.039905:	Training iteration: 82200, Loss: 0.00519770011305809
2018-10-19 14:47:08.567755:	Training iteration: 82400, Loss: 0.005395232699811459
2018-10-19 14:51:27.463150:	Training iteration: 82600, Loss: 0.003245942061766982
2018-10-19 14:55:45.943315:	Training iteration: 82800, Loss: 0.004038098268210888
2018-10-19 15:00:04.455886:	Training iteration: 83000, Loss: 0.008832154795527458
2018-10-19 15:04:23.313132:	Training iteration: 83200, Loss: 0.009761003777384758
2018-10-19 15:08:42.184327:	Training iteration: 83400, Loss: 0.004011679440736771
2018-10-19 15:13:02.754960:	Training iteration: 83600, Loss: 0.010735815390944481
2018-10-19 15:17:22.377463:	Training iteration: 83800, Loss: 0.007905583828687668
2018-10-19 15:21:41.379060:	Training iteration: 84000, Loss: 0.005408911965787411
2018-10-19 15:26:01.112627:	Training iteration: 84200, Loss: 0.009731130674481392
2018-10-19 15:30:20.640935:	Training iteration: 84400, Loss: 0.008091913536190987
2018-10-19 15:34:40.560753:	Training iteration: 84600, Loss: 0.005141085013747215
2018-10-19 15:38:59.953635:	Training iteration: 84800, Loss: 0.007922639138996601
2018-10-19 15:43:19.095087:	Training iteration: 85000, Loss: 0.007001228164881468
2018-10-19 15:47:37.128422:	Training iteration: 85200, Loss: 0.005311177112162113
2018-10-19 15:51:56.356160:	Training iteration: 85400, Loss: 0.010536319576203823
2018-10-19 15:56:15.891589:	Training iteration: 85600, Loss: 0.007310571614652872
2018-10-19 16:00:34.451528:	Training iteration: 85800, Loss: 0.006190160289406776
2018-10-19 16:04:53.203829:	Training iteration: 86000, Loss: 0.0033860900439321995
2018-10-19 16:09:10.968816:	Training iteration: 86200, Loss: 0.00438746577128768
2018-10-19 16:13:29.076725:	Training iteration: 86400, Loss: 0.00589017104357481
2018-10-19 16:17:48.038635:	Training iteration: 86600, Loss: 0.004437957890331745
2018-10-19 16:22:06.283155:	Training iteration: 86800, Loss: 0.003369067795574665
2018-10-19 16:26:25.530635:	Training iteration: 87000, Loss: 0.00356482807546854
2018-10-19 16:30:44.075589:	Training iteration: 87200, Loss: 0.00296035292558372
2018-10-19 16:35:03.042330:	Training iteration: 87400, Loss: 0.003837345167994499
2018-10-19 16:39:22.378793:	Training iteration: 87600, Loss: 0.0053069088608026505
2018-10-19 16:43:41.388139:	Training iteration: 87800, Loss: 0.0031253048218786716
2018-10-19 16:48:00.930895:	Training iteration: 88000, Loss: 0.0028307263273745775
2018-10-19 16:52:19.377195:	Training iteration: 88200, Loss: 0.006021091248840094
2018-10-19 16:56:38.421803:	Training iteration: 88400, Loss: 0.005928764119744301
2018-10-19 17:00:56.940269:	Training iteration: 88600, Loss: 0.003248093416914344
2018-10-19 17:05:16.258263:	Training iteration: 88800, Loss: 0.006783804856240749
2018-10-19 17:09:34.818656:	Training iteration: 89000, Loss: 0.004010714590549469
2018-10-19 17:13:53.348286:	Training iteration: 89200, Loss: 0.005236951634287834
2018-10-19 17:18:11.202785:	Training iteration: 89400, Loss: 0.0024045901373028755
2018-10-19 17:22:29.651495:	Training iteration: 89600, Loss: 0.005119134671986103
2018-10-19 17:26:48.459845:	Training iteration: 89800, Loss: 0.003974304534494877
2018-10-19 17:31:07.481202:	Training iteration: 90000, Loss: 0.004480907693505287
2018-10-19 17:35:25.449368:	Training iteration: 90200, Loss: 0.004005660302937031
2018-10-19 17:39:45.115303:	Training iteration: 90400, Loss: 0.006502870004624128
2018-10-19 17:44:04.159012:	Training iteration: 90600, Loss: 0.008957975544035435
2018-10-19 17:48:23.677350:	Training iteration: 90800, Loss: 0.007641295902431011
2018-10-19 17:52:42.504185:	Training iteration: 91000, Loss: 0.008580170571804047
2018-10-19 17:57:01.590335:	Training iteration: 91200, Loss: 0.0068842750042676926
2018-10-19 18:01:20.247130:	Training iteration: 91400, Loss: 0.009617052972316742
2018-10-19 18:05:39.867667:	Training iteration: 91600, Loss: 0.005375477019697428
2018-10-19 18:09:58.616230:	Training iteration: 91800, Loss: 0.007426468655467033
2018-10-19 18:14:17.886205:	Training iteration: 92000, Loss: 0.001938372734002769
2018-10-19 18:18:36.827085:	Training iteration: 92200, Loss: 0.006311520934104919
2018-10-19 18:22:54.571224:	Training iteration: 92400, Loss: 0.005143926478922367
2018-10-19 18:27:12.384003:	Training iteration: 92600, Loss: 0.0073230937123298645
2018-10-19 18:31:30.369188:	Training iteration: 92800, Loss: 0.004807654768228531
2018-10-19 18:35:48.299558:	Training iteration: 93000, Loss: 0.006656424142420292
2018-10-19 18:40:07.525516:	Training iteration: 93200, Loss: 0.004408870358020067
2018-10-19 18:44:26.889288:	Training iteration: 93400, Loss: 0.005655007436871529
2018-10-19 18:48:45.922463:	Training iteration: 93600, Loss: 0.009360585361719131
2018-10-19 18:53:06.000401:	Training iteration: 93800, Loss: 0.005983908660709858
2018-10-19 18:57:26.202949:	Training iteration: 94000, Loss: 0.008736275136470795
2018-10-19 19:01:46.362834:	Training iteration: 94200, Loss: 0.0030000321567058563
2018-10-19 19:06:05.585246:	Training iteration: 94400, Loss: 0.009469378739595413
2018-10-19 19:10:25.015132:	Training iteration: 94600, Loss: 0.006431362591683865
2018-10-19 19:14:44.396599:	Training iteration: 94800, Loss: 0.006995589938014746
2018-10-19 19:19:04.231729:	Training iteration: 95000, Loss: 0.00939591322094202
2018-10-19 19:23:23.681423:	Training iteration: 95200, Loss: 0.007617506664246321
2018-10-19 19:27:43.154765:	Training iteration: 95400, Loss: 0.005560897756367922
2018-10-19 19:32:03.317540:	Training iteration: 95600, Loss: 0.007668713107705116
2018-10-19 19:36:22.572072:	Training iteration: 95800, Loss: 0.0054076677188277245
2018-10-19 19:40:41.827086:	Training iteration: 96000, Loss: 0.01235989574342966
2018-10-19 19:45:00.116845:	Training iteration: 96200, Loss: 0.00472942553460598
2018-10-19 19:49:18.613890:	Training iteration: 96400, Loss: 0.00459798751398921
2018-10-19 19:53:36.795651:	Training iteration: 96600, Loss: 0.007868918590247631
2018-10-19 19:57:56.772868:	Training iteration: 96800, Loss: 0.009349152445793152
2018-10-19 20:02:15.685798:	Training iteration: 97000, Loss: 0.00495738722383976
2018-10-19 20:06:35.340958:	Training iteration: 97200, Loss: 0.007680280599743128
2018-10-19 20:10:54.801137:	Training iteration: 97400, Loss: 0.004938953090459108
2018-10-19 20:15:14.108080:	Training iteration: 97600, Loss: 0.002591395750641823
2018-10-19 20:19:33.603954:	Training iteration: 97800, Loss: 0.007028804160654545
2018-10-19 20:23:54.016260:	Training iteration: 98000, Loss: 0.006008959375321865
2018-10-19 20:28:12.911897:	Training iteration: 98200, Loss: 0.0034402073360979557
2018-10-19 20:32:32.186196:	Training iteration: 98400, Loss: 0.005283131264150143
2018-10-19 20:36:52.854618:	Training iteration: 98600, Loss: 0.005905890837311745
2018-10-19 20:41:13.095310:	Training iteration: 98800, Loss: 0.003953656647354364
2018-10-19 20:45:31.941304:	Training iteration: 99000, Loss: 0.004275505430996418
2018-10-19 20:49:50.118349:	Training iteration: 99200, Loss: 0.008508289232850075
2018-10-19 20:54:08.575056:	Training iteration: 99400, Loss: 0.003567492589354515
2018-10-19 20:58:26.375622:	Training iteration: 99600, Loss: 0.003992225043475628
2018-10-19 21:02:44.425443:	Training iteration: 99800, Loss: 0.0052703432738780975
2018-10-19 21:07:03.143583:	Training iteration: 100000, Loss: 0.0046125901862978935
2018-10-19 21:11:22.078797:	Training iteration: 100200, Loss: 0.005963698960840702
2018-10-19 21:15:41.621722:	Training iteration: 100400, Loss: 0.0050233760848641396
2018-10-19 21:20:02.087169:	Training iteration: 100600, Loss: 0.0031505743972957134
2018-10-19 21:24:20.902044:	Training iteration: 100800, Loss: 0.005302579142153263
2018-10-19 21:28:40.599527:	Training iteration: 101000, Loss: 0.004197335802018642
2018-10-19 21:32:59.131939:	Training iteration: 101200, Loss: 0.0033439004328101873
2018-10-19 21:37:18.847122:	Training iteration: 101400, Loss: 0.004281231202185154
2018-10-19 21:41:38.116780:	Training iteration: 101600, Loss: 0.00498640350997448
2018-10-19 21:45:57.545437:	Training iteration: 101800, Loss: 0.004037025384604931
2018-10-19 21:50:16.990888:	Training iteration: 102000, Loss: 0.004569792188704014
2018-10-19 21:54:36.544645:	Training iteration: 102200, Loss: 0.0032696882262825966
2018-10-19 21:58:55.021013:	Training iteration: 102400, Loss: 0.005132842808961868
2018-10-19 22:03:13.668372:	Training iteration: 102600, Loss: 0.006117235869169235
2018-10-19 22:07:32.198143:	Training iteration: 102800, Loss: 0.0038247157353907824
2018-10-19 22:11:50.556503:	Training iteration: 103000, Loss: 0.004320255480706692
2018-10-19 22:16:09.138309:	Training iteration: 103200, Loss: 0.008299654349684715
2018-10-19 22:20:27.191722:	Training iteration: 103400, Loss: 0.005738080479204655
2018-10-19 22:24:45.907682:	Training iteration: 103600, Loss: 0.006559666246175766
2018-10-19 22:29:05.140666:	Training iteration: 103800, Loss: 0.007562733255326748
2018-10-19 22:33:23.742091:	Training iteration: 104000, Loss: 0.003397454507648945
2018-10-19 22:37:43.045129:	Training iteration: 104200, Loss: 0.006543151568621397
2018-10-19 22:42:02.224009:	Training iteration: 104400, Loss: 0.00784939993172884
2018-10-19 22:46:21.204733:	Training iteration: 104600, Loss: 0.004986207000911236
2018-10-19 22:50:40.991243:	Training iteration: 104800, Loss: 0.004122253507375717
2018-10-19 22:55:00.801674:	Training iteration: 105000, Loss: 0.006043306086212397
2018-10-19 22:59:20.877147:	Training iteration: 105200, Loss: 0.0038642268627882004
2018-10-19 23:03:42.962249:	Training iteration: 105400, Loss: 0.005256488919258118
2018-10-19 23:08:04.176989:	Training iteration: 105600, Loss: 0.004977221135050058
2018-10-19 23:12:25.868304:	Training iteration: 105800, Loss: 0.006652533542364836
2018-10-19 23:16:47.348087:	Training iteration: 106000, Loss: 0.007629161234945059
2018-10-19 23:21:07.799896:	Training iteration: 106200, Loss: 0.00577242486178875
2018-10-19 23:25:28.267266:	Training iteration: 106400, Loss: 0.006961372215300798
2018-10-19 23:29:49.202272:	Training iteration: 106600, Loss: 0.00562039390206337
2018-10-19 23:34:10.053059:	Training iteration: 106800, Loss: 0.006780634634196758
2018-10-19 23:38:31.938115:	Training iteration: 107000, Loss: 0.008011458441615105
2018-10-19 23:42:54.191446:	Training iteration: 107200, Loss: 0.0069917598739266396
2018-10-19 23:47:18.033000:	Training iteration: 107400, Loss: 0.0027925746981054544
2018-10-19 23:51:40.824493:	Training iteration: 107600, Loss: 0.002497456269338727
2018-10-19 23:56:03.852574:	Training iteration: 107800, Loss: 0.004562150686979294
2018-10-20 00:00:27.165175:	Training iteration: 108000, Loss: 0.005173265002667904
2018-10-20 00:04:50.990390:	Training iteration: 108200, Loss: 0.0033802087418735027
2018-10-20 00:09:14.603677:	Training iteration: 108400, Loss: 0.0031046918593347073
2018-10-20 00:13:38.647145:	Training iteration: 108600, Loss: 0.004809213802218437
2018-10-20 00:18:01.101673:	Training iteration: 108800, Loss: 0.005679472349584103
2018-10-20 00:22:23.289728:	Training iteration: 109000, Loss: 0.00904365349560976
2018-10-20 00:26:44.647672:	Training iteration: 109200, Loss: 0.005274578928947449
2018-10-20 00:31:06.363258:	Training iteration: 109400, Loss: 0.005354651249945164
2018-10-20 00:35:27.761940:	Training iteration: 109600, Loss: 0.00686034606769681
2018-10-20 00:39:49.664246:	Training iteration: 109800, Loss: 0.007479076273739338
2018-10-20 00:44:12.922883:	Training iteration: 110000, Loss: 0.005666084587574005
2018-10-20 00:48:36.172527:	Training iteration: 110200, Loss: 0.00803321786224842
2018-10-20 00:53:00.736540:	Training iteration: 110400, Loss: 0.0059282779693603516
2018-10-20 00:57:24.854454:	Training iteration: 110600, Loss: 0.005447601433843374
2018-10-20 01:01:49.535857:	Training iteration: 110800, Loss: 0.004579317755997181
2018-10-20 01:06:13.096151:	Training iteration: 111000, Loss: 0.005640785675495863
2018-10-20 01:10:37.278720:	Training iteration: 111200, Loss: 0.008044842630624771
2018-10-20 01:15:01.499263:	Training iteration: 111400, Loss: 0.006370857823640108
2018-10-20 01:19:23.930982:	Training iteration: 111600, Loss: 0.005432420410215855
2018-10-20 01:23:45.472513:	Training iteration: 111800, Loss: 0.004026681184768677
2018-10-20 01:28:07.204598:	Training iteration: 112000, Loss: 0.006819607689976692
2018-10-20 01:32:29.657095:	Training iteration: 112200, Loss: 0.005279203876852989
2018-10-20 01:36:51.089477:	Training iteration: 112400, Loss: 0.004444570746272802
2018-10-20 01:41:12.952215:	Training iteration: 112600, Loss: 0.004523604176938534
2018-10-20 01:45:35.884750:	Training iteration: 112800, Loss: 0.005970879457890987
2018-10-20 01:49:59.482903:	Training iteration: 113000, Loss: 0.007360449526458979
2018-10-20 01:54:23.286404:	Training iteration: 113200, Loss: 0.00987353827804327
2018-10-20 01:58:46.937464:	Training iteration: 113400, Loss: 0.008192392066121101
2018-10-20 02:03:10.613545:	Training iteration: 113600, Loss: 0.006067443173378706
2018-10-20 02:07:34.546141:	Training iteration: 113800, Loss: 0.004320577252656221
2018-10-20 02:11:58.993413:	Training iteration: 114000, Loss: 0.008186358958482742
2018-10-20 02:16:22.611683:	Training iteration: 114200, Loss: 0.006505992729216814
2018-10-20 02:20:45.577789:	Training iteration: 114400, Loss: 0.0036262464709579945
2018-10-20 02:25:07.773530:	Training iteration: 114600, Loss: 0.0045284004881978035
2018-10-20 02:29:30.260425:	Training iteration: 114800, Loss: 0.00505113136023283
2018-10-20 02:33:51.669703:	Training iteration: 115000, Loss: 0.004310742951929569
2018-10-20 02:38:13.537672:	Training iteration: 115200, Loss: 0.0057753389701247215
2018-10-20 02:42:35.406134:	Training iteration: 115400, Loss: 0.006233024410903454
2018-10-20 02:46:57.743360:	Training iteration: 115600, Loss: 0.00465845363214612
2018-10-20 02:51:21.235029:	Training iteration: 115800, Loss: 0.0056711044162511826
2018-10-20 02:55:45.387328:	Training iteration: 116000, Loss: 0.0060331374406814575
2018-10-20 03:00:09.190748:	Training iteration: 116200, Loss: 0.005105173215270042
2018-10-20 03:04:32.801782:	Training iteration: 116400, Loss: 0.004666361957788467
2018-10-20 03:08:56.904569:	Training iteration: 116600, Loss: 0.009109143167734146
2018-10-20 03:13:20.774509:	Training iteration: 116800, Loss: 0.007330843713134527
2018-10-20 03:17:44.376767:	Training iteration: 117000, Loss: 0.0044006104581058025
2018-10-20 03:22:07.635911:	Training iteration: 117200, Loss: 0.005284626968204975
2018-10-20 03:26:30.126011:	Training iteration: 117400, Loss: 0.003636607201769948
2018-10-20 03:30:51.862342:	Training iteration: 117600, Loss: 0.0032137122470885515
2018-10-20 03:35:13.861370:	Training iteration: 117800, Loss: 0.008337205275893211
2018-10-20 03:39:35.448662:	Training iteration: 118000, Loss: 0.00784226506948471
2018-10-20 03:43:57.869644:	Training iteration: 118200, Loss: 0.004997679963707924
2018-10-20 03:48:20.387731:	Training iteration: 118400, Loss: 0.003417107742279768
2018-10-20 03:52:44.481605:	Training iteration: 118600, Loss: 0.005721064284443855
2018-10-20 03:57:08.142121:	Training iteration: 118800, Loss: 0.004219577647745609
2018-10-20 04:01:32.333500:	Training iteration: 119000, Loss: 0.004961427766829729
2018-10-20 04:05:56.489236:	Training iteration: 119200, Loss: 0.005428621079772711
2018-10-20 04:10:20.471003:	Training iteration: 119400, Loss: 0.005328238010406494
2018-10-20 04:14:44.734832:	Training iteration: 119600, Loss: 0.004732692614197731
2018-10-20 04:19:08.227372:	Training iteration: 119800, Loss: 0.006048194598406553
2018-10-20 04:22:27.327752:	Epoch 0 finished after 119952 iterations.
Validating
2018-10-20 04:22:27.434160:	Entering validation loop
2018-10-20 04:22:37.599847: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 701 of 1000
2018-10-20 04:22:41.669704: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 04:24:21.662500:	Validation iteration: 200, Loss: 0.003100094385445118
2018-10-20 04:26:02.265884:	Validation iteration: 400, Loss: 0.003906548954546452
2018-10-20 04:27:42.569607:	Validation iteration: 600, Loss: 0.007100021466612816
2018-10-20 04:29:22.664152:	Validation iteration: 800, Loss: 0.007833173498511314
2018-10-20 04:31:02.570895:	Validation iteration: 1000, Loss: 0.007329659070819616
2018-10-20 04:32:42.152390:	Validation iteration: 1200, Loss: 0.00506362272426486
2018-10-20 04:34:21.897300:	Validation iteration: 1400, Loss: 0.004621765110641718
2018-10-20 04:36:01.471277:	Validation iteration: 1600, Loss: 0.004744354169815779
2018-10-20 04:37:41.443867:	Validation iteration: 1800, Loss: 0.006059812381863594
2018-10-20 04:39:21.305772:	Validation iteration: 2000, Loss: 0.004504349082708359
2018-10-20 04:40:15.371152: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 726 of 1000
2018-10-20 04:40:19.006664: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 04:41:13.027874:	Validation iteration: 2200, Loss: 0.004400565754622221
2018-10-20 04:42:52.226047:	Validation iteration: 2400, Loss: 0.006666176021099091
2018-10-20 04:44:31.127293:	Validation iteration: 2600, Loss: 0.006835319567471743
2018-10-20 04:46:10.768380:	Validation iteration: 2800, Loss: 0.006022734101861715
2018-10-20 04:47:50.418465:	Validation iteration: 3000, Loss: 0.007920945063233376
2018-10-20 04:49:30.212287:	Validation iteration: 3200, Loss: 0.008058076724410057
2018-10-20 04:51:10.721979:	Validation iteration: 3400, Loss: 0.005064504686743021
2018-10-20 04:52:51.227343:	Validation iteration: 3600, Loss: 0.004256157204508781
2018-10-20 04:54:32.391877:	Validation iteration: 3800, Loss: 0.004688363056629896
2018-10-20 04:56:12.953749:	Validation iteration: 4000, Loss: 0.005917457863688469
2018-10-20 04:57:51.319604: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 715 of 1000
2018-10-20 04:57:55.034183: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 04:58:06.493530:	Validation iteration: 4200, Loss: 0.004761645570397377
2018-10-20 04:59:45.421016:	Validation iteration: 4400, Loss: 0.005695630796253681
2018-10-20 05:01:26.019314:	Validation iteration: 4600, Loss: 0.007503086235374212
2018-10-20 05:03:06.729126:	Validation iteration: 4800, Loss: 0.005001913756132126
2018-10-20 05:04:47.164568:	Validation iteration: 5000, Loss: 0.005935429595410824
2018-10-20 05:06:27.399425:	Validation iteration: 5200, Loss: 0.006961654871702194
2018-10-20 05:08:08.550998:	Validation iteration: 5400, Loss: 0.007871471345424652
2018-10-20 05:09:49.365312:	Validation iteration: 5600, Loss: 0.0065431720577180386
2018-10-20 05:11:30.169934:	Validation iteration: 5800, Loss: 0.004296026658266783
2018-10-20 05:13:11.627474:	Validation iteration: 6000, Loss: 0.006450322922319174
2018-10-20 05:14:52.676949:	Validation iteration: 6200, Loss: 0.0058209942653775215
2018-10-20 05:15:35.082690: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 717 of 1000
2018-10-20 05:15:38.757580: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 05:16:45.354783:	Validation iteration: 6400, Loss: 0.005395443644374609
2018-10-20 05:18:25.920155:	Validation iteration: 6600, Loss: 0.007869182154536247
2018-10-20 05:20:06.361592:	Validation iteration: 6800, Loss: 0.006565015763044357
2018-10-20 05:21:47.658585:	Validation iteration: 7000, Loss: 0.005228486843407154
2018-10-20 05:23:28.639021:	Validation iteration: 7200, Loss: 0.00565520953387022
2018-10-20 05:25:08.066559:	Validation iteration: 7400, Loss: 0.007552187889814377
2018-10-20 05:26:48.204282:	Validation iteration: 7600, Loss: 0.006101544946432114
2018-10-20 05:28:28.071955:	Validation iteration: 7800, Loss: 0.006952233146876097
2018-10-20 05:30:07.743786:	Validation iteration: 8000, Loss: 0.006811687722802162
2018-10-20 05:31:48.209757:	Validation iteration: 8200, Loss: 0.006009993143379688
2018-10-20 05:33:14.119270: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 666 of 1000
2018-10-20 05:33:18.667235: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 05:33:41.366345:	Validation iteration: 8400, Loss: 0.006703068036586046
2018-10-20 05:35:19.828538:	Validation iteration: 8600, Loss: 0.006158964708447456
2018-10-20 05:36:59.042710:	Validation iteration: 8800, Loss: 0.007472551427781582
2018-10-20 05:38:37.972499:	Validation iteration: 9000, Loss: 0.003295530565083027
2018-10-20 05:40:17.388538:	Validation iteration: 9200, Loss: 0.0028780540451407433
2018-10-20 05:41:57.045662:	Validation iteration: 9400, Loss: 0.005953381769359112
2018-10-20 05:43:36.691651:	Validation iteration: 9600, Loss: 0.005863356404006481
2018-10-20 05:45:16.350894:	Validation iteration: 9800, Loss: 0.003740254556760192
2018-10-20 05:46:55.182230:	Validation iteration: 10000, Loss: 0.005711526144295931
2018-10-20 05:48:35.028868:	Validation iteration: 10200, Loss: 0.004177778959274292
2018-10-20 05:50:15.040283:	Validation iteration: 10400, Loss: 0.004907259717583656
2018-10-20 05:51:55.155611:	Validation iteration: 10600, Loss: 0.007514151278883219
2018-10-20 05:53:35.385119:	Validation iteration: 10800, Loss: 0.0034322263672947884
2018-10-20 05:55:15.643339:	Validation iteration: 11000, Loss: 0.004539027810096741
2018-10-20 05:56:56.385114:	Validation iteration: 11200, Loss: 0.0046263509429991245
Validation check mean loss: 0.006084309635510317
Validation loss has improved!
New best validation cost!
Checkpoint
2018-10-20 05:57:31.465473: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 795 of 1000
2018-10-20 05:57:34.023526: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 05:58:37.189262:	Training iteration: 120000, Loss: 0.005015394650399685
2018-10-20 06:02:58.803696:	Training iteration: 120200, Loss: 0.0033814639318734407
2018-10-20 06:07:21.496073:	Training iteration: 120400, Loss: 0.0035728211514651775
2018-10-20 06:11:44.466373:	Training iteration: 120600, Loss: 0.003978420048952103
2018-10-20 06:16:08.509591:	Training iteration: 120800, Loss: 0.005013428628444672
2018-10-20 06:20:32.321435:	Training iteration: 121000, Loss: 0.0025309016928076744
2018-10-20 06:24:56.109578:	Training iteration: 121200, Loss: 0.0036869309842586517
2018-10-20 06:29:20.805789:	Training iteration: 121400, Loss: 0.0030168090015649796
2018-10-20 06:33:43.648886:	Training iteration: 121600, Loss: 0.0033504548482596874
2018-10-20 06:38:05.631777:	Training iteration: 121800, Loss: 0.003759422805160284
2018-10-20 06:42:27.914505:	Training iteration: 122000, Loss: 0.004785269498825073
2018-10-20 06:46:49.657937:	Training iteration: 122200, Loss: 0.003139644395560026
2018-10-20 06:51:11.530732:	Training iteration: 122400, Loss: 0.0037900072056800127
2018-10-20 06:55:33.514392:	Training iteration: 122600, Loss: 0.004270763136446476
2018-10-20 06:59:56.716806:	Training iteration: 122800, Loss: 0.005054950714111328
2018-10-20 07:04:19.720126:	Training iteration: 123000, Loss: 0.004703924059867859
2018-10-20 07:08:43.755033:	Training iteration: 123200, Loss: 0.004580679349601269
2018-10-20 07:13:06.876836:	Training iteration: 123400, Loss: 0.003274742979556322
2018-10-20 07:17:30.649996:	Training iteration: 123600, Loss: 0.0032333508133888245
2018-10-20 07:21:54.123343:	Training iteration: 123800, Loss: 0.005298630800098181
2018-10-20 07:26:17.924459:	Training iteration: 124000, Loss: 0.004320817068219185
2018-10-20 07:30:41.920518:	Training iteration: 124200, Loss: 0.0035534012131392956
2018-10-20 07:35:04.598340:	Training iteration: 124400, Loss: 0.0037030389066785574
2018-10-20 07:39:26.750429:	Training iteration: 124600, Loss: 0.00544330570846796
2018-10-20 07:43:49.428822:	Training iteration: 124800, Loss: 0.004662211984395981
2018-10-20 07:48:10.393705:	Training iteration: 125000, Loss: 0.003225467400625348
2018-10-20 07:52:32.305540:	Training iteration: 125200, Loss: 0.003247968852519989
2018-10-20 07:56:54.308168:	Training iteration: 125400, Loss: 0.0025060665793716908
2018-10-20 08:01:17.598529:	Training iteration: 125600, Loss: 0.004269945435225964
2018-10-20 08:05:42.209132:	Training iteration: 125800, Loss: 0.005566832143813372
2018-10-20 08:10:06.232484:	Training iteration: 126000, Loss: 0.002847101539373398
2018-10-20 08:14:30.651578:	Training iteration: 126200, Loss: 0.00361111992970109
2018-10-20 08:18:54.886706:	Training iteration: 126400, Loss: 0.005304860416799784
2018-10-20 08:23:19.500064:	Training iteration: 126600, Loss: 0.0034295092336833477
2018-10-20 08:27:43.890723:	Training iteration: 126800, Loss: 0.004701223690062761
2018-10-20 08:32:08.425053:	Training iteration: 127000, Loss: 0.003201592480763793
2018-10-20 08:36:32.369622:	Training iteration: 127200, Loss: 0.002975574927404523
2018-10-20 08:40:55.118375:	Training iteration: 127400, Loss: 0.004293856676667929
2018-10-20 08:45:17.888610:	Training iteration: 127600, Loss: 0.004203385673463345
2018-10-20 08:49:40.808919:	Training iteration: 127800, Loss: 0.0029185391031205654
2018-10-20 08:54:02.516983:	Training iteration: 128000, Loss: 0.00406290777027607
2018-10-20 08:58:24.850501:	Training iteration: 128200, Loss: 0.0051976339891552925
2018-10-20 09:02:48.898699:	Training iteration: 128400, Loss: 0.004756545647978783
2018-10-20 09:07:12.952263:	Training iteration: 128600, Loss: 0.005482031963765621
2018-10-20 09:11:36.662347:	Training iteration: 128800, Loss: 0.003383589442819357
2018-10-20 09:16:01.049293:	Training iteration: 129000, Loss: 0.0039849537424743176
2018-10-20 09:20:24.452630:	Training iteration: 129200, Loss: 0.004129663109779358
2018-10-20 09:24:48.076264:	Training iteration: 129400, Loss: 0.004871027544140816
2018-10-20 09:29:12.187538:	Training iteration: 129600, Loss: 0.006586454808712006
2018-10-20 09:33:36.129266:	Training iteration: 129800, Loss: 0.002983689308166504
2018-10-20 09:37:58.750825:	Training iteration: 130000, Loss: 0.00410092156380415
2018-10-20 09:42:21.936179:	Training iteration: 130200, Loss: 0.005161594599485397
2018-10-20 09:46:44.729772:	Training iteration: 130400, Loss: 0.0032598848920315504
2018-10-20 09:51:07.463794:	Training iteration: 130600, Loss: 0.006692467723041773
2018-10-20 09:55:29.506407:	Training iteration: 130800, Loss: 0.00470787612721324
2018-10-20 09:59:52.226715:	Training iteration: 131000, Loss: 0.003350999439135194
2018-10-20 10:04:14.946076:	Training iteration: 131200, Loss: 0.005223097279667854
2018-10-20 10:08:38.883316:	Training iteration: 131400, Loss: 0.005110264755785465
2018-10-20 10:13:02.898170:	Training iteration: 131600, Loss: 0.005604085978120565
2018-10-20 10:17:26.646740:	Training iteration: 131800, Loss: 0.0053350962698459625
2018-10-20 10:21:50.604133:	Training iteration: 132000, Loss: 0.005795846693217754
2018-10-20 10:26:15.093000:	Training iteration: 132200, Loss: 0.0052419863641262054
2018-10-20 10:29:39.271298: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 763 of 1000
2018-10-20 10:29:42.211512: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 10:30:51.498481:	Training iteration: 132400, Loss: 0.0037813340313732624
2018-10-20 10:35:14.952279:	Training iteration: 132600, Loss: 0.004308875650167465
2018-10-20 10:39:38.309314:	Training iteration: 132800, Loss: 0.003152935765683651
2018-10-20 10:44:01.205084:	Training iteration: 133000, Loss: 0.00338747794739902
2018-10-20 10:48:23.229265:	Training iteration: 133200, Loss: 0.005723339505493641
2018-10-20 10:52:44.957268:	Training iteration: 133400, Loss: 0.005198703147470951
2018-10-20 10:57:06.583941:	Training iteration: 133600, Loss: 0.004378051031380892
2018-10-20 11:01:30.006160:	Training iteration: 133800, Loss: 0.005246419925242662
2018-10-20 11:05:53.418041:	Training iteration: 134000, Loss: 0.00440753111615777
2018-10-20 11:10:17.829051:	Training iteration: 134200, Loss: 0.006512670777738094
2018-10-20 11:14:41.852537:	Training iteration: 134400, Loss: 0.005796328186988831
2018-10-20 11:19:07.082574:	Training iteration: 134600, Loss: 0.003029708517715335
2018-10-20 11:23:30.848504:	Training iteration: 134800, Loss: 0.008757539093494415
2018-10-20 11:27:55.239256:	Training iteration: 135000, Loss: 0.005974967498332262
2018-10-20 11:32:20.275307:	Training iteration: 135200, Loss: 0.005001196637749672
2018-10-20 11:36:45.501944:	Training iteration: 135400, Loss: 0.004675664007663727
2018-10-20 11:41:09.583597:	Training iteration: 135600, Loss: 0.006636162754148245
2018-10-20 11:45:32.692872:	Training iteration: 135800, Loss: 0.0032463124953210354
2018-10-20 11:49:55.610821:	Training iteration: 136000, Loss: 0.005530764348804951
2018-10-20 11:54:19.806433:	Training iteration: 136200, Loss: 0.005585097707808018
2018-10-20 11:58:42.331884:	Training iteration: 136400, Loss: 0.00838519912213087
2018-10-20 12:03:05.350445:	Training iteration: 136600, Loss: 0.004367277957499027
2018-10-20 12:07:29.102320:	Training iteration: 136800, Loss: 0.003458486869931221
2018-10-20 12:11:53.676013:	Training iteration: 137000, Loss: 0.004977466072887182
2018-10-20 12:16:17.584862:	Training iteration: 137200, Loss: 0.005962788593024015
2018-10-20 12:20:42.077213:	Training iteration: 137400, Loss: 0.006948371883481741
2018-10-20 12:25:06.055845:	Training iteration: 137600, Loss: 0.0040906695649027824
2018-10-20 12:29:31.041236:	Training iteration: 137800, Loss: 0.006023967172950506
2018-10-20 12:33:55.873927:	Training iteration: 138000, Loss: 0.004755398258566856
2018-10-20 12:38:18.990578:	Training iteration: 138200, Loss: 0.006038617808371782
2018-10-20 12:42:42.216155:	Training iteration: 138400, Loss: 0.004792281426489353
2018-10-20 12:47:04.471854:	Training iteration: 138600, Loss: 0.0038695549592375755
2018-10-20 12:51:26.680360:	Training iteration: 138800, Loss: 0.00527579989284277
2018-10-20 12:55:48.857342:	Training iteration: 139000, Loss: 0.004312709905207157
2018-10-20 13:00:11.626195:	Training iteration: 139200, Loss: 0.003840870223939419
2018-10-20 13:04:35.109097:	Training iteration: 139400, Loss: 0.004918464459478855
2018-10-20 13:08:59.090142:	Training iteration: 139600, Loss: 0.005906569305807352
2018-10-20 13:13:23.621614:	Training iteration: 139800, Loss: 0.004431884735822678
2018-10-20 13:17:47.707601:	Training iteration: 140000, Loss: 0.005270814523100853
2018-10-20 13:22:11.649537:	Training iteration: 140200, Loss: 0.0030444636940956116
2018-10-20 13:26:36.537779:	Training iteration: 140400, Loss: 0.0038200528360903263
2018-10-20 13:31:01.761290:	Training iteration: 140600, Loss: 0.005043358076363802
2018-10-20 13:35:26.301435:	Training iteration: 140800, Loss: 0.0055570234544575214
2018-10-20 13:39:50.866208:	Training iteration: 141000, Loss: 0.005005554296076298
2018-10-20 13:44:13.616875:	Training iteration: 141200, Loss: 0.004551642574369907
2018-10-20 13:48:36.593027:	Training iteration: 141400, Loss: 0.005030519794672728
2018-10-20 13:52:59.023591:	Training iteration: 141600, Loss: 0.0038183245342224836
2018-10-20 13:57:21.443568:	Training iteration: 141800, Loss: 0.003797720419242978
2018-10-20 14:01:45.780465:	Training iteration: 142000, Loss: 0.007170877419412136
2018-10-20 14:06:17.700184:	Training iteration: 142200, Loss: 0.005035856273025274
2018-10-20 14:10:54.692051:	Training iteration: 142400, Loss: 0.006745859980583191
2018-10-20 14:15:55.665656:	Training iteration: 142600, Loss: 0.004343600012362003
2018-10-20 14:21:11.538538:	Training iteration: 142800, Loss: 0.00847693532705307
2018-10-20 14:26:35.481153:	Training iteration: 143000, Loss: 0.0027998813893646
2018-10-20 14:32:02.268519:	Training iteration: 143200, Loss: 0.0035161515697836876
2018-10-20 14:37:13.103810:	Training iteration: 143400, Loss: 0.0043287258595228195
2018-10-20 14:41:47.804823:	Training iteration: 143600, Loss: 0.006688211113214493
2018-10-20 14:46:16.901150:	Training iteration: 143800, Loss: 0.0047434251755476
2018-10-20 14:50:41.981280:	Training iteration: 144000, Loss: 0.005778555758297443
2018-10-20 14:55:07.953933:	Training iteration: 144200, Loss: 0.0036059259437024593
2018-10-20 14:59:32.487058:	Training iteration: 144400, Loss: 0.006244649179279804
2018-10-20 15:03:56.831470:	Training iteration: 144600, Loss: 0.005903592333197594
2018-10-20 15:08:41.482393:	Training iteration: 144800, Loss: 0.0033923129085451365
2018-10-20 15:09:14.353818: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 712 of 1000
2018-10-20 15:09:18.076355: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 15:14:04.240863:	Training iteration: 145000, Loss: 0.006698560900986195
2018-10-20 15:19:34.869657:	Training iteration: 145200, Loss: 0.004620260559022427
2018-10-20 15:25:13.940599:	Training iteration: 145400, Loss: 0.004974943120032549
2018-10-20 15:30:56.003780:	Training iteration: 145600, Loss: 0.004564191214740276
2018-10-20 15:36:35.303944:	Training iteration: 145800, Loss: 0.004785562865436077
2018-10-20 15:42:06.593749:	Training iteration: 146000, Loss: 0.005489827133715153
2018-10-20 15:47:33.861222:	Training iteration: 146200, Loss: 0.004840097855776548
2018-10-20 15:53:00.568902:	Training iteration: 146400, Loss: 0.005403788294643164
2018-10-20 15:58:25.813530:	Training iteration: 146600, Loss: 0.0047630080953240395
2018-10-20 16:03:53.365136:	Training iteration: 146800, Loss: 0.003962938208132982
2018-10-20 16:09:31.520884:	Training iteration: 147000, Loss: 0.009926547296345234
2018-10-20 16:15:14.620664:	Training iteration: 147200, Loss: 0.005389497149735689
2018-10-20 16:21:01.404133:	Training iteration: 147400, Loss: 0.005928209982812405
2018-10-20 16:26:48.308057:	Training iteration: 147600, Loss: 0.005172629840672016
2018-10-20 16:32:37.374886:	Training iteration: 147800, Loss: 0.003989859484136105
2018-10-20 16:38:16.356032:	Training iteration: 148000, Loss: 0.00694693811237812
2018-10-20 16:43:47.720190:	Training iteration: 148200, Loss: 0.006017504725605249
2018-10-20 16:49:16.219459:	Training iteration: 148400, Loss: 0.005113459657877684
2018-10-20 16:54:41.073549:	Training iteration: 148600, Loss: 0.0056601702235639095
2018-10-20 17:00:07.595002:	Training iteration: 148800, Loss: 0.005478082224726677
2018-10-20 17:05:32.040887:	Training iteration: 149000, Loss: 0.005423142574727535
2018-10-20 17:10:57.619761:	Training iteration: 149200, Loss: 0.007585282437503338
2018-10-20 17:16:20.396186:	Training iteration: 149400, Loss: 0.006496456451714039
2018-10-20 17:21:46.141216:	Training iteration: 149600, Loss: 0.005414013750851154
2018-10-20 17:27:11.867173:	Training iteration: 149800, Loss: 0.003597999457269907
2018-10-20 17:32:35.858114:	Training iteration: 150000, Loss: 0.0035199085250496864
2018-10-20 17:38:02.363462:	Training iteration: 150200, Loss: 0.00447084428742528
2018-10-20 17:43:26.561741:	Training iteration: 150400, Loss: 0.006064532324671745
2018-10-20 17:48:51.928886:	Training iteration: 150600, Loss: 0.004943171516060829
2018-10-20 17:54:16.646148:	Training iteration: 150800, Loss: 0.004079805687069893
2018-10-20 17:59:40.397626:	Training iteration: 151000, Loss: 0.006957225501537323
2018-10-20 18:05:03.564651:	Training iteration: 151200, Loss: 0.004078879486769438
2018-10-20 18:10:34.457142:	Training iteration: 151400, Loss: 0.00381056172773242
2018-10-20 18:16:10.917729:	Training iteration: 151600, Loss: 0.005884738638997078
2018-10-20 18:21:52.904437:	Training iteration: 151800, Loss: 0.005767211318016052
2018-10-20 18:27:36.407979:	Training iteration: 152000, Loss: 0.005564236547797918
2018-10-20 18:33:24.748645:	Training iteration: 152200, Loss: 0.003494352102279663
2018-10-20 18:39:09.202226:	Training iteration: 152400, Loss: 0.004295165650546551
2018-10-20 18:44:42.237367:	Training iteration: 152600, Loss: 0.005063128191977739
2018-10-20 18:50:12.930374:	Training iteration: 152800, Loss: 0.003161870874464512
2018-10-20 18:55:38.948291:	Training iteration: 153000, Loss: 0.0058337729424238205
2018-10-20 19:01:05.693426:	Training iteration: 153200, Loss: 0.007522906642407179
2018-10-20 19:06:29.029748:	Training iteration: 153400, Loss: 0.003989004530012608
2018-10-20 19:11:34.982783:	Training iteration: 153600, Loss: 0.007468069903552532
2018-10-20 19:16:09.756095:	Training iteration: 153800, Loss: 0.004439588636159897
2018-10-20 19:20:40.379566:	Training iteration: 154000, Loss: 0.006242740899324417
2018-10-20 19:25:07.760885:	Training iteration: 154200, Loss: 0.005560329183936119
2018-10-20 19:29:33.954345:	Training iteration: 154400, Loss: 0.006246652454137802
2018-10-20 19:33:58.675262:	Training iteration: 154600, Loss: 0.00507776066660881
2018-10-20 19:38:23.288301:	Training iteration: 154800, Loss: 0.0034944056533277035
2018-10-20 19:42:47.020796:	Training iteration: 155000, Loss: 0.0061704786494374275
2018-10-20 19:47:11.720424:	Training iteration: 155200, Loss: 0.003924248740077019
2018-10-20 19:51:34.989283:	Training iteration: 155400, Loss: 0.004187599755823612
2018-10-20 19:55:57.514172:	Training iteration: 155600, Loss: 0.004809076432138681
2018-10-20 20:00:18.379211:	Training iteration: 155800, Loss: 0.00703047588467598
2018-10-20 20:04:39.106091:	Training iteration: 156000, Loss: 0.00614773016422987
2018-10-20 20:09:01.470788:	Training iteration: 156200, Loss: 0.006120418664067984
2018-10-20 20:13:37.480330:	Training iteration: 156400, Loss: 0.003638060763478279
2018-10-20 20:18:39.164253:	Training iteration: 156600, Loss: 0.0037287832237780094
2018-10-20 20:23:54.143256:	Training iteration: 156800, Loss: 0.003984251990914345
2018-10-20 20:29:23.068681:	Training iteration: 157000, Loss: 0.00700278440490365
2018-10-20 20:34:58.965598:	Training iteration: 157200, Loss: 0.005962139926850796
2018-10-20 20:37:15.354535: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 678 of 1000
2018-10-20 20:37:19.549162: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 20:40:47.298944:	Training iteration: 157400, Loss: 0.004763099364936352
2018-10-20 20:46:17.138794:	Training iteration: 157600, Loss: 0.0062897587195038795
2018-10-20 20:51:07.130797:	Training iteration: 157800, Loss: 0.003815789707005024
2018-10-20 20:55:39.559124:	Training iteration: 158000, Loss: 0.003957405686378479
2018-10-20 21:00:06.416309:	Training iteration: 158200, Loss: 0.003166494658216834
2018-10-20 21:04:31.002877:	Training iteration: 158400, Loss: 0.004072178620845079
2018-10-20 21:08:54.898440:	Training iteration: 158600, Loss: 0.00399342505261302
2018-10-20 21:13:22.978590:	Training iteration: 158800, Loss: 0.0030194816645234823
2018-10-20 21:17:52.980831:	Training iteration: 159000, Loss: 0.006192108616232872
2018-10-20 21:22:24.265529:	Training iteration: 159200, Loss: 0.003917376510798931
2018-10-20 21:26:58.325119:	Training iteration: 159400, Loss: 0.006065689958631992
2018-10-20 21:31:32.971135:	Training iteration: 159600, Loss: 0.003074536332860589
2018-10-20 21:36:07.965230:	Training iteration: 159800, Loss: 0.007359039504081011
2018-10-20 21:40:40.857171:	Training iteration: 160000, Loss: 0.0030555054545402527
2018-10-20 21:45:13.704575:	Training iteration: 160200, Loss: 0.0032206871546804905
2018-10-20 21:49:46.277832:	Training iteration: 160400, Loss: 0.0035944373812526464
2018-10-20 21:54:20.189834:	Training iteration: 160600, Loss: 0.003780648810788989
2018-10-20 21:58:51.950952:	Training iteration: 160800, Loss: 0.004585199989378452
2018-10-20 22:03:23.561294:	Training iteration: 161000, Loss: 0.005291673354804516
2018-10-20 22:07:57.064421:	Training iteration: 161200, Loss: 0.007192200981080532
2018-10-20 22:12:30.050602:	Training iteration: 161400, Loss: 0.003569297958165407
2018-10-20 22:17:04.630118:	Training iteration: 161600, Loss: 0.0026041597593575716
2018-10-20 22:21:38.500238:	Training iteration: 161800, Loss: 0.004350544884800911
2018-10-20 22:26:13.146842:	Training iteration: 162000, Loss: 0.004724418744444847
2018-10-20 22:30:46.587395:	Training iteration: 162200, Loss: 0.003897419199347496
2018-10-20 22:35:19.502974:	Training iteration: 162400, Loss: 0.00744490884244442
2018-10-20 22:39:51.059193:	Training iteration: 162600, Loss: 0.00449582701548934
2018-10-20 22:44:22.792874:	Training iteration: 162800, Loss: 0.005661348812282085
2018-10-20 22:48:54.362842:	Training iteration: 163000, Loss: 0.0030309867579489946
2018-10-20 22:53:27.429596:	Training iteration: 163200, Loss: 0.004300553351640701
2018-10-20 22:58:00.938870:	Training iteration: 163400, Loss: 0.0035039368085563183
2018-10-20 23:02:32.801881:	Training iteration: 163600, Loss: 0.0023773889988660812
2018-10-20 23:07:07.522863:	Training iteration: 163800, Loss: 0.0035568010061979294
2018-10-20 23:11:41.497158:	Training iteration: 164000, Loss: 0.003484067041426897
2018-10-20 23:16:14.514733:	Training iteration: 164200, Loss: 0.005739491432905197
2018-10-20 23:20:49.002806:	Training iteration: 164400, Loss: 0.0035804230719804764
2018-10-20 23:25:23.074243:	Training iteration: 164600, Loss: 0.003328346647322178
2018-10-20 23:29:57.332690:	Training iteration: 164800, Loss: 0.0034861541353166103
2018-10-20 23:34:29.884158:	Training iteration: 165000, Loss: 0.0025169048458337784
2018-10-20 23:39:02.240224:	Training iteration: 165200, Loss: 0.003619419177994132
2018-10-20 23:43:33.523236:	Training iteration: 165400, Loss: 0.002325918525457382
2018-10-20 23:48:06.942016:	Training iteration: 165600, Loss: 0.0038422225043177605
2018-10-20 23:52:38.296540:	Training iteration: 165800, Loss: 0.0033289603888988495
2018-10-20 23:57:11.336654:	Training iteration: 166000, Loss: 0.0024133222177624702
2018-10-21 00:01:43.857223:	Training iteration: 166200, Loss: 0.00310713704675436
2018-10-21 00:06:17.204563:	Training iteration: 166400, Loss: 0.004506118595600128
2018-10-21 00:10:50.414262:	Training iteration: 166600, Loss: 0.003585841041058302
2018-10-21 00:15:24.140309:	Training iteration: 166800, Loss: 0.004876320715993643
2018-10-21 00:19:57.038909:	Training iteration: 167000, Loss: 0.003478660713881254
2018-10-21 00:24:29.305105:	Training iteration: 167200, Loss: 0.0035445187240839005
2018-10-21 00:29:01.428507:	Training iteration: 167400, Loss: 0.005055810324847698
2018-10-21 00:33:34.738462:	Training iteration: 167600, Loss: 0.0056060440838336945
2018-10-21 00:38:05.533055:	Training iteration: 167800, Loss: 0.004518944304436445
2018-10-21 00:42:38.283795:	Training iteration: 168000, Loss: 0.0027063689194619656
2018-10-21 00:47:10.465516:	Training iteration: 168200, Loss: 0.004251575097441673
2018-10-21 00:51:45.347843:	Training iteration: 168400, Loss: 0.0037422592286020517
2018-10-21 00:56:17.931740:	Training iteration: 168600, Loss: 0.004181405529379845
2018-10-21 01:00:52.125741:	Training iteration: 168800, Loss: 0.005226629786193371
2018-10-21 01:05:27.696683:	Training iteration: 169000, Loss: 0.005168519914150238
2018-10-21 01:10:01.405714:	Training iteration: 169200, Loss: 0.0056655872613191605
2018-10-21 01:14:34.574839:	Training iteration: 169400, Loss: 0.004158037714660168
2018-10-21 01:19:08.990780:	Training iteration: 169600, Loss: 0.006783634424209595
2018-10-21 01:23:40.366051:	Training iteration: 169800, Loss: 0.004479621071368456
2018-10-21 01:28:13.110754:	Training iteration: 170000, Loss: 0.005104458425194025
2018-10-21 01:32:43.735544:	Training iteration: 170200, Loss: 0.003945333417505026
