INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "149"
Experiment ID: 149
Preparing dataset
Dataset ready
2018-10-21 12:14:35.671933: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-10-21 12:14:35.942835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-21 12:14:35.944391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:26:00.0
totalMemory: 10.91GiB freeMemory: 10.76GiB
2018-10-21 12:14:35.944412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Running initialisation test
Starting testing
2018-10-21 12:14:48.391711:	Entering test loop
2018-10-21 12:14:58.922376: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 469 of 1000
2018-10-21 12:15:08.019252: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 12:15:16.731448:	Testing iteration: 0, Loss: 0.0067664943635463715
2018-10-21 12:17:10.747443:	Testing iteration: 200, Loss: 0.0048614260740578175
2018-10-21 12:19:05.567822:	Testing iteration: 400, Loss: 0.005713187158107758
2018-10-21 12:21:05.916506:	Testing iteration: 600, Loss: 0.007568668108433485
2018-10-21 12:23:08.580985:	Testing iteration: 800, Loss: 0.003867474151775241
2018-10-21 12:25:15.186852:	Testing iteration: 1000, Loss: 0.004866656381636858
2018-10-21 12:26:14.237378: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 586 of 1000
2018-10-21 12:26:20.700023: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 12:27:45.771777:	Testing iteration: 1200, Loss: 0.004695537965744734
2018-10-21 12:30:00.034911:	Testing iteration: 1400, Loss: 0.006452595815062523
2018-10-21 12:32:15.929225:	Testing iteration: 1600, Loss: 0.005985687952488661
2018-10-21 12:34:37.727806:	Testing iteration: 1800, Loss: 0.005522201303392649
2018-10-21 12:37:04.175368:	Testing iteration: 2000, Loss: 0.0063097551465034485
2018-10-21 12:39:04.275037: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 603 of 1000
2018-10-21 12:39:10.462029: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 12:39:46.463969:	Testing iteration: 2200, Loss: 0.0067658028565347195
2018-10-21 12:42:15.951047:	Testing iteration: 2400, Loss: 0.009105042554438114
2018-10-21 12:44:47.889483:	Testing iteration: 2600, Loss: 0.010166799649596214
2018-10-21 12:47:23.332191:	Testing iteration: 2800, Loss: 0.007524014916270971
2018-10-21 12:50:01.176643:	Testing iteration: 3000, Loss: 0.007472439203411341
2018-10-21 12:52:44.563422:	Testing iteration: 3200, Loss: 0.004548377823084593
2018-10-21 12:53:17.577103: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 606 of 1000
2018-10-21 12:53:23.446316: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 12:55:44.357811:	Testing iteration: 3400, Loss: 0.010156634263694286
2018-10-21 12:58:34.138067:	Testing iteration: 3600, Loss: 0.0041009485721588135
2018-10-21 13:01:38.682377:	Testing iteration: 3800, Loss: 0.0069780475459992886
2018-10-21 13:05:00.094384:	Testing iteration: 4000, Loss: 0.005777793470770121
2018-10-21 13:08:22.914739:	Testing iteration: 4200, Loss: 0.0068930648267269135
2018-10-21 13:10:21.267283: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 618 of 1000
2018-10-21 13:10:27.615506: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 13:12:02.825152:	Testing iteration: 4400, Loss: 0.006802053656429052
2018-10-21 13:15:14.416328:	Testing iteration: 4600, Loss: 0.009296963922679424
2018-10-21 13:18:22.872658:	Testing iteration: 4800, Loss: 0.012321126647293568
2018-10-21 13:21:32.716080:	Testing iteration: 5000, Loss: 0.006777389440685511
2018-10-21 13:24:45.477632:	Testing iteration: 5200, Loss: 0.009041144512593746
2018-10-21 13:28:02.278496:	Testing iteration: 5400, Loss: 0.007462214212864637
2018-10-21 13:31:19.869434:	Testing iteration: 5600, Loss: 0.0066007934510707855
2018-10-21 13:34:41.569262:	Testing iteration: 5800, Loss: 0.008079192601144314
2018-10-21 13:38:06.825837:	Testing iteration: 6000, Loss: 0.009128127247095108
2018-10-21 13:41:34.191175:	Testing iteration: 6200, Loss: 0.004442439414560795
Test pass complete
Mean loss over test set: 0.006722370703282082
Data saved to dumps/149 for later audio metric calculation
Starting training
2018-10-21 13:43:10.516080: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 692 of 1000
2018-10-21 13:43:14.507791: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 13:44:42.705171:	Training iteration: 200, Loss: 0.0037726934533566236
2018-10-21 13:46:18.734494:	Training iteration: 400, Loss: 0.004040947183966637
2018-10-21 13:48:01.059356:	Training iteration: 600, Loss: 0.0041722399182617664
2018-10-21 13:49:45.051175:	Training iteration: 800, Loss: 0.005352379288524389
2018-10-21 13:51:30.012897:	Training iteration: 1000, Loss: 0.0031208230648189783
2018-10-21 13:53:18.196084:	Training iteration: 1200, Loss: 0.0037995281163603067
2018-10-21 13:55:06.110586:	Training iteration: 1400, Loss: 0.004604616668075323
2018-10-21 13:56:54.032369:	Training iteration: 1600, Loss: 0.004598718602210283
2018-10-21 13:58:40.685123:	Training iteration: 1800, Loss: 0.003837287425994873
2018-10-21 14:00:28.058159:	Training iteration: 2000, Loss: 0.004872603341937065
2018-10-21 14:02:14.847446:	Training iteration: 2200, Loss: 0.002542734844610095
2018-10-21 14:04:03.736764:	Training iteration: 2400, Loss: 0.006266981363296509
2018-10-21 14:05:53.204489:	Training iteration: 2600, Loss: 0.004347190726548433
2018-10-21 14:07:43.584004:	Training iteration: 2800, Loss: 0.004915768280625343
2018-10-21 14:09:27.815328:	Training iteration: 3000, Loss: 0.005650512408465147
2018-10-21 14:11:13.144621:	Training iteration: 3200, Loss: 0.003979028668254614
2018-10-21 14:12:58.923009:	Training iteration: 3400, Loss: 0.003765036351978779
2018-10-21 14:14:44.395628:	Training iteration: 3600, Loss: 0.0040371776558458805
2018-10-21 14:16:29.852519:	Training iteration: 3800, Loss: 0.004439252428710461
2018-10-21 14:18:17.041870:	Training iteration: 4000, Loss: 0.005553100723773241
2018-10-21 14:20:04.432413:	Training iteration: 4200, Loss: 0.002881592372432351
2018-10-21 14:21:51.686121:	Training iteration: 4400, Loss: 0.0034376420080661774
2018-10-21 14:23:38.179938:	Training iteration: 4600, Loss: 0.004397931043058634
2018-10-21 14:25:24.365100:	Training iteration: 4800, Loss: 0.005932917352765799
2018-10-21 14:27:09.538034:	Training iteration: 5000, Loss: 0.005023652222007513
2018-10-21 14:28:55.893225:	Training iteration: 5200, Loss: 0.004208942409604788
2018-10-21 14:30:42.569083:	Training iteration: 5400, Loss: 0.0034269343595951796
2018-10-21 14:32:30.491559:	Training iteration: 5600, Loss: 0.005241965409368277
2018-10-21 14:34:17.875079:	Training iteration: 5800, Loss: 0.0036563079338520765
2018-10-21 14:36:04.372649:	Training iteration: 6000, Loss: 0.003989153541624546
2018-10-21 14:37:49.826573:	Training iteration: 6200, Loss: 0.0046133059076964855
2018-10-21 14:39:38.120326:	Training iteration: 6400, Loss: 0.004190834239125252
2018-10-21 14:41:26.610189:	Training iteration: 6600, Loss: 0.0034785978496074677
2018-10-21 14:43:15.860765:	Training iteration: 6800, Loss: 0.004301533568650484
2018-10-21 14:45:03.964146:	Training iteration: 7000, Loss: 0.004884280730038881
2018-10-21 14:46:53.450525:	Training iteration: 7200, Loss: 0.004045052453875542
2018-10-21 14:48:42.176476:	Training iteration: 7400, Loss: 0.004717858508229256
2018-10-21 14:50:30.438927:	Training iteration: 7600, Loss: 0.0040185716934502125
2018-10-21 14:52:17.896466:	Training iteration: 7800, Loss: 0.004369428846985102
2018-10-21 14:54:03.657399:	Training iteration: 8000, Loss: 0.005783951375633478
2018-10-21 14:55:51.746180:	Training iteration: 8200, Loss: 0.004078024532645941
2018-10-21 14:56:35.048587: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 642 of 1000
2018-10-21 14:56:39.950467: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 14:57:47.895101:	Training iteration: 8400, Loss: 0.006537502631545067
2018-10-21 14:59:34.319926:	Training iteration: 8600, Loss: 0.00686138728633523
2018-10-21 15:01:24.916625:	Training iteration: 8800, Loss: 0.004093351308256388
2018-10-21 15:03:11.164668:	Training iteration: 9000, Loss: 0.004184307996183634
2018-10-21 15:04:57.962730:	Training iteration: 9200, Loss: 0.003728957613930106
2018-10-21 15:06:47.812309:	Training iteration: 9400, Loss: 0.004763621371239424
2018-10-21 15:08:35.342489:	Training iteration: 9600, Loss: 0.004674328025430441
2018-10-21 15:10:24.075431:	Training iteration: 9800, Loss: 0.005813965108245611
2018-10-21 15:12:14.430022:	Training iteration: 10000, Loss: 0.004321030806750059
Checkpoint
2018-10-21 15:14:18.754365:	Training iteration: 10200, Loss: 0.005622869823127985
2018-10-21 15:16:02.065809:	Training iteration: 10400, Loss: 0.004407168831676245
2018-10-21 15:17:48.856836:	Training iteration: 10600, Loss: 0.004608117043972015
2018-10-21 15:19:35.239700:	Training iteration: 10800, Loss: 0.004551224410533905
2018-10-21 15:21:21.443409:	Training iteration: 11000, Loss: 0.005580951925367117
2018-10-21 15:23:07.663243:	Training iteration: 11200, Loss: 0.004218424204736948
2018-10-21 15:24:53.954434:	Training iteration: 11400, Loss: 0.0055897594429552555
2018-10-21 15:26:38.478281:	Training iteration: 11600, Loss: 0.00651344982907176
2018-10-21 15:28:24.189356:	Training iteration: 11800, Loss: 0.004975136369466782
2018-10-21 15:30:11.520507:	Training iteration: 12000, Loss: 0.005701734218746424
2018-10-21 15:31:56.948342:	Training iteration: 12200, Loss: 0.004095959011465311
2018-10-21 15:33:43.401207:	Training iteration: 12400, Loss: 0.003544008359313011
2018-10-21 15:35:29.623278:	Training iteration: 12600, Loss: 0.005294140428304672
2018-10-21 15:37:19.424392:	Training iteration: 12800, Loss: 0.003959173336625099
2018-10-21 15:38:51.865793:	Training iteration: 13000, Loss: 0.004244996700435877
2018-10-21 15:40:19.446772:	Training iteration: 13200, Loss: 0.0038694695103913546
2018-10-21 15:41:46.582434:	Training iteration: 13400, Loss: 0.005783988628536463
2018-10-21 15:43:12.582497:	Training iteration: 13600, Loss: 0.005817478056997061
2018-10-21 15:44:37.649067:	Training iteration: 13800, Loss: 0.00456068804487586
2018-10-21 15:46:05.099871:	Training iteration: 14000, Loss: 0.0047765509225428104
2018-10-21 15:47:38.657402:	Training iteration: 14200, Loss: 0.0044247801415622234
2018-10-21 15:49:19.961366:	Training iteration: 14400, Loss: 0.005338374059647322
2018-10-21 15:51:04.353967:	Training iteration: 14600, Loss: 0.004980985075235367
2018-10-21 15:53:05.113451:	Training iteration: 14800, Loss: 0.004015162121504545
2018-10-21 15:54:47.690795:	Training iteration: 15000, Loss: 0.004945915192365646
2018-10-21 15:56:36.191905:	Training iteration: 15200, Loss: 0.0036298104096204042
2018-10-21 15:58:27.340586:	Training iteration: 15400, Loss: 0.005385936703532934
2018-10-21 16:00:20.801355:	Training iteration: 15600, Loss: 0.0037600763607770205
2018-10-21 16:02:19.387355:	Training iteration: 15800, Loss: 0.0045217047445476055
2018-10-21 16:04:19.866428:	Training iteration: 16000, Loss: 0.0048203556798398495
2018-10-21 16:06:20.698215:	Training iteration: 16200, Loss: 0.0050347899086773396
2018-10-21 16:08:20.279528:	Training iteration: 16400, Loss: 0.005184689536690712
2018-10-21 16:10:13.089956: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 631 of 1000
2018-10-21 16:10:17.933958: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 16:10:28.916511:	Training iteration: 16600, Loss: 0.006448822561651468
2018-10-21 16:12:19.916481:	Training iteration: 16800, Loss: 0.004867831710726023
2018-10-21 16:14:12.702258:	Training iteration: 17000, Loss: 0.005336580332368612
2018-10-21 16:15:59.775076:	Training iteration: 17200, Loss: 0.005315958056598902
2018-10-21 16:17:47.150681:	Training iteration: 17400, Loss: 0.008662131614983082
2018-10-21 16:19:34.630007:	Training iteration: 17600, Loss: 0.0060807145200669765
2018-10-21 16:21:22.873845:	Training iteration: 17800, Loss: 0.004506553988903761
2018-10-21 16:23:11.351942:	Training iteration: 18000, Loss: 0.005689407233148813
2018-10-21 16:25:01.864518:	Training iteration: 18200, Loss: 0.004799462854862213
2018-10-21 16:26:51.311924:	Training iteration: 18400, Loss: 0.0050361971370875835
2018-10-21 16:28:39.637904:	Training iteration: 18600, Loss: 0.005273330956697464
2018-10-21 16:30:27.044609:	Training iteration: 18800, Loss: 0.005923168733716011
2018-10-21 16:32:13.707545:	Training iteration: 19000, Loss: 0.005755996331572533
2018-10-21 16:34:03.608357:	Training iteration: 19200, Loss: 0.004024003632366657
2018-10-21 16:35:53.890201:	Training iteration: 19400, Loss: 0.005356708075851202
2018-10-21 16:37:42.381874:	Training iteration: 19600, Loss: 0.004822164308279753
2018-10-21 16:39:31.567287:	Training iteration: 19800, Loss: 0.004129207693040371
2018-10-21 16:41:18.732259:	Training iteration: 20000, Loss: 0.004732923582196236
Checkpoint
2018-10-21 16:43:49.232100:	Training iteration: 20200, Loss: 0.005631677806377411
2018-10-21 16:45:29.798003:	Training iteration: 20400, Loss: 0.004930447321385145
2018-10-21 16:47:13.274050:	Training iteration: 20600, Loss: 0.006695280317217112
2018-10-21 16:48:56.900395:	Training iteration: 20800, Loss: 0.0048722680658102036
2018-10-21 16:50:43.127346:	Training iteration: 21000, Loss: 0.004515289794653654
2018-10-21 16:52:30.038543:	Training iteration: 21200, Loss: 0.006878615822643042
2018-10-21 16:54:18.130137:	Training iteration: 21400, Loss: 0.005369042977690697
2018-10-21 16:56:05.168008:	Training iteration: 21600, Loss: 0.0032893826719373465
2018-10-21 16:57:52.073184:	Training iteration: 21800, Loss: 0.00567741459235549
2018-10-21 16:59:39.319099:	Training iteration: 22000, Loss: 0.003762936219573021
2018-10-21 17:01:25.217605:	Training iteration: 22200, Loss: 0.00709480931982398
2018-10-21 17:03:13.750735:	Training iteration: 22400, Loss: 0.004565655253827572
2018-10-21 17:05:01.974266:	Training iteration: 22600, Loss: 0.004134416114538908
2018-10-21 17:06:48.959794:	Training iteration: 22800, Loss: 0.00445933360606432
2018-10-21 17:08:37.102621:	Training iteration: 23000, Loss: 0.0047761257737874985
2018-10-21 17:10:26.503938:	Training iteration: 23200, Loss: 0.006490966770797968
2018-10-21 17:12:17.668040:	Training iteration: 23400, Loss: 0.005479301791638136
2018-10-21 17:14:09.857462:	Training iteration: 23600, Loss: 0.004304150119423866
2018-10-21 17:16:01.892162:	Training iteration: 23800, Loss: 0.005655287299305201
2018-10-21 17:17:52.463087:	Training iteration: 24000, Loss: 0.0054082684218883514
2018-10-21 17:19:39.997575:	Training iteration: 24200, Loss: 0.006090777460485697
2018-10-21 17:21:26.586942:	Training iteration: 24400, Loss: 0.00369894877076149
2018-10-21 17:23:13.746373:	Training iteration: 24600, Loss: 0.004281382542103529
2018-10-21 17:25:02.814095:	Training iteration: 24800, Loss: 0.006234075874090195
2018-10-21 17:25:57.698491: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 661 of 1000
2018-10-21 17:26:02.183969: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 17:27:00.817918:	Training iteration: 25000, Loss: 0.004215410444885492
2018-10-21 17:28:48.354826:	Training iteration: 25200, Loss: 0.003845197381451726
2018-10-21 17:30:36.433575:	Training iteration: 25400, Loss: 0.003920992370694876
2018-10-21 17:32:24.945351:	Training iteration: 25600, Loss: 0.0044617061503231525
2018-10-21 17:34:11.977530:	Training iteration: 25800, Loss: 0.003642231924459338
2018-10-21 17:36:00.893938:	Training iteration: 26000, Loss: 0.006205336656421423
2018-10-21 17:37:49.996472:	Training iteration: 26200, Loss: 0.0042540766298770905
2018-10-21 17:39:36.673851:	Training iteration: 26400, Loss: 0.004998540971428156
2018-10-21 17:41:21.722012:	Training iteration: 26600, Loss: 0.0040438175201416016
2018-10-21 17:43:12.527214:	Training iteration: 26800, Loss: 0.006615743041038513
2018-10-21 17:45:00.132862:	Training iteration: 27000, Loss: 0.004113791044801474
2018-10-21 17:46:48.820223:	Training iteration: 27200, Loss: 0.00529438117519021
2018-10-21 17:48:36.402742:	Training iteration: 27400, Loss: 0.006298985332250595
2018-10-21 17:50:23.992535:	Training iteration: 27600, Loss: 0.0037615057080984116
2018-10-21 17:52:12.154586:	Training iteration: 27800, Loss: 0.003439035266637802
2018-10-21 17:54:02.311487:	Training iteration: 28000, Loss: 0.005555763375014067
2018-10-21 17:55:51.687642:	Training iteration: 28200, Loss: 0.0050086588598787785
2018-10-21 17:57:41.264807:	Training iteration: 28400, Loss: 0.005677816923707724
2018-10-21 17:59:31.512824:	Training iteration: 28600, Loss: 0.0038973602931946516
2018-10-21 18:01:21.112813:	Training iteration: 28800, Loss: 0.0036103276070207357
2018-10-21 18:03:12.733046:	Training iteration: 29000, Loss: 0.0027827646117657423
2018-10-21 18:05:02.849546:	Training iteration: 29200, Loss: 0.004931362811475992
2018-10-21 18:06:50.370787:	Training iteration: 29400, Loss: 0.005077969282865524
2018-10-21 18:08:38.718541:	Training iteration: 29600, Loss: 0.003868143307045102
2018-10-21 18:10:27.813376:	Training iteration: 29800, Loss: 0.004204252269119024
2018-10-21 18:12:17.611475:	Training iteration: 30000, Loss: 0.00598873570561409
Checkpoint
2018-10-21 18:14:07.844306:	Training iteration: 30200, Loss: 0.005401221569627523
2018-10-21 18:15:56.489219:	Training iteration: 30400, Loss: 0.0028752803336828947
2018-10-21 18:17:44.391083:	Training iteration: 30600, Loss: 0.004662145860493183
2018-10-21 18:19:34.397654:	Training iteration: 30800, Loss: 0.005600946489721537
2018-10-21 18:21:27.141674:	Training iteration: 31000, Loss: 0.003795548574998975
2018-10-21 18:23:18.743779:	Training iteration: 31200, Loss: 0.003380751935765147
2018-10-21 18:25:11.622190:	Training iteration: 31400, Loss: 0.004735768307000399
2018-10-21 18:27:03.809690:	Training iteration: 31600, Loss: 0.004456364084035158
2018-10-21 18:28:54.880417:	Training iteration: 31800, Loss: 0.004860505927354097
2018-10-21 18:30:44.750552:	Training iteration: 32000, Loss: 0.004325216170400381
2018-10-21 18:32:34.373465:	Training iteration: 32200, Loss: 0.0036490755155682564
2018-10-21 18:34:25.684988:	Training iteration: 32400, Loss: 0.004856365267187357
2018-10-21 18:36:17.377821:	Training iteration: 32600, Loss: 0.005225112196058035
2018-10-21 18:38:08.771215:	Training iteration: 32800, Loss: 0.005779445171356201
2018-10-21 18:39:57.824869:	Training iteration: 33000, Loss: 0.003518257988616824
2018-10-21 18:41:49.510369:	Training iteration: 33200, Loss: 0.004583823960274458
2018-10-21 18:43:41.634450:	Training iteration: 33400, Loss: 0.004115989897400141
2018-10-21 18:45:23.176103: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 720 of 1000
2018-10-21 18:45:26.302523: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 18:45:43.920517:	Training iteration: 33600, Loss: 0.007016906514763832
2018-10-21 18:47:31.724022:	Training iteration: 33800, Loss: 0.006462448742240667
2018-10-21 18:49:20.988146:	Training iteration: 34000, Loss: 0.008300799876451492
2018-10-21 18:51:10.366501:	Training iteration: 34200, Loss: 0.004849076736718416
2018-10-21 18:53:01.189127:	Training iteration: 34400, Loss: 0.006728440057486296
2018-10-21 18:54:51.495967:	Training iteration: 34600, Loss: 0.0066247135400772095
2018-10-21 18:56:41.164599:	Training iteration: 34800, Loss: 0.004763510078191757
2018-10-21 18:58:30.868513:	Training iteration: 35000, Loss: 0.003498772857710719
2018-10-21 19:00:20.668272:	Training iteration: 35200, Loss: 0.003877554088830948
2018-10-21 19:02:10.010893:	Training iteration: 35400, Loss: 0.005445197224617004
2018-10-21 19:03:59.363413:	Training iteration: 35600, Loss: 0.00542322127148509
2018-10-21 19:05:47.424124:	Training iteration: 35800, Loss: 0.00835817214101553
2018-10-21 19:07:35.573690:	Training iteration: 36000, Loss: 0.009067405946552753
2018-10-21 19:09:23.006396:	Training iteration: 36200, Loss: 0.004740412812680006
2018-10-21 19:11:11.312857:	Training iteration: 36400, Loss: 0.004996582865715027
2018-10-21 19:12:59.738065:	Training iteration: 36600, Loss: 0.003388046519830823
2018-10-21 19:14:49.178671:	Training iteration: 36800, Loss: 0.004853413440287113
2018-10-21 19:16:37.975754:	Training iteration: 37000, Loss: 0.004671166185289621
2018-10-21 19:18:28.340955:	Training iteration: 37200, Loss: 0.0032426349353045225
2018-10-21 19:20:18.544840:	Training iteration: 37400, Loss: 0.005789484828710556
2018-10-21 19:22:10.097244:	Training iteration: 37600, Loss: 0.005435949191451073
2018-10-21 19:23:57.518116:	Training iteration: 37800, Loss: 0.005204298533499241
2018-10-21 19:25:45.697186:	Training iteration: 38000, Loss: 0.005562978330999613
2018-10-21 19:27:35.648752:	Training iteration: 38200, Loss: 0.006859338376671076
2018-10-21 19:29:26.197090:	Training iteration: 38400, Loss: 0.007833237759768963
2018-10-21 19:31:16.693010:	Training iteration: 38600, Loss: 0.004051674623042345
2018-10-21 19:33:05.780081:	Training iteration: 38800, Loss: 0.004250612575560808
2018-10-21 19:34:55.057271:	Training iteration: 39000, Loss: 0.005702211055904627
2018-10-21 19:36:45.005570:	Training iteration: 39200, Loss: 0.004606613889336586
2018-10-21 19:38:34.474302:	Training iteration: 39400, Loss: 0.003071095561608672
2018-10-21 19:40:22.188439:	Training iteration: 39600, Loss: 0.0034194334875792265
2018-10-21 19:42:09.985496:	Training iteration: 39800, Loss: 0.003488769056275487
2018-10-21 19:44:00.765473:	Training iteration: 40000, Loss: 0.004917967598885298
Checkpoint
2018-10-21 19:45:52.219644:	Training iteration: 40200, Loss: 0.008124075829982758
2018-10-21 19:47:40.073883:	Training iteration: 40400, Loss: 0.003511110320687294
2018-10-21 19:49:29.823503:	Training iteration: 40600, Loss: 0.00648132711648941
2018-10-21 19:51:22.287258:	Training iteration: 40800, Loss: 0.005947371479123831
2018-10-21 19:53:14.524159:	Training iteration: 41000, Loss: 0.00721761817112565
2018-10-21 19:55:09.435771:	Training iteration: 41200, Loss: 0.003917214926332235
2018-10-21 19:57:03.676043:	Training iteration: 41400, Loss: 0.006819050759077072
2018-10-21 19:58:56.238330:	Training iteration: 41600, Loss: 0.005684154573827982
2018-10-21 20:00:52.105677:	Training iteration: 41800, Loss: 0.0035674944519996643
2018-10-21 20:02:46.062738:	Training iteration: 42000, Loss: 0.00548192672431469
2018-10-21 20:04:42.280955:	Training iteration: 42200, Loss: 0.006606787908822298
2018-10-21 20:06:34.962933:	Training iteration: 42400, Loss: 0.006742683704942465
2018-10-21 20:08:29.608815:	Training iteration: 42600, Loss: 0.00912133976817131
2018-10-21 20:10:24.447003:	Training iteration: 42800, Loss: 0.0047606793232262135
2018-10-21 20:12:17.576260:	Training iteration: 43000, Loss: 0.006663425359874964
2018-10-21 20:14:08.894814:	Training iteration: 43200, Loss: 0.00766144460067153
2018-10-21 20:16:00.127203:	Training iteration: 43400, Loss: 0.0063780671916902065
2018-10-21 20:17:49.599204:	Training iteration: 43600, Loss: 0.004957374185323715
2018-10-21 20:19:39.860029:	Training iteration: 43800, Loss: 0.005125824827700853
2018-10-21 20:21:30.747317:	Training iteration: 44000, Loss: 0.005597698036581278
2018-10-21 20:23:21.269289:	Training iteration: 44200, Loss: 0.00700734369456768
2018-10-21 20:25:11.189444:	Training iteration: 44400, Loss: 0.0038762844633311033
2018-10-21 20:27:01.840648:	Training iteration: 44600, Loss: 0.007153382059186697
2018-10-21 20:28:50.572578:	Training iteration: 44800, Loss: 0.004779379814863205
2018-10-21 20:30:38.247130:	Training iteration: 45000, Loss: 0.010891617275774479
2018-10-21 20:32:27.399927:	Training iteration: 45200, Loss: 0.00746935373172164
2018-10-21 20:34:15.342283:	Training iteration: 45400, Loss: 0.0066754757426679134
2018-10-21 20:36:04.501287:	Training iteration: 45600, Loss: 0.00799452792853117
2018-10-21 20:37:53.077259:	Training iteration: 45800, Loss: 0.006988395005464554
2018-10-21 20:39:44.206365:	Training iteration: 46000, Loss: 0.004932987969368696
2018-10-21 20:41:33.460869:	Training iteration: 46200, Loss: 0.004527607467025518
2018-10-21 20:43:21.611722:	Training iteration: 46400, Loss: 0.007987109944224358
2018-10-21 20:45:07.416030:	Training iteration: 46600, Loss: 0.006446583662182093
2018-10-21 20:46:54.331150:	Training iteration: 46800, Loss: 0.00825469195842743
2018-10-21 20:48:40.917495:	Training iteration: 47000, Loss: 0.0041452632285654545
2018-10-21 20:50:27.378832:	Training iteration: 47200, Loss: 0.006689500063657761
2018-10-21 20:52:15.236420:	Training iteration: 47400, Loss: 0.006478801369667053
2018-10-21 20:54:04.183832:	Training iteration: 47600, Loss: 0.0053203110583126545
2018-10-21 20:55:51.572881:	Training iteration: 47800, Loss: 0.0042138188146054745
2018-10-21 20:57:40.869157:	Training iteration: 48000, Loss: 0.004259031731635332
2018-10-21 20:59:14.883371:	Training iteration: 48200, Loss: 0.005187679547816515
2018-10-21 21:00:55.215061:	Training iteration: 48400, Loss: 0.005395069252699614
2018-10-21 21:02:39.635977:	Training iteration: 48600, Loss: 0.00831468403339386
2018-10-21 21:04:25.635311:	Training iteration: 48800, Loss: 0.0069135078229010105
2018-10-21 21:06:12.438078:	Training iteration: 49000, Loss: 0.008034073747694492
2018-10-21 21:07:58.635610:	Training iteration: 49200, Loss: 0.005280064418911934
2018-10-21 21:09:44.917984:	Training iteration: 49400, Loss: 0.00743850925937295
2018-10-21 21:11:31.984862:	Training iteration: 49600, Loss: 0.00570099800825119
2018-10-21 21:13:19.256390:	Training iteration: 49800, Loss: 0.007264683023095131
2018-10-21 21:15:08.060132:	Training iteration: 50000, Loss: 0.006581742316484451
Checkpoint
2018-10-21 21:16:57.898114:	Training iteration: 50200, Loss: 0.0052603743970394135
2018-10-21 21:18:45.017888:	Training iteration: 50400, Loss: 0.007216181606054306
2018-10-21 21:20:31.391430:	Training iteration: 50600, Loss: 0.005863734986633062
2018-10-21 21:22:20.200229:	Training iteration: 50800, Loss: 0.003995532635599375
2018-10-21 21:24:08.440480:	Training iteration: 51000, Loss: 0.008589275181293488
2018-10-21 21:25:57.128139:	Training iteration: 51200, Loss: 0.006887430790811777
2018-10-21 21:27:45.852798:	Training iteration: 51400, Loss: 0.004514607135206461
2018-10-21 21:29:33.993947:	Training iteration: 51600, Loss: 0.011661347001791
2018-10-21 21:31:22.332120:	Training iteration: 51800, Loss: 0.007009891327470541
2018-10-21 21:33:12.293536:	Training iteration: 52000, Loss: 0.004554403480142355
2018-10-21 21:35:02.769896:	Training iteration: 52200, Loss: 0.004638811107724905
2018-10-21 21:36:50.880417:	Training iteration: 52400, Loss: 0.007954242639243603
2018-10-21 21:38:38.809190:	Training iteration: 52600, Loss: 0.009285062551498413
2018-10-21 21:40:27.526409:	Training iteration: 52800, Loss: 0.0061895283870399
2018-10-21 21:42:15.141852:	Training iteration: 53000, Loss: 0.004750473890453577
2018-10-21 21:44:02.986071:	Training iteration: 53200, Loss: 0.0037820052821189165
2018-10-21 21:45:51.364109:	Training iteration: 53400, Loss: 0.004379295278340578
2018-10-21 21:47:39.609356:	Training iteration: 53600, Loss: 0.004329513292759657
2018-10-21 21:49:27.160589:	Training iteration: 53800, Loss: 0.007002662867307663
2018-10-21 21:51:14.249440:	Training iteration: 54000, Loss: 0.005219649523496628
2018-10-21 21:53:00.943336:	Training iteration: 54200, Loss: 0.0033481114078313112
2018-10-21 21:54:49.631433:	Training iteration: 54400, Loss: 0.0051173013634979725
2018-10-21 21:56:38.254795:	Training iteration: 54600, Loss: 0.0031820200383663177
2018-10-21 21:58:27.852030:	Training iteration: 54800, Loss: 0.006685107480734587
2018-10-21 22:00:15.160931:	Training iteration: 55000, Loss: 0.004308927338570356
2018-10-21 22:02:00.791062:	Training iteration: 55200, Loss: 0.007068139035254717
2018-10-21 22:03:47.827020:	Training iteration: 55400, Loss: 0.0038737349677830935
2018-10-21 22:05:34.592266:	Training iteration: 55600, Loss: 0.005362195428460836
2018-10-21 22:07:23.321101:	Training iteration: 55800, Loss: 0.007059136405587196
2018-10-21 22:09:13.243848:	Training iteration: 56000, Loss: 0.008466645143926144
2018-10-21 22:11:01.753710:	Training iteration: 56200, Loss: 0.004992831964045763
2018-10-21 22:12:53.752650:	Training iteration: 56400, Loss: 0.008902009576559067
2018-10-21 22:14:44.077182:	Training iteration: 56600, Loss: 0.004504029173403978
2018-10-21 22:16:33.426825:	Training iteration: 56800, Loss: 0.0053314934484660625
2018-10-21 22:18:21.467687:	Training iteration: 57000, Loss: 0.008844410069286823
2018-10-21 22:20:10.424271:	Training iteration: 57200, Loss: 0.00563775934278965
2018-10-21 22:22:00.173255:	Training iteration: 57400, Loss: 0.0036998651921749115
2018-10-21 22:23:51.368391:	Training iteration: 57600, Loss: 0.006746351253241301
2018-10-21 22:25:42.595893:	Training iteration: 57800, Loss: 0.005356110632419586
2018-10-21 22:27:32.861515:	Training iteration: 58000, Loss: 0.0028189278673380613
2018-10-21 22:29:20.867874:	Training iteration: 58200, Loss: 0.005045974627137184
2018-10-21 22:31:10.355391:	Training iteration: 58400, Loss: 0.0041547599248588085
2018-10-21 22:32:56.111448:	Training iteration: 58600, Loss: 0.0054999166168272495
2018-10-21 22:34:44.778063:	Training iteration: 58800, Loss: 0.007408348377794027
2018-10-21 22:36:34.171631:	Training iteration: 59000, Loss: 0.003856225870549679
2018-10-21 22:38:21.638179:	Training iteration: 59200, Loss: 0.004190923646092415
2018-10-21 22:40:09.706513:	Training iteration: 59400, Loss: 0.0050157043151557446
2018-10-21 22:41:58.428474:	Training iteration: 59600, Loss: 0.0045067667961120605
2018-10-21 22:43:48.160321:	Training iteration: 59800, Loss: 0.004989513661712408
2018-10-21 22:45:39.453804:	Training iteration: 60000, Loss: 0.004545015748590231
Checkpoint
2018-10-21 22:47:31.208618:	Training iteration: 60200, Loss: 0.007426382973790169
2018-10-21 22:49:19.993147:	Training iteration: 60400, Loss: 0.005902457982301712
2018-10-21 22:51:07.831673:	Training iteration: 60600, Loss: 0.005054414737969637
2018-10-21 22:52:56.402820:	Training iteration: 60800, Loss: 0.008741769939661026
2018-10-21 22:54:44.803997:	Training iteration: 61000, Loss: 0.004400686826556921
2018-10-21 22:56:34.106083:	Training iteration: 61200, Loss: 0.006048053037375212
2018-10-21 22:58:21.477343:	Training iteration: 61400, Loss: 0.004618094768375158
2018-10-21 23:00:09.325263:	Training iteration: 61600, Loss: 0.008037727326154709
2018-10-21 23:01:58.611285:	Training iteration: 61800, Loss: 0.005752407014369965
2018-10-21 23:03:45.351454:	Training iteration: 62000, Loss: 0.005562891718000174
2018-10-21 23:05:30.660493:	Training iteration: 62200, Loss: 0.0061959158629179
2018-10-21 23:07:16.174994:	Training iteration: 62400, Loss: 0.0035639237612485886
2018-10-21 23:09:03.560804:	Training iteration: 62600, Loss: 0.009478184394538403
2018-10-21 23:10:51.882877:	Training iteration: 62800, Loss: 0.0060341390781104565
2018-10-21 23:12:38.673544:	Training iteration: 63000, Loss: 0.009359347634017467
2018-10-21 23:14:24.778845:	Training iteration: 63200, Loss: 0.004075673408806324
2018-10-21 23:16:11.465206:	Training iteration: 63400, Loss: 0.00820866134017706
2018-10-21 23:17:59.283124:	Training iteration: 63600, Loss: 0.006848569493740797
2018-10-21 23:19:46.240971:	Training iteration: 63800, Loss: 0.008320939727127552
2018-10-21 23:21:35.062506:	Training iteration: 64000, Loss: 0.00840615201741457
2018-10-21 23:23:23.364405:	Training iteration: 64200, Loss: 0.012011236511170864
2018-10-21 23:25:10.773357:	Training iteration: 64400, Loss: 0.0038216973189264536
2018-10-21 23:26:58.994904:	Training iteration: 64600, Loss: 0.003594551933929324
2018-10-21 23:28:46.461470:	Training iteration: 64800, Loss: 0.004863489884883165
2018-10-21 23:30:34.624518:	Training iteration: 65000, Loss: 0.004581726621836424
2018-10-21 23:32:22.209738:	Training iteration: 65200, Loss: 0.0053322226740419865
2018-10-21 23:34:10.749343:	Training iteration: 65400, Loss: 0.005856575444340706
2018-10-21 23:35:58.999867:	Training iteration: 65600, Loss: 0.005004414822906256
2018-10-21 23:37:46.100889:	Training iteration: 65800, Loss: 0.005387600511312485
2018-10-21 23:39:33.891490:	Training iteration: 66000, Loss: 0.00997703056782484
2018-10-21 23:41:21.992682:	Training iteration: 66200, Loss: 0.004865328315645456
2018-10-21 23:43:10.760331:	Training iteration: 66400, Loss: 0.004437791649252176
2018-10-21 23:44:59.972627:	Training iteration: 66600, Loss: 0.004555757623165846
2018-10-21 23:46:48.585754:	Training iteration: 66800, Loss: 0.004114571958780289
2018-10-21 23:48:38.277391:	Training iteration: 67000, Loss: 0.007015701849013567
2018-10-21 23:50:30.142972:	Training iteration: 67200, Loss: 0.00414067879319191
2018-10-21 23:52:21.559679:	Training iteration: 67400, Loss: 0.004924467299133539
2018-10-21 23:54:13.089070:	Training iteration: 67600, Loss: 0.004344699438661337
2018-10-21 23:56:05.349687:	Training iteration: 67800, Loss: 0.005461327265948057
2018-10-21 23:57:58.901996:	Training iteration: 68000, Loss: 0.004373180214315653
2018-10-21 23:59:49.101198:	Training iteration: 68200, Loss: 0.003957963082939386
2018-10-22 00:01:40.730905:	Training iteration: 68400, Loss: 0.007538292091339827
2018-10-22 00:03:34.218588:	Training iteration: 68600, Loss: 0.005127046722918749
2018-10-22 00:05:27.364788:	Training iteration: 68800, Loss: 0.005450190510600805
2018-10-22 00:07:22.356099:	Training iteration: 69000, Loss: 0.006613302510231733
2018-10-22 00:09:15.757678:	Training iteration: 69200, Loss: 0.004688641522079706
2018-10-22 00:11:07.255345:	Training iteration: 69400, Loss: 0.004088318906724453
2018-10-22 00:12:56.182496:	Training iteration: 69600, Loss: 0.005764196161180735
2018-10-22 00:14:44.712046:	Training iteration: 69800, Loss: 0.0049310773611068726
2018-10-22 00:16:32.473672:	Training iteration: 70000, Loss: 0.005547078792005777
Checkpoint
2018-10-22 00:18:24.087618:	Training iteration: 70200, Loss: 0.003518499666824937
2018-10-22 00:20:14.748895:	Training iteration: 70400, Loss: 0.005047349724918604
2018-10-22 00:22:06.491155:	Training iteration: 70600, Loss: 0.006914117839187384
2018-10-22 00:23:57.217854:	Training iteration: 70800, Loss: 0.006084675434976816
2018-10-22 00:25:47.994722:	Training iteration: 71000, Loss: 0.006733367685228586
2018-10-22 00:27:38.360075:	Training iteration: 71200, Loss: 0.006653787102550268
2018-10-22 00:29:26.352988:	Training iteration: 71400, Loss: 0.005024620797485113
2018-10-22 00:31:14.151102:	Training iteration: 71600, Loss: 0.005913719534873962
2018-10-22 00:33:02.539809:	Training iteration: 71800, Loss: 0.006393488496541977
2018-10-22 00:34:50.854726:	Training iteration: 72000, Loss: 0.005049120634794235
2018-10-22 00:36:37.653963:	Training iteration: 72200, Loss: 0.007887756451964378
2018-10-22 00:38:24.390701:	Training iteration: 72400, Loss: 0.004456155002117157
2018-10-22 00:40:11.695095:	Training iteration: 72600, Loss: 0.004428191110491753
2018-10-22 00:41:58.966039:	Training iteration: 72800, Loss: 0.005547558888792992
2018-10-22 00:43:47.356157:	Training iteration: 73000, Loss: 0.004498774651437998
2018-10-22 00:45:34.486972:	Training iteration: 73200, Loss: 0.005369298625737429
2018-10-22 00:47:20.726384:	Training iteration: 73400, Loss: 0.0058089750818908215
2018-10-22 00:49:08.547620:	Training iteration: 73600, Loss: 0.004876564722508192
2018-10-22 00:50:58.036551:	Training iteration: 73800, Loss: 0.005642857402563095
2018-10-22 00:52:48.455860:	Training iteration: 74000, Loss: 0.006148507818579674
2018-10-22 00:54:36.105494:	Training iteration: 74200, Loss: 0.004900874104350805
2018-10-22 00:56:22.628673:	Training iteration: 74400, Loss: 0.004755750764161348
2018-10-22 00:58:13.445720:	Training iteration: 74600, Loss: 0.0061731901951134205
2018-10-22 01:00:06.493917:	Training iteration: 74800, Loss: 0.005568691994994879
2018-10-22 01:01:57.464860:	Training iteration: 75000, Loss: 0.0044552721083164215
2018-10-22 01:03:49.043836:	Training iteration: 75200, Loss: 0.008381289429962635
2018-10-22 01:05:39.696876:	Training iteration: 75400, Loss: 0.0072968690656125546
2018-10-22 01:07:30.890112:	Training iteration: 75600, Loss: 0.005308844149112701
2018-10-22 01:09:21.540035:	Training iteration: 75800, Loss: 0.0073171635158360004
2018-10-22 01:11:11.011505:	Training iteration: 76000, Loss: 0.008531083352863789
2018-10-22 01:12:58.310878:	Training iteration: 76200, Loss: 0.005357093643397093
2018-10-22 01:14:47.690269:	Training iteration: 76400, Loss: 0.006265776231884956
2018-10-22 01:16:33.486351:	Training iteration: 76600, Loss: 0.004960687831044197
2018-10-22 01:18:21.616221:	Training iteration: 76800, Loss: 0.006640932057052851
2018-10-22 01:20:11.018634:	Training iteration: 77000, Loss: 0.005018012132495642
2018-10-22 01:22:02.616237:	Training iteration: 77200, Loss: 0.005124275106936693
2018-10-22 01:23:52.845806:	Training iteration: 77400, Loss: 0.005454763770103455
2018-10-22 01:25:42.252749:	Training iteration: 77600, Loss: 0.0072085014544427395
2018-10-22 01:27:31.819117:	Training iteration: 77800, Loss: 0.0035268592182546854
2018-10-22 01:29:21.933872:	Training iteration: 78000, Loss: 0.004741842392832041
2018-10-22 01:31:11.803937:	Training iteration: 78200, Loss: 0.005787333473563194
2018-10-22 01:33:01.907300:	Training iteration: 78400, Loss: 0.004705002065747976
2018-10-22 01:34:49.960490:	Training iteration: 78600, Loss: 0.003663487732410431
2018-10-22 01:36:37.882122:	Training iteration: 78800, Loss: 0.005464873742312193
2018-10-22 01:38:25.101016:	Training iteration: 79000, Loss: 0.006244182121008635
2018-10-22 01:40:11.554209:	Training iteration: 79200, Loss: 0.008034499362111092
2018-10-22 01:41:59.975251:	Training iteration: 79400, Loss: 0.011049437336623669
2018-10-22 01:43:49.626353:	Training iteration: 79600, Loss: 0.006899021565914154
2018-10-22 01:45:38.012794:	Training iteration: 79800, Loss: 0.007506217807531357
2018-10-22 01:47:09.548374:	Epoch 0 finished after 79969 iterations.
Validating
2018-10-22 01:47:13.299487:	Entering validation loop
2018-10-22 01:47:23.919931: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 597 of 1000
2018-10-22 01:47:30.008100: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 01:48:09.762692:	Validation iteration: 200, Loss: 0.006744939833879471
2018-10-22 01:48:53.783434:	Validation iteration: 400, Loss: 0.006857837084680796
2018-10-22 01:49:40.925241:	Validation iteration: 600, Loss: 0.004964159335941076
2018-10-22 01:50:26.217427:	Validation iteration: 800, Loss: 0.00623699463903904
2018-10-22 01:51:11.689760:	Validation iteration: 1000, Loss: 0.003957616165280342
2018-10-22 01:51:56.740695:	Validation iteration: 1200, Loss: 0.0068263523280620575
2018-10-22 01:52:50.167412: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 611 of 1000
2018-10-22 01:52:56.055189: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 01:52:57.687178:	Validation iteration: 1400, Loss: 0.00614334037527442
2018-10-22 01:53:38.197247:	Validation iteration: 1600, Loss: 0.005921881180256605
2018-10-22 01:54:22.680850:	Validation iteration: 1800, Loss: 0.006983477622270584
2018-10-22 01:55:07.492375:	Validation iteration: 2000, Loss: 0.0057160635478794575
2018-10-22 01:55:54.590205:	Validation iteration: 2200, Loss: 0.0065560657531023026
2018-10-22 01:56:39.595265:	Validation iteration: 2400, Loss: 0.006138049066066742
2018-10-22 01:57:23.298692:	Validation iteration: 2600, Loss: 0.004462870769202709
2018-10-22 01:58:15.007474: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 456 of 1000
2018-10-22 01:58:22.425667: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 01:58:25.523322:	Validation iteration: 2800, Loss: 0.0054214331321418285
2018-10-22 01:59:06.171369:	Validation iteration: 3000, Loss: 0.004878150764852762
2018-10-22 01:59:50.488332:	Validation iteration: 3200, Loss: 0.005133924074470997
2018-10-22 02:00:35.349649:	Validation iteration: 3400, Loss: 0.005993615835905075
2018-10-22 02:01:19.767274:	Validation iteration: 3600, Loss: 0.006654147524386644
2018-10-22 02:02:05.121289:	Validation iteration: 3800, Loss: 0.007160215172916651
2018-10-22 02:02:50.052880:	Validation iteration: 4000, Loss: 0.005284463986754417
2018-10-22 02:03:39.363570: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 672 of 1000
2018-10-22 02:03:43.783349: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:03:48.476098:	Validation iteration: 4200, Loss: 0.006545318756252527
2018-10-22 02:04:30.624627:	Validation iteration: 4400, Loss: 0.009286249987781048
2018-10-22 02:05:16.526972:	Validation iteration: 4600, Loss: 0.0065985447727143764
2018-10-22 02:06:00.914055:	Validation iteration: 4800, Loss: 0.006089319940656424
2018-10-22 02:06:45.041873:	Validation iteration: 5000, Loss: 0.006759652402251959
2018-10-22 02:07:29.744450:	Validation iteration: 5200, Loss: 0.006282883230596781
2018-10-22 02:08:14.313662:	Validation iteration: 5400, Loss: 0.007638797163963318
2018-10-22 02:09:02.273598: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 492 of 1000
2018-10-22 02:09:10.567899: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:09:16.719831:	Validation iteration: 5600, Loss: 0.004355830606073141
2018-10-22 02:09:56.898582:	Validation iteration: 5800, Loss: 0.004534932319074869
2018-10-22 02:10:40.449500:	Validation iteration: 6000, Loss: 0.007900687865912914
2018-10-22 02:11:23.992870:	Validation iteration: 6200, Loss: 0.0079395342618227
2018-10-22 02:12:07.305728:	Validation iteration: 6400, Loss: 0.0065101576037704945
2018-10-22 02:12:51.296825:	Validation iteration: 6600, Loss: 0.006383089814335108
2018-10-22 02:13:36.572165:	Validation iteration: 6800, Loss: 0.005180991720408201
2018-10-22 02:14:20.825836:	Validation iteration: 7000, Loss: 0.00549321947619319
2018-10-22 02:15:04.694761:	Validation iteration: 7200, Loss: 0.008476425893604755
2018-10-22 02:15:49.052903:	Validation iteration: 7400, Loss: 0.006481265649199486
Validation check mean loss: 0.005970435526692621
Validation loss has improved!
New best validation cost!
2018-10-22 02:16:21.407784: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 689 of 1000
2018-10-22 02:16:25.479268: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:16:39.745625:	Training iteration: 80000, Loss: 0.004513969179242849
Checkpoint
2018-10-22 02:18:33.759464:	Training iteration: 80200, Loss: 0.0033721725922077894
2018-10-22 02:20:20.846084:	Training iteration: 80400, Loss: 0.0046802847646176815
2018-10-22 02:22:09.005031:	Training iteration: 80600, Loss: 0.00429380452260375
2018-10-22 02:23:56.410231:	Training iteration: 80800, Loss: 0.0037243098486214876
2018-10-22 02:25:44.247832:	Training iteration: 81000, Loss: 0.0034300757106393576
2018-10-22 02:27:32.786949:	Training iteration: 81200, Loss: 0.004624588880687952
2018-10-22 02:29:24.020271:	Training iteration: 81400, Loss: 0.005282364785671234
2018-10-22 02:31:15.176748:	Training iteration: 81600, Loss: 0.006241717841476202
2018-10-22 02:33:05.836107:	Training iteration: 81800, Loss: 0.004056263715028763
2018-10-22 02:34:55.751232:	Training iteration: 82000, Loss: 0.005353828426450491
2018-10-22 02:36:44.199677:	Training iteration: 82200, Loss: 0.006928803864866495
2018-10-22 02:38:32.398335:	Training iteration: 82400, Loss: 0.004018602892756462
2018-10-22 02:40:21.429006:	Training iteration: 82600, Loss: 0.005182372871786356
2018-10-22 02:42:08.942714:	Training iteration: 82800, Loss: 0.004154888913035393
2018-10-22 02:43:56.326585:	Training iteration: 83000, Loss: 0.004561448935419321
2018-10-22 02:45:43.458044:	Training iteration: 83200, Loss: 0.0034662168473005295
2018-10-22 02:47:30.046955:	Training iteration: 83400, Loss: 0.004353388678282499
2018-10-22 02:49:19.288756:	Training iteration: 83600, Loss: 0.00579808047041297
2018-10-22 02:51:08.596062:	Training iteration: 83800, Loss: 0.003977720160037279
2018-10-22 02:52:58.645850:	Training iteration: 84000, Loss: 0.0030321229714900255
2018-10-22 02:54:48.200831:	Training iteration: 84200, Loss: 0.004620676394551992
2018-10-22 02:56:38.288233:	Training iteration: 84400, Loss: 0.004294427577406168
2018-10-22 02:58:27.173066:	Training iteration: 84600, Loss: 0.003742433153092861
2018-10-22 03:00:16.450961:	Training iteration: 84800, Loss: 0.004211239982396364
2018-10-22 03:02:07.641250:	Training iteration: 85000, Loss: 0.004115115385502577
2018-10-22 03:03:58.734187:	Training iteration: 85200, Loss: 0.005387384910136461
2018-10-22 03:05:50.304695:	Training iteration: 85400, Loss: 0.005924752447754145
2018-10-22 03:07:41.362909:	Training iteration: 85600, Loss: 0.005862133577466011
2018-10-22 03:09:32.568729:	Training iteration: 85800, Loss: 0.0034369148779660463
2018-10-22 03:11:22.835202:	Training iteration: 86000, Loss: 0.005681672599166632
2018-10-22 03:13:13.791767:	Training iteration: 86200, Loss: 0.003869185922667384
2018-10-22 03:15:01.108773:	Training iteration: 86400, Loss: 0.0047508967109024525
2018-10-22 03:16:49.001285:	Training iteration: 86600, Loss: 0.004217421170324087
2018-10-22 03:18:36.503675:	Training iteration: 86800, Loss: 0.0033454063814133406
2018-10-22 03:20:25.327544:	Training iteration: 87000, Loss: 0.0029771907720714808
2018-10-22 03:22:13.786132:	Training iteration: 87200, Loss: 0.005623176693916321
2018-10-22 03:24:03.496921:	Training iteration: 87400, Loss: 0.006185585167258978
2018-10-22 03:25:53.468788:	Training iteration: 87600, Loss: 0.005335379391908646
2018-10-22 03:27:41.943314:	Training iteration: 87800, Loss: 0.0033585724886506796
2018-10-22 03:29:30.857434:	Training iteration: 88000, Loss: 0.004075656179338694
2018-10-22 03:31:19.551428:	Training iteration: 88200, Loss: 0.003616020083427429
2018-10-22 03:31:46.720739: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 652 of 1000
2018-10-22 03:31:51.519242: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 03:33:16.824945:	Training iteration: 88400, Loss: 0.005502550397068262
2018-10-22 03:35:04.882163:	Training iteration: 88600, Loss: 0.004255661275237799
2018-10-22 03:36:56.029463:	Training iteration: 88800, Loss: 0.0036543256137520075
2018-10-22 03:38:46.312699:	Training iteration: 89000, Loss: 0.004760124254971743
2018-10-22 03:40:36.832683:	Training iteration: 89200, Loss: 0.005277858581393957
2018-10-22 03:42:28.361203:	Training iteration: 89400, Loss: 0.005699096713215113
2018-10-22 03:44:20.194888:	Training iteration: 89600, Loss: 0.004906842950731516
2018-10-22 03:46:08.212503:	Training iteration: 89800, Loss: 0.004471533466130495
2018-10-22 03:47:57.333337:	Training iteration: 90000, Loss: 0.003849926171824336
Checkpoint
2018-10-22 03:49:48.979894:	Training iteration: 90200, Loss: 0.003888505743816495
2018-10-22 03:51:39.316342:	Training iteration: 90400, Loss: 0.00403545331209898
2018-10-22 03:53:27.880031:	Training iteration: 90600, Loss: 0.00447881780564785
2018-10-22 03:55:14.036728:	Training iteration: 90800, Loss: 0.005128644406795502
2018-10-22 03:57:00.629553:	Training iteration: 91000, Loss: 0.004030976444482803
2018-10-22 03:58:47.001148:	Training iteration: 91200, Loss: 0.006510943174362183
2018-10-22 04:00:35.421086:	Training iteration: 91400, Loss: 0.007629450410604477
2018-10-22 04:02:23.802617:	Training iteration: 91600, Loss: 0.0064164274372160435
2018-10-22 04:04:11.378148:	Training iteration: 91800, Loss: 0.004908181726932526
2018-10-22 04:05:59.480917:	Training iteration: 92000, Loss: 0.0035498712677508593
2018-10-22 04:07:47.299674:	Training iteration: 92200, Loss: 0.004521768540143967
2018-10-22 04:09:35.264685:	Training iteration: 92400, Loss: 0.004944149404764175
2018-10-22 04:11:23.775788:	Training iteration: 92600, Loss: 0.0045512765645980835
2018-10-22 04:13:11.910275:	Training iteration: 92800, Loss: 0.004095533397048712
2018-10-22 04:14:59.049840:	Training iteration: 93000, Loss: 0.007226273883134127
2018-10-22 04:16:46.982673:	Training iteration: 93200, Loss: 0.007038322743028402
2018-10-22 04:18:33.153611:	Training iteration: 93400, Loss: 0.0057656108401715755
2018-10-22 04:20:21.034395:	Training iteration: 93600, Loss: 0.003914929926395416
2018-10-22 04:22:08.044229:	Training iteration: 93800, Loss: 0.008004301227629185
2018-10-22 04:23:54.309517:	Training iteration: 94000, Loss: 0.0047346726059913635
2018-10-22 04:25:41.266653:	Training iteration: 94200, Loss: 0.005000115837901831
2018-10-22 04:27:28.900122:	Training iteration: 94400, Loss: 0.004547791555523872
2018-10-22 04:29:17.743914:	Training iteration: 94600, Loss: 0.004911277908831835
2018-10-22 04:31:07.918868:	Training iteration: 94800, Loss: 0.004984027240425348
2018-10-22 04:32:58.241099:	Training iteration: 95000, Loss: 0.005180466920137405
2018-10-22 04:34:47.190115:	Training iteration: 95200, Loss: 0.005267970263957977
2018-10-22 04:36:36.482670:	Training iteration: 95400, Loss: 0.005576420575380325
2018-10-22 04:38:25.061815:	Training iteration: 95600, Loss: 0.004910005256533623
2018-10-22 04:40:11.749924:	Training iteration: 95800, Loss: 0.004368361551314592
2018-10-22 04:42:01.082015:	Training iteration: 96000, Loss: 0.004147535655647516
2018-10-22 04:43:49.277594:	Training iteration: 96200, Loss: 0.00413426011800766
2018-10-22 04:45:37.862545:	Training iteration: 96400, Loss: 0.004802224691957235
2018-10-22 04:47:04.804731: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 680 of 1000
2018-10-22 04:47:08.926942: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 04:47:35.326910:	Training iteration: 96600, Loss: 0.006995069328695536
2018-10-22 04:49:20.044367:	Training iteration: 96800, Loss: 0.0056195869110524654
2018-10-22 04:51:07.640635:	Training iteration: 97000, Loss: 0.0056176818907260895
2018-10-22 04:52:55.440357:	Training iteration: 97200, Loss: 0.0042433603666722775
2018-10-22 04:54:43.884827:	Training iteration: 97400, Loss: 0.0053368923254311085
2018-10-22 04:56:30.273985:	Training iteration: 97600, Loss: 0.005202579777687788
2018-10-22 04:58:15.278425:	Training iteration: 97800, Loss: 0.004559702705591917
2018-10-22 05:00:02.187071:	Training iteration: 98000, Loss: 0.004518516361713409
2018-10-22 05:01:49.268010:	Training iteration: 98200, Loss: 0.005338850896805525
2018-10-22 05:03:36.485493:	Training iteration: 98400, Loss: 0.0030584298074245453
2018-10-22 05:05:23.061430:	Training iteration: 98600, Loss: 0.004868345335125923
2018-10-22 05:07:09.942342:	Training iteration: 98800, Loss: 0.006842608097940683
2018-10-22 05:08:56.794789:	Training iteration: 99000, Loss: 0.004499603062868118
2018-10-22 05:10:45.224351:	Training iteration: 99200, Loss: 0.005513206589967012
2018-10-22 05:12:30.011580:	Training iteration: 99400, Loss: 0.004855745937675238
2018-10-22 05:14:16.752935:	Training iteration: 99600, Loss: 0.006751274690032005
2018-10-22 05:16:02.609448:	Training iteration: 99800, Loss: 0.005532457958906889
2018-10-22 05:17:49.097942:	Training iteration: 100000, Loss: 0.004392996896058321
Checkpoint
2018-10-22 05:19:37.310654:	Training iteration: 100200, Loss: 0.004305216949433088
2018-10-22 05:21:23.329228:	Training iteration: 100400, Loss: 0.005346307065337896
2018-10-22 05:23:10.070817:	Training iteration: 100600, Loss: 0.0037687362637370825
2018-10-22 05:24:58.404757:	Training iteration: 100800, Loss: 0.0042801303789019585
2018-10-22 05:26:44.736983:	Training iteration: 101000, Loss: 0.0036649403627961874
2018-10-22 05:28:31.968634:	Training iteration: 101200, Loss: 0.0037330768536776304
2018-10-22 05:30:18.840428:	Training iteration: 101400, Loss: 0.006149856373667717
2018-10-22 05:32:06.282770:	Training iteration: 101600, Loss: 0.004757680464535952
2018-10-22 05:33:53.370816:	Training iteration: 101800, Loss: 0.006766524165868759
2018-10-22 05:35:40.309326:	Training iteration: 102000, Loss: 0.004174368921667337
2018-10-22 05:37:27.965492:	Training iteration: 102200, Loss: 0.005580151919275522
2018-10-22 05:39:15.594329:	Training iteration: 102400, Loss: 0.005615388974547386
2018-10-22 05:41:02.518200:	Training iteration: 102600, Loss: 0.0071727982722222805
2018-10-22 05:42:49.470147:	Training iteration: 102800, Loss: 0.004713414702564478
2018-10-22 05:44:35.374065:	Training iteration: 103000, Loss: 0.005937869194895029
2018-10-22 05:46:21.859368:	Training iteration: 103200, Loss: 0.004458706360310316
2018-10-22 05:48:10.105254:	Training iteration: 103400, Loss: 0.003939331043511629
2018-10-22 05:49:56.162265:	Training iteration: 103600, Loss: 0.004405206069350243
2018-10-22 05:51:43.095230:	Training iteration: 103800, Loss: 0.004450791049748659
2018-10-22 05:53:28.069646:	Training iteration: 104000, Loss: 0.0049581523053348064
2018-10-22 05:55:15.014543:	Training iteration: 104200, Loss: 0.008623114787042141
2018-10-22 05:57:02.504669:	Training iteration: 104400, Loss: 0.005669110920280218
2018-10-22 05:58:51.370650:	Training iteration: 104600, Loss: 0.00495051359757781
2018-10-22 06:00:40.183196:	Training iteration: 104800, Loss: 0.004266046918928623
2018-10-22 06:01:18.335500: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 659 of 1000
2018-10-22 06:01:22.654435: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 06:02:36.591557:	Training iteration: 105000, Loss: 0.004728769883513451
2018-10-22 06:04:22.078094:	Training iteration: 105200, Loss: 0.003960566129535437
2018-10-22 06:06:08.680611:	Training iteration: 105400, Loss: 0.003891815198585391
2018-10-22 06:07:57.505044:	Training iteration: 105600, Loss: 0.0024866655003279448
2018-10-22 06:09:46.203740:	Training iteration: 105800, Loss: 0.0038869641721248627
2018-10-22 06:11:35.192862:	Training iteration: 106000, Loss: 0.0035817541647702456
2018-10-22 06:13:24.782247:	Training iteration: 106200, Loss: 0.004953681956976652
2018-10-22 06:15:10.978405:	Training iteration: 106400, Loss: 0.004675381351262331
2018-10-22 06:16:58.427879:	Training iteration: 106600, Loss: 0.0033278700429946184
2018-10-22 06:18:45.555943:	Training iteration: 106800, Loss: 0.003496613586321473
2018-10-22 06:20:31.503856:	Training iteration: 107000, Loss: 0.0038886733818799257
2018-10-22 06:22:16.776402:	Training iteration: 107200, Loss: 0.006040047388523817
2018-10-22 06:24:02.811740:	Training iteration: 107400, Loss: 0.0030365868005901575
2018-10-22 06:25:49.708232:	Training iteration: 107600, Loss: 0.004690275061875582
2018-10-22 06:27:34.712359:	Training iteration: 107800, Loss: 0.003464776324108243
2018-10-22 06:29:19.386066:	Training iteration: 108000, Loss: 0.0038504283875226974
2018-10-22 06:31:03.697941:	Training iteration: 108200, Loss: 0.004785103723406792
2018-10-22 06:32:48.898180:	Training iteration: 108400, Loss: 0.004935434553772211
2018-10-22 06:34:33.615268:	Training iteration: 108600, Loss: 0.003959326539188623
2018-10-22 06:36:19.094741:	Training iteration: 108800, Loss: 0.006327961105853319
2018-10-22 06:38:04.478956:	Training iteration: 109000, Loss: 0.0054171145893633366
2018-10-22 06:39:49.785569:	Training iteration: 109200, Loss: 0.0033446263987571
2018-10-22 06:41:34.763011:	Training iteration: 109400, Loss: 0.005261907819658518
2018-10-22 06:43:19.391760:	Training iteration: 109600, Loss: 0.0048383441753685474
2018-10-22 06:45:03.847028:	Training iteration: 109800, Loss: 0.0034317236859351397
2018-10-22 06:46:48.040355:	Training iteration: 110000, Loss: 0.0038076138589531183
Checkpoint
2018-10-22 06:48:32.982805:	Training iteration: 110200, Loss: 0.005393424537032843
2018-10-22 06:50:17.849151:	Training iteration: 110400, Loss: 0.003392326645553112
2018-10-22 06:52:03.701790:	Training iteration: 110600, Loss: 0.00459284009411931
2018-10-22 06:53:49.088613:	Training iteration: 110800, Loss: 0.002900158753618598
2018-10-22 06:55:33.920595:	Training iteration: 111000, Loss: 0.005844354163855314
2018-10-22 06:57:18.651650:	Training iteration: 111200, Loss: 0.003356743371114135
2018-10-22 06:59:04.247483:	Training iteration: 111400, Loss: 0.004678503144532442
2018-10-22 07:00:51.038223:	Training iteration: 111600, Loss: 0.004345212131738663
2018-10-22 07:02:37.969304:	Training iteration: 111800, Loss: 0.004115859512239695
2018-10-22 07:04:23.814878:	Training iteration: 112000, Loss: 0.004918075632303953
2018-10-22 07:06:09.227592:	Training iteration: 112200, Loss: 0.004537621047347784
2018-10-22 07:07:54.554335:	Training iteration: 112400, Loss: 0.006059771403670311
2018-10-22 07:09:41.258814:	Training iteration: 112600, Loss: 0.0033611140679568052
2018-10-22 07:11:24.640068:	Training iteration: 112800, Loss: 0.0038506004493683577
2018-10-22 07:13:09.922387:	Training iteration: 113000, Loss: 0.003550683381035924
2018-10-22 07:14:53.996140:	Training iteration: 113200, Loss: 0.0048821368254721165
2018-10-22 07:16:37.033294:	Training iteration: 113400, Loss: 0.004032615572214127
2018-10-22 07:17:54.832818: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 664 of 1000
2018-10-22 07:17:59.040776: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 07:18:31.717076:	Training iteration: 113600, Loss: 0.006049970630556345
2018-10-22 07:20:14.526329:	Training iteration: 113800, Loss: 0.003951527178287506
2018-10-22 07:21:57.681963:	Training iteration: 114000, Loss: 0.004927827510982752
2018-10-22 07:23:42.405108:	Training iteration: 114200, Loss: 0.0056079961359500885
2018-10-22 07:25:23.233499:	Training iteration: 114400, Loss: 0.005927730817347765
2018-10-22 07:27:05.949353:	Training iteration: 114600, Loss: 0.0031341174617409706
2018-10-22 07:28:49.590132:	Training iteration: 114800, Loss: 0.003887988394126296
2018-10-22 07:30:33.569678:	Training iteration: 115000, Loss: 0.005063104908913374
2018-10-22 07:32:20.421135:	Training iteration: 115200, Loss: 0.00401883153244853
2018-10-22 07:34:05.812124:	Training iteration: 115400, Loss: 0.005655964370816946
2018-10-22 07:35:51.981918:	Training iteration: 115600, Loss: 0.008883458562195301
2018-10-22 07:37:37.740489:	Training iteration: 115800, Loss: 0.008090081624686718
2018-10-22 07:39:22.703326:	Training iteration: 116000, Loss: 0.007497405167669058
2018-10-22 07:41:07.447868:	Training iteration: 116200, Loss: 0.004937028978019953
2018-10-22 07:42:52.659372:	Training iteration: 116400, Loss: 0.005406104493886232
2018-10-22 07:44:36.678629:	Training iteration: 116600, Loss: 0.005402179900556803
2018-10-22 07:46:21.723529:	Training iteration: 116800, Loss: 0.004320499487221241
2018-10-22 07:48:06.186710:	Training iteration: 117000, Loss: 0.005083128344267607
2018-10-22 07:49:50.052087:	Training iteration: 117200, Loss: 0.008899698965251446
2018-10-22 07:51:34.115267:	Training iteration: 117400, Loss: 0.00578699866309762
2018-10-22 07:53:17.940411:	Training iteration: 117600, Loss: 0.004489387851208448
2018-10-22 07:55:01.299308:	Training iteration: 117800, Loss: 0.005913118366152048
2018-10-22 07:56:42.847415:	Training iteration: 118000, Loss: 0.00586478179320693
2018-10-22 07:58:26.117020:	Training iteration: 118200, Loss: 0.005648352205753326
2018-10-22 08:00:09.037429:	Training iteration: 118400, Loss: 0.005824262276291847
2018-10-22 08:01:51.346127:	Training iteration: 118600, Loss: 0.004400497768074274
2018-10-22 08:03:33.049962:	Training iteration: 118800, Loss: 0.0047450014390051365
2018-10-22 08:05:17.568321:	Training iteration: 119000, Loss: 0.004945374559611082
2018-10-22 08:07:00.963525:	Training iteration: 119200, Loss: 0.0035586438607424498
2018-10-22 08:08:46.301070:	Training iteration: 119400, Loss: 0.00418520625680685
2018-10-22 08:10:30.589944:	Training iteration: 119600, Loss: 0.004280091729015112
2018-10-22 08:12:14.883984:	Training iteration: 119800, Loss: 0.0050271302461624146
2018-10-22 08:13:58.901885:	Training iteration: 120000, Loss: 0.00617217319086194
Checkpoint
2018-10-22 08:15:45.035455:	Training iteration: 120200, Loss: 0.004805454518646002
2018-10-22 08:17:29.081336:	Training iteration: 120400, Loss: 0.003651060163974762
2018-10-22 08:19:12.964762:	Training iteration: 120600, Loss: 0.004905844572931528
2018-10-22 08:20:55.443064:	Training iteration: 120800, Loss: 0.006375855300575495
2018-10-22 08:22:40.026772:	Training iteration: 121000, Loss: 0.004266895819455385
2018-10-22 08:24:23.938816:	Training iteration: 121200, Loss: 0.004772070795297623
2018-10-22 08:26:07.670651:	Training iteration: 121400, Loss: 0.004916600417345762
2018-10-22 08:27:50.304119:	Training iteration: 121600, Loss: 0.006192445755004883
2018-10-22 08:29:32.392145:	Training iteration: 121800, Loss: 0.006274659186601639
2018-10-22 08:31:16.431321:	Training iteration: 122000, Loss: 0.004559928551316261
2018-10-22 08:33:01.042183:	Training iteration: 122200, Loss: 0.00903030950576067
2018-10-22 08:34:45.591228:	Training iteration: 122400, Loss: 0.005325356964021921
2018-10-22 08:36:31.282309:	Training iteration: 122600, Loss: 0.005823708605021238
2018-10-22 08:38:18.033228:	Training iteration: 122800, Loss: 0.006784610450267792
2018-10-22 08:40:03.979214:	Training iteration: 123000, Loss: 0.009988218545913696
2018-10-22 08:41:51.419843:	Training iteration: 123200, Loss: 0.005057962145656347
2018-10-22 08:43:37.424033:	Training iteration: 123400, Loss: 0.003773221978917718
2018-10-22 08:45:23.249969:	Training iteration: 123600, Loss: 0.005259152036160231
2018-10-22 08:47:08.011069:	Training iteration: 123800, Loss: 0.003366057528182864
2018-10-22 08:48:52.664150:	Training iteration: 124000, Loss: 0.00500241806730628
2018-10-22 08:50:38.011014:	Training iteration: 124200, Loss: 0.006040697451680899
2018-10-22 08:52:21.427345:	Training iteration: 124400, Loss: 0.008804370649158955
2018-10-22 08:54:06.521430:	Training iteration: 124600, Loss: 0.004736247938126326
2018-10-22 08:55:50.684707:	Training iteration: 124800, Loss: 0.007046742830425501
2018-10-22 08:57:33.251256:	Training iteration: 125000, Loss: 0.007278007920831442
2018-10-22 08:59:17.306997:	Training iteration: 125200, Loss: 0.005028955172747374
2018-10-22 09:01:01.066479:	Training iteration: 125400, Loss: 0.006963036954402924
2018-10-22 09:02:46.215975:	Training iteration: 125600, Loss: 0.008279585279524326
2018-10-22 09:04:30.088017:	Training iteration: 125800, Loss: 0.004262290429323912
2018-10-22 09:06:13.704927:	Training iteration: 126000, Loss: 0.0066383820958435535
2018-10-22 09:07:57.108689:	Training iteration: 126200, Loss: 0.003969002980738878
2018-10-22 09:09:42.241468:	Training iteration: 126400, Loss: 0.006385814864188433
2018-10-22 09:11:24.797902:	Training iteration: 126600, Loss: 0.005933528300374746
2018-10-22 09:13:08.611825:	Training iteration: 126800, Loss: 0.0063968501053750515
2018-10-22 09:14:53.475673:	Training iteration: 127000, Loss: 0.0044780634343624115
2018-10-22 09:16:37.743233:	Training iteration: 127200, Loss: 0.006385338958352804
2018-10-22 09:18:22.260263:	Training iteration: 127400, Loss: 0.005762232001870871
2018-10-22 09:20:06.038617:	Training iteration: 127600, Loss: 0.0032692484091967344
2018-10-22 09:21:50.148029:	Training iteration: 127800, Loss: 0.004718191921710968
2018-10-22 09:23:34.781737:	Training iteration: 128000, Loss: 0.0066087860614061356
2018-10-22 09:25:18.088840:	Training iteration: 128200, Loss: 0.006654039025306702
2018-10-22 09:27:01.099401:	Training iteration: 128400, Loss: 0.005403528455644846
2018-10-22 09:28:44.876369:	Training iteration: 128600, Loss: 0.00831462349742651
2018-10-22 09:30:27.406490:	Training iteration: 128800, Loss: 0.005537562072277069
2018-10-22 09:32:10.067366:	Training iteration: 129000, Loss: 0.006795156747102737
2018-10-22 09:33:53.052893:	Training iteration: 129200, Loss: 0.00880294106900692
2018-10-22 09:35:36.871021:	Training iteration: 129400, Loss: 0.005420377943664789
2018-10-22 09:37:20.379565:	Training iteration: 129600, Loss: 0.006630655378103256
2018-10-22 09:39:03.158124:	Training iteration: 129800, Loss: 0.006929128896445036
2018-10-22 09:40:48.097909:	Training iteration: 130000, Loss: 0.007750466000288725
Checkpoint
2018-10-22 09:42:32.266035:	Training iteration: 130200, Loss: 0.0052608405239880085
2018-10-22 09:44:16.541003:	Training iteration: 130400, Loss: 0.0048646326176822186
2018-10-22 09:46:01.696452:	Training iteration: 130600, Loss: 0.006494118366390467
2018-10-22 09:47:46.985180:	Training iteration: 130800, Loss: 0.005817183759063482
2018-10-22 09:49:31.219008:	Training iteration: 131000, Loss: 0.008822176605463028
2018-10-22 09:51:15.849974:	Training iteration: 131200, Loss: 0.006352003663778305
2018-10-22 09:52:59.492549:	Training iteration: 131400, Loss: 0.0051276725716888905
2018-10-22 09:54:43.112534:	Training iteration: 131600, Loss: 0.004994450602680445
2018-10-22 09:56:28.271450:	Training iteration: 131800, Loss: 0.005926227662712336
2018-10-22 09:58:12.752824:	Training iteration: 132000, Loss: 0.008313721977174282
2018-10-22 09:59:56.173790:	Training iteration: 132200, Loss: 0.0053155794739723206
2018-10-22 10:01:41.709244:	Training iteration: 132400, Loss: 0.0053652264177799225
2018-10-22 10:03:26.341756:	Training iteration: 132600, Loss: 0.005383639130741358
2018-10-22 10:05:10.342447:	Training iteration: 132800, Loss: 0.004017689730972052
2018-10-22 10:06:55.404878:	Training iteration: 133000, Loss: 0.006021826062351465
2018-10-22 10:08:39.012244:	Training iteration: 133200, Loss: 0.004002690780907869
2018-10-22 10:10:22.839038:	Training iteration: 133400, Loss: 0.005556137766689062
2018-10-22 10:12:08.215119:	Training iteration: 133600, Loss: 0.007086575031280518
2018-10-22 10:13:51.065595:	Training iteration: 133800, Loss: 0.007682472001761198
2018-10-22 10:15:36.215171:	Training iteration: 134000, Loss: 0.005815097596496344
2018-10-22 10:17:20.489190:	Training iteration: 134200, Loss: 0.004478906746953726
2018-10-22 10:19:04.417802:	Training iteration: 134400, Loss: 0.005382988601922989
2018-10-22 10:20:46.876588:	Training iteration: 134600, Loss: 0.003318268805742264
2018-10-22 10:22:31.223436:	Training iteration: 134800, Loss: 0.004567671567201614
2018-10-22 10:24:14.781045:	Training iteration: 135000, Loss: 0.004162488039582968
2018-10-22 10:25:59.100861:	Training iteration: 135200, Loss: 0.004290225449949503
2018-10-22 10:27:41.929774:	Training iteration: 135400, Loss: 0.006878294050693512
2018-10-22 10:29:25.197817:	Training iteration: 135600, Loss: 0.006677133496850729
2018-10-22 10:31:08.706304:	Training iteration: 135800, Loss: 0.0056318421848118305
2018-10-22 10:32:51.748303:	Training iteration: 136000, Loss: 0.005651236977428198
2018-10-22 10:34:34.568389:	Training iteration: 136200, Loss: 0.004689743276685476
2018-10-22 10:36:17.567973:	Training iteration: 136400, Loss: 0.00823794025927782
2018-10-22 10:37:59.810430:	Training iteration: 136600, Loss: 0.008672752417623997
2018-10-22 10:39:42.919019:	Training iteration: 136800, Loss: 0.006982378661632538
2018-10-22 10:41:26.436709:	Training iteration: 137000, Loss: 0.004518297966569662
2018-10-22 10:43:09.363500:	Training iteration: 137200, Loss: 0.00631724065169692
2018-10-22 10:44:52.949198:	Training iteration: 137400, Loss: 0.005579243879765272
2018-10-22 10:46:36.586536:	Training iteration: 137600, Loss: 0.005427466705441475
2018-10-22 10:48:20.617104:	Training iteration: 137800, Loss: 0.0073791202157735825
2018-10-22 10:50:04.080408:	Training iteration: 138000, Loss: 0.004998130723834038
2018-10-22 10:51:48.025852:	Training iteration: 138200, Loss: 0.004579869564622641
2018-10-22 10:53:29.750587:	Training iteration: 138400, Loss: 0.004151348490267992
2018-10-22 10:55:13.886163:	Training iteration: 138600, Loss: 0.0031683091074228287
2018-10-22 10:56:56.589208:	Training iteration: 138800, Loss: 0.0050485869869589806
2018-10-22 10:58:39.645276:	Training iteration: 139000, Loss: 0.0057209753431379795
2018-10-22 11:00:22.634611:	Training iteration: 139200, Loss: 0.004467849154025316
2018-10-22 11:02:05.392602:	Training iteration: 139400, Loss: 0.005883061792701483
2018-10-22 11:03:47.323910:	Training iteration: 139600, Loss: 0.005334984511137009
2018-10-22 11:05:30.359967:	Training iteration: 139800, Loss: 0.004753293469548225
2018-10-22 11:07:12.012088:	Training iteration: 140000, Loss: 0.0036317892372608185
Checkpoint
2018-10-22 11:08:56.897812:	Training iteration: 140200, Loss: 0.004155741073191166
2018-10-22 11:10:39.917419:	Training iteration: 140400, Loss: 0.007689972873777151
2018-10-22 11:12:24.091164:	Training iteration: 140600, Loss: 0.0038016820326447487
2018-10-22 11:14:06.271949:	Training iteration: 140800, Loss: 0.003479684004560113
2018-10-22 11:15:49.360277:	Training iteration: 141000, Loss: 0.004479629453271627
2018-10-22 11:17:32.756811:	Training iteration: 141200, Loss: 0.004708310589194298
2018-10-22 11:19:16.586465:	Training iteration: 141400, Loss: 0.004054963123053312
2018-10-22 11:21:00.452608:	Training iteration: 141600, Loss: 0.008609510958194733
2018-10-22 11:22:43.144759:	Training iteration: 141800, Loss: 0.005221670959144831
2018-10-22 11:24:26.189402:	Training iteration: 142000, Loss: 0.003971954341977835
2018-10-22 11:26:09.632688:	Training iteration: 142200, Loss: 0.006251148413866758
2018-10-22 11:27:52.494354:	Training iteration: 142400, Loss: 0.007085777353495359
2018-10-22 11:29:35.828655:	Training iteration: 142600, Loss: 0.010655130259692669
2018-10-22 11:31:20.247532:	Training iteration: 142800, Loss: 0.006905729416757822
2018-10-22 11:33:04.561252:	Training iteration: 143000, Loss: 0.00589715363457799
2018-10-22 11:34:48.333982:	Training iteration: 143200, Loss: 0.00924630556255579
2018-10-22 11:36:33.622514:	Training iteration: 143400, Loss: 0.007214836776256561
2018-10-22 11:38:16.695110:	Training iteration: 143600, Loss: 0.0044238450936973095
2018-10-22 11:40:00.274382:	Training iteration: 143800, Loss: 0.007095595821738243
2018-10-22 11:41:43.611145:	Training iteration: 144000, Loss: 0.008293875493109226
2018-10-22 11:43:27.950953:	Training iteration: 144200, Loss: 0.009901865385472775
2018-10-22 11:45:13.101115:	Training iteration: 144400, Loss: 0.00608416460454464
2018-10-22 11:46:56.532728:	Training iteration: 144600, Loss: 0.005537820979952812
2018-10-22 11:48:41.293090:	Training iteration: 144800, Loss: 0.003950537648051977
2018-10-22 11:50:24.719682:	Training iteration: 145000, Loss: 0.005033465567976236
2018-10-22 11:52:08.378924:	Training iteration: 145200, Loss: 0.004737401846796274
2018-10-22 11:53:52.217545:	Training iteration: 145400, Loss: 0.003208161098882556
2018-10-22 11:55:36.155397:	Training iteration: 145600, Loss: 0.0037093565333634615
2018-10-22 11:57:20.133070:	Training iteration: 145800, Loss: 0.005290175322443247
2018-10-22 11:59:03.229083:	Training iteration: 146000, Loss: 0.005679893773049116
2018-10-22 12:00:47.722388:	Training iteration: 146200, Loss: 0.006183698773384094
2018-10-22 12:02:36.279117:	Training iteration: 146400, Loss: 0.004964565858244896
2018-10-22 12:04:21.102587:	Training iteration: 146600, Loss: 0.0052217659540474415
2018-10-22 12:06:04.543475:	Training iteration: 146800, Loss: 0.0062857321463525295
2018-10-22 12:07:46.622220:	Training iteration: 147000, Loss: 0.004865508992224932
2018-10-22 12:09:31.307956:	Training iteration: 147200, Loss: 0.005135819781571627
2018-10-22 12:11:14.135557:	Training iteration: 147400, Loss: 0.004055052530020475
2018-10-22 12:12:56.828206:	Training iteration: 147600, Loss: 0.008193482644855976
2018-10-22 12:14:39.629371:	Training iteration: 147800, Loss: 0.006268370896577835
2018-10-22 12:16:21.973986:	Training iteration: 148000, Loss: 0.005877370480448008
2018-10-22 12:18:05.435535:	Training iteration: 148200, Loss: 0.0034111745189875364
2018-10-22 12:19:46.786224:	Training iteration: 148400, Loss: 0.007131345570087433
2018-10-22 12:21:29.667779:	Training iteration: 148600, Loss: 0.004902899265289307
2018-10-22 12:23:13.031244:	Training iteration: 148800, Loss: 0.007940378971397877
2018-10-22 12:24:56.176926:	Training iteration: 149000, Loss: 0.005221080034971237
2018-10-22 12:26:38.552146:	Training iteration: 149200, Loss: 0.005780769977718592
2018-10-22 12:28:19.701719:	Training iteration: 149400, Loss: 0.006527113262563944
2018-10-22 12:30:02.187155:	Training iteration: 149600, Loss: 0.00617734482511878
2018-10-22 12:31:45.436119:	Training iteration: 149800, Loss: 0.004344369750469923
2018-10-22 12:33:27.758214:	Training iteration: 150000, Loss: 0.004308001603931189
Checkpoint
2018-10-22 12:35:11.739045:	Training iteration: 150200, Loss: 0.003861998440697789
2018-10-22 12:37:11.132831:	Training iteration: 150400, Loss: 0.004731586202979088
2018-10-22 12:38:46.820920:	Training iteration: 150600, Loss: 0.005563138518482447
2018-10-22 12:40:27.217634:	Training iteration: 150800, Loss: 0.004688514396548271
2018-10-22 12:42:08.246002:	Training iteration: 151000, Loss: 0.009119492955505848
2018-10-22 12:43:50.633284:	Training iteration: 151200, Loss: 0.0043808273039758205
2018-10-22 12:45:34.136078:	Training iteration: 151400, Loss: 0.006336860358715057
2018-10-22 12:47:16.585678:	Training iteration: 151600, Loss: 0.008823097683489323
2018-10-22 12:49:00.024560:	Training iteration: 151800, Loss: 0.005570759531110525
2018-10-22 12:50:45.996371:	Training iteration: 152000, Loss: 0.006029097363352776
2018-10-22 12:52:29.032241:	Training iteration: 152200, Loss: 0.004009409807622433
2018-10-22 12:54:12.631895:	Training iteration: 152400, Loss: 0.004380297381430864
2018-10-22 12:55:57.281494:	Training iteration: 152600, Loss: 0.0046631949953734875
2018-10-22 12:57:40.575243:	Training iteration: 152800, Loss: 0.00571015989407897
2018-10-22 12:59:25.038107:	Training iteration: 153000, Loss: 0.002624066546559334
2018-10-22 13:01:08.060062:	Training iteration: 153200, Loss: 0.007178951054811478
2018-10-22 13:02:50.961156:	Training iteration: 153400, Loss: 0.004721019417047501
2018-10-22 13:04:36.242920:	Training iteration: 153600, Loss: 0.004889692645519972
2018-10-22 13:06:18.769936:	Training iteration: 153800, Loss: 0.006830174475908279
2018-10-22 13:08:01.923568:	Training iteration: 154000, Loss: 0.005730328615754843
2018-10-22 13:09:44.666490:	Training iteration: 154200, Loss: 0.0046921526081860065
2018-10-22 13:11:27.312358:	Training iteration: 154400, Loss: 0.004369496833533049
2018-10-22 13:13:12.052204:	Training iteration: 154600, Loss: 0.005547253414988518
2018-10-22 13:14:56.320243:	Training iteration: 154800, Loss: 0.005661800038069487
2018-10-22 13:16:40.517164:	Training iteration: 155000, Loss: 0.005307493265718222
2018-10-22 13:18:24.752990:	Training iteration: 155200, Loss: 0.005426220595836639
2018-10-22 13:20:08.411679:	Training iteration: 155400, Loss: 0.008104448206722736
2018-10-22 13:21:53.574331:	Training iteration: 155600, Loss: 0.00744851678609848
2018-10-22 13:23:36.737423:	Training iteration: 155800, Loss: 0.005309304688125849
2018-10-22 13:25:19.938668:	Training iteration: 156000, Loss: 0.0059441071934998035
2018-10-22 13:27:04.803828:	Training iteration: 156200, Loss: 0.005153157282620668
2018-10-22 13:28:49.862420:	Training iteration: 156400, Loss: 0.00713927811011672
2018-10-22 13:30:34.568283:	Training iteration: 156600, Loss: 0.005019382108002901
2018-10-22 13:32:20.603927:	Training iteration: 156800, Loss: 0.004014423117041588
2018-10-22 13:34:04.414510:	Training iteration: 157000, Loss: 0.004024841357022524
2018-10-22 13:35:49.622181:	Training iteration: 157200, Loss: 0.004497996065765619
2018-10-22 13:37:34.095119:	Training iteration: 157400, Loss: 0.0063239592127501965
2018-10-22 13:39:15.816964:	Training iteration: 157600, Loss: 0.005705614108592272
2018-10-22 13:40:59.263772:	Training iteration: 157800, Loss: 0.006166770588606596
2018-10-22 13:42:43.100969:	Training iteration: 158000, Loss: 0.0033097295090556145
2018-10-22 13:44:26.516031:	Training iteration: 158200, Loss: 0.006149932276457548
2018-10-22 13:46:10.051551:	Training iteration: 158400, Loss: 0.004841126501560211
2018-10-22 13:47:53.339211:	Training iteration: 158600, Loss: 0.006565111223608255
2018-10-22 13:49:36.733980:	Training iteration: 158800, Loss: 0.005997361149638891
2018-10-22 13:51:18.641012:	Training iteration: 159000, Loss: 0.003821229562163353
2018-10-22 13:53:02.454192:	Training iteration: 159200, Loss: 0.004304928705096245
2018-10-22 13:54:45.109341:	Training iteration: 159400, Loss: 0.005719762295484543
2018-10-22 13:56:28.298919:	Training iteration: 159600, Loss: 0.004351674113422632
2018-10-22 13:58:12.340183:	Training iteration: 159800, Loss: 0.006702200975269079
2018-10-22 13:59:21.127486:	Epoch 1 finished after 159937 iterations.
Validating
2018-10-22 13:59:21.309205:	Entering validation loop
2018-10-22 13:59:31.343088: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 604 of 1000
2018-10-22 13:59:37.363961: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:00:16.236390:	Validation iteration: 200, Loss: 0.00789600145071745
2018-10-22 14:00:58.656180:	Validation iteration: 400, Loss: 0.006316151935607195
2018-10-22 14:01:42.325768:	Validation iteration: 600, Loss: 0.005670974496752024
2018-10-22 14:02:25.692602:	Validation iteration: 800, Loss: 0.005041914060711861
2018-10-22 14:03:11.601406:	Validation iteration: 1000, Loss: 0.008029120974242687
2018-10-22 14:03:54.865407:	Validation iteration: 1200, Loss: 0.006902220193296671
2018-10-22 14:04:46.355181: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 631 of 1000
2018-10-22 14:04:51.989474: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:04:53.611703:	Validation iteration: 1400, Loss: 0.006992841139435768
2018-10-22 14:05:32.395296:	Validation iteration: 1600, Loss: 0.007428698241710663
2018-10-22 14:06:14.612632:	Validation iteration: 1800, Loss: 0.007410941179841757
2018-10-22 14:06:57.398967:	Validation iteration: 2000, Loss: 0.007254500407725573
2018-10-22 14:07:41.464217:	Validation iteration: 2200, Loss: 0.005472511053085327
2018-10-22 14:08:25.357269:	Validation iteration: 2400, Loss: 0.006418462842702866
2018-10-22 14:09:08.361255:	Validation iteration: 2600, Loss: 0.006230778526514769
2018-10-22 14:09:58.076074: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 596 of 1000
2018-10-22 14:10:04.023486: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:10:07.155290:	Validation iteration: 2800, Loss: 0.006298007443547249
2018-10-22 14:10:46.127885:	Validation iteration: 3000, Loss: 0.007080326322466135
2018-10-22 14:11:28.704281:	Validation iteration: 3200, Loss: 0.006176235619932413
2018-10-22 14:12:12.251317:	Validation iteration: 3400, Loss: 0.007394677493721247
2018-10-22 14:12:55.832781:	Validation iteration: 3600, Loss: 0.005533245857805014
2018-10-22 14:13:39.555109:	Validation iteration: 3800, Loss: 0.0044950442388653755
2018-10-22 14:14:23.253040:	Validation iteration: 4000, Loss: 0.006061470601707697
2018-10-22 14:15:12.015329: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 603 of 1000
2018-10-22 14:15:17.657056: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:15:22.396618:	Validation iteration: 4200, Loss: 0.007193078752607107
2018-10-22 14:16:01.674773:	Validation iteration: 4400, Loss: 0.006727028638124466
2018-10-22 14:16:45.242986:	Validation iteration: 4600, Loss: 0.004572769161313772
2018-10-22 14:17:29.007164:	Validation iteration: 4800, Loss: 0.0076669566333293915
2018-10-22 14:18:12.980207:	Validation iteration: 5000, Loss: 0.005871823523193598
2018-10-22 14:18:57.022178:	Validation iteration: 5200, Loss: 0.008370761759579182
2018-10-22 14:19:41.964789:	Validation iteration: 5400, Loss: 0.007698278408497572
2018-10-22 14:20:28.352348: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 561 of 1000
2018-10-22 14:20:35.008847: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:20:41.118612:	Validation iteration: 5600, Loss: 0.007864672690629959
2018-10-22 14:21:20.772900:	Validation iteration: 5800, Loss: 0.006457535084336996
2018-10-22 14:22:03.621203:	Validation iteration: 6000, Loss: 0.005662186071276665
2018-10-22 14:22:46.670012:	Validation iteration: 6200, Loss: 0.0033240194898098707
2018-10-22 14:23:29.031627:	Validation iteration: 6400, Loss: 0.00432561943307519
2018-10-22 14:24:13.206153:	Validation iteration: 6600, Loss: 0.004781050607562065
2018-10-22 14:24:56.710703:	Validation iteration: 6800, Loss: 0.005963955540210009
2018-10-22 14:25:41.493406:	Validation iteration: 7000, Loss: 0.004778286907821894
2018-10-22 14:26:24.960231:	Validation iteration: 7200, Loss: 0.004841184243559837
2018-10-22 14:27:08.871169:	Validation iteration: 7400, Loss: 0.003809058340266347
Validation check mean loss: 0.005959838641531444
Validation loss has improved!
New best validation cost!
2018-10-22 14:27:40.540500: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 679 of 1000
2018-10-22 14:27:44.744681: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:28:13.749042:	Training iteration: 160000, Loss: 0.004967920016497374
Checkpoint
2018-10-22 14:29:57.098449:	Training iteration: 160200, Loss: 0.0038224151358008385
2018-10-22 14:31:41.003385:	Training iteration: 160400, Loss: 0.0036995727568864822
2018-10-22 14:33:26.189107:	Training iteration: 160600, Loss: 0.005197481717914343
2018-10-22 14:35:09.634974:	Training iteration: 160800, Loss: 0.0038199082482606173
2018-10-22 14:36:53.977346:	Training iteration: 161000, Loss: 0.005632750224322081
2018-10-22 14:38:37.721572:	Training iteration: 161200, Loss: 0.004574358928948641
2018-10-22 14:40:22.820207:	Training iteration: 161400, Loss: 0.0039780656807124615
2018-10-22 14:42:05.012691:	Training iteration: 161600, Loss: 0.005054083187133074
2018-10-22 14:43:48.926913:	Training iteration: 161800, Loss: 0.004156568553298712
2018-10-22 14:45:33.455706:	Training iteration: 162000, Loss: 0.00452291639521718
2018-10-22 14:47:18.322761:	Training iteration: 162200, Loss: 0.005430700723081827
2018-10-22 14:49:02.600503:	Training iteration: 162400, Loss: 0.005176343489438295
2018-10-22 14:50:46.639333:	Training iteration: 162600, Loss: 0.00515827676281333
2018-10-22 14:52:31.939971:	Training iteration: 162800, Loss: 0.002843550406396389
2018-10-22 14:54:16.219253:	Training iteration: 163000, Loss: 0.0038279565051198006
2018-10-22 14:56:01.592540:	Training iteration: 163200, Loss: 0.005409928038716316
2018-10-22 14:57:44.405819:	Training iteration: 163400, Loss: 0.005109114572405815
2018-10-22 14:59:29.745958:	Training iteration: 163600, Loss: 0.0035168828908354044
2018-10-22 15:01:15.529889:	Training iteration: 163800, Loss: 0.003992476966232061
2018-10-22 15:03:00.553034:	Training iteration: 164000, Loss: 0.0033974095713347197
2018-10-22 15:04:44.301910:	Training iteration: 164200, Loss: 0.0038649237249046564
2018-10-22 15:06:28.850317:	Training iteration: 164400, Loss: 0.0061081149615347385
2018-10-22 15:08:13.616490:	Training iteration: 164600, Loss: 0.003027735510841012
2018-10-22 15:09:56.362299:	Training iteration: 164800, Loss: 0.002359120175242424
2018-10-22 15:11:39.495516:	Training iteration: 165000, Loss: 0.004739950876682997
2018-10-22 15:13:22.001303:	Training iteration: 165200, Loss: 0.006050227675586939
2018-10-22 15:15:04.872391:	Training iteration: 165400, Loss: 0.006415870040655136
2018-10-22 15:16:48.064793:	Training iteration: 165600, Loss: 0.0038327977526932955
2018-10-22 15:18:30.997602:	Training iteration: 165800, Loss: 0.005216527730226517
2018-10-22 15:20:13.073305:	Training iteration: 166000, Loss: 0.0038992695044726133
2018-10-22 15:21:56.196761:	Training iteration: 166200, Loss: 0.0043610683642327785
2018-10-22 15:23:39.512757:	Training iteration: 166400, Loss: 0.005529490765184164
2018-10-22 15:25:22.415161:	Training iteration: 166600, Loss: 0.0027498577255755663
2018-10-22 15:27:06.255769:	Training iteration: 166800, Loss: 0.004772060550749302
2018-10-22 15:28:49.348003:	Training iteration: 167000, Loss: 0.00567970983684063
2018-10-22 15:30:33.106213:	Training iteration: 167200, Loss: 0.00474562868475914
2018-10-22 15:32:15.800769:	Training iteration: 167400, Loss: 0.006245551165193319
2018-10-22 15:33:58.159095:	Training iteration: 167600, Loss: 0.004054997116327286
2018-10-22 15:35:43.579085:	Training iteration: 167800, Loss: 0.004686631262302399
2018-10-22 15:37:29.508469:	Training iteration: 168000, Loss: 0.004886115901172161
2018-10-22 15:39:15.284472:	Training iteration: 168200, Loss: 0.004758866038173437
2018-10-22 15:39:25.344074: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 656 of 1000
2018-10-22 15:39:29.983026: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 15:41:09.346561:	Training iteration: 168400, Loss: 0.006943332497030497
2018-10-22 15:42:54.431550:	Training iteration: 168600, Loss: 0.006024142261594534
2018-10-22 15:44:39.963061:	Training iteration: 168800, Loss: 0.005139817018061876
2018-10-22 15:46:24.074273:	Training iteration: 169000, Loss: 0.004888537805527449
2018-10-22 15:48:08.267445:	Training iteration: 169200, Loss: 0.004275248385965824
2018-10-22 15:49:52.242453:	Training iteration: 169400, Loss: 0.0037531505804508924
2018-10-22 15:51:40.713918:	Training iteration: 169600, Loss: 0.004363650921732187
2018-10-22 15:53:28.283925:	Training iteration: 169800, Loss: 0.005162922665476799
2018-10-22 15:55:15.412924:	Training iteration: 170000, Loss: 0.00597212603315711
Checkpoint
2018-10-22 15:57:02.190704:	Training iteration: 170200, Loss: 0.00684186490252614
2018-10-22 15:58:47.815916:	Training iteration: 170400, Loss: 0.00415662070736289
2018-10-22 16:00:33.100854:	Training iteration: 170600, Loss: 0.0058450959622859955
2018-10-22 16:02:17.960439:	Training iteration: 170800, Loss: 0.004312989767640829
2018-10-22 16:04:02.405404:	Training iteration: 171000, Loss: 0.004182890523225069
2018-10-22 16:05:46.362737:	Training iteration: 171200, Loss: 0.004034759011119604
2018-10-22 16:07:30.921954:	Training iteration: 171400, Loss: 0.006417388562113047
2018-10-22 16:09:17.934329:	Training iteration: 171600, Loss: 0.004506519064307213
2018-10-22 16:11:02.948454:	Training iteration: 171800, Loss: 0.006430823355913162
2018-10-22 16:12:49.426067:	Training iteration: 172000, Loss: 0.007324788719415665
2018-10-22 16:14:34.964947:	Training iteration: 172200, Loss: 0.0049799284897744656
2018-10-22 16:16:19.088089:	Training iteration: 172400, Loss: 0.005005190148949623
2018-10-22 16:18:04.628881:	Training iteration: 172600, Loss: 0.004410557448863983
2018-10-22 16:19:47.798301:	Training iteration: 172800, Loss: 0.005784634500741959
2018-10-22 16:21:31.894680:	Training iteration: 173000, Loss: 0.005819657351821661
2018-10-22 16:23:16.332193:	Training iteration: 173200, Loss: 0.004650356713682413
2018-10-22 16:25:02.185406:	Training iteration: 173400, Loss: 0.007141027599573135
2018-10-22 16:26:47.720298:	Training iteration: 173600, Loss: 0.005495724733918905
2018-10-22 16:28:33.199592:	Training iteration: 173800, Loss: 0.0039004564750939608
2018-10-22 16:30:19.273045:	Training iteration: 174000, Loss: 0.0047957440838217735
2018-10-22 16:32:03.970020:	Training iteration: 174200, Loss: 0.004804177209734917
2018-10-22 16:33:48.950158:	Training iteration: 174400, Loss: 0.004501855932176113
2018-10-22 16:35:33.937709:	Training iteration: 174600, Loss: 0.00560313044115901
2018-10-22 16:37:17.878813:	Training iteration: 174800, Loss: 0.004381267819553614
2018-10-22 16:39:02.786550:	Training iteration: 175000, Loss: 0.0044662863947451115
2018-10-22 16:40:47.257378:	Training iteration: 175200, Loss: 0.005151951219886541
2018-10-22 16:42:30.425334:	Training iteration: 175400, Loss: 0.003443480236455798
2018-10-22 16:44:14.717640:	Training iteration: 175600, Loss: 0.006352428812533617
2018-10-22 16:45:58.409511:	Training iteration: 175800, Loss: 0.004635939374566078
2018-10-22 16:47:42.712699:	Training iteration: 176000, Loss: 0.005060413386672735
2018-10-22 16:49:24.760010:	Training iteration: 176200, Loss: 0.0034135766327381134
2018-10-22 16:51:06.930240:	Training iteration: 176400, Loss: 0.0032247246708720922
2018-10-22 16:52:14.782659: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 644 of 1000
2018-10-22 16:52:19.394314: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 16:53:00.121276:	Training iteration: 176600, Loss: 0.004422538448125124
2018-10-22 16:54:42.776213:	Training iteration: 176800, Loss: 0.004635240416973829
2018-10-22 16:56:24.964173:	Training iteration: 177000, Loss: 0.0038374552968889475
2018-10-22 16:58:07.819203:	Training iteration: 177200, Loss: 0.005641559604555368
2018-10-22 16:59:50.944573:	Training iteration: 177400, Loss: 0.003835557261481881
2018-10-22 17:01:34.819388:	Training iteration: 177600, Loss: 0.006157842930406332
2018-10-22 17:03:19.326549:	Training iteration: 177800, Loss: 0.004848645534366369
2018-10-22 17:05:02.467880:	Training iteration: 178000, Loss: 0.004954085219651461
2018-10-22 17:06:46.147974:	Training iteration: 178200, Loss: 0.005639944225549698
2018-10-22 17:08:31.291599:	Training iteration: 178400, Loss: 0.0048674121499061584
2018-10-22 17:10:16.778600:	Training iteration: 178600, Loss: 0.004809423815459013
2018-10-22 17:12:01.755317:	Training iteration: 178800, Loss: 0.003602653741836548
2018-10-22 17:13:48.170022:	Training iteration: 179000, Loss: 0.004415441304445267
2018-10-22 17:15:33.099073:	Training iteration: 179200, Loss: 0.004182992968708277
2018-10-22 17:17:17.446368:	Training iteration: 179400, Loss: 0.004394222982227802
2018-10-22 17:19:00.306475:	Training iteration: 179600, Loss: 0.004109052941203117
2018-10-22 17:20:46.955585:	Training iteration: 179800, Loss: 0.005876276642084122
2018-10-22 17:22:32.831051:	Training iteration: 180000, Loss: 0.0044904653914272785
Checkpoint
2018-10-22 17:24:19.412153:	Training iteration: 180200, Loss: 0.0046472144313156605
2018-10-22 17:26:05.377653:	Training iteration: 180400, Loss: 0.00444156676530838
2018-10-22 17:27:48.662311:	Training iteration: 180600, Loss: 0.0046587237156927586
2018-10-22 17:29:33.185029:	Training iteration: 180800, Loss: 0.007131517399102449
2018-10-22 17:31:17.363636:	Training iteration: 181000, Loss: 0.005821188446134329
2018-10-22 17:33:02.388516:	Training iteration: 181200, Loss: 0.004820871166884899
2018-10-22 17:34:48.027604:	Training iteration: 181400, Loss: 0.007044267375022173
2018-10-22 17:36:30.967043:	Training iteration: 181600, Loss: 0.004639497492462397
2018-10-22 17:38:14.396822:	Training iteration: 181800, Loss: 0.006547898519784212
2018-10-22 17:39:59.265729:	Training iteration: 182000, Loss: 0.004084499087184668
2018-10-22 17:41:50.307710:	Training iteration: 182200, Loss: 0.0036520594730973244
2018-10-22 17:43:34.680609:	Training iteration: 182400, Loss: 0.006483849138021469
2018-10-22 17:45:21.031014:	Training iteration: 182600, Loss: 0.005006775725632906
2018-10-22 17:47:04.878508:	Training iteration: 182800, Loss: 0.005234507378190756
2018-10-22 17:48:50.820343:	Training iteration: 183000, Loss: 0.005678211804479361
2018-10-22 17:50:33.961268:	Training iteration: 183200, Loss: 0.005161674227565527
2018-10-22 17:52:00.598971:	Training iteration: 183400, Loss: 0.004656398668885231
2018-10-22 17:53:26.421830:	Training iteration: 183600, Loss: 0.004843438975512981
2018-10-22 17:54:51.978394:	Training iteration: 183800, Loss: 0.0049076019786298275
2018-10-22 17:56:17.292069:	Training iteration: 184000, Loss: 0.006588019896298647
2018-10-22 17:57:43.282857:	Training iteration: 184200, Loss: 0.00442681647837162
2018-10-22 17:59:09.081035:	Training iteration: 184400, Loss: 0.0050370884127914906
2018-10-22 18:00:35.468326:	Training iteration: 184600, Loss: 0.003113768994808197
2018-10-22 18:02:01.643893:	Training iteration: 184800, Loss: 0.0045522525906562805
2018-10-22 18:02:19.420605: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 582 of 1000
2018-10-22 18:02:24.627921: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 18:03:42.982064:	Training iteration: 185000, Loss: 0.004982894752174616
2018-10-22 18:05:09.038910:	Training iteration: 185200, Loss: 0.0047037131153047085
2018-10-22 18:06:34.369976:	Training iteration: 185400, Loss: 0.0029467802960425615
2018-10-22 18:08:00.432037:	Training iteration: 185600, Loss: 0.0031843383330851793
2018-10-22 18:09:26.341377:	Training iteration: 185800, Loss: 0.004527072887867689
2018-10-22 18:10:51.698420:	Training iteration: 186000, Loss: 0.003193162614479661
2018-10-22 18:12:17.737566:	Training iteration: 186200, Loss: 0.003977564629167318
2018-10-22 18:13:44.181632:	Training iteration: 186400, Loss: 0.004444799851626158
2018-10-22 18:15:09.520786:	Training iteration: 186600, Loss: 0.004038509447127581
2018-10-22 18:16:35.586999:	Training iteration: 186800, Loss: 0.0034721686970442533
2018-10-22 18:18:01.182684:	Training iteration: 187000, Loss: 0.0037624321412295103
2018-10-22 18:19:26.632953:	Training iteration: 187200, Loss: 0.003873566398397088
2018-10-22 18:20:52.228765:	Training iteration: 187400, Loss: 0.005945775657892227
2018-10-22 18:22:18.030857:	Training iteration: 187600, Loss: 0.004709774162620306
2018-10-22 18:23:42.759261:	Training iteration: 187800, Loss: 0.0030847780872136354
2018-10-22 18:25:08.120327:	Training iteration: 188000, Loss: 0.005908682476729155
2018-10-22 18:26:33.334187:	Training iteration: 188200, Loss: 0.0030522493179887533
2018-10-22 18:27:58.491227:	Training iteration: 188400, Loss: 0.0033419046085327864
2018-10-22 18:29:23.462955:	Training iteration: 188600, Loss: 0.0029307848308235407
2018-10-22 18:30:48.832073:	Training iteration: 188800, Loss: 0.0029367816168814898
2018-10-22 18:32:14.427589:	Training iteration: 189000, Loss: 0.004067862406373024
2018-10-22 18:33:39.541747:	Training iteration: 189200, Loss: 0.004509581718593836
2018-10-22 18:35:04.401515:	Training iteration: 189400, Loss: 0.004257263150066137
2018-10-22 18:36:29.494051:	Training iteration: 189600, Loss: 0.004909955430775881
2018-10-22 18:37:54.803699:	Training iteration: 189800, Loss: 0.00466722110286355
2018-10-22 18:39:19.522849:	Training iteration: 190000, Loss: 0.0045973458327353
Checkpoint
2018-10-22 18:40:48.903636:	Training iteration: 190200, Loss: 0.0028304271399974823
2018-10-22 18:42:14.002665:	Training iteration: 190400, Loss: 0.00280401180498302
2018-10-22 18:43:38.933099:	Training iteration: 190600, Loss: 0.004181666299700737
2018-10-22 18:45:03.148031:	Training iteration: 190800, Loss: 0.0021356248762458563
2018-10-22 18:46:28.876369:	Training iteration: 191000, Loss: 0.003698676824569702
2018-10-22 18:47:54.103643:	Training iteration: 191200, Loss: 0.004406111780554056
2018-10-22 18:49:18.728541:	Training iteration: 191400, Loss: 0.004204389173537493
2018-10-22 18:50:43.539804:	Training iteration: 191600, Loss: 0.004134702030569315
2018-10-22 18:52:08.852216:	Training iteration: 191800, Loss: 0.003924528602510691
2018-10-22 18:53:34.980819:	Training iteration: 192000, Loss: 0.004685206804424524
2018-10-22 18:55:00.456100:	Training iteration: 192200, Loss: 0.004014583304524422
2018-10-22 18:56:25.873496:	Training iteration: 192400, Loss: 0.0033836814109236
2018-10-22 18:57:50.885696:	Training iteration: 192600, Loss: 0.004023750312626362
2018-10-22 18:59:16.611119:	Training iteration: 192800, Loss: 0.003744391957297921
2018-10-22 19:00:41.580904:	Training iteration: 193000, Loss: 0.003227242035791278
2018-10-22 19:02:09.776069:	Training iteration: 193200, Loss: 0.0032958395313471556
2018-10-22 19:03:34.276830:	Training iteration: 193400, Loss: 0.004884313326328993
2018-10-22 19:04:26.075370: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 831 of 1000
2018-10-22 19:04:27.969259: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 19:05:10.297232:	Training iteration: 193600, Loss: 0.008501417003571987
2018-10-22 19:06:34.535549:	Training iteration: 193800, Loss: 0.005680564325302839
2018-10-22 19:07:59.502013:	Training iteration: 194000, Loss: 0.006030000746250153
2018-10-22 19:09:24.234159:	Training iteration: 194200, Loss: 0.004535766784101725
2018-10-22 19:10:48.601995:	Training iteration: 194400, Loss: 0.005717841442674398
2018-10-22 19:12:12.617771:	Training iteration: 194600, Loss: 0.0035005996469408274
2018-10-22 19:13:38.263694:	Training iteration: 194800, Loss: 0.005093633662909269
2018-10-22 19:15:02.773890:	Training iteration: 195000, Loss: 0.003312427317723632
2018-10-22 19:16:27.362450:	Training iteration: 195200, Loss: 0.0031308706384152174
2018-10-22 19:17:52.401859:	Training iteration: 195400, Loss: 0.00720421364530921
2018-10-22 19:19:16.462408:	Training iteration: 195600, Loss: 0.004773878492414951
2018-10-22 19:20:40.150615:	Training iteration: 195800, Loss: 0.006910009775310755
2018-10-22 19:22:04.275290:	Training iteration: 196000, Loss: 0.009199744090437889
2018-10-22 19:23:28.170525:	Training iteration: 196200, Loss: 0.0038376152515411377
2018-10-22 19:24:52.124618:	Training iteration: 196400, Loss: 0.0031473066192120314
2018-10-22 19:26:15.554785:	Training iteration: 196600, Loss: 0.003526482032611966
2018-10-22 19:27:39.426165:	Training iteration: 196800, Loss: 0.004641483072191477
2018-10-22 19:29:02.975916:	Training iteration: 197000, Loss: 0.004401260521262884
2018-10-22 19:30:26.842508:	Training iteration: 197200, Loss: 0.004530655685812235
2018-10-22 19:31:50.900172:	Training iteration: 197400, Loss: 0.007565113250166178
2018-10-22 19:33:14.735206:	Training iteration: 197600, Loss: 0.005066162906587124
2018-10-22 19:34:37.900792:	Training iteration: 197800, Loss: 0.007169836666435003
2018-10-22 19:36:01.886435:	Training iteration: 198000, Loss: 0.004777607973664999
2018-10-22 19:37:25.574714:	Training iteration: 198200, Loss: 0.004868000280112028
2018-10-22 19:38:49.457764:	Training iteration: 198400, Loss: 0.004060814622789621
2018-10-22 19:40:13.166883:	Training iteration: 198600, Loss: 0.00323843932710588
2018-10-22 19:41:38.221607:	Training iteration: 198800, Loss: 0.008457023650407791
2018-10-22 19:43:02.948289:	Training iteration: 199000, Loss: 0.004426749888807535
2018-10-22 19:44:27.591429:	Training iteration: 199200, Loss: 0.005043426528573036
2018-10-22 19:45:52.378851:	Training iteration: 199400, Loss: 0.0036659620236605406
2018-10-22 19:47:17.340940:	Training iteration: 199600, Loss: 0.004168665502220392
2018-10-22 19:48:41.884749:	Training iteration: 199800, Loss: 0.004198736045509577
2018-10-22 19:50:06.729460:	Training iteration: 200000, Loss: 0.005620906595140696
Checkpoint
2018-10-22 19:51:33.785499:	Training iteration: 200200, Loss: 0.002796148881316185
2018-10-22 19:52:58.751274:	Training iteration: 200400, Loss: 0.00456504849717021
2018-10-22 19:54:23.524317:	Training iteration: 200600, Loss: 0.00459279352799058
2018-10-22 19:55:48.447188:	Training iteration: 200800, Loss: 0.006232539191842079
2018-10-22 19:57:12.995578:	Training iteration: 201000, Loss: 0.004626626614481211
2018-10-22 19:58:38.113813:	Training iteration: 201200, Loss: 0.0048145572654902935
2018-10-22 20:00:02.341259:	Training iteration: 201400, Loss: 0.004930260591208935
2018-10-22 20:01:26.534643:	Training iteration: 201600, Loss: 0.0040248967707157135
2018-10-22 20:02:51.301555:	Training iteration: 201800, Loss: 0.003629007376730442
2018-10-22 20:04:15.937877:	Training iteration: 202000, Loss: 0.004909526091068983
2018-10-22 20:05:40.428641:	Training iteration: 202200, Loss: 0.003225088119506836
2018-10-22 20:07:05.275852:	Training iteration: 202400, Loss: 0.0036345943808555603
2018-10-22 20:08:29.417970:	Training iteration: 202600, Loss: 0.006463551428169012
2018-10-22 20:09:53.953533:	Training iteration: 202800, Loss: 0.004938914440572262
2018-10-22 20:11:19.351915:	Training iteration: 203000, Loss: 0.007254272699356079
2018-10-22 20:12:43.566296:	Training iteration: 203200, Loss: 0.007768627256155014
2018-10-22 20:14:07.993372:	Training iteration: 203400, Loss: 0.004260613117367029
2018-10-22 20:15:32.219110:	Training iteration: 203600, Loss: 0.0036840650718659163
2018-10-22 20:16:56.458028:	Training iteration: 203800, Loss: 0.005483461078256369
2018-10-22 20:18:20.642897:	Training iteration: 204000, Loss: 0.0086913937702775
2018-10-22 20:19:44.414008:	Training iteration: 204200, Loss: 0.007501245941966772
2018-10-22 20:21:08.907634:	Training iteration: 204400, Loss: 0.006057223305106163
2018-10-22 20:22:33.161491:	Training iteration: 204600, Loss: 0.008356347680091858
2018-10-22 20:23:57.385777:	Training iteration: 204800, Loss: 0.007785780355334282
2018-10-22 20:25:21.677502:	Training iteration: 205000, Loss: 0.005926682148128748
2018-10-22 20:26:45.331362:	Training iteration: 205200, Loss: 0.0059516276232898235
2018-10-22 20:28:10.052048:	Training iteration: 205400, Loss: 0.006105313543230295
2018-10-22 20:29:34.234013:	Training iteration: 205600, Loss: 0.006301999092102051
2018-10-22 20:30:58.100001:	Training iteration: 205800, Loss: 0.0042399014346301556
2018-10-22 20:32:21.540379:	Training iteration: 206000, Loss: 0.0034585464745759964
2018-10-22 20:33:45.925098:	Training iteration: 206200, Loss: 0.0036996507551521063
2018-10-22 20:35:10.411430:	Training iteration: 206400, Loss: 0.007961983792483807
2018-10-22 20:36:34.912956:	Training iteration: 206600, Loss: 0.006784370634704828
2018-10-22 20:37:59.168841:	Training iteration: 206800, Loss: 0.007691498380154371
2018-10-22 20:39:23.773417:	Training iteration: 207000, Loss: 0.005524723324924707
2018-10-22 20:40:48.553698:	Training iteration: 207200, Loss: 0.0064461734145879745
2018-10-22 20:42:12.343242:	Training iteration: 207400, Loss: 0.00505093252286315
2018-10-22 20:43:37.170893:	Training iteration: 207600, Loss: 0.006437980104237795
2018-10-22 20:45:02.311561:	Training iteration: 207800, Loss: 0.0046639940701425076
2018-10-22 20:46:26.854906:	Training iteration: 208000, Loss: 0.0064167059026658535
2018-10-22 20:47:51.281009:	Training iteration: 208200, Loss: 0.0067402333952486515
2018-10-22 20:49:16.506216:	Training iteration: 208400, Loss: 0.0038938287179917097
2018-10-22 20:50:40.856048:	Training iteration: 208600, Loss: 0.008214162662625313
2018-10-22 20:52:05.826340:	Training iteration: 208800, Loss: 0.004318188410252333
2018-10-22 20:53:30.574771:	Training iteration: 209000, Loss: 0.007385519798845053
2018-10-22 20:54:54.752462:	Training iteration: 209200, Loss: 0.007022510748356581
2018-10-22 20:56:19.717464:	Training iteration: 209400, Loss: 0.0096202427521348
2018-10-22 20:57:44.801596:	Training iteration: 209600, Loss: 0.005939988419413567
2018-10-22 20:59:09.526949:	Training iteration: 209800, Loss: 0.0038912321906536818
2018-10-22 21:00:34.305806:	Training iteration: 210000, Loss: 0.005042810458689928
Checkpoint
2018-10-22 21:02:02.535776:	Training iteration: 210200, Loss: 0.005715858191251755
2018-10-22 21:03:27.444151:	Training iteration: 210400, Loss: 0.0048684109933674335
2018-10-22 21:04:51.698872:	Training iteration: 210600, Loss: 0.004625402856618166
2018-10-22 21:06:17.984791:	Training iteration: 210800, Loss: 0.006950384471565485
2018-10-22 21:07:42.463277:	Training iteration: 211000, Loss: 0.004859786480665207
2018-10-22 21:09:06.925150:	Training iteration: 211200, Loss: 0.006422135513275862
2018-10-22 21:10:30.819157:	Training iteration: 211400, Loss: 0.004775234032422304
2018-10-22 21:11:55.021964:	Training iteration: 211600, Loss: 0.006379814352840185
2018-10-22 21:13:19.200562:	Training iteration: 211800, Loss: 0.009187125600874424
2018-10-22 21:14:43.384721:	Training iteration: 212000, Loss: 0.008048065938055515
2018-10-22 21:16:07.332317:	Training iteration: 212200, Loss: 0.003798219608142972
2018-10-22 21:17:31.085812:	Training iteration: 212400, Loss: 0.006714186165481806
2018-10-22 21:18:55.120395:	Training iteration: 212600, Loss: 0.00397301884368062
2018-10-22 21:20:18.543882:	Training iteration: 212800, Loss: 0.005537411663681269
2018-10-22 21:21:42.636397:	Training iteration: 213000, Loss: 0.003910867962986231
2018-10-22 21:23:06.367492:	Training iteration: 213200, Loss: 0.0043647377751767635
2018-10-22 21:24:30.573993:	Training iteration: 213400, Loss: 0.004468109458684921
2018-10-22 21:25:55.033580:	Training iteration: 213600, Loss: 0.0050553516484797
2018-10-22 21:27:18.639747:	Training iteration: 213800, Loss: 0.005022482480853796
2018-10-22 21:28:42.466349:	Training iteration: 214000, Loss: 0.0038046122062951326
2018-10-22 21:30:06.349242:	Training iteration: 214200, Loss: 0.0042158085852861404
2018-10-22 21:31:30.441061:	Training iteration: 214400, Loss: 0.00552491145208478
2018-10-22 21:32:54.764143:	Training iteration: 214600, Loss: 0.0031064434442669153
2018-10-22 21:34:19.156919:	Training iteration: 214800, Loss: 0.005863219499588013
2018-10-22 21:35:43.349398:	Training iteration: 215000, Loss: 0.006567607168108225
2018-10-22 21:37:07.730455:	Training iteration: 215200, Loss: 0.004575433675199747
2018-10-22 21:38:32.880491:	Training iteration: 215400, Loss: 0.0052959248423576355
2018-10-22 21:39:57.274802:	Training iteration: 215600, Loss: 0.006136032287031412
2018-10-22 21:41:20.866960:	Training iteration: 215800, Loss: 0.004808933008462191
2018-10-22 21:42:45.606186:	Training iteration: 216000, Loss: 0.004629557020962238
2018-10-22 21:44:10.469302:	Training iteration: 216200, Loss: 0.005968574434518814
2018-10-22 21:45:35.146375:	Training iteration: 216400, Loss: 0.005399256944656372
2018-10-22 21:46:59.560666:	Training iteration: 216600, Loss: 0.004379883408546448
2018-10-22 21:48:24.248783:	Training iteration: 216800, Loss: 0.004560787696391344
2018-10-22 21:49:48.348502:	Training iteration: 217000, Loss: 0.004637678619474173
2018-10-22 21:51:13.188778:	Training iteration: 217200, Loss: 0.005031564738601446
2018-10-22 21:52:37.711285:	Training iteration: 217400, Loss: 0.005742558743804693
2018-10-22 21:54:02.498249:	Training iteration: 217600, Loss: 0.00431009940803051
2018-10-22 21:55:27.423887:	Training iteration: 217800, Loss: 0.0037559110205620527
2018-10-22 21:56:51.796853:	Training iteration: 218000, Loss: 0.0036029843613505363
2018-10-22 21:58:15.981195:	Training iteration: 218200, Loss: 0.005018456373363733
2018-10-22 21:59:40.621787:	Training iteration: 218400, Loss: 0.005333427339792252
2018-10-22 22:01:05.193099:	Training iteration: 218600, Loss: 0.0062921177595853806
2018-10-22 22:02:29.187433:	Training iteration: 218800, Loss: 0.006866893265396357
2018-10-22 22:03:53.782386:	Training iteration: 219000, Loss: 0.00362461362965405
2018-10-22 22:05:17.543013:	Training iteration: 219200, Loss: 0.0052233911119401455
2018-10-22 22:06:41.482017:	Training iteration: 219400, Loss: 0.004871805664151907
2018-10-22 22:08:05.830408:	Training iteration: 219600, Loss: 0.004689177963882685
2018-10-22 22:09:29.630387:	Training iteration: 219800, Loss: 0.005559112410992384
2018-10-22 22:10:53.690598:	Training iteration: 220000, Loss: 0.003987676464021206
Checkpoint
2018-10-22 22:12:20.774423:	Training iteration: 220200, Loss: 0.009549219161272049
2018-10-22 22:13:44.292998:	Training iteration: 220400, Loss: 0.009385941550135612
2018-10-22 22:15:07.990963:	Training iteration: 220600, Loss: 0.0036765504628419876
2018-10-22 22:16:32.341641:	Training iteration: 220800, Loss: 0.00740438885986805
2018-10-22 22:17:55.766566:	Training iteration: 221000, Loss: 0.0064142742194235325
2018-10-22 22:19:19.503617:	Training iteration: 221200, Loss: 0.004213748499751091
2018-10-22 22:20:42.996037:	Training iteration: 221400, Loss: 0.008393974974751472
2018-10-22 22:22:07.273333:	Training iteration: 221600, Loss: 0.006134297698736191
2018-10-22 22:23:31.593766:	Training iteration: 221800, Loss: 0.004021437373012304
2018-10-22 22:24:55.659382:	Training iteration: 222000, Loss: 0.004853275138884783
2018-10-22 22:26:19.985416:	Training iteration: 222200, Loss: 0.003932794090360403
2018-10-22 22:27:43.759415:	Training iteration: 222400, Loss: 0.004601737949997187
2018-10-22 22:29:08.550120:	Training iteration: 222600, Loss: 0.004873621743172407
2018-10-22 22:30:33.535463:	Training iteration: 222800, Loss: 0.012196180410683155
2018-10-22 22:31:58.504005:	Training iteration: 223000, Loss: 0.009855369105935097
2018-10-22 22:33:23.450946:	Training iteration: 223200, Loss: 0.0061146654188632965
2018-10-22 22:34:47.478396:	Training iteration: 223400, Loss: 0.006123846862465143
2018-10-22 22:36:11.919743:	Training iteration: 223600, Loss: 0.0049938359297811985
2018-10-22 22:37:37.437216:	Training iteration: 223800, Loss: 0.0076985121704638
2018-10-22 22:39:01.730899:	Training iteration: 224000, Loss: 0.0073726363480091095
2018-10-22 22:40:26.398866:	Training iteration: 224200, Loss: 0.013930746354162693
2018-10-22 22:41:51.245886:	Training iteration: 224400, Loss: 0.00932862889021635
2018-10-22 22:43:15.632748:	Training iteration: 224600, Loss: 0.007201289292424917
2018-10-22 22:44:39.736425:	Training iteration: 224800, Loss: 0.0050419350154697895
2018-10-22 22:46:04.844262:	Training iteration: 225000, Loss: 0.0031426374334841967
2018-10-22 22:47:29.301945:	Training iteration: 225200, Loss: 0.005649862810969353
2018-10-22 22:48:53.879348:	Training iteration: 225400, Loss: 0.002947154687717557
2018-10-22 22:50:19.174260:	Training iteration: 225600, Loss: 0.005529545713216066
2018-10-22 22:51:43.942385:	Training iteration: 225800, Loss: 0.005103823728859425
2018-10-22 22:53:08.051534:	Training iteration: 226000, Loss: 0.006531921681016684
2018-10-22 22:54:32.454948:	Training iteration: 226200, Loss: 0.0055668237619102
2018-10-22 22:55:56.564286:	Training iteration: 226400, Loss: 0.004953465890139341
2018-10-22 22:57:21.039509:	Training iteration: 226600, Loss: 0.006525496486574411
2018-10-22 22:58:45.431038:	Training iteration: 226800, Loss: 0.0035738309379667044
2018-10-22 23:00:09.433958:	Training iteration: 227000, Loss: 0.00524519756436348
2018-10-22 23:01:33.630393:	Training iteration: 227200, Loss: 0.004435041453689337
2018-10-22 23:02:57.205895:	Training iteration: 227400, Loss: 0.003557957476004958
2018-10-22 23:04:21.224968:	Training iteration: 227600, Loss: 0.003645359305664897
2018-10-22 23:05:45.626544:	Training iteration: 227800, Loss: 0.004528445191681385
2018-10-22 23:07:09.448243:	Training iteration: 228000, Loss: 0.004174458794295788
2018-10-22 23:08:33.019724:	Training iteration: 228200, Loss: 0.006954304873943329
2018-10-22 23:09:57.485856:	Training iteration: 228400, Loss: 0.0076424796134233475
2018-10-22 23:11:21.288673:	Training iteration: 228600, Loss: 0.006396008189767599
2018-10-22 23:12:44.880745:	Training iteration: 228800, Loss: 0.009110352955758572
2018-10-22 23:14:09.232920:	Training iteration: 229000, Loss: 0.007149409037083387
2018-10-22 23:15:32.733191:	Training iteration: 229200, Loss: 0.006062536034733057
2018-10-22 23:16:56.372950:	Training iteration: 229400, Loss: 0.007178860250860453
2018-10-22 23:18:20.378311:	Training iteration: 229600, Loss: 0.005817104130983353
2018-10-22 23:19:43.985224:	Training iteration: 229800, Loss: 0.005667110905051231
2018-10-22 23:21:08.236512:	Training iteration: 230000, Loss: 0.004909558687359095
Checkpoint
2018-10-22 23:22:37.216812:	Training iteration: 230200, Loss: 0.004564179573208094
2018-10-22 23:24:01.348198:	Training iteration: 230400, Loss: 0.0045365639962255955
2018-10-22 23:25:25.913136:	Training iteration: 230600, Loss: 0.003406414994969964
2018-10-22 23:26:50.115416:	Training iteration: 230800, Loss: 0.004139144439250231
2018-10-22 23:28:14.491641:	Training iteration: 231000, Loss: 0.005171814933419228
2018-10-22 23:29:39.360652:	Training iteration: 231200, Loss: 0.007664510514587164
2018-10-22 23:31:03.450479:	Training iteration: 231400, Loss: 0.0043127890676259995
2018-10-22 23:32:28.030339:	Training iteration: 231600, Loss: 0.004846247378736734
2018-10-22 23:33:52.209979:	Training iteration: 231800, Loss: 0.009217528626322746
2018-10-22 23:35:17.108924:	Training iteration: 232000, Loss: 0.004582629073411226
2018-10-22 23:36:41.403387:	Training iteration: 232200, Loss: 0.0047288513742387295
2018-10-22 23:38:06.168122:	Training iteration: 232400, Loss: 0.004067708272486925
2018-10-22 23:39:30.542525:	Training iteration: 232600, Loss: 0.007399767637252808
2018-10-22 23:40:54.406707:	Training iteration: 232800, Loss: 0.008894109167158604
2018-10-22 23:42:19.458202:	Training iteration: 233000, Loss: 0.005225088447332382
2018-10-22 23:43:43.879412:	Training iteration: 233200, Loss: 0.004565884824842215
2018-10-22 23:45:09.124007:	Training iteration: 233400, Loss: 0.00582518195733428
2018-10-22 23:46:34.205835:	Training iteration: 233600, Loss: 0.007697677705436945
2018-10-22 23:47:58.274720:	Training iteration: 233800, Loss: 0.006870826240628958
2018-10-22 23:49:23.527347:	Training iteration: 234000, Loss: 0.005416309926658869
2018-10-22 23:50:48.072258:	Training iteration: 234200, Loss: 0.005643242504447699
2018-10-22 23:52:12.185026:	Training iteration: 234400, Loss: 0.0037777533289045095
2018-10-22 23:53:36.216827:	Training iteration: 234600, Loss: 0.004425215069204569
2018-10-22 23:55:00.864720:	Training iteration: 234800, Loss: 0.0042557124979794025
2018-10-22 23:56:25.146346:	Training iteration: 235000, Loss: 0.007077626883983612
2018-10-22 23:57:49.111394:	Training iteration: 235200, Loss: 0.0055069890804588795
2018-10-22 23:59:13.735107:	Training iteration: 235400, Loss: 0.006596285849809647
2018-10-23 00:00:37.285728:	Training iteration: 235600, Loss: 0.004514913074672222
2018-10-23 00:02:01.229143:	Training iteration: 235800, Loss: 0.006431614514440298
2018-10-23 00:03:25.236995:	Training iteration: 236000, Loss: 0.0067510176450014114
2018-10-23 00:04:48.918170:	Training iteration: 236200, Loss: 0.004141532350331545
2018-10-23 00:06:12.372611:	Training iteration: 236400, Loss: 0.006023753900080919
2018-10-23 00:07:36.116434:	Training iteration: 236600, Loss: 0.004199750255793333
2018-10-23 00:09:00.156081:	Training iteration: 236800, Loss: 0.0059135048650205135
2018-10-23 00:10:24.092377:	Training iteration: 237000, Loss: 0.006544014904648066
2018-10-23 00:11:48.018219:	Training iteration: 237200, Loss: 0.005091797560453415
2018-10-23 00:13:11.555000:	Training iteration: 237400, Loss: 0.003178957151249051
2018-10-23 00:14:35.961746:	Training iteration: 237600, Loss: 0.005527853965759277
2018-10-23 00:16:00.100563:	Training iteration: 237800, Loss: 0.006769208237528801
2018-10-23 00:17:24.675286:	Training iteration: 238000, Loss: 0.005090213846415281
2018-10-23 00:18:48.737872:	Training iteration: 238200, Loss: 0.003674668027088046
2018-10-23 00:20:13.500613:	Training iteration: 238400, Loss: 0.005448728799819946
2018-10-23 00:21:38.047128:	Training iteration: 238600, Loss: 0.005326293874531984
2018-10-23 00:23:02.835205:	Training iteration: 238800, Loss: 0.0052327862940728664
2018-10-23 00:24:27.522709:	Training iteration: 239000, Loss: 0.00393105112016201
2018-10-23 00:25:52.657101:	Training iteration: 239200, Loss: 0.009043306112289429
2018-10-23 00:27:17.184115:	Training iteration: 239400, Loss: 0.00617242231965065
2018-10-23 00:28:41.603010:	Training iteration: 239600, Loss: 0.0051211039535701275
2018-10-23 00:30:05.918975:	Training iteration: 239800, Loss: 0.004222618415951729
2018-10-23 00:30:49.875169:	Epoch 2 finished after 239905 iterations.
Validating
2018-10-23 00:30:49.983344:	Entering validation loop
2018-10-23 00:31:00.007352: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 649 of 1000
2018-10-23 00:31:04.846580: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:31:40.614331:	Validation iteration: 200, Loss: 0.004943173844367266
2018-10-23 00:32:16.662456:	Validation iteration: 400, Loss: 0.004927926696836948
2018-10-23 00:32:53.188324:	Validation iteration: 600, Loss: 0.007887603715062141
2018-10-23 00:33:30.383534:	Validation iteration: 800, Loss: 0.003496601479128003
2018-10-23 00:34:06.860372:	Validation iteration: 1000, Loss: 0.00602959468960762
2018-10-23 00:34:43.979492:	Validation iteration: 1200, Loss: 0.006292422767728567
2018-10-23 00:35:30.056814: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 669 of 1000
2018-10-23 00:35:34.582824: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:35:35.961068:	Validation iteration: 1400, Loss: 0.005200291518121958
2018-10-23 00:36:11.814365:	Validation iteration: 1600, Loss: 0.006542359013110399
2018-10-23 00:36:47.875957:	Validation iteration: 1800, Loss: 0.006999887526035309
2018-10-23 00:37:24.510898:	Validation iteration: 2000, Loss: 0.005528055131435394
2018-10-23 00:38:01.665235:	Validation iteration: 2200, Loss: 0.007565019186586142
2018-10-23 00:38:38.028970:	Validation iteration: 2400, Loss: 0.008825051598250866
2018-10-23 00:39:14.867030:	Validation iteration: 2600, Loss: 0.007205257657915354
2018-10-23 00:39:59.149754: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 529 of 1000
2018-10-23 00:40:05.753423: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:40:08.458477:	Validation iteration: 2800, Loss: 0.007542847190052271
2018-10-23 00:40:44.689934:	Validation iteration: 3000, Loss: 0.006372521165758371
2018-10-23 00:41:21.414808:	Validation iteration: 3200, Loss: 0.004947233945131302
2018-10-23 00:41:58.393814:	Validation iteration: 3400, Loss: 0.005190922878682613
2018-10-23 00:42:34.620010:	Validation iteration: 3600, Loss: 0.005888741463422775
2018-10-23 00:43:11.533670:	Validation iteration: 3800, Loss: 0.004539630841463804
2018-10-23 00:43:48.130098:	Validation iteration: 4000, Loss: 0.0048099178820848465
2018-10-23 00:44:31.357774: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 650 of 1000
2018-10-23 00:44:36.010543: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:44:40.012730:	Validation iteration: 4200, Loss: 0.0063221328891813755
2018-10-23 00:45:15.496044:	Validation iteration: 4400, Loss: 0.0063096764497458935
2018-10-23 00:45:51.085159:	Validation iteration: 4600, Loss: 0.007123095449060202
2018-10-23 00:46:27.431671:	Validation iteration: 4800, Loss: 0.006741993594914675
2018-10-23 00:47:04.350537:	Validation iteration: 5000, Loss: 0.006478708237409592
2018-10-23 00:47:40.689855:	Validation iteration: 5200, Loss: 0.007115040440112352
2018-10-23 00:48:17.054441:	Validation iteration: 5400, Loss: 0.005508077796548605
2018-10-23 00:48:58.327324: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 614 of 1000
2018-10-23 00:49:04.125565: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:49:09.501742:	Validation iteration: 5600, Loss: 0.004498508293181658
2018-10-23 00:49:45.316535:	Validation iteration: 5800, Loss: 0.00619850680232048
2018-10-23 00:50:21.668219:	Validation iteration: 6000, Loss: 0.006168274208903313
2018-10-23 00:50:57.329894:	Validation iteration: 6200, Loss: 0.0041971332393586636
2018-10-23 00:51:33.428628:	Validation iteration: 6400, Loss: 0.005319857969880104
2018-10-23 00:52:12.570435:	Validation iteration: 6600, Loss: 0.004390560556203127
2018-10-23 00:52:48.784001:	Validation iteration: 6800, Loss: 0.005994038190692663
2018-10-23 00:53:26.061078:	Validation iteration: 7000, Loss: 0.0037116315215826035
2018-10-23 00:54:02.933829:	Validation iteration: 7200, Loss: 0.00591307645663619
2018-10-23 00:54:39.425351:	Validation iteration: 7400, Loss: 0.006434659007936716
Validation check mean loss: 0.005929216462873478
Validation loss has improved!
New best validation cost!
2018-10-23 00:55:06.773639: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 735 of 1000
2018-10-23 00:55:10.117226: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:55:49.560013:	Training iteration: 240000, Loss: 0.004540411289781332
Checkpoint
2018-10-23 00:57:16.952358:	Training iteration: 240200, Loss: 0.0061303251422941685
2018-10-23 00:58:41.326326:	Training iteration: 240400, Loss: 0.0049539715982973576
2018-10-23 01:00:06.052687:	Training iteration: 240600, Loss: 0.003411757992580533
2018-10-23 01:01:30.348142:	Training iteration: 240800, Loss: 0.0045048207975924015
2018-10-23 01:02:54.731360:	Training iteration: 241000, Loss: 0.003208782523870468
2018-10-23 01:04:19.584032:	Training iteration: 241200, Loss: 0.003896136535331607
2018-10-23 01:05:43.905379:	Training iteration: 241400, Loss: 0.004024892579764128
2018-10-23 01:07:07.825750:	Training iteration: 241600, Loss: 0.005019893404096365
2018-10-23 01:08:32.619104:	Training iteration: 241800, Loss: 0.002660207450389862
2018-10-23 01:09:57.551139:	Training iteration: 242000, Loss: 0.0034264184068888426
2018-10-23 01:11:22.703736:	Training iteration: 242200, Loss: 0.0026216537225991488
2018-10-23 01:12:47.280332:	Training iteration: 242400, Loss: 0.006379773374646902
2018-10-23 01:14:12.385002:	Training iteration: 242600, Loss: 0.006015724036842585
2018-10-23 01:15:37.551216:	Training iteration: 242800, Loss: 0.004443930462002754
2018-10-23 01:17:02.305231:	Training iteration: 243000, Loss: 0.003603390185162425
2018-10-23 01:18:27.413383:	Training iteration: 243200, Loss: 0.003477308200672269
2018-10-23 01:19:52.758353:	Training iteration: 243400, Loss: 0.004605141934007406
2018-10-23 01:21:17.660216:	Training iteration: 243600, Loss: 0.0043185134418308735
2018-10-23 01:22:42.825021:	Training iteration: 243800, Loss: 0.0037175510078668594
2018-10-23 01:24:07.237055:	Training iteration: 244000, Loss: 0.004659172613173723
2018-10-23 01:25:32.952982:	Training iteration: 244200, Loss: 0.0029986214358359575
2018-10-23 01:26:57.989005:	Training iteration: 244400, Loss: 0.004206677433103323
2018-10-23 01:28:23.311314:	Training iteration: 244600, Loss: 0.004584969952702522
2018-10-23 01:29:48.328783:	Training iteration: 244800, Loss: 0.0033829074818640947
2018-10-23 01:31:13.527794:	Training iteration: 245000, Loss: 0.0047049145214259624
2018-10-23 01:32:38.593139:	Training iteration: 245200, Loss: 0.003681482747197151
2018-10-23 01:34:04.165624:	Training iteration: 245400, Loss: 0.003974221646785736
2018-10-23 01:35:29.277609:	Training iteration: 245600, Loss: 0.005046279635280371
2018-10-23 01:36:54.514586:	Training iteration: 245800, Loss: 0.003644282231107354
2018-10-23 01:38:19.928882:	Training iteration: 246000, Loss: 0.004453785251826048
2018-10-23 01:39:44.463128:	Training iteration: 246200, Loss: 0.0034940491896122694
2018-10-23 01:41:09.649839:	Training iteration: 246400, Loss: 0.004894180689007044
2018-10-23 01:42:34.661851:	Training iteration: 246600, Loss: 0.004784644115716219
2018-10-23 01:43:59.572781:	Training iteration: 246800, Loss: 0.004080293700098991
2018-10-23 01:45:24.374171:	Training iteration: 247000, Loss: 0.005031624808907509
2018-10-23 01:46:49.249276:	Training iteration: 247200, Loss: 0.0032118104863911867
2018-10-23 01:48:13.625913:	Training iteration: 247400, Loss: 0.004394873045384884
2018-10-23 01:49:37.968441:	Training iteration: 247600, Loss: 0.006182258483022451
2018-10-23 01:51:02.655333:	Training iteration: 247800, Loss: 0.003991845995187759
2018-10-23 01:52:27.156551:	Training iteration: 248000, Loss: 0.006261270027607679
2018-10-23 01:53:48.247450: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 594 of 1000
2018-10-23 01:53:53.508338: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 01:54:06.333023:	Training iteration: 248200, Loss: 0.0051005203276872635
2018-10-23 01:55:29.175993:	Training iteration: 248400, Loss: 0.004746283870190382
2018-10-23 01:56:53.129518:	Training iteration: 248600, Loss: 0.0037760932464152575
2018-10-23 01:58:17.068681:	Training iteration: 248800, Loss: 0.004447161685675383
2018-10-23 01:59:41.310060:	Training iteration: 249000, Loss: 0.005396615713834763
2018-10-23 02:01:06.289120:	Training iteration: 249200, Loss: 0.004357778932899237
2018-10-23 02:02:30.240920:	Training iteration: 249400, Loss: 0.0047706072218716145
2018-10-23 02:03:54.577304:	Training iteration: 249600, Loss: 0.004579664207994938
2018-10-23 02:05:19.688680:	Training iteration: 249800, Loss: 0.0043820771388709545
2018-10-23 02:06:44.169195:	Training iteration: 250000, Loss: 0.0041297911666333675
Checkpoint
2018-10-23 02:08:11.677781:	Training iteration: 250200, Loss: 0.004413563758134842
2018-10-23 02:09:36.267044:	Training iteration: 250400, Loss: 0.00565386563539505
2018-10-23 02:11:00.915797:	Training iteration: 250600, Loss: 0.005835283547639847
2018-10-23 02:12:25.686714:	Training iteration: 250800, Loss: 0.005219269543886185
2018-10-23 02:13:50.813409:	Training iteration: 251000, Loss: 0.004072295036166906
2018-10-23 02:15:15.898029:	Training iteration: 251200, Loss: 0.0052909343503415585
2018-10-23 02:16:41.045752:	Training iteration: 251400, Loss: 0.0038588764145970345
2018-10-23 02:18:05.527158:	Training iteration: 251600, Loss: 0.0041675325483083725
2018-10-23 02:19:30.946951:	Training iteration: 251800, Loss: 0.005774704273790121
2018-10-23 02:20:55.838599:	Training iteration: 252000, Loss: 0.0035849381238222122
2018-10-23 02:22:20.908044:	Training iteration: 252200, Loss: 0.003776178928092122
2018-10-23 02:23:45.556288:	Training iteration: 252400, Loss: 0.0045724003575742245
2018-10-23 02:25:10.450435:	Training iteration: 252600, Loss: 0.006627222057431936
2018-10-23 02:26:35.869313:	Training iteration: 252800, Loss: 0.004584502428770065
2018-10-23 02:28:01.018384:	Training iteration: 253000, Loss: 0.0038384057115763426
2018-10-23 02:29:26.325074:	Training iteration: 253200, Loss: 0.00438601104542613
2018-10-23 02:30:50.742668:	Training iteration: 253400, Loss: 0.004346322733908892
2018-10-23 02:32:16.252497:	Training iteration: 253600, Loss: 0.005627525504678488
2018-10-23 02:33:41.147604:	Training iteration: 253800, Loss: 0.004422375932335854
2018-10-23 02:35:05.937608:	Training iteration: 254000, Loss: 0.005588797386735678
2018-10-23 02:36:30.915007:	Training iteration: 254200, Loss: 0.006139291916042566
2018-10-23 02:37:55.858118:	Training iteration: 254400, Loss: 0.004901979584246874
2018-10-23 02:39:20.462327:	Training iteration: 254600, Loss: 0.004534274805337191
2018-10-23 02:40:45.515262:	Training iteration: 254800, Loss: 0.004761762451380491
2018-10-23 02:42:10.080406:	Training iteration: 255000, Loss: 0.004871793556958437
2018-10-23 02:43:35.199695:	Training iteration: 255200, Loss: 0.005767903756350279
2018-10-23 02:44:59.533154:	Training iteration: 255400, Loss: 0.004035928752273321
2018-10-23 02:46:24.008707:	Training iteration: 255600, Loss: 0.0037443230394273996
2018-10-23 02:47:48.376649:	Training iteration: 255800, Loss: 0.002811051206663251
2018-10-23 02:49:12.647904:	Training iteration: 256000, Loss: 0.004061064682900906
2018-10-23 02:50:36.681305:	Training iteration: 256200, Loss: 0.004769949708133936
2018-10-23 02:52:01.168149:	Training iteration: 256400, Loss: 0.0034815678372979164
2018-10-23 02:52:44.630569: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 718 of 1000
2018-10-23 02:52:48.060344: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 02:53:37.172988:	Training iteration: 256600, Loss: 0.004321616608649492
2018-10-23 02:55:01.602191:	Training iteration: 256800, Loss: 0.004496304783970118
2018-10-23 02:56:26.369901:	Training iteration: 257000, Loss: 0.006105508655309677
2018-10-23 02:57:50.581943:	Training iteration: 257200, Loss: 0.00509141618385911
2018-10-23 02:59:15.146988:	Training iteration: 257400, Loss: 0.005342879798263311
2018-10-23 03:00:39.242347:	Training iteration: 257600, Loss: 0.005900456104427576
2018-10-23 03:02:03.896946:	Training iteration: 257800, Loss: 0.0032049568835645914
2018-10-23 03:03:29.637938:	Training iteration: 258000, Loss: 0.0059088715352118015
2018-10-23 03:04:54.479955:	Training iteration: 258200, Loss: 0.004692342132329941
2018-10-23 03:06:20.929276:	Training iteration: 258400, Loss: 0.00391189381480217
2018-10-23 03:07:45.863364:	Training iteration: 258600, Loss: 0.006075885146856308
2018-10-23 03:09:13.307421:	Training iteration: 258800, Loss: 0.0036282241344451904
2018-10-23 03:10:37.923635:	Training iteration: 259000, Loss: 0.004732516128569841
2018-10-23 03:12:02.982239:	Training iteration: 259200, Loss: 0.005432553123682737
2018-10-23 03:13:27.821813:	Training iteration: 259400, Loss: 0.00518996873870492
2018-10-23 03:14:55.391391:	Training iteration: 259600, Loss: 0.0034204116091132164
2018-10-23 03:16:20.130125:	Training iteration: 259800, Loss: 0.005942333955317736
2018-10-23 03:17:46.038443:	Training iteration: 260000, Loss: 0.005419857334345579
Checkpoint
2018-10-23 03:19:14.435218:	Training iteration: 260200, Loss: 0.006976487580686808
2018-10-23 03:20:38.992693:	Training iteration: 260400, Loss: 0.004731399472802877
2018-10-23 03:22:04.584585:	Training iteration: 260600, Loss: 0.004231275990605354
2018-10-23 03:23:29.216729:	Training iteration: 260800, Loss: 0.0048077767714858055
2018-10-23 03:24:54.118513:	Training iteration: 261000, Loss: 0.0031574645545333624
2018-10-23 03:26:19.016080:	Training iteration: 261200, Loss: 0.004259979818016291
2018-10-23 03:27:45.394619:	Training iteration: 261400, Loss: 0.00474320026114583
2018-10-23 03:29:10.502855:	Training iteration: 261600, Loss: 0.0048961094580590725
2018-10-23 03:30:35.451125:	Training iteration: 261800, Loss: 0.005210084840655327
2018-10-23 03:31:59.910140:	Training iteration: 262000, Loss: 0.005416788626462221
2018-10-23 03:33:24.843234:	Training iteration: 262200, Loss: 0.0061333361081779
2018-10-23 03:34:48.935082:	Training iteration: 262400, Loss: 0.0042057037353515625
2018-10-23 03:36:13.715232:	Training iteration: 262600, Loss: 0.003883672645315528
2018-10-23 03:37:38.459222:	Training iteration: 262800, Loss: 0.005404740571975708
2018-10-23 03:39:02.457516:	Training iteration: 263000, Loss: 0.006221145391464233
2018-10-23 03:40:26.881242:	Training iteration: 263200, Loss: 0.0073546445928514
2018-10-23 03:41:51.302805:	Training iteration: 263400, Loss: 0.0039239302277565
2018-10-23 03:43:16.034734:	Training iteration: 263600, Loss: 0.004619855899363756
2018-10-23 03:44:40.472341:	Training iteration: 263800, Loss: 0.004476919770240784
2018-10-23 03:46:05.306471:	Training iteration: 264000, Loss: 0.005753235425800085
2018-10-23 03:47:29.686321:	Training iteration: 264200, Loss: 0.005506306421011686
2018-10-23 03:48:54.005999:	Training iteration: 264400, Loss: 0.004194668959826231
2018-10-23 03:50:17.953914:	Training iteration: 264600, Loss: 0.006044598761945963
2018-10-23 03:51:46.254989: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 689 of 1000
2018-10-23 03:51:50.301611: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 03:51:56.060867:	Training iteration: 264800, Loss: 0.005070238374173641
2018-10-23 03:53:19.619229:	Training iteration: 265000, Loss: 0.005766888614743948
2018-10-23 03:54:44.296404:	Training iteration: 265200, Loss: 0.003114132909104228
2018-10-23 03:56:08.639708:	Training iteration: 265400, Loss: 0.006478667724877596
2018-10-23 03:57:34.019698:	Training iteration: 265600, Loss: 0.005774026736617088
2018-10-23 03:58:58.457193:	Training iteration: 265800, Loss: 0.0029264118056744337
2018-10-23 04:00:24.129718:	Training iteration: 266000, Loss: 0.00340641220100224
2018-10-23 04:01:48.656565:	Training iteration: 266200, Loss: 0.0036856967490166426
2018-10-23 04:03:13.652798:	Training iteration: 266400, Loss: 0.0036442112177610397
2018-10-23 04:04:38.756668:	Training iteration: 266600, Loss: 0.00587495556101203
2018-10-23 04:06:03.849152:	Training iteration: 266800, Loss: 0.004070552531629801
2018-10-23 04:07:29.577310:	Training iteration: 267000, Loss: 0.005491414573043585
2018-10-23 04:08:53.487161:	Training iteration: 267200, Loss: 0.004796633496880531
2018-10-23 04:10:18.597146:	Training iteration: 267400, Loss: 0.004315605387091637
2018-10-23 04:11:44.085351:	Training iteration: 267600, Loss: 0.005503110587596893
2018-10-23 04:13:08.896915:	Training iteration: 267800, Loss: 0.00533019145950675
2018-10-23 04:14:33.953854:	Training iteration: 268000, Loss: 0.0031001169700175524
2018-10-23 04:15:59.477419:	Training iteration: 268200, Loss: 0.002993215573951602
2018-10-23 04:17:24.273370:	Training iteration: 268400, Loss: 0.0037783216685056686
2018-10-23 04:18:49.962858:	Training iteration: 268600, Loss: 0.0033713001757860184
2018-10-23 04:20:15.374656:	Training iteration: 268800, Loss: 0.0031945195514708757
2018-10-23 04:21:40.664591:	Training iteration: 269000, Loss: 0.0040938411839306355
2018-10-23 04:23:05.526853:	Training iteration: 269200, Loss: 0.004034589510411024
2018-10-23 04:24:30.849172:	Training iteration: 269400, Loss: 0.004889327567070723
2018-10-23 04:25:55.555862:	Training iteration: 269600, Loss: 0.003574036294594407
2018-10-23 04:27:20.888201:	Training iteration: 269800, Loss: 0.003550758585333824
2018-10-23 04:28:45.727012:	Training iteration: 270000, Loss: 0.004688102286309004
Checkpoint
2018-10-23 04:30:12.793538:	Training iteration: 270200, Loss: 0.003587904619053006
2018-10-23 04:31:37.121265:	Training iteration: 270400, Loss: 0.0043927631340920925
2018-10-23 04:33:02.055460:	Training iteration: 270600, Loss: 0.0042841569520533085
2018-10-23 04:34:26.395725:	Training iteration: 270800, Loss: 0.00599574064835906
2018-10-23 04:35:51.079982:	Training iteration: 271000, Loss: 0.0038585152942687273
2018-10-23 04:37:15.390223:	Training iteration: 271200, Loss: 0.0037569766864180565
2018-10-23 04:38:39.859295:	Training iteration: 271400, Loss: 0.005221343133598566
2018-10-23 04:40:04.385658:	Training iteration: 271600, Loss: 0.0029114081989973783
2018-10-23 04:41:28.897415:	Training iteration: 271800, Loss: 0.0038135144859552383
2018-10-23 04:42:53.627025:	Training iteration: 272000, Loss: 0.004291930701583624
2018-10-23 04:44:18.998370:	Training iteration: 272200, Loss: 0.0032733974512666464
2018-10-23 04:45:43.483630:	Training iteration: 272400, Loss: 0.005481796804815531
2018-10-23 04:47:08.148848:	Training iteration: 272600, Loss: 0.004975773394107819
2018-10-23 04:48:32.971922:	Training iteration: 272800, Loss: 0.0050745438784360886
2018-10-23 04:49:57.762922:	Training iteration: 273000, Loss: 0.005458307918161154
2018-10-23 04:51:22.506774:	Training iteration: 273200, Loss: 0.0035751918330788612
2018-10-23 04:52:47.150951:	Training iteration: 273400, Loss: 0.0036308381240814924
2018-10-23 04:53:25.541899: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 645 of 1000
2018-10-23 04:53:29.457064: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 04:54:24.583277:	Training iteration: 273600, Loss: 0.004971932619810104
2018-10-23 04:55:49.043542:	Training iteration: 273800, Loss: 0.00436142273247242
2018-10-23 04:57:12.489151:	Training iteration: 274000, Loss: 0.007456956896930933
2018-10-23 04:58:37.524470:	Training iteration: 274200, Loss: 0.0049247960560023785
2018-10-23 05:00:02.826175:	Training iteration: 274400, Loss: 0.006852210033684969
2018-10-23 05:01:27.474855:	Training iteration: 274600, Loss: 0.005436751991510391
2018-10-23 05:02:52.066728:	Training iteration: 274800, Loss: 0.003520337864756584
2018-10-23 05:04:16.960966:	Training iteration: 275000, Loss: 0.0029206902254372835
2018-10-23 05:05:42.452099:	Training iteration: 275200, Loss: 0.006909344810992479
2018-10-23 05:07:07.709673:	Training iteration: 275400, Loss: 0.010017328895628452
2018-10-23 05:08:31.303546:	Training iteration: 275600, Loss: 0.007224878761917353
2018-10-23 05:09:55.628203:	Training iteration: 275800, Loss: 0.007641410920768976
2018-10-23 05:11:20.480659:	Training iteration: 276000, Loss: 0.0038308333605527878
2018-10-23 05:12:44.983032:	Training iteration: 276200, Loss: 0.004732434172183275
2018-10-23 05:14:09.591638:	Training iteration: 276400, Loss: 0.004481828305870295
2018-10-23 05:15:34.125254:	Training iteration: 276600, Loss: 0.0036644740030169487
2018-10-23 05:16:58.809361:	Training iteration: 276800, Loss: 0.004089090507477522
2018-10-23 05:18:23.959530:	Training iteration: 277000, Loss: 0.004919794853776693
2018-10-23 05:19:48.344493:	Training iteration: 277200, Loss: 0.006487933453172445
2018-10-23 05:21:13.314652:	Training iteration: 277400, Loss: 0.009249995462596416
2018-10-23 05:22:37.906826:	Training iteration: 277600, Loss: 0.004287257324904203
2018-10-23 05:24:02.509051:	Training iteration: 277800, Loss: 0.006078364793211222
2018-10-23 05:25:27.080430:	Training iteration: 278000, Loss: 0.0061220452189445496
2018-10-23 05:26:51.203684:	Training iteration: 278200, Loss: 0.006708489265292883
2018-10-23 05:28:15.624891:	Training iteration: 278400, Loss: 0.004527457524091005
2018-10-23 05:29:40.420622:	Training iteration: 278600, Loss: 0.0038837685715407133
2018-10-23 05:31:04.392083:	Training iteration: 278800, Loss: 0.009271170012652874
2018-10-23 05:32:28.157663:	Training iteration: 279000, Loss: 0.006072232499718666
2018-10-23 05:33:52.727948:	Training iteration: 279200, Loss: 0.0034369423519819975
2018-10-23 05:35:17.132563:	Training iteration: 279400, Loss: 0.004106924403458834
2018-10-23 05:36:41.332304:	Training iteration: 279600, Loss: 0.003491326468065381
2018-10-23 05:38:05.094039:	Training iteration: 279800, Loss: 0.0037504853680729866
2018-10-23 05:39:29.700932:	Training iteration: 280000, Loss: 0.005015298258513212
Checkpoint
2018-10-23 05:40:56.423972:	Training iteration: 280200, Loss: 0.004540794063359499
2018-10-23 05:42:20.426394:	Training iteration: 280400, Loss: 0.00465208338573575
2018-10-23 05:43:45.185633:	Training iteration: 280600, Loss: 0.004924119915813208
2018-10-23 05:45:09.399793:	Training iteration: 280800, Loss: 0.003679730696603656
2018-10-23 05:46:32.786129:	Training iteration: 281000, Loss: 0.005943296942859888
2018-10-23 05:47:57.545496:	Training iteration: 281200, Loss: 0.003948007710278034
2018-10-23 05:49:22.049546:	Training iteration: 281400, Loss: 0.006294913589954376
2018-10-23 05:50:47.312520:	Training iteration: 281600, Loss: 0.00474024610593915
2018-10-23 05:52:11.512912:	Training iteration: 281800, Loss: 0.004122509155422449
2018-10-23 05:53:35.693517:	Training iteration: 282000, Loss: 0.004200128838419914
2018-10-23 05:54:59.994253:	Training iteration: 282200, Loss: 0.00349438120611012
2018-10-23 05:56:24.716663:	Training iteration: 282400, Loss: 0.005062159616500139
2018-10-23 05:57:48.902203:	Training iteration: 282600, Loss: 0.0038826807867735624
2018-10-23 05:59:14.124781:	Training iteration: 282800, Loss: 0.0055005974136292934
2018-10-23 06:00:38.883058:	Training iteration: 283000, Loss: 0.006792211439460516
2018-10-23 06:02:02.726172:	Training iteration: 283200, Loss: 0.004407473374158144
2018-10-23 06:03:28.049573:	Training iteration: 283400, Loss: 0.005831323564052582
2018-10-23 06:04:54.567139:	Training iteration: 283600, Loss: 0.0067749000154435635
2018-10-23 06:06:19.012969:	Training iteration: 283800, Loss: 0.005311686545610428
2018-10-23 06:07:43.648663:	Training iteration: 284000, Loss: 0.005356306675821543
2018-10-23 06:09:08.247374:	Training iteration: 284200, Loss: 0.006478739436715841
2018-10-23 06:10:32.774065:	Training iteration: 284400, Loss: 0.004519290756434202
2018-10-23 06:11:57.733101:	Training iteration: 284600, Loss: 0.005554328206926584
2018-10-23 06:13:22.333197:	Training iteration: 284800, Loss: 0.0068352906964719296
2018-10-23 06:14:47.105795:	Training iteration: 285000, Loss: 0.005328273866325617
2018-10-23 06:16:11.307825:	Training iteration: 285200, Loss: 0.009875424206256866
2018-10-23 06:17:36.202934:	Training iteration: 285400, Loss: 0.004370806273072958
2018-10-23 06:19:00.401666:	Training iteration: 285600, Loss: 0.007253685500472784
2018-10-23 06:20:24.749877:	Training iteration: 285800, Loss: 0.005997943226248026
2018-10-23 06:21:49.161884:	Training iteration: 286000, Loss: 0.005564613733440638
2018-10-23 06:23:13.187903:	Training iteration: 286200, Loss: 0.002712169662117958
2018-10-23 06:24:37.036609:	Training iteration: 286400, Loss: 0.010096156969666481
2018-10-23 06:26:02.312445:	Training iteration: 286600, Loss: 0.006727123633027077
2018-10-23 06:28:00.187703:	Training iteration: 286800, Loss: 0.00707256281748414
2018-10-23 06:39:58.749533:	Training iteration: 287000, Loss: 0.006696542724967003
2018-10-23 06:57:59.706960:	Training iteration: 287200, Loss: 0.003348207101225853
2018-10-23 06:59:13.933084:	Training iteration: 287400, Loss: 0.004841626156121492
2018-10-23 07:00:30.425550:	Training iteration: 287600, Loss: 0.0044386484660208225
2018-10-23 07:01:49.088378:	Training iteration: 287800, Loss: 0.004373843315988779
2018-10-23 07:03:09.394585:	Training iteration: 288000, Loss: 0.005088191479444504
2018-10-23 07:04:30.425837:	Training iteration: 288200, Loss: 0.004398609045892954
2018-10-23 07:05:55.060418:	Training iteration: 288400, Loss: 0.0082034170627594
2018-10-23 07:19:25.745096:	Training iteration: 288600, Loss: 0.008080695755779743
2018-10-23 07:20:39.858121:	Training iteration: 288800, Loss: 0.0049512190744280815
2018-10-23 07:21:54.689016:	Training iteration: 289000, Loss: 0.005395233165472746
2018-10-23 07:23:12.956264:	Training iteration: 289200, Loss: 0.007070703897625208
2018-10-23 07:24:35.223365:	Training iteration: 289400, Loss: 0.005305187311023474
2018-10-23 07:25:58.450219:	Training iteration: 289600, Loss: 0.009071165695786476
2018-10-23 07:40:25.012709:	Training iteration: 289800, Loss: 0.004667004104703665
2018-10-23 07:41:44.925472:	Training iteration: 290000, Loss: 0.0038396550808101892
Checkpoint
2018-10-23 08:02:30.656282:	Training iteration: 290200, Loss: 0.004636072088032961
2018-10-23 09:46:45.393035:	Training iteration: 290400, Loss: 0.0047430661506950855
2018-10-23 09:47:59.241517:	Training iteration: 290600, Loss: 0.004147726576775312
2018-10-23 09:49:13.177804:	Training iteration: 290800, Loss: 0.0044205873273313046
2018-10-23 09:50:27.233159:	Training iteration: 291000, Loss: 0.00709001487120986
2018-10-23 09:51:41.738082:	Training iteration: 291200, Loss: 0.005560414399951696
2018-10-23 09:52:57.465880:	Training iteration: 291400, Loss: 0.0068868123926222324
2018-10-23 09:54:14.108799:	Training iteration: 291600, Loss: 0.007097531110048294
2018-10-23 09:55:30.821439:	Training iteration: 291800, Loss: 0.005477439612150192
2018-10-23 09:56:48.018278:	Training iteration: 292000, Loss: 0.00592069374397397
2018-10-23 09:58:05.239990:	Training iteration: 292200, Loss: 0.00989782065153122
2018-10-23 09:59:23.235610:	Training iteration: 292400, Loss: 0.0100410720333457
2018-10-23 10:00:41.086891:	Training iteration: 292600, Loss: 0.008737554773688316
2018-10-23 10:01:58.972935:	Training iteration: 292800, Loss: 0.008852136321365833
2018-10-23 10:03:17.004474:	Training iteration: 293000, Loss: 0.0045293825678527355
2018-10-23 10:04:35.363318:	Training iteration: 293200, Loss: 0.004314066376537085
2018-10-23 10:05:53.989653:	Training iteration: 293400, Loss: 0.007183700799942017
2018-10-23 10:07:12.558256:	Training iteration: 293600, Loss: 0.002744395285844803
2018-10-23 10:08:31.101761:	Training iteration: 293800, Loss: 0.0048780255019664764
2018-10-23 10:09:50.156032:	Training iteration: 294000, Loss: 0.006856721360236406
2018-10-23 10:11:08.855278:	Training iteration: 294200, Loss: 0.003491561859846115
2018-10-23 10:12:27.340813:	Training iteration: 294400, Loss: 0.004463976249098778
2018-10-23 10:13:46.359375:	Training iteration: 294600, Loss: 0.005126423668116331
2018-10-23 10:15:05.277129:	Training iteration: 294800, Loss: 0.004336744546890259
2018-10-23 10:16:24.002743:	Training iteration: 295000, Loss: 0.007551342248916626
2018-10-23 10:17:43.093911:	Training iteration: 295200, Loss: 0.004879377316683531
2018-10-23 10:19:02.264792:	Training iteration: 295400, Loss: 0.00812841858714819
2018-10-23 10:20:21.580272:	Training iteration: 295600, Loss: 0.004166582133620977
2018-10-23 10:21:40.676337:	Training iteration: 295800, Loss: 0.0054127792827785015
2018-10-23 10:22:59.975609:	Training iteration: 296000, Loss: 0.009489166550338268
2018-10-23 10:24:19.316305:	Training iteration: 296200, Loss: 0.0035322762560099363
2018-10-23 10:25:38.297525:	Training iteration: 296400, Loss: 0.007895770482718945
2018-10-23 10:26:57.471896:	Training iteration: 296600, Loss: 0.005116862710565329
2018-10-23 10:28:16.952135:	Training iteration: 296800, Loss: 0.007217520382255316
2018-10-23 10:29:35.884349:	Training iteration: 297000, Loss: 0.005291726440191269
2018-10-23 10:30:55.110678:	Training iteration: 297200, Loss: 0.004407609347254038
2018-10-23 10:32:14.638488:	Training iteration: 297400, Loss: 0.004938409198075533
2018-10-23 10:33:33.945611:	Training iteration: 297600, Loss: 0.0036312981974333525
2018-10-23 10:34:53.120813:	Training iteration: 297800, Loss: 0.005254516378045082
2018-10-23 10:36:12.382938:	Training iteration: 298000, Loss: 0.004298523534089327
2018-10-23 10:37:31.490723:	Training iteration: 298200, Loss: 0.004972010850906372
2018-10-23 10:38:50.478168:	Training iteration: 298400, Loss: 0.005107790697365999
2018-10-23 10:40:09.068438:	Training iteration: 298600, Loss: 0.0035423028748482466
2018-10-23 10:41:28.094083:	Training iteration: 298800, Loss: 0.005380270536988974
2018-10-23 10:42:47.033420:	Training iteration: 299000, Loss: 0.004380783531814814
2018-10-23 10:44:05.883954:	Training iteration: 299200, Loss: 0.003910834435373545
2018-10-23 10:45:24.413811:	Training iteration: 299400, Loss: 0.004103975836187601
2018-10-23 10:46:43.323392:	Training iteration: 299600, Loss: 0.005903594195842743
2018-10-23 10:48:02.197523:	Training iteration: 299800, Loss: 0.0050207446329295635
2018-10-23 10:49:20.813959:	Training iteration: 300000, Loss: 0.0037038952577859163
Checkpoint
2018-10-23 10:50:50.619133:	Training iteration: 300200, Loss: 0.01014469563961029
2018-10-23 10:52:09.616709:	Training iteration: 300400, Loss: 0.00788277667015791
2018-10-23 10:53:28.386215:	Training iteration: 300600, Loss: 0.0072793238796293736
2018-10-23 10:54:46.825241:	Training iteration: 300800, Loss: 0.007107289973646402
2018-10-23 10:56:05.793428:	Training iteration: 301000, Loss: 0.005976694170385599
2018-10-23 10:57:24.658799:	Training iteration: 301200, Loss: 0.005529084708541632
2018-10-23 10:58:43.074448:	Training iteration: 301400, Loss: 0.005901578813791275
2018-10-23 11:00:02.204784:	Training iteration: 301600, Loss: 0.006106188055127859
2018-10-23 11:01:21.302663:	Training iteration: 301800, Loss: 0.005469894036650658
2018-10-23 11:02:40.524800:	Training iteration: 302000, Loss: 0.0063600786961615086
2018-10-23 11:04:00.171719:	Training iteration: 302200, Loss: 0.007472333963960409
2018-10-23 11:05:20.160304:	Training iteration: 302400, Loss: 0.008520283736288548
2018-10-23 11:06:39.804468:	Training iteration: 302600, Loss: 0.008497446775436401
2018-10-23 11:07:59.560721:	Training iteration: 302800, Loss: 0.005776350852102041
2018-10-23 11:09:19.935463:	Training iteration: 303000, Loss: 0.006768984254449606
2018-10-23 11:10:39.807479:	Training iteration: 303200, Loss: 0.0059908428229391575
2018-10-23 11:11:59.748227:	Training iteration: 303400, Loss: 0.00511140888556838
2018-10-23 11:13:20.083109:	Training iteration: 303600, Loss: 0.006457198411226273
2018-10-23 11:14:40.009867:	Training iteration: 303800, Loss: 0.006265351083129644
2018-10-23 11:15:59.851391:	Training iteration: 304000, Loss: 0.00828588381409645
2018-10-23 11:17:20.194353:	Training iteration: 304200, Loss: 0.006532222032546997
2018-10-23 11:18:40.441023:	Training iteration: 304400, Loss: 0.003329812316223979
2018-10-23 11:20:00.694135:	Training iteration: 304600, Loss: 0.006729259621351957
2018-10-23 11:21:20.332375:	Training iteration: 304800, Loss: 0.006127601955085993
2018-10-23 11:22:40.378398:	Training iteration: 305000, Loss: 0.006537009030580521
2018-10-23 11:24:00.532377:	Training iteration: 305200, Loss: 0.005851095542311668
2018-10-23 11:25:20.643639:	Training iteration: 305400, Loss: 0.004173055291175842
2018-10-23 11:26:40.726770:	Training iteration: 305600, Loss: 0.006898084189742804
2018-10-23 11:28:00.874021:	Training iteration: 305800, Loss: 0.003880681237205863
2018-10-23 11:29:21.236893:	Training iteration: 306000, Loss: 0.006500949617475271
2018-10-23 11:30:41.296181:	Training iteration: 306200, Loss: 0.004968176130205393
2018-10-23 11:32:01.361174:	Training iteration: 306400, Loss: 0.005448196083307266
2018-10-23 11:33:21.445309:	Training iteration: 306600, Loss: 0.0038994327187538147
2018-10-23 11:34:42.050273:	Training iteration: 306800, Loss: 0.0050408486276865005
2018-10-23 11:36:02.022047:	Training iteration: 307000, Loss: 0.004589755088090897
2018-10-23 11:37:22.308367:	Training iteration: 307200, Loss: 0.003913705237209797
2018-10-23 11:38:42.497896:	Training iteration: 307400, Loss: 0.006081053521484137
2018-10-23 11:40:02.271045:	Training iteration: 307600, Loss: 0.006794004235416651
2018-10-23 11:41:22.479668:	Training iteration: 307800, Loss: 0.002828819677233696
2018-10-23 11:42:42.012983:	Training iteration: 308000, Loss: 0.0048871394246816635
2018-10-23 11:44:01.326384:	Training iteration: 308200, Loss: 0.0038379512261599302
2018-10-23 11:45:20.582349:	Training iteration: 308400, Loss: 0.008830226957798004
2018-10-23 11:46:40.396049:	Training iteration: 308600, Loss: 0.005488247144967318
2018-10-23 11:47:59.924237:	Training iteration: 308800, Loss: 0.007244454231113195
2018-10-23 11:49:19.203862:	Training iteration: 309000, Loss: 0.005968703422695398
2018-10-23 11:50:38.540897:	Training iteration: 309200, Loss: 0.004553000908344984
2018-10-23 11:51:58.259505:	Training iteration: 309400, Loss: 0.006339117418974638
2018-10-23 11:53:17.326785:	Training iteration: 309600, Loss: 0.0041432492434978485
2018-10-23 11:54:36.400462:	Training iteration: 309800, Loss: 0.005221386905759573
2018-10-23 11:55:55.437013:	Training iteration: 310000, Loss: 0.004550882149487734
Checkpoint
2018-10-23 11:57:16.876212:	Training iteration: 310200, Loss: 0.005177323240786791
2018-10-23 11:58:36.327274:	Training iteration: 310400, Loss: 0.005534952040761709
2018-10-23 11:59:55.393007:	Training iteration: 310600, Loss: 0.005157656501978636
2018-10-23 12:01:14.906535:	Training iteration: 310800, Loss: 0.005711319390684366
2018-10-23 12:02:34.293657:	Training iteration: 311000, Loss: 0.006236393004655838
2018-10-23 12:03:53.788671:	Training iteration: 311200, Loss: 0.006785043980926275
2018-10-23 12:05:13.432676:	Training iteration: 311400, Loss: 0.0040606241673231125
2018-10-23 12:06:33.653629:	Training iteration: 311600, Loss: 0.006193955894559622
2018-10-23 12:07:53.495104:	Training iteration: 311800, Loss: 0.004928066860884428
2018-10-23 12:09:13.501293:	Training iteration: 312000, Loss: 0.005449675489217043
2018-10-23 12:10:33.449208:	Training iteration: 312200, Loss: 0.005020396318286657
2018-10-23 12:11:56.210200:	Training iteration: 312400, Loss: 0.004284258466213942
2018-10-23 12:13:15.400389:	Training iteration: 312600, Loss: 0.005048041697591543
2018-10-23 12:14:35.866180:	Training iteration: 312800, Loss: 0.003824452171102166
2018-10-23 12:15:56.152284:	Training iteration: 313000, Loss: 0.004904622677713633
2018-10-23 12:17:16.318017:	Training iteration: 313200, Loss: 0.0062615214847028255
2018-10-23 12:18:36.124032:	Training iteration: 313400, Loss: 0.006642904132604599
2018-10-23 12:19:56.543803:	Training iteration: 313600, Loss: 0.006436455994844437
2018-10-23 12:21:16.866940:	Training iteration: 313800, Loss: 0.006457423325628042
2018-10-23 12:22:36.800434:	Training iteration: 314000, Loss: 0.0043832650408148766
2018-10-23 12:23:57.212711:	Training iteration: 314200, Loss: 0.005043400451540947
2018-10-23 12:25:17.791448:	Training iteration: 314400, Loss: 0.004080201964825392
2018-10-23 12:26:38.021497:	Training iteration: 314600, Loss: 0.00425769854336977
2018-10-23 12:27:58.372343:	Training iteration: 314800, Loss: 0.003631533356383443
2018-10-23 12:29:19.066748:	Training iteration: 315000, Loss: 0.007721222471445799
2018-10-23 12:30:39.342022:	Training iteration: 315200, Loss: 0.005485009867697954
2018-10-23 12:31:59.408298:	Training iteration: 315400, Loss: 0.006559278815984726
2018-10-23 12:33:19.973337:	Training iteration: 315600, Loss: 0.007264129817485809
2018-10-23 12:34:40.557515:	Training iteration: 315800, Loss: 0.007639577612280846
2018-10-23 12:36:00.817545:	Training iteration: 316000, Loss: 0.005466973874717951
2018-10-23 12:37:21.119654:	Training iteration: 316200, Loss: 0.006059272680431604
2018-10-23 12:38:41.215130:	Training iteration: 316400, Loss: 0.004584821406751871
2018-10-23 12:40:01.686206:	Training iteration: 316600, Loss: 0.0057406178675591946
2018-10-23 12:41:21.398460:	Training iteration: 316800, Loss: 0.005631000269204378
2018-10-23 12:42:40.987179:	Training iteration: 317000, Loss: 0.006074349861592054
2018-10-23 12:44:00.814762:	Training iteration: 317200, Loss: 0.0046941363252699375
2018-10-23 12:45:19.828890:	Training iteration: 317400, Loss: 0.006167656276375055
2018-10-23 12:46:39.636550:	Training iteration: 317600, Loss: 0.006892643868923187
2018-10-23 12:47:59.383076:	Training iteration: 317800, Loss: 0.004113880917429924
2018-10-23 12:49:18.614994:	Training iteration: 318000, Loss: 0.004624096676707268
2018-10-23 12:50:37.805694:	Training iteration: 318200, Loss: 0.006667726207524538
2018-10-23 12:51:57.342279:	Training iteration: 318400, Loss: 0.0066617620177567005
2018-10-23 12:53:16.913854:	Training iteration: 318600, Loss: 0.004959681537002325
2018-10-23 12:54:36.163759:	Training iteration: 318800, Loss: 0.0036145129706710577
2018-10-23 12:55:55.321396:	Training iteration: 319000, Loss: 0.005235848482698202
2018-10-23 12:57:14.868827:	Training iteration: 319200, Loss: 0.005524171981960535
2018-10-23 12:58:34.102238:	Training iteration: 319400, Loss: 0.005949238780885935
2018-10-23 12:59:53.523423:	Training iteration: 319600, Loss: 0.00435016630217433
2018-10-23 13:01:12.885168:	Training iteration: 319800, Loss: 0.0050222245045006275
2018-10-23 13:01:41.212593:	Epoch 3 finished after 319873 iterations.
Validating
2018-10-23 13:01:41.265448:	Entering validation loop
2018-10-23 13:01:51.287138: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 677 of 1000
2018-10-23 13:01:55.527259: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 13:02:27.700758:	Validation iteration: 200, Loss: 0.005589397624135017
2018-10-23 13:03:00.891085:	Validation iteration: 400, Loss: 0.004344318527728319
2018-10-23 13:03:34.361041:	Validation iteration: 600, Loss: 0.0048721362836658955
2018-10-23 13:04:07.921848:	Validation iteration: 800, Loss: 0.0077641792595386505
2018-10-23 13:04:41.452169:	Validation iteration: 1000, Loss: 0.008637059479951859
2018-10-23 13:05:15.153109:	Validation iteration: 1200, Loss: 0.0053703803569078445
2018-10-23 13:05:58.289419: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 715 of 1000
2018-10-23 13:06:01.978316: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 13:06:03.385193:	Validation iteration: 1400, Loss: 0.0066350423730909824
2018-10-23 13:06:37.694904:	Validation iteration: 1600, Loss: 0.007195936981588602
2018-10-23 13:07:10.937083:	Validation iteration: 1800, Loss: 0.007074801251292229
2018-10-23 13:07:44.751091:	Validation iteration: 2000, Loss: 0.0060348971746861935
2018-10-23 13:08:18.388594:	Validation iteration: 2200, Loss: 0.007412215229123831
2018-10-23 13:08:51.982059:	Validation iteration: 2400, Loss: 0.006495032459497452
2018-10-23 13:09:26.298799:	Validation iteration: 2600, Loss: 0.005840543191879988
2018-10-23 13:10:07.873436: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 693 of 1000
2018-10-23 13:10:11.850093: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 13:10:14.472883:	Validation iteration: 2800, Loss: 0.005602648016065359
2018-10-23 13:10:47.117766:	Validation iteration: 3000, Loss: 0.004086285829544067
2018-10-23 13:11:20.763526:	Validation iteration: 3200, Loss: 0.005866695195436478
2018-10-23 13:11:54.382677:	Validation iteration: 3400, Loss: 0.007030741777271032
2018-10-23 13:12:27.957509:	Validation iteration: 3600, Loss: 0.005267426371574402
2018-10-23 13:13:01.796348:	Validation iteration: 3800, Loss: 0.004439568612724543
2018-10-23 13:13:36.035011:	Validation iteration: 4000, Loss: 0.004880067426711321
2018-10-23 13:14:16.217259: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 710 of 1000
2018-10-23 13:14:19.967831: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 13:14:23.824109:	Validation iteration: 4200, Loss: 0.0058069913648068905
2018-10-23 13:14:56.382044:	Validation iteration: 4400, Loss: 0.006391060072928667
2018-10-23 13:15:30.121565:	Validation iteration: 4600, Loss: 0.006463959347456694
2018-10-23 13:16:03.639324:	Validation iteration: 4800, Loss: 0.007227172609418631
2018-10-23 13:16:37.413412:	Validation iteration: 5000, Loss: 0.00923829060047865
2018-10-23 13:17:11.275019:	Validation iteration: 5200, Loss: 0.004858385771512985
2018-10-23 13:17:45.499628:	Validation iteration: 5400, Loss: 0.006415093317627907
2018-10-23 13:18:24.434741: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 655 of 1000
2018-10-23 13:18:29.183846: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 13:18:34.287804:	Validation iteration: 5600, Loss: 0.004657654091715813
2018-10-23 13:19:07.045496:	Validation iteration: 5800, Loss: 0.0033376123756170273
2018-10-23 13:19:40.461456:	Validation iteration: 6000, Loss: 0.004049292299896479
2018-10-23 13:20:14.451927:	Validation iteration: 6200, Loss: 0.004193481523543596
2018-10-23 13:20:48.201805:	Validation iteration: 6400, Loss: 0.005759734660387039
2018-10-23 13:21:21.885621:	Validation iteration: 6600, Loss: 0.0032963522244244814
2018-10-23 13:21:55.850690:	Validation iteration: 6800, Loss: 0.0034256719518452883
2018-10-23 13:22:29.916899:	Validation iteration: 7000, Loss: 0.00423813983798027
2018-10-23 13:23:03.658849:	Validation iteration: 7200, Loss: 0.0035137508530169725
2018-10-23 13:23:38.020700:	Validation iteration: 7400, Loss: 0.004269507247954607
Validation check mean loss: 0.005958478824743764
Validation loss has worsened. worse_val_checks = 1
2018-10-23 13:24:04.806752: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 787 of 1000
2018-10-23 13:24:07.302960: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 13:24:56.870688:	Training iteration: 320000, Loss: 0.004315302241593599
Checkpoint
2018-10-23 13:26:18.918883:	Training iteration: 320200, Loss: 0.004640880506485701
2018-10-23 13:27:38.780392:	Training iteration: 320400, Loss: 0.004376299679279327
2018-10-23 13:28:58.387465:	Training iteration: 320600, Loss: 0.004303514491766691
2018-10-23 13:30:17.890013:	Training iteration: 320800, Loss: 0.004683030303567648
2018-10-23 13:31:37.704652:	Training iteration: 321000, Loss: 0.0042932103388011456
2018-10-23 13:32:57.040576:	Training iteration: 321200, Loss: 0.0042927754111588
2018-10-23 13:34:16.208657:	Training iteration: 321400, Loss: 0.005831995513290167
2018-10-23 13:35:35.674006:	Training iteration: 321600, Loss: 0.004357669968158007
2018-10-23 13:36:54.930947:	Training iteration: 321800, Loss: 0.0071317716501653194
2018-10-23 13:38:13.703764:	Training iteration: 322000, Loss: 0.0036189977545291185
2018-10-23 13:39:33.691560:	Training iteration: 322200, Loss: 0.004346475936472416
2018-10-23 13:40:52.509954:	Training iteration: 322400, Loss: 0.00280868005938828
2018-10-23 13:42:11.116490:	Training iteration: 322600, Loss: 0.0036237749736756086
2018-10-23 13:43:28.882672:	Training iteration: 322800, Loss: 0.005512120667845011
2018-10-23 13:44:47.169543:	Training iteration: 323000, Loss: 0.005126813426613808
2018-10-23 13:46:05.218376:	Training iteration: 323200, Loss: 0.0037242949474602938
2018-10-23 13:47:22.783399:	Training iteration: 323400, Loss: 0.004892612341791391
2018-10-23 13:48:40.383217:	Training iteration: 323600, Loss: 0.0030215184669941664
2018-10-23 13:49:58.364397:	Training iteration: 323800, Loss: 0.0054608359932899475
2018-10-23 13:51:15.682715:	Training iteration: 324000, Loss: 0.0029803086072206497
2018-10-23 13:52:33.359856:	Training iteration: 324200, Loss: 0.00390612636692822
2018-10-23 13:53:51.023504:	Training iteration: 324400, Loss: 0.006226578261703253
2018-10-23 13:55:08.521065:	Training iteration: 324600, Loss: 0.004501255229115486
2018-10-23 13:56:26.289157:	Training iteration: 324800, Loss: 0.003730209544301033
2018-10-23 13:57:43.438367:	Training iteration: 325000, Loss: 0.004218447487801313
2018-10-23 13:59:00.900416:	Training iteration: 325200, Loss: 0.003997768275439739
2018-10-23 14:00:18.284821:	Training iteration: 325400, Loss: 0.004193045198917389
2018-10-23 14:01:35.399197:	Training iteration: 325600, Loss: 0.004103735089302063
2018-10-23 14:02:55.475117:	Training iteration: 325800, Loss: 0.0039580329321324825
2018-10-23 14:04:12.709745:	Training iteration: 326000, Loss: 0.003709506941959262
2018-10-23 14:05:32.034248:	Training iteration: 326200, Loss: 0.0044254823587834835
2018-10-23 14:06:49.657279:	Training iteration: 326400, Loss: 0.005084789823740721
2018-10-23 14:08:06.978703:	Training iteration: 326600, Loss: 0.003228827379643917
2018-10-23 14:09:24.913772:	Training iteration: 326800, Loss: 0.0036059487611055374
2018-10-23 14:10:42.472595:	Training iteration: 327000, Loss: 0.004610419739037752
2018-10-23 14:12:01.651578:	Training iteration: 327200, Loss: 0.00358186406083405
2018-10-23 14:13:19.314385:	Training iteration: 327400, Loss: 0.004226944409310818
2018-10-23 14:14:36.791881:	Training iteration: 327600, Loss: 0.0041342503391206264
2018-10-23 14:15:54.716749:	Training iteration: 327800, Loss: 0.00615753373131156
2018-10-23 14:17:12.984501:	Training iteration: 328000, Loss: 0.003250288078561425
2018-10-23 14:18:15.777013: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 761 of 1000
2018-10-23 14:18:18.560026: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 14:18:42.465338:	Training iteration: 328200, Loss: 0.004523629322648048
2018-10-23 14:19:59.439490:	Training iteration: 328400, Loss: 0.006558158900588751
2018-10-23 14:21:17.142612:	Training iteration: 328600, Loss: 0.004237764980643988
2018-10-23 14:22:35.634421:	Training iteration: 328800, Loss: 0.004618884529918432
2018-10-23 14:23:53.418675:	Training iteration: 329000, Loss: 0.005133162718266249
2018-10-23 14:25:10.823028:	Training iteration: 329200, Loss: 0.005885211285203695
2018-10-23 14:26:28.306931:	Training iteration: 329400, Loss: 0.006856419146060944
2018-10-23 14:27:46.135193:	Training iteration: 329600, Loss: 0.003482294036075473
2018-10-23 14:29:03.370029:	Training iteration: 329800, Loss: 0.0063504367135465145
2018-10-23 14:30:21.129263:	Training iteration: 330000, Loss: 0.006120818201452494
Checkpoint
2018-10-23 14:31:40.720406:	Training iteration: 330200, Loss: 0.0058888536877930164
2018-10-23 14:32:58.354924:	Training iteration: 330400, Loss: 0.004116742871701717
2018-10-23 14:34:16.001865:	Training iteration: 330600, Loss: 0.004462055861949921
2018-10-23 14:35:33.544828:	Training iteration: 330800, Loss: 0.004195222165435553
2018-10-23 14:36:51.334945:	Training iteration: 331000, Loss: 0.005663065705448389
2018-10-23 14:38:09.087138:	Training iteration: 331200, Loss: 0.0037108815740793943
2018-10-23 14:39:26.479884:	Training iteration: 331400, Loss: 0.005502887070178986
2018-10-23 14:40:44.292918:	Training iteration: 331600, Loss: 0.004172779154032469
2018-10-23 14:42:01.869464:	Training iteration: 331800, Loss: 0.003529544221237302
2018-10-23 14:43:19.901936:	Training iteration: 332000, Loss: 0.004473135340958834
2018-10-23 14:44:37.623448:	Training iteration: 332200, Loss: 0.003860615426674485
2018-10-23 14:45:54.702971:	Training iteration: 332400, Loss: 0.004401168320327997
2018-10-23 14:47:12.226890:	Training iteration: 332600, Loss: 0.0035405345261096954
2018-10-23 14:48:29.358561:	Training iteration: 332800, Loss: 0.006000037770718336
2018-10-23 14:49:46.546870:	Training iteration: 333000, Loss: 0.004106893669813871
2018-10-23 14:51:03.773678:	Training iteration: 333200, Loss: 0.006855411920696497
2018-10-23 14:52:20.562385:	Training iteration: 333400, Loss: 0.004598858766257763
2018-10-23 14:53:37.692451:	Training iteration: 333600, Loss: 0.006497176829725504
2018-10-23 14:54:54.563227:	Training iteration: 333800, Loss: 0.004677996505051851
2018-10-23 14:56:11.715569:	Training iteration: 334000, Loss: 0.0051439604721963406
2018-10-23 14:57:28.773478:	Training iteration: 334200, Loss: 0.004725802689790726
2018-10-23 14:58:45.656410:	Training iteration: 334400, Loss: 0.004483354277908802
2018-10-23 15:00:02.892241:	Training iteration: 334600, Loss: 0.004628696013242006
2018-10-23 15:01:19.403671:	Training iteration: 334800, Loss: 0.006359979975968599
2018-10-23 15:02:36.486173:	Training iteration: 335000, Loss: 0.00492641469463706
2018-10-23 15:03:53.243634:	Training iteration: 335200, Loss: 0.004234350752085447
2018-10-23 15:05:10.270145:	Training iteration: 335400, Loss: 0.004268808290362358
2018-10-23 15:06:27.288317:	Training iteration: 335600, Loss: 0.004651899915188551
2018-10-23 15:07:43.968319:	Training iteration: 335800, Loss: 0.004859659820795059
2018-10-23 15:09:01.298504:	Training iteration: 336000, Loss: 0.003895159112289548
2018-10-23 15:10:18.424817:	Training iteration: 336200, Loss: 0.004460341762751341
2018-10-23 15:11:35.878197:	Training iteration: 336400, Loss: 0.0035164933651685715
2018-10-23 15:12:04.516643: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 785 of 1000
2018-10-23 15:12:06.988989: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 15:13:04.650317:	Training iteration: 336600, Loss: 0.0038547387812286615
2018-10-23 15:14:21.736480:	Training iteration: 336800, Loss: 0.003716137958690524
2018-10-23 15:15:39.291809:	Training iteration: 337000, Loss: 0.0050281439907848835
2018-10-23 15:16:56.435363:	Training iteration: 337200, Loss: 0.005106428172439337
2018-10-23 15:18:13.881443:	Training iteration: 337400, Loss: 0.004160627257078886
2018-10-23 15:19:31.325708:	Training iteration: 337600, Loss: 0.006335353013128042
2018-10-23 15:20:48.639653:	Training iteration: 337800, Loss: 0.002639163052663207
2018-10-23 15:22:06.321961:	Training iteration: 338000, Loss: 0.004229322541505098
2018-10-23 15:23:23.836110:	Training iteration: 338200, Loss: 0.005799515638500452
2018-10-23 15:24:41.485046:	Training iteration: 338400, Loss: 0.005765413399785757
2018-10-23 15:25:59.102245:	Training iteration: 338600, Loss: 0.005992119666188955
2018-10-23 15:27:16.414166:	Training iteration: 338800, Loss: 0.004504311364144087
2018-10-23 15:28:34.286851:	Training iteration: 339000, Loss: 0.007171253208070993
2018-10-23 15:29:51.779296:	Training iteration: 339200, Loss: 0.004877228289842606
2018-10-23 15:31:09.398346:	Training iteration: 339400, Loss: 0.003672900842502713
2018-10-23 15:32:27.336725:	Training iteration: 339600, Loss: 0.00508333882316947
2018-10-23 15:33:44.610567:	Training iteration: 339800, Loss: 0.005672696977853775
2018-10-23 15:35:02.446029:	Training iteration: 340000, Loss: 0.005455721169710159
Checkpoint
2018-10-23 15:36:22.227808:	Training iteration: 340200, Loss: 0.0039879740215837955
2018-10-23 15:37:39.707386:	Training iteration: 340400, Loss: 0.0033780240919440985
2018-10-23 15:38:57.358473:	Training iteration: 340600, Loss: 0.003724694950506091
2018-10-23 15:40:14.853033:	Training iteration: 340800, Loss: 0.003343909978866577
2018-10-23 15:41:32.509481:	Training iteration: 341000, Loss: 0.005399372428655624
2018-10-23 15:42:50.181261:	Training iteration: 341200, Loss: 0.006215027067810297
2018-10-23 15:44:07.607652:	Training iteration: 341400, Loss: 0.006742391735315323
2018-10-23 15:45:25.383815:	Training iteration: 341600, Loss: 0.005231431219726801
2018-10-23 15:46:43.013200:	Training iteration: 341800, Loss: 0.004946419037878513
2018-10-23 15:48:00.480953:	Training iteration: 342000, Loss: 0.005802882369607687
2018-10-23 15:49:18.103923:	Training iteration: 342200, Loss: 0.004368311259895563
2018-10-23 15:50:35.188326:	Training iteration: 342400, Loss: 0.004030016716569662
2018-10-23 15:51:52.920873:	Training iteration: 342600, Loss: 0.004251276608556509
2018-10-23 15:53:10.297540:	Training iteration: 342800, Loss: 0.004376367200165987
2018-10-23 15:54:27.619277:	Training iteration: 343000, Loss: 0.004811879713088274
2018-10-23 15:55:45.327472:	Training iteration: 343200, Loss: 0.005098721478134394
2018-10-23 15:57:02.840066:	Training iteration: 343400, Loss: 0.007695915177464485
2018-10-23 15:58:20.353437:	Training iteration: 343600, Loss: 0.004799913614988327
2018-10-23 15:59:37.811874:	Training iteration: 343800, Loss: 0.0048760599456727505
2018-10-23 16:00:54.804011:	Training iteration: 344000, Loss: 0.005085689015686512
2018-10-23 16:02:12.228222:	Training iteration: 344200, Loss: 0.004948618356138468
2018-10-23 16:03:29.197006:	Training iteration: 344400, Loss: 0.0048656705766916275
2018-10-23 16:04:46.529467:	Training iteration: 344600, Loss: 0.00460066506639123
2018-10-23 16:05:55.894752: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 765 of 1000
2018-10-23 16:05:58.767985: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 16:06:15.948864:	Training iteration: 344800, Loss: 0.004714876879006624
2018-10-23 16:07:32.067431:	Training iteration: 345000, Loss: 0.003831223351880908
2018-10-23 16:08:49.253159:	Training iteration: 345200, Loss: 0.0038369817193597555
2018-10-23 16:10:06.363404:	Training iteration: 345400, Loss: 0.005276139359921217
2018-10-23 16:11:25.022520:	Training iteration: 345600, Loss: 0.0037504236679524183
2018-10-23 16:12:42.062675:	Training iteration: 345800, Loss: 0.0045721507631242275
2018-10-23 16:13:59.123636:	Training iteration: 346000, Loss: 0.004034763667732477
2018-10-23 16:15:16.256459:	Training iteration: 346200, Loss: 0.003537679323926568
2018-10-23 16:16:33.105180:	Training iteration: 346400, Loss: 0.005720290821045637
2018-10-23 16:17:50.104730:	Training iteration: 346600, Loss: 0.0035975428763777018
2018-10-23 16:19:06.910998:	Training iteration: 346800, Loss: 0.003953825682401657
2018-10-23 16:20:24.215031:	Training iteration: 347000, Loss: 0.003771999618038535
2018-10-23 16:21:41.187854:	Training iteration: 347200, Loss: 0.0033172715920954943
2018-10-23 16:22:58.399731:	Training iteration: 347400, Loss: 0.003567031817510724
2018-10-23 16:24:15.708429:	Training iteration: 347600, Loss: 0.003796745091676712
2018-10-23 16:25:33.135544:	Training iteration: 347800, Loss: 0.0030430431943386793
2018-10-23 16:26:50.683689:	Training iteration: 348000, Loss: 0.003794432384893298
2018-10-23 16:28:07.993459:	Training iteration: 348200, Loss: 0.004146473947912455
2018-10-23 16:29:25.778511:	Training iteration: 348400, Loss: 0.003724496578797698
2018-10-23 16:30:43.268344:	Training iteration: 348600, Loss: 0.003802924184128642
2018-10-23 16:32:00.777025:	Training iteration: 348800, Loss: 0.004308543633669615
2018-10-23 16:33:18.534938:	Training iteration: 349000, Loss: 0.006560587789863348
2018-10-23 16:34:35.881681:	Training iteration: 349200, Loss: 0.004234855528920889
2018-10-23 16:35:53.561432:	Training iteration: 349400, Loss: 0.0070044309832155704
2018-10-23 16:37:11.256442:	Training iteration: 349600, Loss: 0.0037947872187942266
2018-10-23 16:38:30.064088:	Training iteration: 349800, Loss: 0.005087119992822409
2018-10-23 16:39:48.061431:	Training iteration: 350000, Loss: 0.005078380461782217
Checkpoint
2018-10-23 16:41:08.003163:	Training iteration: 350200, Loss: 0.0040539707988500595
2018-10-23 16:42:25.467930:	Training iteration: 350400, Loss: 0.0037884239573031664
2018-10-23 16:43:44.290177:	Training iteration: 350600, Loss: 0.004384303465485573
2018-10-23 16:45:01.580488:	Training iteration: 350800, Loss: 0.00579925999045372
2018-10-23 16:46:19.577027:	Training iteration: 351000, Loss: 0.004833466839045286
2018-10-23 16:47:37.293871:	Training iteration: 351200, Loss: 0.00666198693215847
2018-10-23 16:48:54.901371:	Training iteration: 351400, Loss: 0.004239852074533701
2018-10-23 16:50:13.312353:	Training iteration: 351600, Loss: 0.004090907517820597
2018-10-23 16:51:30.780911:	Training iteration: 351800, Loss: 0.004242445342242718
2018-10-23 16:52:48.432060:	Training iteration: 352000, Loss: 0.006063398439437151
2018-10-23 16:54:06.083543:	Training iteration: 352200, Loss: 0.004622507840394974
2018-10-23 16:55:23.288451:	Training iteration: 352400, Loss: 0.004635067656636238
2018-10-23 16:56:40.888729:	Training iteration: 352600, Loss: 0.003543275408446789
2018-10-23 16:57:58.955428:	Training iteration: 352800, Loss: 0.005175330210477114
2018-10-23 16:59:16.784976:	Training iteration: 353000, Loss: 0.004319294821470976
2018-10-23 17:00:34.410228:	Training iteration: 353200, Loss: 0.00454748934134841
2018-10-23 17:01:51.631607:	Training iteration: 353400, Loss: 0.004028942435979843
2018-10-23 17:02:14.744937: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 834 of 1000
2018-10-23 17:02:16.506005: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 17:03:19.244301:	Training iteration: 353600, Loss: 0.008636618964374065
2018-10-23 17:04:36.252014:	Training iteration: 353800, Loss: 0.004199788440018892
2018-10-23 17:05:53.455496:	Training iteration: 354000, Loss: 0.005772537086158991
2018-10-23 17:07:10.725503:	Training iteration: 354200, Loss: 0.0064601958729326725
2018-10-23 17:08:28.057640:	Training iteration: 354400, Loss: 0.006399401929229498
2018-10-23 17:09:44.712514:	Training iteration: 354600, Loss: 0.005871079862117767
2018-10-23 17:11:02.099092:	Training iteration: 354800, Loss: 0.0038229841738939285
2018-10-23 17:12:19.241710:	Training iteration: 355000, Loss: 0.004263636190444231
2018-10-23 17:13:36.349776:	Training iteration: 355200, Loss: 0.0050255837850272655
2018-10-23 17:14:53.139480:	Training iteration: 355400, Loss: 0.0049429237842559814
2018-10-23 17:16:10.158899:	Training iteration: 355600, Loss: 0.007015090901404619
2018-10-23 17:17:26.635609:	Training iteration: 355800, Loss: 0.005831432994455099
2018-10-23 17:18:43.253090:	Training iteration: 356000, Loss: 0.0038966659922152758
2018-10-23 17:19:59.857620:	Training iteration: 356200, Loss: 0.005520234350115061
2018-10-23 17:21:16.233616:	Training iteration: 356400, Loss: 0.0030928237829357386
2018-10-23 17:22:32.738404:	Training iteration: 356600, Loss: 0.004513569641858339
2018-10-23 17:23:48.927109:	Training iteration: 356800, Loss: 0.002792813815176487
2018-10-23 17:25:05.473847:	Training iteration: 357000, Loss: 0.006856830324977636
2018-10-23 17:26:21.679834:	Training iteration: 357200, Loss: 0.004948326852172613
2018-10-23 17:27:38.430557:	Training iteration: 357400, Loss: 0.003801648737862706
2018-10-23 17:28:55.899846:	Training iteration: 357600, Loss: 0.0037639092188328505
2018-10-23 17:30:12.455279:	Training iteration: 357800, Loss: 0.00514355069026351
2018-10-23 17:31:28.249289:	Training iteration: 358000, Loss: 0.007626965641975403
2018-10-23 17:32:44.052697:	Training iteration: 358200, Loss: 0.006576416548341513
2018-10-23 17:34:00.640739:	Training iteration: 358400, Loss: 0.008941764943301678
2018-10-23 17:35:16.911845:	Training iteration: 358600, Loss: 0.0041608624160289764
2018-10-23 17:36:33.146564:	Training iteration: 358800, Loss: 0.003128844080492854
2018-10-23 17:37:49.812997:	Training iteration: 359000, Loss: 0.003876093775033951
2018-10-23 17:39:06.262219:	Training iteration: 359200, Loss: 0.004166719038039446
2018-10-23 17:40:23.483249:	Training iteration: 359400, Loss: 0.0032310138922184706
2018-10-23 17:41:40.383204:	Training iteration: 359600, Loss: 0.003090726910158992
2018-10-23 17:42:57.674003:	Training iteration: 359800, Loss: 0.0034257241059094667
2018-10-23 17:44:14.913513:	Training iteration: 360000, Loss: 0.006195297930389643
Checkpoint
2018-10-23 17:45:34.500812:	Training iteration: 360200, Loss: 0.005365973804146051
2018-10-23 17:46:52.741692:	Training iteration: 360400, Loss: 0.0035574177745729685
2018-10-23 17:48:10.591534:	Training iteration: 360600, Loss: 0.005461222026497126
2018-10-23 17:49:29.896854:	Training iteration: 360800, Loss: 0.007500655949115753
2018-10-23 17:50:48.381768:	Training iteration: 361000, Loss: 0.0040167104452848434
2018-10-23 17:52:06.720072:	Training iteration: 361200, Loss: 0.00498003838583827
2018-10-23 17:53:25.494810:	Training iteration: 361400, Loss: 0.003978084772825241
2018-10-23 17:54:45.502524:	Training iteration: 361600, Loss: 0.0047772591933608055
2018-10-23 17:56:04.568710:	Training iteration: 361800, Loss: 0.0043206545524299145
2018-10-23 17:57:22.271310:	Training iteration: 362000, Loss: 0.005365148186683655
2018-10-23 17:58:39.063642:	Training iteration: 362200, Loss: 0.003708736738190055
2018-10-23 17:59:55.402131:	Training iteration: 362400, Loss: 0.0039184014312922955
2018-10-23 18:01:11.932054:	Training iteration: 362600, Loss: 0.004117223899811506
2018-10-23 18:02:29.090478:	Training iteration: 362800, Loss: 0.006057557184249163
2018-10-23 18:03:46.017589:	Training iteration: 363000, Loss: 0.007912875153124332
2018-10-23 18:05:03.015301:	Training iteration: 363200, Loss: 0.004826620686799288
2018-10-23 18:06:20.913783:	Training iteration: 363400, Loss: 0.0050682323053479195
2018-10-23 18:07:39.952459:	Training iteration: 363600, Loss: 0.004232863429933786
2018-10-23 18:09:04.551247:	Training iteration: 363800, Loss: 0.007739314343780279
2018-10-23 18:10:24.877892:	Training iteration: 364000, Loss: 0.008241306990385056
2018-10-23 18:11:45.534328:	Training iteration: 364200, Loss: 0.005599891301244497
2018-10-23 18:13:06.717316:	Training iteration: 364400, Loss: 0.005373530089855194
2018-10-23 18:14:27.930110:	Training iteration: 364600, Loss: 0.006321358028799295
2018-10-23 18:15:49.466130:	Training iteration: 364800, Loss: 0.007976558059453964
2018-10-23 18:17:10.779529:	Training iteration: 365000, Loss: 0.00942582730203867
2018-10-23 18:18:31.890039:	Training iteration: 365200, Loss: 0.0078036286868155
2018-10-23 18:19:52.824579:	Training iteration: 365400, Loss: 0.006667772773653269
2018-10-23 18:21:13.736288:	Training iteration: 365600, Loss: 0.006838435772806406
2018-10-23 18:22:34.644758:	Training iteration: 365800, Loss: 0.003381083719432354
2018-10-23 18:23:55.789451:	Training iteration: 366000, Loss: 0.008122310973703861
2018-10-23 18:25:16.378795:	Training iteration: 366200, Loss: 0.007369920611381531
2018-10-23 18:26:37.078645:	Training iteration: 366400, Loss: 0.004988888744264841
2018-10-23 18:27:57.855911:	Training iteration: 366600, Loss: 0.008412598632276058
2018-10-23 18:29:18.449351:	Training iteration: 366800, Loss: 0.0050686984322965145
2018-10-23 18:30:38.944045:	Training iteration: 367000, Loss: 0.004653125070035458
2018-10-23 18:31:59.143446:	Training iteration: 367200, Loss: 0.005846533924341202
2018-10-23 18:33:19.496483:	Training iteration: 367400, Loss: 0.008161826990544796
2018-10-23 18:34:39.747359:	Training iteration: 367600, Loss: 0.00645011430606246
2018-10-23 18:36:00.159669:	Training iteration: 367800, Loss: 0.005019502714276314
2018-10-23 18:37:20.042019:	Training iteration: 368000, Loss: 0.007173157762736082
2018-10-23 18:38:40.744222:	Training iteration: 368200, Loss: 0.005963796284049749
2018-10-23 18:40:01.128948:	Training iteration: 368400, Loss: 0.005908890161663294
2018-10-23 18:41:23.545264:	Training iteration: 368600, Loss: 0.006626442074775696
2018-10-23 18:42:44.297525:	Training iteration: 368800, Loss: 0.006765392143279314
2018-10-23 18:44:04.933821:	Training iteration: 369000, Loss: 0.005922921001911163
2018-10-23 18:45:25.703262:	Training iteration: 369200, Loss: 0.007802899926900864
2018-10-23 18:46:46.485958:	Training iteration: 369400, Loss: 0.006638217251747847
2018-10-23 18:48:07.549171:	Training iteration: 369600, Loss: 0.0061677731573581696
2018-10-23 18:49:28.428964:	Training iteration: 369800, Loss: 0.004679481033235788
2018-10-23 18:50:49.281459:	Training iteration: 370000, Loss: 0.005335817579180002
Checkpoint
2018-10-23 18:52:12.299955:	Training iteration: 370200, Loss: 0.00532849645242095
2018-10-23 18:53:33.491460:	Training iteration: 370400, Loss: 0.003234229749068618
2018-10-23 18:54:54.293519:	Training iteration: 370600, Loss: 0.005830736365169287
2018-10-23 18:56:15.445661:	Training iteration: 370800, Loss: 0.005951263010501862
2018-10-23 18:57:36.678466:	Training iteration: 371000, Loss: 0.00827066507190466
2018-10-23 18:58:58.040836:	Training iteration: 371200, Loss: 0.007689908612519503
2018-10-23 19:00:19.573354:	Training iteration: 371400, Loss: 0.005717792082577944
2018-10-23 19:01:40.613811:	Training iteration: 371600, Loss: 0.007344414945691824
2018-10-23 19:03:02.150284:	Training iteration: 371800, Loss: 0.00840537715703249
2018-10-23 19:04:23.180729:	Training iteration: 372000, Loss: 0.0027714089956134558
2018-10-23 19:05:44.739725:	Training iteration: 372200, Loss: 0.008504926227033138
2018-10-23 19:07:06.021929:	Training iteration: 372400, Loss: 0.008183802478015423
2018-10-23 19:08:27.300173:	Training iteration: 372600, Loss: 0.011442574672400951
2018-10-23 19:09:48.855288:	Training iteration: 372800, Loss: 0.004668690264225006
2018-10-23 19:11:10.220046:	Training iteration: 373000, Loss: 0.003358346177265048
2018-10-23 19:12:31.896523:	Training iteration: 373200, Loss: 0.005444031674414873
2018-10-23 19:13:53.303672:	Training iteration: 373400, Loss: 0.007592926267534494
2018-10-23 19:15:15.143604:	Training iteration: 373600, Loss: 0.009296268224716187
2018-10-23 19:16:37.040233:	Training iteration: 373800, Loss: 0.0047078812494874
2018-10-23 19:17:58.855600:	Training iteration: 374000, Loss: 0.007485745474696159
2018-10-23 19:19:20.669509:	Training iteration: 374200, Loss: 0.003939099609851837
2018-10-23 19:20:42.040644:	Training iteration: 374400, Loss: 0.005709649529308081
2018-10-23 19:22:03.532596:	Training iteration: 374600, Loss: 0.006314691621810198
2018-10-23 19:23:24.970576:	Training iteration: 374800, Loss: 0.0050298781134188175
2018-10-23 19:24:45.975794:	Training iteration: 375000, Loss: 0.004815986845642328
2018-10-23 19:26:07.039717:	Training iteration: 375200, Loss: 0.00561818853020668
2018-10-23 19:27:28.324890:	Training iteration: 375400, Loss: 0.008262512274086475
2018-10-23 19:28:49.733748:	Training iteration: 375600, Loss: 0.003827278269454837
2018-10-23 19:30:11.041658:	Training iteration: 375800, Loss: 0.012481637299060822
2018-10-23 19:31:31.848724:	Training iteration: 376000, Loss: 0.007501110900193453
2018-10-23 19:32:52.781288:	Training iteration: 376200, Loss: 0.004635721445083618
2018-10-23 19:34:13.870177:	Training iteration: 376400, Loss: 0.00518958643078804
2018-10-23 19:35:34.786717:	Training iteration: 376600, Loss: 0.00491129606962204
2018-10-23 19:36:55.485646:	Training iteration: 376800, Loss: 0.005473676603287458
2018-10-23 19:38:16.170559:	Training iteration: 377000, Loss: 0.005294788163155317
2018-10-23 19:39:37.004179:	Training iteration: 377200, Loss: 0.007718794047832489
2018-10-23 19:40:57.765077:	Training iteration: 377400, Loss: 0.005266272928565741
2018-10-23 19:42:18.772945:	Training iteration: 377600, Loss: 0.004845339339226484
2018-10-23 19:43:39.907834:	Training iteration: 377800, Loss: 0.004257021006196737
2018-10-23 19:45:01.188819:	Training iteration: 378000, Loss: 0.004869547672569752
2018-10-23 19:46:22.783689:	Training iteration: 378200, Loss: 0.004316648002713919
2018-10-23 19:47:44.487695:	Training iteration: 378400, Loss: 0.00672844797372818
2018-10-23 19:49:06.132559:	Training iteration: 378600, Loss: 0.0038094872143119574
2018-10-23 19:50:27.708405:	Training iteration: 378800, Loss: 0.005141464062035084
2018-10-23 19:51:49.258279:	Training iteration: 379000, Loss: 0.004154234658926725
2018-10-23 19:53:11.015484:	Training iteration: 379200, Loss: 0.0061180382035672665
2018-10-23 19:54:32.203243:	Training iteration: 379400, Loss: 0.003793962299823761
2018-10-23 19:55:53.543068:	Training iteration: 379600, Loss: 0.004910679999738932
2018-10-23 19:57:15.241558:	Training iteration: 379800, Loss: 0.0039469110779464245
2018-10-23 19:58:36.945425:	Training iteration: 380000, Loss: 0.004698872100561857
Checkpoint
2018-10-23 20:00:00.770171:	Training iteration: 380200, Loss: 0.010112063027918339
2018-10-23 20:01:24.299353:	Training iteration: 380400, Loss: 0.005296558141708374
2018-10-23 20:02:45.791608:	Training iteration: 380600, Loss: 0.007067100610584021
2018-10-23 20:04:07.155914:	Training iteration: 380800, Loss: 0.0054665300995111465
2018-10-23 20:05:28.619777:	Training iteration: 381000, Loss: 0.00409081531688571
2018-10-23 20:06:49.689647:	Training iteration: 381200, Loss: 0.005095026921480894
2018-10-23 20:08:10.705648:	Training iteration: 381400, Loss: 0.006062394008040428
2018-10-23 20:09:33.562015:	Training iteration: 381600, Loss: 0.0063583157025277615
2018-10-23 20:10:54.858007:	Training iteration: 381800, Loss: 0.004058895166963339
2018-10-23 20:12:16.200121:	Training iteration: 382000, Loss: 0.0060446676798164845
2018-10-23 20:13:37.656973:	Training iteration: 382200, Loss: 0.007900298573076725
2018-10-23 20:14:59.269821:	Training iteration: 382400, Loss: 0.006510225590318441
2018-10-23 20:16:20.948328:	Training iteration: 382600, Loss: 0.0038737161085009575
2018-10-23 20:17:42.825596:	Training iteration: 382800, Loss: 0.009335130453109741
2018-10-23 20:19:03.987426:	Training iteration: 383000, Loss: 0.005962108727544546
2018-10-23 20:20:25.251791:	Training iteration: 383200, Loss: 0.007934014312922955
2018-10-23 20:21:46.350854:	Training iteration: 383400, Loss: 0.004855914507061243
2018-10-23 20:23:07.505059:	Training iteration: 383600, Loss: 0.005541152786463499
2018-10-23 20:24:28.415808:	Training iteration: 383800, Loss: 0.005330575164407492
2018-10-23 20:25:49.628702:	Training iteration: 384000, Loss: 0.007063431199640036
2018-10-23 20:27:10.206067:	Training iteration: 384200, Loss: 0.004243839532136917
2018-10-23 20:28:30.977704:	Training iteration: 384400, Loss: 0.006682420615106821
2018-10-23 20:29:51.327933:	Training iteration: 384600, Loss: 0.007138121873140335
2018-10-23 20:31:11.682669:	Training iteration: 384800, Loss: 0.004275347106158733
2018-10-23 20:32:32.303691:	Training iteration: 385000, Loss: 0.006088830530643463
2018-10-23 20:33:52.617185:	Training iteration: 385200, Loss: 0.0027005914598703384
2018-10-23 20:35:12.821833:	Training iteration: 385400, Loss: 0.005735897924751043
2018-10-23 20:36:32.754411:	Training iteration: 385600, Loss: 0.003324408084154129
2018-10-23 20:37:53.006999:	Training iteration: 385800, Loss: 0.003065644996240735
2018-10-23 20:39:12.897032:	Training iteration: 386000, Loss: 0.0033611527178436518
2018-10-23 20:40:32.728563:	Training iteration: 386200, Loss: 0.008492641150951385
2018-10-23 20:41:53.298509:	Training iteration: 386400, Loss: 0.0046604108065366745
2018-10-23 20:43:13.534934:	Training iteration: 386600, Loss: 0.0036611463874578476
2018-10-23 20:44:34.073211:	Training iteration: 386800, Loss: 0.005348860751837492
2018-10-23 20:45:54.600789:	Training iteration: 387000, Loss: 0.0038924196269363165
2018-10-23 20:47:15.064400:	Training iteration: 387200, Loss: 0.004014588426798582
2018-10-23 20:48:35.368833:	Training iteration: 387400, Loss: 0.0061332713812589645
2018-10-23 20:49:56.238418:	Training iteration: 387600, Loss: 0.004917226731777191
2018-10-23 20:51:16.807407:	Training iteration: 387800, Loss: 0.004105756059288979
2018-10-23 20:52:37.325341:	Training iteration: 388000, Loss: 0.002583852270618081
2018-10-23 20:53:58.282839:	Training iteration: 388200, Loss: 0.004897172097116709
2018-10-23 20:55:19.333937:	Training iteration: 388400, Loss: 0.006361259613186121
2018-10-23 20:56:40.050468:	Training iteration: 388600, Loss: 0.005549713969230652
2018-10-23 20:58:00.955898:	Training iteration: 388800, Loss: 0.007126232609152794
2018-10-23 20:59:22.050698:	Training iteration: 389000, Loss: 0.006771689746528864
2018-10-23 21:00:42.722601:	Training iteration: 389200, Loss: 0.0028174796607345343
2018-10-23 21:02:03.651268:	Training iteration: 389400, Loss: 0.005656833294779062
2018-10-23 21:03:24.721804:	Training iteration: 389600, Loss: 0.003737865714356303
2018-10-23 21:04:45.536539:	Training iteration: 389800, Loss: 0.005872347857803106
2018-10-23 21:06:06.763208:	Training iteration: 390000, Loss: 0.005379226058721542
Checkpoint
2018-10-23 21:07:30.707055:	Training iteration: 390200, Loss: 0.006939323153346777
2018-10-23 21:08:50.860526:	Training iteration: 390400, Loss: 0.006813072133809328
2018-10-23 21:10:11.600493:	Training iteration: 390600, Loss: 0.004092444200068712
2018-10-23 21:11:32.747618:	Training iteration: 390800, Loss: 0.004321918822824955
2018-10-23 21:12:53.889096:	Training iteration: 391000, Loss: 0.0053887018002569675
2018-10-23 21:14:14.847085:	Training iteration: 391200, Loss: 0.007123306393623352
2018-10-23 21:15:35.741598:	Training iteration: 391400, Loss: 0.0040557305328547955
2018-10-23 21:16:56.371058:	Training iteration: 391600, Loss: 0.005643319338560104
2018-10-23 21:18:17.338397:	Training iteration: 391800, Loss: 0.006667595822364092
2018-10-23 21:19:37.977396:	Training iteration: 392000, Loss: 0.005786605644971132
2018-10-23 21:20:58.866170:	Training iteration: 392200, Loss: 0.0036582008469849825
2018-10-23 21:22:19.741218:	Training iteration: 392400, Loss: 0.004571112804114819
2018-10-23 21:23:40.618081:	Training iteration: 392600, Loss: 0.00512394355610013
2018-10-23 21:25:01.333004:	Training iteration: 392800, Loss: 0.0035804733633995056
2018-10-23 21:26:21.696517:	Training iteration: 393000, Loss: 0.003617743728682399
2018-10-23 21:27:42.416927:	Training iteration: 393200, Loss: 0.004572572186589241
2018-10-23 21:29:02.998347:	Training iteration: 393400, Loss: 0.005039017181843519
2018-10-23 21:30:23.189118:	Training iteration: 393600, Loss: 0.0032971708569675684
2018-10-23 21:31:43.522404:	Training iteration: 393800, Loss: 0.004645937588065863
2018-10-23 21:33:03.424415:	Training iteration: 394000, Loss: 0.005174923222512007
2018-10-23 21:34:23.567216:	Training iteration: 394200, Loss: 0.0038077549543231726
2018-10-23 21:35:43.699577:	Training iteration: 394400, Loss: 0.004954247269779444
2018-10-23 21:37:03.918539:	Training iteration: 394600, Loss: 0.006302542984485626
2018-10-23 21:38:24.002249:	Training iteration: 394800, Loss: 0.0035535746719688177
2018-10-23 21:39:44.052887:	Training iteration: 395000, Loss: 0.005870867520570755
2018-10-23 21:41:04.011981:	Training iteration: 395200, Loss: 0.01082244236022234
2018-10-23 21:42:24.142696:	Training iteration: 395400, Loss: 0.006260822992771864
2018-10-23 21:43:44.099974:	Training iteration: 395600, Loss: 0.0073527805507183075
2018-10-23 21:45:03.979923:	Training iteration: 395800, Loss: 0.0048723905347287655
2018-10-23 21:46:23.859393:	Training iteration: 396000, Loss: 0.005350814666599035
2018-10-23 21:47:43.911389:	Training iteration: 396200, Loss: 0.005627464037388563
2018-10-23 21:49:04.218474:	Training iteration: 396400, Loss: 0.004729167558252811
2018-10-23 21:50:24.348489:	Training iteration: 396600, Loss: 0.0031799878925085068
2018-10-23 21:51:44.973167:	Training iteration: 396800, Loss: 0.005941517651081085
2018-10-23 21:53:05.074215:	Training iteration: 397000, Loss: 0.00578106427565217
2018-10-23 21:54:25.763866:	Training iteration: 397200, Loss: 0.00762091064825654
2018-10-23 21:55:46.277404:	Training iteration: 397400, Loss: 0.008534613065421581
2018-10-23 21:57:07.185163:	Training iteration: 397600, Loss: 0.004676729440689087
2018-10-23 21:58:27.917202:	Training iteration: 397800, Loss: 0.00760965421795845
2018-10-23 21:59:48.597004:	Training iteration: 398000, Loss: 0.0033486734610050917
2018-10-23 22:01:09.525244:	Training iteration: 398200, Loss: 0.005037178751081228
2018-10-23 22:02:30.295329:	Training iteration: 398400, Loss: 0.004477867856621742
2018-10-23 22:03:51.102970:	Training iteration: 398600, Loss: 0.0062710256315767765
2018-10-23 22:05:12.196217:	Training iteration: 398800, Loss: 0.004270227625966072
2018-10-23 22:06:33.086483:	Training iteration: 399000, Loss: 0.00470050610601902
2018-10-23 22:07:54.109763:	Training iteration: 399200, Loss: 0.009961101226508617
2018-10-23 22:09:15.206023:	Training iteration: 399400, Loss: 0.006231662351638079
2018-10-23 22:10:36.312026:	Training iteration: 399600, Loss: 0.007749117445200682
2018-10-23 22:11:57.424448:	Training iteration: 399800, Loss: 0.005352642387151718
2018-10-23 22:12:13.403688:	Epoch 4 finished after 399841 iterations.
Validating
2018-10-23 22:12:13.421739:	Entering validation loop
2018-10-23 22:12:23.440226: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 677 of 1000
2018-10-23 22:12:27.915355: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 22:13:00.682960:	Validation iteration: 200, Loss: 0.00625594099983573
2018-10-23 22:13:35.462397:	Validation iteration: 400, Loss: 0.006681249011307955
2018-10-23 22:14:09.601733:	Validation iteration: 600, Loss: 0.005320632830262184
2018-10-23 22:14:43.867453:	Validation iteration: 800, Loss: 0.0071371193043887615
2018-10-23 22:15:18.087316:	Validation iteration: 1000, Loss: 0.005870219320058823
2018-10-23 22:15:52.695185:	Validation iteration: 1200, Loss: 0.007008494809269905
2018-10-23 22:16:36.156465: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 705 of 1000
2018-10-23 22:16:40.140853: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 22:16:41.507082:	Validation iteration: 1400, Loss: 0.005208481103181839
2018-10-23 22:17:14.234165:	Validation iteration: 1600, Loss: 0.005443160887807608
2018-10-23 22:17:49.604032:	Validation iteration: 1800, Loss: 0.005281440448015928
2018-10-23 22:18:23.682674:	Validation iteration: 2000, Loss: 0.006215507630258799
2018-10-23 22:18:58.044182:	Validation iteration: 2200, Loss: 0.0059059299528598785
2018-10-23 22:19:32.274425:	Validation iteration: 2400, Loss: 0.008158196695148945
2018-10-23 22:20:06.582405:	Validation iteration: 2600, Loss: 0.006176656577736139
2018-10-23 22:20:48.026597: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 689 of 1000
2018-10-23 22:20:52.123940: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 22:20:54.729989:	Validation iteration: 2800, Loss: 0.006757363677024841
2018-10-23 22:21:27.363342:	Validation iteration: 3000, Loss: 0.005000170785933733
2018-10-23 22:22:01.410631:	Validation iteration: 3200, Loss: 0.004677258897572756
2018-10-23 22:22:35.488440:	Validation iteration: 3400, Loss: 0.005631579086184502
2018-10-23 22:23:09.754306:	Validation iteration: 3600, Loss: 0.005864763166755438
2018-10-23 22:23:44.736256:	Validation iteration: 3800, Loss: 0.0049729798920452595
2018-10-23 22:24:19.312537:	Validation iteration: 4000, Loss: 0.005302288103848696
2018-10-23 22:24:59.719923: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 703 of 1000
2018-10-23 22:25:03.585506: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 22:25:07.441361:	Validation iteration: 4200, Loss: 0.006709592882543802
2018-10-23 22:25:40.127176:	Validation iteration: 4400, Loss: 0.007541883271187544
2018-10-23 22:26:13.921094:	Validation iteration: 4600, Loss: 0.0051589407958090305
2018-10-23 22:26:48.143360:	Validation iteration: 4800, Loss: 0.006738975178450346
2018-10-23 22:27:22.282134:	Validation iteration: 5000, Loss: 0.006550021935254335
2018-10-23 22:27:56.201711:	Validation iteration: 5200, Loss: 0.005766075570136309
2018-10-23 22:28:30.512760:	Validation iteration: 5400, Loss: 0.005189783405512571
2018-10-23 22:29:09.439063: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 656 of 1000
2018-10-23 22:29:16.988891: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 22:29:22.044718:	Validation iteration: 5600, Loss: 0.004027100745588541
2018-10-23 22:29:54.425119:	Validation iteration: 5800, Loss: 0.006707977969199419
2018-10-23 22:30:28.279372:	Validation iteration: 6000, Loss: 0.006160194519907236
2018-10-23 22:31:02.209126:	Validation iteration: 6200, Loss: 0.0047786724753677845
2018-10-23 22:31:36.171103:	Validation iteration: 6400, Loss: 0.006100100930780172
2018-10-23 22:32:10.313648:	Validation iteration: 6600, Loss: 0.006298986729234457
2018-10-23 22:32:44.448924:	Validation iteration: 6800, Loss: 0.005365705117583275
2018-10-23 22:33:18.532014:	Validation iteration: 7000, Loss: 0.004867138806730509
2018-10-23 22:33:52.557021:	Validation iteration: 7200, Loss: 0.005681145936250687
2018-10-23 22:34:26.614692:	Validation iteration: 7400, Loss: 0.006778462324291468
Validation check mean loss: 0.0059275077943456355
Validation loss has improved!
New best validation cost!
2018-10-23 22:34:53.191874: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 765 of 1000
2018-10-23 22:34:56.048994: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 22:35:58.644660:	Training iteration: 400000, Loss: 0.0035980604588985443
Checkpoint
2018-10-23 22:37:21.289017:	Training iteration: 400200, Loss: 0.005228313151746988
2018-10-23 22:38:41.717923:	Training iteration: 400400, Loss: 0.00390395219437778
2018-10-23 22:40:02.426720:	Training iteration: 400600, Loss: 0.004223082214593887
2018-10-23 22:41:22.863663:	Training iteration: 400800, Loss: 0.00387769378721714
2018-10-23 22:42:43.150555:	Training iteration: 401000, Loss: 0.003658488392829895
2018-10-23 22:44:03.485394:	Training iteration: 401200, Loss: 0.003922064322978258
2018-10-23 22:45:23.963166:	Training iteration: 401400, Loss: 0.004688646644353867
2018-10-23 22:46:44.176430:	Training iteration: 401600, Loss: 0.004618298728018999
2018-10-23 22:48:04.816443:	Training iteration: 401800, Loss: 0.0027525026816874743
2018-10-23 22:49:25.335842:	Training iteration: 402000, Loss: 0.004247923847287893
2018-10-23 22:50:46.976478:	Training iteration: 402200, Loss: 0.0035430844873189926
2018-10-23 22:52:08.129675:	Training iteration: 402400, Loss: 0.004491697996854782
2018-10-23 22:53:28.927819:	Training iteration: 402600, Loss: 0.0039152223616838455
2018-10-23 22:54:50.434369:	Training iteration: 402800, Loss: 0.0034647139254957438
2018-10-23 22:56:11.575927:	Training iteration: 403000, Loss: 0.006058637518435717
2018-10-23 22:57:33.281428:	Training iteration: 403200, Loss: 0.004986314103007317
2018-10-23 22:58:54.499069:	Training iteration: 403400, Loss: 0.005249299108982086
2018-10-23 23:00:16.001139:	Training iteration: 403600, Loss: 0.004513282794505358
2018-10-23 23:01:39.209007:	Training iteration: 403800, Loss: 0.0037320449482649565
2018-10-23 23:03:00.658800:	Training iteration: 404000, Loss: 0.004949918482452631
2018-10-23 23:04:24.648953:	Training iteration: 404200, Loss: 0.004390574060380459
2018-10-23 23:05:46.590639:	Training iteration: 404400, Loss: 0.0033279850613325834
2018-10-23 23:07:08.076383:	Training iteration: 404600, Loss: 0.0048630028031766415
2018-10-23 23:08:29.680433:	Training iteration: 404800, Loss: 0.0034387223422527313
2018-10-23 23:09:51.377504:	Training iteration: 405000, Loss: 0.005150509532541037
2018-10-23 23:11:13.030204:	Training iteration: 405200, Loss: 0.0035941542591899633
2018-10-23 23:12:35.003013:	Training iteration: 405400, Loss: 0.004606329370290041
2018-10-23 23:13:56.540089:	Training iteration: 405600, Loss: 0.003939240705221891
2018-10-23 23:15:18.172766:	Training iteration: 405800, Loss: 0.007316677365452051
2018-10-23 23:16:39.590867:	Training iteration: 406000, Loss: 0.004890215117484331
2018-10-23 23:18:01.022438:	Training iteration: 406200, Loss: 0.0036941885482519865
2018-10-23 23:19:22.576380:	Training iteration: 406400, Loss: 0.003751949639990926
2018-10-23 23:20:44.093476:	Training iteration: 406600, Loss: 0.004541853908449411
2018-10-23 23:22:05.500428:	Training iteration: 406800, Loss: 0.004626155365258455
2018-10-23 23:23:27.331739:	Training iteration: 407000, Loss: 0.005041966214776039
2018-10-23 23:24:48.614310:	Training iteration: 407200, Loss: 0.0031913721468299627
2018-10-23 23:26:10.054410:	Training iteration: 407400, Loss: 0.004005350638180971
2018-10-23 23:27:31.644372:	Training iteration: 407600, Loss: 0.005510328803211451
2018-10-23 23:28:52.894301:	Training iteration: 407800, Loss: 0.004256603308022022
2018-10-23 23:30:14.176577:	Training iteration: 408000, Loss: 0.003313997760415077
2018-10-23 23:31:06.440317: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 754 of 1000
2018-10-23 23:31:09.469690: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 23:31:46.797727:	Training iteration: 408200, Loss: 0.004620256833732128
2018-10-23 23:33:07.542193:	Training iteration: 408400, Loss: 0.005043205339461565
2018-10-23 23:34:28.547257:	Training iteration: 408600, Loss: 0.0049483394250273705
2018-10-23 23:35:49.490188:	Training iteration: 408800, Loss: 0.00577651197090745
2018-10-23 23:37:10.333078:	Training iteration: 409000, Loss: 0.004767559934407473
2018-10-23 23:38:31.129733:	Training iteration: 409200, Loss: 0.003948697820305824
2018-10-23 23:39:52.120869:	Training iteration: 409400, Loss: 0.004536369349807501
2018-10-23 23:41:12.644558:	Training iteration: 409600, Loss: 0.004308335017412901
2018-10-23 23:42:33.500784:	Training iteration: 409800, Loss: 0.0033730838913470507
2018-10-23 23:43:54.229597:	Training iteration: 410000, Loss: 0.003767792135477066
Checkpoint
2018-10-23 23:45:17.201871:	Training iteration: 410200, Loss: 0.004085004795342684
2018-10-23 23:46:38.633072:	Training iteration: 410400, Loss: 0.003949511330574751
2018-10-23 23:47:59.252048:	Training iteration: 410600, Loss: 0.004139693453907967
2018-10-23 23:49:19.870923:	Training iteration: 410800, Loss: 0.0064584375359117985
2018-10-23 23:50:40.066675:	Training iteration: 411000, Loss: 0.004860094282776117
2018-10-23 23:52:01.230381:	Training iteration: 411200, Loss: 0.004114695359021425
2018-10-23 23:53:21.984113:	Training iteration: 411400, Loss: 0.004677394405007362
2018-10-23 23:54:43.462717:	Training iteration: 411600, Loss: 0.004656681325286627
2018-10-23 23:56:04.838875:	Training iteration: 411800, Loss: 0.004848110023885965
2018-10-23 23:57:26.319222:	Training iteration: 412000, Loss: 0.004455530550330877
2018-10-23 23:58:47.806461:	Training iteration: 412200, Loss: 0.004583131987601519
2018-10-24 00:00:09.495782:	Training iteration: 412400, Loss: 0.004806478042155504
2018-10-24 00:01:31.009179:	Training iteration: 412600, Loss: 0.00516193313524127
2018-10-24 00:02:52.735974:	Training iteration: 412800, Loss: 0.004692962858825922
2018-10-24 00:04:14.399531:	Training iteration: 413000, Loss: 0.004081264603883028
2018-10-24 00:05:37.380207:	Training iteration: 413200, Loss: 0.006156084593385458
2018-10-24 00:06:59.116219:	Training iteration: 413400, Loss: 0.005017183721065521
2018-10-24 00:08:20.693826:	Training iteration: 413600, Loss: 0.0039170850068330765
2018-10-24 00:09:42.620289:	Training iteration: 413800, Loss: 0.00362287531606853
2018-10-24 00:11:04.359904:	Training iteration: 414000, Loss: 0.0036189488600939512
2018-10-24 00:12:25.421733:	Training iteration: 414200, Loss: 0.005141750443726778
2018-10-24 00:13:47.286962:	Training iteration: 414400, Loss: 0.003958776593208313
2018-10-24 00:15:09.077900:	Training iteration: 414600, Loss: 0.0038529771845787764
2018-10-24 00:16:30.989865:	Training iteration: 414800, Loss: 0.004319129977375269
2018-10-24 00:17:53.261227:	Training iteration: 415000, Loss: 0.003726450726389885
2018-10-24 00:19:14.970298:	Training iteration: 415200, Loss: 0.00497442064806819
2018-10-24 00:20:36.763230:	Training iteration: 415400, Loss: 0.00469674589112401
2018-10-24 00:21:58.600753:	Training iteration: 415600, Loss: 0.0038580161053687334
2018-10-24 00:23:20.364459:	Training iteration: 415800, Loss: 0.0045376732014119625
2018-10-24 00:24:42.270132:	Training iteration: 416000, Loss: 0.0049360268749296665
2018-10-24 00:26:04.130690:	Training iteration: 416200, Loss: 0.005811659153550863
2018-10-24 00:27:26.129916:	Training iteration: 416400, Loss: 0.0061094737611711025
2018-10-24 00:27:42.756645: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 767 of 1000
2018-10-24 00:27:45.515277: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 00:28:59.078879:	Training iteration: 416600, Loss: 0.004871221724897623
2018-10-24 00:30:20.250666:	Training iteration: 416800, Loss: 0.004837801214307547
2018-10-24 00:31:41.300763:	Training iteration: 417000, Loss: 0.003401469672098756
2018-10-24 00:33:01.902661:	Training iteration: 417200, Loss: 0.007284520193934441
2018-10-24 00:34:23.572599:	Training iteration: 417400, Loss: 0.005937155336141586
2018-10-24 00:35:44.201158:	Training iteration: 417600, Loss: 0.005737882107496262
2018-10-24 00:37:04.837194:	Training iteration: 417800, Loss: 0.0036834964994341135
2018-10-24 00:38:26.859068:	Training iteration: 418000, Loss: 0.004667768254876137
2018-10-24 00:39:47.355250:	Training iteration: 418200, Loss: 0.004223664756864309
2018-10-24 00:41:08.027309:	Training iteration: 418400, Loss: 0.005762577056884766
2018-10-24 00:42:28.648048:	Training iteration: 418600, Loss: 0.005858646240085363
2018-10-24 00:43:49.415219:	Training iteration: 418800, Loss: 0.004743555095046759
2018-10-24 00:45:09.860814:	Training iteration: 419000, Loss: 0.005457419902086258
2018-10-24 00:46:30.398853:	Training iteration: 419200, Loss: 0.0049561443738639355
2018-10-24 00:47:51.021893:	Training iteration: 419400, Loss: 0.005416968371719122
2018-10-24 00:49:11.967654:	Training iteration: 419600, Loss: 0.006388682872056961
2018-10-24 00:50:32.632447:	Training iteration: 419800, Loss: 0.005085792858153582
2018-10-24 00:51:53.175688:	Training iteration: 420000, Loss: 0.005525434855371714
Checkpoint
2018-10-24 00:53:16.235190:	Training iteration: 420200, Loss: 0.0033298172056674957
2018-10-24 00:54:37.370143:	Training iteration: 420400, Loss: 0.006174344569444656
2018-10-24 00:55:58.772221:	Training iteration: 420600, Loss: 0.005593064706772566
2018-10-24 00:57:20.146253:	Training iteration: 420800, Loss: 0.003501647850498557
2018-10-24 00:58:41.311967:	Training iteration: 421000, Loss: 0.007144020404666662
2018-10-24 01:00:02.903311:	Training iteration: 421200, Loss: 0.004098938778042793
2018-10-24 01:01:25.925126:	Training iteration: 421400, Loss: 0.005252068396657705
2018-10-24 01:02:47.250074:	Training iteration: 421600, Loss: 0.0039441874250769615
2018-10-24 01:04:08.779186:	Training iteration: 421800, Loss: 0.003859183518216014
2018-10-24 01:05:30.540551:	Training iteration: 422000, Loss: 0.006244407966732979
2018-10-24 01:06:52.173316:	Training iteration: 422200, Loss: 0.004496041219681501
2018-10-24 01:08:13.668796:	Training iteration: 422400, Loss: 0.005190238822251558
2018-10-24 01:09:35.277728:	Training iteration: 422600, Loss: 0.0054618255235254765
2018-10-24 01:10:56.821898:	Training iteration: 422800, Loss: 0.004772316198796034
2018-10-24 01:12:18.510920:	Training iteration: 423000, Loss: 0.00594759127125144
2018-10-24 01:13:40.183652:	Training iteration: 423200, Loss: 0.005255309399217367
2018-10-24 01:15:01.932287:	Training iteration: 423400, Loss: 0.005105616990476847
2018-10-24 01:16:23.672706:	Training iteration: 423600, Loss: 0.006246535573154688
2018-10-24 01:17:45.182539:	Training iteration: 423800, Loss: 0.006930215749889612
2018-10-24 01:19:07.026802:	Training iteration: 424000, Loss: 0.005591386463493109
2018-10-24 01:20:28.729786:	Training iteration: 424200, Loss: 0.0054894108325243
2018-10-24 01:21:50.550041:	Training iteration: 424400, Loss: 0.007982754148542881
2018-10-24 01:23:12.393065:	Training iteration: 424600, Loss: 0.004746295977383852
2018-10-24 01:24:12.395025: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 772 of 1000
2018-10-24 01:24:15.174987: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 01:24:45.635757:	Training iteration: 424800, Loss: 0.004729799926280975
2018-10-24 01:26:07.271727:	Training iteration: 425000, Loss: 0.002870765281841159
2018-10-24 01:27:29.202076:	Training iteration: 425200, Loss: 0.00419915234670043
2018-10-24 01:28:51.002138:	Training iteration: 425400, Loss: 0.0031005593482404947
2018-10-24 01:30:12.908449:	Training iteration: 425600, Loss: 0.004008182790130377
2018-10-24 01:31:35.261658:	Training iteration: 425800, Loss: 0.003238203702494502
2018-10-24 01:32:56.752284:	Training iteration: 426000, Loss: 0.007319655269384384
2018-10-24 01:34:18.033917:	Training iteration: 426200, Loss: 0.004237579647451639
2018-10-24 01:35:39.301135:	Training iteration: 426400, Loss: 0.0043043941259384155
2018-10-24 01:37:00.468676:	Training iteration: 426600, Loss: 0.0038431852590292692
2018-10-24 01:38:20.958079:	Training iteration: 426800, Loss: 0.002784794894978404
2018-10-24 01:39:41.807070:	Training iteration: 427000, Loss: 0.004535037558525801
2018-10-24 01:41:02.607060:	Training iteration: 427200, Loss: 0.0044068279676139355
2018-10-24 01:42:23.378451:	Training iteration: 427400, Loss: 0.004986064974218607
2018-10-24 01:43:43.955369:	Training iteration: 427600, Loss: 0.0029219237621873617
2018-10-24 01:45:04.920002:	Training iteration: 427800, Loss: 0.0028209127485752106
2018-10-24 01:46:26.029782:	Training iteration: 428000, Loss: 0.00448929239064455
2018-10-24 01:47:46.800055:	Training iteration: 428200, Loss: 0.0036380011588335037
2018-10-24 01:49:07.686234:	Training iteration: 428400, Loss: 0.0035650748759508133
2018-10-24 01:50:29.391580:	Training iteration: 428600, Loss: 0.0057866256684064865
2018-10-24 01:51:50.022771:	Training iteration: 428800, Loss: 0.004100102931261063
2018-10-24 01:53:10.544756:	Training iteration: 429000, Loss: 0.0038032198790460825
2018-10-24 01:54:31.390689:	Training iteration: 429200, Loss: 0.005170401651412249
2018-10-24 01:55:52.435037:	Training iteration: 429400, Loss: 0.005604354199022055
2018-10-24 01:57:13.523936:	Training iteration: 429600, Loss: 0.005542365368455648
2018-10-24 01:58:34.630932:	Training iteration: 429800, Loss: 0.006754899863153696
2018-10-24 01:59:55.466815:	Training iteration: 430000, Loss: 0.0035291884560137987
Checkpoint
2018-10-24 02:01:28.921274:	Training iteration: 430200, Loss: 0.003496328601613641
2018-10-24 02:02:49.496482:	Training iteration: 430400, Loss: 0.0037508169189095497
2018-10-24 02:04:11.248225:	Training iteration: 430600, Loss: 0.006671153008937836
2018-10-24 02:05:32.762835:	Training iteration: 430800, Loss: 0.003645108314231038
2018-10-24 02:06:54.300848:	Training iteration: 431000, Loss: 0.0040275477804243565
2018-10-24 02:08:16.887917:	Training iteration: 431200, Loss: 0.003894207300618291
2018-10-24 02:09:38.448558:	Training iteration: 431400, Loss: 0.0045518167316913605
2018-10-24 02:11:00.079603:	Training iteration: 431600, Loss: 0.006366213317960501
2018-10-24 02:12:21.948852:	Training iteration: 431800, Loss: 0.00356745976023376
2018-10-24 02:13:43.548630:	Training iteration: 432000, Loss: 0.0039998809807002544
2018-10-24 02:15:05.206007:	Training iteration: 432200, Loss: 0.004298890009522438
2018-10-24 02:16:26.818563:	Training iteration: 432400, Loss: 0.003454922465607524
2018-10-24 02:17:48.420076:	Training iteration: 432600, Loss: 0.004072023555636406
2018-10-24 02:19:10.148749:	Training iteration: 432800, Loss: 0.0037857491988688707
2018-10-24 02:20:32.049179:	Training iteration: 433000, Loss: 0.005621116608381271
2018-10-24 02:21:53.872274:	Training iteration: 433200, Loss: 0.0047281780280172825
2018-10-24 02:23:15.754033:	Training iteration: 433400, Loss: 0.004365246742963791
2018-10-24 02:23:26.575638: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 884 of 1000
2018-10-24 02:23:27.940709: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 02:24:47.463196:	Training iteration: 433600, Loss: 0.004144902341067791
2018-10-24 02:26:08.975846:	Training iteration: 433800, Loss: 0.008519517257809639
2018-10-24 02:27:30.545662:	Training iteration: 434000, Loss: 0.004458046983927488
2018-10-24 02:28:52.253835:	Training iteration: 434200, Loss: 0.004515075124800205
2018-10-24 02:30:13.728431:	Training iteration: 434400, Loss: 0.0059549384750425816
2018-10-24 02:31:35.611261:	Training iteration: 434600, Loss: 0.0060682822950184345
2018-10-24 02:32:56.971138:	Training iteration: 434800, Loss: 0.004934271331876516
2018-10-24 02:34:18.270193:	Training iteration: 435000, Loss: 0.0034476632717996836
2018-10-24 02:35:39.582929:	Training iteration: 435200, Loss: 0.0029459025245159864
2018-10-24 02:37:00.435061:	Training iteration: 435400, Loss: 0.007750243414193392
2018-10-24 02:38:21.248356:	Training iteration: 435600, Loss: 0.006287324707955122
2018-10-24 02:39:41.825939:	Training iteration: 435800, Loss: 0.005345944780856371
2018-10-24 02:41:02.708920:	Training iteration: 436000, Loss: 0.004486825782805681
2018-10-24 02:42:23.058189:	Training iteration: 436200, Loss: 0.004222234711050987
2018-10-24 02:43:43.336157:	Training iteration: 436400, Loss: 0.00415403675287962
2018-10-24 02:45:03.495049:	Training iteration: 436600, Loss: 0.004731422755867243
2018-10-24 02:46:23.885754:	Training iteration: 436800, Loss: 0.004689021036028862
2018-10-24 02:47:44.583664:	Training iteration: 437000, Loss: 0.004990291316062212
2018-10-24 02:49:04.918475:	Training iteration: 437200, Loss: 0.005755656864494085
2018-10-24 02:50:25.303911:	Training iteration: 437400, Loss: 0.0035383563954383135
2018-10-24 02:51:45.605713:	Training iteration: 437600, Loss: 0.006960701197385788
2018-10-24 02:53:05.962774:	Training iteration: 437800, Loss: 0.005535684060305357
2018-10-24 02:54:26.576296:	Training iteration: 438000, Loss: 0.006117828190326691
2018-10-24 02:55:46.937073:	Training iteration: 438200, Loss: 0.00895137432962656
2018-10-24 02:57:07.718812:	Training iteration: 438400, Loss: 0.00604887492954731
2018-10-24 02:58:28.499404:	Training iteration: 438600, Loss: 0.006736295763403177
2018-10-24 02:59:49.525762:	Training iteration: 438800, Loss: 0.004308047238737345
2018-10-24 03:01:10.474220:	Training iteration: 439000, Loss: 0.004448385443538427
2018-10-24 03:02:31.619609:	Training iteration: 439200, Loss: 0.004682940896600485
2018-10-24 03:03:53.056781:	Training iteration: 439400, Loss: 0.005442088935524225
2018-10-24 03:05:14.355493:	Training iteration: 439600, Loss: 0.003451623721048236
2018-10-24 03:06:35.757251:	Training iteration: 439800, Loss: 0.0061757490038871765
2018-10-24 03:07:57.179877:	Training iteration: 440000, Loss: 0.004293763544410467
Checkpoint
2018-10-24 03:09:24.013738:	Training iteration: 440200, Loss: 0.007246376480907202
2018-10-24 03:10:45.132361:	Training iteration: 440400, Loss: 0.004315201658755541
2018-10-24 03:12:06.640849:	Training iteration: 440600, Loss: 0.004879538435488939
2018-10-24 03:13:27.854638:	Training iteration: 440800, Loss: 0.004256062209606171
2018-10-24 03:14:49.425832:	Training iteration: 441000, Loss: 0.003031756030395627
2018-10-24 03:16:10.760393:	Training iteration: 441200, Loss: 0.006136266048997641
2018-10-24 03:17:31.902348:	Training iteration: 441400, Loss: 0.003449082374572754
2018-10-24 03:18:53.269127:	Training iteration: 441600, Loss: 0.0023620130959898233
2018-10-24 03:20:14.461986:	Training iteration: 441800, Loss: 0.005326279904693365
2018-10-24 03:21:35.817986:	Training iteration: 442000, Loss: 0.004245401825755835
2018-10-24 03:22:56.720690:	Training iteration: 442200, Loss: 0.006052028387784958
2018-10-24 03:24:18.214745:	Training iteration: 442400, Loss: 0.009371032007038593
2018-10-24 03:25:39.464972:	Training iteration: 442600, Loss: 0.007122233975678682
2018-10-24 03:27:00.815971:	Training iteration: 442800, Loss: 0.008580059744417667
2018-10-24 03:28:22.206680:	Training iteration: 443000, Loss: 0.005773971322923899
2018-10-24 03:29:43.752168:	Training iteration: 443200, Loss: 0.0038879092317074537
2018-10-24 03:31:05.346911:	Training iteration: 443400, Loss: 0.004685336258262396
2018-10-24 03:32:26.849897:	Training iteration: 443600, Loss: 0.002984333783388138
2018-10-24 03:33:48.140622:	Training iteration: 443800, Loss: 0.005045214667916298
2018-10-24 03:35:09.233041:	Training iteration: 444000, Loss: 0.0051000225357711315
2018-10-24 03:36:30.391082:	Training iteration: 444200, Loss: 0.007074939552694559
2018-10-24 03:37:51.319580:	Training iteration: 444400, Loss: 0.009369042702019215
2018-10-24 03:39:12.371720:	Training iteration: 444600, Loss: 0.004176512826234102
2018-10-24 03:40:33.131867:	Training iteration: 444800, Loss: 0.003739959793165326
2018-10-24 03:41:53.671190:	Training iteration: 445000, Loss: 0.0062151276506483555
2018-10-24 03:43:14.280570:	Training iteration: 445200, Loss: 0.008754500187933445
2018-10-24 03:44:34.841873:	Training iteration: 445400, Loss: 0.006215082015842199
2018-10-24 03:45:55.886471:	Training iteration: 445600, Loss: 0.004740748088806868
2018-10-24 03:47:16.415496:	Training iteration: 445800, Loss: 0.0052179316990077496
2018-10-24 03:48:36.871775:	Training iteration: 446000, Loss: 0.006857665721327066
2018-10-24 03:49:57.358084:	Training iteration: 446200, Loss: 0.007299505174160004
2018-10-24 03:51:17.776735:	Training iteration: 446400, Loss: 0.004906116519123316
2018-10-24 03:52:38.147608:	Training iteration: 446600, Loss: 0.005034288857132196
2018-10-24 03:53:58.585177:	Training iteration: 446800, Loss: 0.0029914204496890306
2018-10-24 03:55:18.898202:	Training iteration: 447000, Loss: 0.0057702683843672276
2018-10-24 03:56:39.237171:	Training iteration: 447200, Loss: 0.0073366351425647736
2018-10-24 03:57:59.875163:	Training iteration: 447400, Loss: 0.006808087229728699
2018-10-24 03:59:20.715829:	Training iteration: 447600, Loss: 0.007463672664016485
2018-10-24 04:00:41.586937:	Training iteration: 447800, Loss: 0.0037831843364983797
2018-10-24 04:02:02.822460:	Training iteration: 448000, Loss: 0.00621207058429718
2018-10-24 04:03:24.111195:	Training iteration: 448200, Loss: 0.004393255338072777
2018-10-24 04:04:45.210470:	Training iteration: 448400, Loss: 0.006028597708791494
2018-10-24 04:06:06.435862:	Training iteration: 448600, Loss: 0.008372284471988678
2018-10-24 04:07:27.332161:	Training iteration: 448800, Loss: 0.006013445556163788
2018-10-24 04:08:48.703476:	Training iteration: 449000, Loss: 0.004825118463486433
2018-10-24 04:10:10.211840:	Training iteration: 449200, Loss: 0.006859909743070602
2018-10-24 04:11:31.264955:	Training iteration: 449400, Loss: 0.007212453987449408
2018-10-24 04:12:52.845224:	Training iteration: 449600, Loss: 0.007830042392015457
2018-10-24 04:14:14.153470:	Training iteration: 449800, Loss: 0.004121317062526941
2018-10-24 04:15:35.498423:	Training iteration: 450000, Loss: 0.006872018799185753
Checkpoint
2018-10-24 04:16:59.311398:	Training iteration: 450200, Loss: 0.004772374406456947
2018-10-24 04:18:20.533641:	Training iteration: 450400, Loss: 0.004236835986375809
2018-10-24 04:19:41.845972:	Training iteration: 450600, Loss: 0.0055803232826292515
2018-10-24 04:21:03.276036:	Training iteration: 450800, Loss: 0.0038835012819617987
2018-10-24 04:22:24.812142:	Training iteration: 451000, Loss: 0.006802885327488184
2018-10-24 04:23:46.388869:	Training iteration: 451200, Loss: 0.005513810086995363
2018-10-24 04:25:07.838667:	Training iteration: 451400, Loss: 0.005586635787039995
2018-10-24 04:26:29.140081:	Training iteration: 451600, Loss: 0.00473677646368742
2018-10-24 04:27:50.636112:	Training iteration: 451800, Loss: 0.00728420028463006
2018-10-24 04:29:12.177871:	Training iteration: 452000, Loss: 0.00453172717243433
2018-10-24 04:30:33.933855:	Training iteration: 452200, Loss: 0.007319232448935509
2018-10-24 04:31:55.429126:	Training iteration: 452400, Loss: 0.005059611983597279
2018-10-24 04:33:16.703384:	Training iteration: 452600, Loss: 0.004947700537741184
2018-10-24 04:34:37.967887:	Training iteration: 452800, Loss: 0.004753191955387592
2018-10-24 04:35:59.406413:	Training iteration: 453000, Loss: 0.0047937724739313126
2018-10-24 04:37:20.381065:	Training iteration: 453200, Loss: 0.0035541346296668053
2018-10-24 04:38:41.456081:	Training iteration: 453400, Loss: 0.00512760179117322
2018-10-24 04:40:02.381442:	Training iteration: 453600, Loss: 0.004765758756548166
2018-10-24 04:41:22.947148:	Training iteration: 453800, Loss: 0.006490903440862894
2018-10-24 04:42:43.977465:	Training iteration: 454000, Loss: 0.007450401782989502
2018-10-24 04:44:04.711532:	Training iteration: 454200, Loss: 0.005221115425229073
2018-10-24 04:45:25.505426:	Training iteration: 454400, Loss: 0.004584813490509987
2018-10-24 04:46:45.845057:	Training iteration: 454600, Loss: 0.0029203405138105154
2018-10-24 04:48:06.633969:	Training iteration: 454800, Loss: 0.006625914480537176
2018-10-24 04:49:27.031839:	Training iteration: 455000, Loss: 0.005441330373287201
2018-10-24 04:50:47.459253:	Training iteration: 455200, Loss: 0.004093362018465996
2018-10-24 04:52:07.887814:	Training iteration: 455400, Loss: 0.008569230325520039
2018-10-24 04:53:28.227001:	Training iteration: 455600, Loss: 0.005819555371999741
2018-10-24 04:54:48.565350:	Training iteration: 455800, Loss: 0.007144941482692957
2018-10-24 04:56:08.701429:	Training iteration: 456000, Loss: 0.0036774456966668367
2018-10-24 04:57:28.923764:	Training iteration: 456200, Loss: 0.005388721823692322
2018-10-24 04:58:49.038044:	Training iteration: 456400, Loss: 0.011893760412931442
2018-10-24 05:00:09.911566:	Training iteration: 456600, Loss: 0.007406134158372879
2018-10-24 05:01:30.241258:	Training iteration: 456800, Loss: 0.004954956937581301
2018-10-24 05:02:51.108089:	Training iteration: 457000, Loss: 0.005753472447395325
2018-10-24 05:04:11.445091:	Training iteration: 457200, Loss: 0.005729209631681442
2018-10-24 05:05:32.678022:	Training iteration: 457400, Loss: 0.004136649891734123
2018-10-24 05:06:53.792017:	Training iteration: 457600, Loss: 0.00522884214296937
2018-10-24 05:08:14.945910:	Training iteration: 457800, Loss: 0.006925974041223526
2018-10-24 05:09:35.870492:	Training iteration: 458000, Loss: 0.004819441121071577
2018-10-24 05:10:56.886048:	Training iteration: 458200, Loss: 0.003977370914071798
2018-10-24 05:12:18.243876:	Training iteration: 458400, Loss: 0.0038858663756400347
2018-10-24 05:13:39.317554:	Training iteration: 458600, Loss: 0.004731229972094297
2018-10-24 05:15:00.628771:	Training iteration: 458800, Loss: 0.0039309910498559475
2018-10-24 05:16:21.494279:	Training iteration: 459000, Loss: 0.005352191627025604
2018-10-24 05:17:42.850234:	Training iteration: 459200, Loss: 0.004342847038060427
2018-10-24 05:19:03.646031:	Training iteration: 459400, Loss: 0.007370853330940008
2018-10-24 05:20:24.903602:	Training iteration: 459600, Loss: 0.006514218170195818
2018-10-24 05:21:46.211094:	Training iteration: 459800, Loss: 0.0035610245540738106
2018-10-24 05:23:07.200382:	Training iteration: 460000, Loss: 0.003892773762345314
Checkpoint
2018-10-24 05:24:30.800637:	Training iteration: 460200, Loss: 0.009790713898837566
2018-10-24 05:25:51.963884:	Training iteration: 460400, Loss: 0.012227353639900684
2018-10-24 05:27:12.745344:	Training iteration: 460600, Loss: 0.003292475128546357
2018-10-24 05:28:33.883061:	Training iteration: 460800, Loss: 0.008188673295080662
2018-10-24 05:29:55.091411:	Training iteration: 461000, Loss: 0.004361693747341633
2018-10-24 05:31:16.125157:	Training iteration: 461200, Loss: 0.007254405412822962
2018-10-24 05:32:36.964280:	Training iteration: 461400, Loss: 0.006198766175657511
2018-10-24 05:33:58.132454:	Training iteration: 461600, Loss: 0.010171475820243359
2018-10-24 05:35:19.426988:	Training iteration: 461800, Loss: 0.005327971186488867
2018-10-24 05:36:40.155109:	Training iteration: 462000, Loss: 0.006118310149759054
2018-10-24 05:38:01.176587:	Training iteration: 462200, Loss: 0.004739055875688791
2018-10-24 05:39:21.896368:	Training iteration: 462400, Loss: 0.009096822701394558
2018-10-24 05:40:42.332164:	Training iteration: 462600, Loss: 0.0032586429733783007
2018-10-24 05:42:02.833380:	Training iteration: 462800, Loss: 0.006349954288452864
2018-10-24 05:43:23.425125:	Training iteration: 463000, Loss: 0.008647277019917965
2018-10-24 05:44:43.890972:	Training iteration: 463200, Loss: 0.0066535137593746185
2018-10-24 05:46:04.135531:	Training iteration: 463400, Loss: 0.005485085304826498
2018-10-24 05:47:24.498585:	Training iteration: 463600, Loss: 0.0053310212679207325
2018-10-24 05:48:44.862100:	Training iteration: 463800, Loss: 0.00503205182030797
2018-10-24 05:50:05.188254:	Training iteration: 464000, Loss: 0.009422200731933117
2018-10-24 05:51:25.224129:	Training iteration: 464200, Loss: 0.0033799249213188887
2018-10-24 05:52:45.545803:	Training iteration: 464400, Loss: 0.005415686871856451
2018-10-24 05:54:05.613479:	Training iteration: 464600, Loss: 0.004005590919405222
2018-10-24 05:55:25.869051:	Training iteration: 464800, Loss: 0.004359090700745583
2018-10-24 05:56:46.034910:	Training iteration: 465000, Loss: 0.005085197743028402
2018-10-24 05:58:05.891834:	Training iteration: 465200, Loss: 0.006109616253525019
2018-10-24 05:59:25.779272:	Training iteration: 465400, Loss: 0.005903452634811401
2018-10-24 06:00:45.962720:	Training iteration: 465600, Loss: 0.004142298828810453
2018-10-24 06:02:07.495636:	Training iteration: 465800, Loss: 0.006424345076084137
2018-10-24 06:03:27.844966:	Training iteration: 466000, Loss: 0.005169968586415052
2018-10-24 06:04:48.385337:	Training iteration: 466200, Loss: 0.0053106932900846004
2018-10-24 06:06:08.984489:	Training iteration: 466400, Loss: 0.0043561733327806
2018-10-24 06:07:29.767250:	Training iteration: 466600, Loss: 0.006350962445139885
2018-10-24 06:08:50.775641:	Training iteration: 466800, Loss: 0.005081817973405123
2018-10-24 06:10:11.694225:	Training iteration: 467000, Loss: 0.0030825536232441664
2018-10-24 06:11:32.562737:	Training iteration: 467200, Loss: 0.004912879783660173
2018-10-24 06:12:53.534870:	Training iteration: 467400, Loss: 0.006475596223026514
2018-10-24 06:14:14.747941:	Training iteration: 467600, Loss: 0.003345312550663948
2018-10-24 06:15:35.789492:	Training iteration: 467800, Loss: 0.004830044694244862
2018-10-24 06:16:57.036788:	Training iteration: 468000, Loss: 0.004539738409221172
2018-10-24 06:18:18.094770:	Training iteration: 468200, Loss: 0.004005555994808674
2018-10-24 06:19:39.194472:	Training iteration: 468400, Loss: 0.006440419238060713
2018-10-24 06:21:00.503476:	Training iteration: 468600, Loss: 0.008509506471455097
2018-10-24 06:22:21.624782:	Training iteration: 468800, Loss: 0.007031966000795364
2018-10-24 06:23:42.869733:	Training iteration: 469000, Loss: 0.0068380278535187244
2018-10-24 06:25:03.438600:	Training iteration: 469200, Loss: 0.007641103584319353
2018-10-24 06:26:24.716151:	Training iteration: 469400, Loss: 0.006948405411094427
2018-10-24 06:27:45.816576:	Training iteration: 469600, Loss: 0.008000373840332031
2018-10-24 06:29:06.945689:	Training iteration: 469800, Loss: 0.0063699656166136265
2018-10-24 06:30:28.251581:	Training iteration: 470000, Loss: 0.003464482491835952
Checkpoint
2018-10-24 06:31:51.414404:	Training iteration: 470200, Loss: 0.0048997849225997925
2018-10-24 06:33:12.711067:	Training iteration: 470400, Loss: 0.005120094399899244
2018-10-24 06:34:34.051338:	Training iteration: 470600, Loss: 0.004517275374382734
2018-10-24 06:35:54.865273:	Training iteration: 470800, Loss: 0.004971880465745926
2018-10-24 06:37:16.352969:	Training iteration: 471000, Loss: 0.006770292297005653
2018-10-24 06:38:37.424928:	Training iteration: 471200, Loss: 0.006228173617273569
2018-10-24 06:39:58.408780:	Training iteration: 471400, Loss: 0.0046379659324884415
2018-10-24 06:41:19.461898:	Training iteration: 471600, Loss: 0.00485938461497426
2018-10-24 06:42:40.044633:	Training iteration: 471800, Loss: 0.005123320966959
2018-10-24 06:44:00.696281:	Training iteration: 472000, Loss: 0.005200330633670092
2018-10-24 06:45:21.368950:	Training iteration: 472200, Loss: 0.004406014457345009
2018-10-24 06:46:42.017338:	Training iteration: 472400, Loss: 0.004861494991928339
2018-10-24 06:48:02.427909:	Training iteration: 472600, Loss: 0.008218775503337383
2018-10-24 06:49:22.447265:	Training iteration: 472800, Loss: 0.006209615617990494
2018-10-24 06:50:43.119374:	Training iteration: 473000, Loss: 0.0074577233754098415
2018-10-24 06:52:03.448042:	Training iteration: 473200, Loss: 0.006171936634927988
2018-10-24 06:53:23.529025:	Training iteration: 473400, Loss: 0.008600957691669464
2018-10-24 06:54:43.755617:	Training iteration: 473600, Loss: 0.005604880396276712
2018-10-24 06:56:03.865340:	Training iteration: 473800, Loss: 0.0063369967974722385
2018-10-24 06:57:23.953762:	Training iteration: 474000, Loss: 0.004460904281586409
2018-10-24 06:58:43.959042:	Training iteration: 474200, Loss: 0.004780335817486048
2018-10-24 07:00:04.227331:	Training iteration: 474400, Loss: 0.0050311461091041565
2018-10-24 07:01:23.928990:	Training iteration: 474600, Loss: 0.004426642786711454
2018-10-24 07:02:43.884938:	Training iteration: 474800, Loss: 0.003055694280192256
2018-10-24 07:04:03.882311:	Training iteration: 475000, Loss: 0.0059529319405555725
2018-10-24 07:05:23.804703:	Training iteration: 475200, Loss: 0.005863122642040253
2018-10-24 07:06:43.996305:	Training iteration: 475400, Loss: 0.0051374416798353195
2018-10-24 07:08:04.414696:	Training iteration: 475600, Loss: 0.006769505795091391
2018-10-24 07:09:25.004442:	Training iteration: 475800, Loss: 0.005359532777220011
2018-10-24 07:10:45.628968:	Training iteration: 476000, Loss: 0.006054371129721403
2018-10-24 07:12:05.987553:	Training iteration: 476200, Loss: 0.005675293039530516
2018-10-24 07:13:26.593345:	Training iteration: 476400, Loss: 0.004146154969930649
2018-10-24 07:14:47.399457:	Training iteration: 476600, Loss: 0.0061131310649216175
2018-10-24 07:16:08.185308:	Training iteration: 476800, Loss: 0.004921641666442156
2018-10-24 07:17:28.966660:	Training iteration: 477000, Loss: 0.003706969553604722
2018-10-24 07:18:49.984341:	Training iteration: 477200, Loss: 0.0036305412650108337
2018-10-24 07:20:10.986202:	Training iteration: 477400, Loss: 0.005068882834166288
2018-10-24 07:21:31.947826:	Training iteration: 477600, Loss: 0.005094047635793686
2018-10-24 07:22:52.954773:	Training iteration: 477800, Loss: 0.003383144037798047
2018-10-24 07:24:14.170529:	Training iteration: 478000, Loss: 0.0052420757710933685
2018-10-24 07:25:35.283437:	Training iteration: 478200, Loss: 0.005340235307812691
2018-10-24 07:26:55.455364:	Training iteration: 478400, Loss: 0.005735809449106455
2018-10-24 07:28:14.533252:	Training iteration: 478600, Loss: 0.0039529562927782536
2018-10-24 07:29:32.743536:	Training iteration: 478800, Loss: 0.005477720405906439
2018-10-24 07:30:50.203789:	Training iteration: 479000, Loss: 0.0059530106373131275
2018-10-24 07:32:07.080727:	Training iteration: 479200, Loss: 0.0059370421804487705
2018-10-24 07:33:24.081264:	Training iteration: 479400, Loss: 0.006723982747644186
2018-10-24 07:34:41.902865:	Training iteration: 479600, Loss: 0.004337478894740343
2018-10-24 07:35:58.922410:	Training iteration: 479800, Loss: 0.006908399518579245
2018-10-24 07:36:01.779377:	Epoch 5 finished after 479809 iterations.
Validating
2018-10-24 07:36:01.796054:	Entering validation loop
2018-10-24 07:36:11.856577: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 711 of 1000
2018-10-24 07:36:15.667733: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 07:36:46.937895:	Validation iteration: 200, Loss: 0.007113542407751083
2018-10-24 07:37:19.452048:	Validation iteration: 400, Loss: 0.006857300642877817
2018-10-24 07:37:51.766913:	Validation iteration: 600, Loss: 0.007893434725701809
2018-10-24 07:38:24.100418:	Validation iteration: 800, Loss: 0.004566698335111141
2018-10-24 07:38:56.519634:	Validation iteration: 1000, Loss: 0.008163430728018284
2018-10-24 07:39:29.581121:	Validation iteration: 1200, Loss: 0.005515077617019415
2018-10-24 07:40:11.354775: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 721 of 1000
2018-10-24 07:40:14.981652: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 07:40:16.328073:	Validation iteration: 1400, Loss: 0.004747506231069565
2018-10-24 07:40:47.530948:	Validation iteration: 1600, Loss: 0.007181847933679819
2018-10-24 07:41:20.184985:	Validation iteration: 1800, Loss: 0.007773077581077814
2018-10-24 07:41:52.592772:	Validation iteration: 2000, Loss: 0.00631168857216835
2018-10-24 07:42:24.889415:	Validation iteration: 2200, Loss: 0.007070751860737801
2018-10-24 07:42:57.092066:	Validation iteration: 2400, Loss: 0.006716100033372641
2018-10-24 07:43:29.808023:	Validation iteration: 2600, Loss: 0.005889111664146185
2018-10-24 07:44:10.214410: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 712 of 1000
2018-10-24 07:44:13.996658: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 07:44:16.532613:	Validation iteration: 2800, Loss: 0.005354527849704027
2018-10-24 07:44:47.686239:	Validation iteration: 3000, Loss: 0.004940354265272617
2018-10-24 07:45:19.247748:	Validation iteration: 3200, Loss: 0.004583894740790129
2018-10-24 07:45:51.301660:	Validation iteration: 3400, Loss: 0.007050005719065666
2018-10-24 07:46:24.138143:	Validation iteration: 3600, Loss: 0.00554178049787879
2018-10-24 07:46:56.809169:	Validation iteration: 3800, Loss: 0.0056114234030246735
2018-10-24 07:47:29.387515:	Validation iteration: 4000, Loss: 0.005805790890008211
2018-10-24 07:48:08.114787: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 704 of 1000
2018-10-24 07:48:11.997769: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 07:48:15.809787:	Validation iteration: 4200, Loss: 0.0057242983020842075
2018-10-24 07:48:48.501818:	Validation iteration: 4400, Loss: 0.008321210741996765
2018-10-24 07:49:20.229107:	Validation iteration: 4600, Loss: 0.00818349327892065
2018-10-24 07:49:52.142030:	Validation iteration: 4800, Loss: 0.007746723014861345
2018-10-24 07:50:24.296600:	Validation iteration: 5000, Loss: 0.007003132253885269
2018-10-24 07:50:56.522561:	Validation iteration: 5200, Loss: 0.006936425808817148
2018-10-24 07:51:29.354334:	Validation iteration: 5400, Loss: 0.008311840705573559
2018-10-24 07:52:06.768776: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 671 of 1000
2018-10-24 07:52:11.222310: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 07:52:16.322254:	Validation iteration: 5600, Loss: 0.006216153502464294
2018-10-24 07:52:47.501820:	Validation iteration: 5800, Loss: 0.005355408415198326
2018-10-24 07:53:19.162284:	Validation iteration: 6000, Loss: 0.004765695426613092
2018-10-24 07:53:51.813880:	Validation iteration: 6200, Loss: 0.004431946203112602
2018-10-24 07:54:24.350519:	Validation iteration: 6400, Loss: 0.0038063994143158197
2018-10-24 07:54:56.928559:	Validation iteration: 6600, Loss: 0.005055947229266167
2018-10-24 07:55:29.668008:	Validation iteration: 6800, Loss: 0.0043548778630793095
2018-10-24 07:56:02.134297:	Validation iteration: 7000, Loss: 0.004219949711114168
2018-10-24 07:56:34.615121:	Validation iteration: 7200, Loss: 0.00415227934718132
2018-10-24 07:57:07.018974:	Validation iteration: 7400, Loss: 0.005106132011860609
Validation check mean loss: 0.005975303621660838
Validation loss has worsened. worse_val_checks = 1
2018-10-24 07:57:32.842347: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 786 of 1000
2018-10-24 07:57:37.934068: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 07:58:49.019233:	Training iteration: 480000, Loss: 0.003704017261043191
Checkpoint
2018-10-24 08:00:06.867280:	Training iteration: 480200, Loss: 0.0038164935540407896
2018-10-24 08:01:22.685387:	Training iteration: 480400, Loss: 0.0046754637733101845
2018-10-24 08:02:39.054408:	Training iteration: 480600, Loss: 0.0036838653031736612
2018-10-24 08:03:54.892289:	Training iteration: 480800, Loss: 0.004418760072439909
2018-10-24 08:05:11.051849:	Training iteration: 481000, Loss: 0.003651002421975136
2018-10-24 08:06:27.242556:	Training iteration: 481200, Loss: 0.005020382348448038
2018-10-24 08:07:44.495694:	Training iteration: 481400, Loss: 0.0038212649524211884
2018-10-24 08:09:02.969840:	Training iteration: 481600, Loss: 0.00433107977733016
2018-10-24 08:10:21.143436:	Training iteration: 481800, Loss: 0.004091636743396521
2018-10-24 08:11:40.211224:	Training iteration: 482000, Loss: 0.0033655462320894003
2018-10-24 08:12:59.364322:	Training iteration: 482200, Loss: 0.0036326900590211153
2018-10-24 08:14:19.091478:	Training iteration: 482400, Loss: 0.004293425474315882
2018-10-24 08:15:38.738759:	Training iteration: 482600, Loss: 0.004159220028668642
2018-10-24 08:16:59.513161:	Training iteration: 482800, Loss: 0.005726830568164587
2018-10-24 08:18:19.693609:	Training iteration: 483000, Loss: 0.005610101390630007
2018-10-24 08:19:39.966007:	Training iteration: 483200, Loss: 0.004957135766744614
2018-10-24 08:21:00.757859:	Training iteration: 483400, Loss: 0.004151962231844664
2018-10-24 08:22:21.574251:	Training iteration: 483600, Loss: 0.004012336954474449
2018-10-24 08:23:42.408196:	Training iteration: 483800, Loss: 0.0055387686006724834
2018-10-24 08:25:03.284730:	Training iteration: 484000, Loss: 0.004752545151859522
2018-10-24 08:26:24.394262:	Training iteration: 484200, Loss: 0.004865454044193029
2018-10-24 08:27:45.296089:	Training iteration: 484400, Loss: 0.004045980051159859
2018-10-24 08:29:06.586168:	Training iteration: 484600, Loss: 0.003035387024283409
2018-10-24 08:30:27.767945:	Training iteration: 484800, Loss: 0.0034019064623862505
2018-10-24 08:31:48.873221:	Training iteration: 485000, Loss: 0.003038546768948436
2018-10-24 08:33:10.567285:	Training iteration: 485200, Loss: 0.00353178009390831
2018-10-24 08:34:31.541302:	Training iteration: 485400, Loss: 0.004016056656837463
2018-10-24 08:35:52.489168:	Training iteration: 485600, Loss: 0.004148548003286123
2018-10-24 08:37:13.597964:	Training iteration: 485800, Loss: 0.004998849239200354
2018-10-24 08:38:34.837662:	Training iteration: 486000, Loss: 0.0038365765940397978
2018-10-24 08:39:56.028371:	Training iteration: 486200, Loss: 0.005903671961277723
2018-10-24 08:41:17.222926:	Training iteration: 486400, Loss: 0.004062863998115063
2018-10-24 08:42:38.506052:	Training iteration: 486600, Loss: 0.0047554378397762775
2018-10-24 08:43:59.825938:	Training iteration: 486800, Loss: 0.0037568628322333097
2018-10-24 08:45:21.048378:	Training iteration: 487000, Loss: 0.0035931316670030355
2018-10-24 08:46:42.566736:	Training iteration: 487200, Loss: 0.0047074295580387115
2018-10-24 08:48:03.533545:	Training iteration: 487400, Loss: 0.0027157431468367577
2018-10-24 08:49:24.644098:	Training iteration: 487600, Loss: 0.005317178089171648
2018-10-24 08:50:45.241216:	Training iteration: 487800, Loss: 0.003747412236407399
2018-10-24 08:52:06.113205:	Training iteration: 488000, Loss: 0.003470843657851219
2018-10-24 08:52:45.353478: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 740 of 1000
2018-10-24 08:52:48.539233: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 08:53:38.363494:	Training iteration: 488200, Loss: 0.0041448804549872875
2018-10-24 08:54:58.700103:	Training iteration: 488400, Loss: 0.004745250102132559
2018-10-24 08:56:19.264762:	Training iteration: 488600, Loss: 0.0061258492060005665
2018-10-24 08:57:39.638691:	Training iteration: 488800, Loss: 0.0044722892343997955
2018-10-24 08:58:59.597124:	Training iteration: 489000, Loss: 0.005338260438293219
2018-10-24 09:00:21.262176:	Training iteration: 489200, Loss: 0.007288350258022547
2018-10-24 09:01:52.491086:	Training iteration: 489400, Loss: 0.005818056408315897
2018-10-24 09:03:12.213312:	Training iteration: 489600, Loss: 0.0038538428489118814
2018-10-24 09:04:32.566549:	Training iteration: 489800, Loss: 0.004120898898690939
2018-10-24 09:05:54.006251:	Training iteration: 490000, Loss: 0.0057822358794510365
Checkpoint
2018-10-24 09:07:16.567609:	Training iteration: 490200, Loss: 0.005644554737955332
2018-10-24 09:08:36.703267:	Training iteration: 490400, Loss: 0.0045877061784267426
2018-10-24 09:09:56.818014:	Training iteration: 490600, Loss: 0.004154277499765158
2018-10-24 09:11:17.243657:	Training iteration: 490800, Loss: 0.005617524031549692
2018-10-24 09:12:37.651363:	Training iteration: 491000, Loss: 0.004198726266622543
2018-10-24 09:13:58.145143:	Training iteration: 491200, Loss: 0.005443412810564041
2018-10-24 09:15:18.750486:	Training iteration: 491400, Loss: 0.004052374046295881
2018-10-24 09:16:39.151986:	Training iteration: 491600, Loss: 0.006270373705774546
2018-10-24 09:17:59.594885:	Training iteration: 491800, Loss: 0.004242649767547846
2018-10-24 09:19:20.702646:	Training iteration: 492000, Loss: 0.004690919071435928
2018-10-24 09:20:41.600795:	Training iteration: 492200, Loss: 0.0030298393685370684
2018-10-24 09:22:02.480644:	Training iteration: 492400, Loss: 0.005282794591039419
2018-10-24 09:23:23.623726:	Training iteration: 492600, Loss: 0.004456900991499424
2018-10-24 09:24:44.776753:	Training iteration: 492800, Loss: 0.004801975097507238
2018-10-24 09:26:06.388371:	Training iteration: 493000, Loss: 0.005671671126037836
2018-10-24 09:27:27.337328:	Training iteration: 493200, Loss: 0.004424843937158585
2018-10-24 09:28:48.918325:	Training iteration: 493400, Loss: 0.0030161961913108826
2018-10-24 09:30:10.210615:	Training iteration: 493600, Loss: 0.005554743576794863
2018-10-24 09:31:31.441425:	Training iteration: 493800, Loss: 0.003927011974155903
2018-10-24 09:32:53.013883:	Training iteration: 494000, Loss: 0.005040809977799654
2018-10-24 09:34:14.244471:	Training iteration: 494200, Loss: 0.0032847991678863764
2018-10-24 09:35:35.627529:	Training iteration: 494400, Loss: 0.0034366603940725327
2018-10-24 09:36:57.095882:	Training iteration: 494600, Loss: 0.004426165483891964
2018-10-24 09:38:18.550419:	Training iteration: 494800, Loss: 0.008292331360280514
2018-10-24 09:39:40.047380:	Training iteration: 495000, Loss: 0.004369017668068409
2018-10-24 09:41:01.613122:	Training iteration: 495200, Loss: 0.004792868625372648
2018-10-24 09:42:23.054741:	Training iteration: 495400, Loss: 0.004627616610378027
2018-10-24 09:43:44.495171:	Training iteration: 495600, Loss: 0.00362200103700161
2018-10-24 09:45:05.761471:	Training iteration: 495800, Loss: 0.003927657846361399
2018-10-24 09:46:27.459509:	Training iteration: 496000, Loss: 0.004768598824739456
2018-10-24 09:47:49.138579:	Training iteration: 496200, Loss: 0.004085593856871128
2018-10-24 09:49:14.124647: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 764 of 1000
2018-10-24 09:49:16.970575: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 09:49:23.074403:	Training iteration: 496400, Loss: 0.005871622357517481
2018-10-24 09:50:42.883396:	Training iteration: 496600, Loss: 0.003832503454759717
2018-10-24 09:52:03.609851:	Training iteration: 496800, Loss: 0.004647191613912582
2018-10-24 09:53:24.233607:	Training iteration: 497000, Loss: 0.006155604962259531
2018-10-24 09:54:44.574824:	Training iteration: 497200, Loss: 0.004524867981672287
2018-10-24 09:56:05.374286:	Training iteration: 497400, Loss: 0.003654828993603587
2018-10-24 09:57:25.890467:	Training iteration: 497600, Loss: 0.004723771009594202
2018-10-24 09:58:46.363927:	Training iteration: 497800, Loss: 0.005310302134603262
2018-10-24 10:00:06.851344:	Training iteration: 498000, Loss: 0.005794297903776169
2018-10-24 10:01:27.023313:	Training iteration: 498200, Loss: 0.004105302970856428
2018-10-24 10:02:49.230092:	Training iteration: 498400, Loss: 0.007910293526947498
2018-10-24 10:04:10.612775:	Training iteration: 498600, Loss: 0.00439315102994442
2018-10-24 10:05:30.680861:	Training iteration: 498800, Loss: 0.006082909647375345
2018-10-24 10:06:50.762296:	Training iteration: 499000, Loss: 0.005290234927088022
2018-10-24 10:08:10.948349:	Training iteration: 499200, Loss: 0.006031265016645193
2018-10-24 10:09:30.863242:	Training iteration: 499400, Loss: 0.005432822275906801
2018-10-24 10:10:51.190407:	Training iteration: 499600, Loss: 0.004702315200120211
2018-10-24 10:12:11.335269:	Training iteration: 499800, Loss: 0.004755282774567604
2018-10-24 10:13:31.995300:	Training iteration: 500000, Loss: 0.004269087687134743
Checkpoint
2018-10-24 10:14:54.659935:	Training iteration: 500200, Loss: 0.00477813184261322
2018-10-24 10:16:15.262782:	Training iteration: 500400, Loss: 0.003287729574367404
2018-10-24 10:17:35.897975:	Training iteration: 500600, Loss: 0.004087007138878107
2018-10-24 10:18:56.549530:	Training iteration: 500800, Loss: 0.005526477470993996
2018-10-24 10:20:17.191298:	Training iteration: 501000, Loss: 0.005456257611513138
2018-10-24 10:21:37.622498:	Training iteration: 501200, Loss: 0.00654916325584054
2018-10-24 10:22:58.538162:	Training iteration: 501400, Loss: 0.0054209306836128235
2018-10-24 10:24:19.150344:	Training iteration: 501600, Loss: 0.005176797974854708
2018-10-24 10:25:40.373344:	Training iteration: 501800, Loss: 0.0037856714334338903
2018-10-24 10:27:01.546259:	Training iteration: 502000, Loss: 0.006031608674675226
2018-10-24 10:28:22.653775:	Training iteration: 502200, Loss: 0.004046113695949316
2018-10-24 10:29:43.484069:	Training iteration: 502400, Loss: 0.003752397373318672
2018-10-24 10:31:04.937342:	Training iteration: 502600, Loss: 0.003246727166697383
2018-10-24 10:32:26.607098:	Training iteration: 502800, Loss: 0.007057417184114456
2018-10-24 10:33:48.365808:	Training iteration: 503000, Loss: 0.004960877355188131
2018-10-24 10:35:10.280886:	Training iteration: 503200, Loss: 0.005821797996759415
2018-10-24 10:36:32.155455:	Training iteration: 503400, Loss: 0.0029834285378456116
2018-10-24 10:37:53.516762:	Training iteration: 503600, Loss: 0.0051407646387815475
2018-10-24 10:39:15.447794:	Training iteration: 503800, Loss: 0.004799854476004839
2018-10-24 10:40:37.291986:	Training iteration: 504000, Loss: 0.0029664079193025827
2018-10-24 10:41:59.201062:	Training iteration: 504200, Loss: 0.004960793070495129
2018-10-24 10:43:21.219615:	Training iteration: 504400, Loss: 0.004674920812249184
2018-10-24 10:44:43.130874:	Training iteration: 504600, Loss: 0.003913227468729019
2018-10-24 10:45:29.995875: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 774 of 1000
2018-10-24 10:45:32.826749: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 10:46:16.439819:	Training iteration: 504800, Loss: 0.005124940071254969
2018-10-24 10:47:38.260156:	Training iteration: 505000, Loss: 0.005864283535629511
2018-10-24 10:48:59.981287:	Training iteration: 505200, Loss: 0.00439797667786479
2018-10-24 10:50:21.849538:	Training iteration: 505400, Loss: 0.0034456246066838503
2018-10-24 10:51:43.576131:	Training iteration: 505600, Loss: 0.006624121684581041
2018-10-24 10:53:05.058515:	Training iteration: 505800, Loss: 0.003264219732955098
2018-10-24 10:54:26.424409:	Training iteration: 506000, Loss: 0.004449726548045874
2018-10-24 10:55:47.689902:	Training iteration: 506200, Loss: 0.0038171422202140093
2018-10-24 10:57:08.604711:	Training iteration: 506400, Loss: 0.003995220642536879
2018-10-24 10:58:30.456427:	Training iteration: 506600, Loss: 0.0038059838116168976
2018-10-24 10:59:51.360398:	Training iteration: 506800, Loss: 0.0035467178095132113
2018-10-24 11:01:11.847997:	Training iteration: 507000, Loss: 0.004650650545954704
2018-10-24 11:02:32.712092:	Training iteration: 507200, Loss: 0.0034938121680170298
2018-10-24 11:03:53.432128:	Training iteration: 507400, Loss: 0.005994849372655153
2018-10-24 11:05:14.075437:	Training iteration: 507600, Loss: 0.0038001530338078737
2018-10-24 11:06:39.621450:	Training iteration: 507800, Loss: 0.00602003512904048
2018-10-24 11:08:00.228504:	Training iteration: 508000, Loss: 0.005365269724279642
2018-10-24 11:09:20.776149:	Training iteration: 508200, Loss: 0.003927987068891525
2018-10-24 11:10:41.052214:	Training iteration: 508400, Loss: 0.0037979362532496452
2018-10-24 11:12:01.466041:	Training iteration: 508600, Loss: 0.002383145969361067
2018-10-24 11:13:21.786868:	Training iteration: 508800, Loss: 0.005749875213950872
2018-10-24 11:14:41.984167:	Training iteration: 509000, Loss: 0.004158837255090475
2018-10-24 11:16:03.291832:	Training iteration: 509200, Loss: 0.003766546957194805
2018-10-24 11:17:23.931288:	Training iteration: 509400, Loss: 0.0029539167881011963
2018-10-24 11:18:44.682479:	Training iteration: 509600, Loss: 0.004619532264769077
2018-10-24 11:20:05.798117:	Training iteration: 509800, Loss: 0.0048552281223237514
2018-10-24 11:21:26.843938:	Training iteration: 510000, Loss: 0.00390796922147274
Checkpoint
2018-10-24 11:22:50.139382:	Training iteration: 510200, Loss: 0.004136623814702034
2018-10-24 11:24:11.075650:	Training iteration: 510400, Loss: 0.003157290630042553
2018-10-24 11:25:32.064177:	Training iteration: 510600, Loss: 0.004319767002016306
2018-10-24 11:26:53.042380:	Training iteration: 510800, Loss: 0.004043142776936293
2018-10-24 11:28:13.839007:	Training iteration: 511000, Loss: 0.003608780913054943
2018-10-24 11:29:34.784831:	Training iteration: 511200, Loss: 0.003741811728104949
2018-10-24 11:30:55.881893:	Training iteration: 511400, Loss: 0.004174795001745224
2018-10-24 11:32:17.185556:	Training iteration: 511600, Loss: 0.004581006709486246
2018-10-24 11:33:38.883234:	Training iteration: 511800, Loss: 0.004885779228061438
2018-10-24 11:35:00.218809:	Training iteration: 512000, Loss: 0.00408819317817688
2018-10-24 11:36:21.350592:	Training iteration: 512200, Loss: 0.003450099378824234
2018-10-24 11:37:42.755871:	Training iteration: 512400, Loss: 0.0033012486528605223
2018-10-24 11:39:04.348741:	Training iteration: 512600, Loss: 0.006078370381146669
2018-10-24 11:40:25.867669:	Training iteration: 512800, Loss: 0.0026973795611411333
2018-10-24 11:41:47.175483:	Training iteration: 513000, Loss: 0.005531603936105967
2018-10-24 11:43:08.977827:	Training iteration: 513200, Loss: 0.0037480054888874292
2018-10-24 11:44:28.551012: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 878 of 1000
2018-10-24 11:44:29.844030: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 11:44:41.229357:	Training iteration: 513400, Loss: 0.009389004670083523
2018-10-24 11:46:02.481096:	Training iteration: 513600, Loss: 0.006473701447248459
2018-10-24 11:47:24.025396:	Training iteration: 513800, Loss: 0.008770197629928589
2018-10-24 11:48:45.201175:	Training iteration: 514000, Loss: 0.004505084827542305
2018-10-24 11:50:06.137695:	Training iteration: 514200, Loss: 0.0038190260529518127
2018-10-24 11:51:27.663914:	Training iteration: 514400, Loss: 0.004293367732316256
2018-10-24 11:52:48.850052:	Training iteration: 514600, Loss: 0.004098853562027216
2018-10-24 11:54:10.076445:	Training iteration: 514800, Loss: 0.006181301083415747
2018-10-24 11:55:31.164081:	Training iteration: 515000, Loss: 0.004945062100887299
2018-10-24 11:56:51.997798:	Training iteration: 515200, Loss: 0.007644484285265207
2018-10-24 11:58:12.617839:	Training iteration: 515400, Loss: 0.00631725275889039
2018-10-24 11:59:33.026575:	Training iteration: 515600, Loss: 0.007206501439213753
2018-10-24 12:00:53.522690:	Training iteration: 515800, Loss: 0.0054874420166015625
2018-10-24 12:02:13.628141:	Training iteration: 516000, Loss: 0.004799886606633663
2018-10-24 12:03:33.817870:	Training iteration: 516200, Loss: 0.003022422082722187
2018-10-24 12:04:53.960218:	Training iteration: 516400, Loss: 0.003563047619536519
2018-10-24 12:06:14.100791:	Training iteration: 516600, Loss: 0.003496095770969987
2018-10-24 12:07:34.415580:	Training iteration: 516800, Loss: 0.00430546747520566
2018-10-24 12:08:54.524791:	Training iteration: 517000, Loss: 0.004860779270529747
2018-10-24 12:10:14.576903:	Training iteration: 517200, Loss: 0.0026289287488907576
2018-10-24 12:11:34.677979:	Training iteration: 517400, Loss: 0.004413079470396042
2018-10-24 12:12:56.657216:	Training iteration: 517600, Loss: 0.004794893320649862
2018-10-24 12:14:16.892333:	Training iteration: 517800, Loss: 0.006672699004411697
2018-10-24 12:15:36.899605:	Training iteration: 518000, Loss: 0.007676917593926191
2018-10-24 12:16:57.084933:	Training iteration: 518200, Loss: 0.006577529478818178
2018-10-24 12:18:17.370572:	Training iteration: 518400, Loss: 0.007137524429708719
2018-10-24 12:19:37.984060:	Training iteration: 518600, Loss: 0.0031748805195093155
2018-10-24 12:20:58.460827:	Training iteration: 518800, Loss: 0.004951351787894964
2018-10-24 12:22:19.073069:	Training iteration: 519000, Loss: 0.003497194731608033
2018-10-24 12:23:39.830668:	Training iteration: 519200, Loss: 0.0071125328540802
2018-10-24 12:25:00.781124:	Training iteration: 519400, Loss: 0.0030323676764965057
2018-10-24 12:26:21.686546:	Training iteration: 519600, Loss: 0.003964162897318602
2018-10-24 12:27:42.298414:	Training iteration: 519800, Loss: 0.006732604932039976
2018-10-24 12:29:03.145484:	Training iteration: 520000, Loss: 0.004704692866653204
Checkpoint
2018-10-24 12:30:26.118017:	Training iteration: 520200, Loss: 0.005308119114488363
2018-10-24 12:31:46.765324:	Training iteration: 520400, Loss: 0.004089334513992071
2018-10-24 12:33:07.838711:	Training iteration: 520600, Loss: 0.004795566201210022
2018-10-24 12:34:28.720210:	Training iteration: 520800, Loss: 0.006341239903122187
2018-10-24 12:35:49.694236:	Training iteration: 521000, Loss: 0.0037355374079197645
2018-10-24 12:37:10.732719:	Training iteration: 521200, Loss: 0.007000422570854425
2018-10-24 12:38:31.783116:	Training iteration: 521400, Loss: 0.004266401287168264
2018-10-24 12:39:53.001660:	Training iteration: 521600, Loss: 0.006234169006347656
2018-10-24 12:41:14.112403:	Training iteration: 521800, Loss: 0.0031375170219689608
2018-10-24 12:42:35.224470:	Training iteration: 522000, Loss: 0.004820183385163546
2018-10-24 12:43:56.116553:	Training iteration: 522200, Loss: 0.002432533772662282
2018-10-24 12:45:17.378079:	Training iteration: 522400, Loss: 0.006028305739164352
2018-10-24 12:46:38.085803:	Training iteration: 522600, Loss: 0.006018552463501692
2018-10-24 12:47:59.615858:	Training iteration: 522800, Loss: 0.0062723965384066105
2018-10-24 12:49:20.836751:	Training iteration: 523000, Loss: 0.006542351562529802
2018-10-24 12:50:42.147641:	Training iteration: 523200, Loss: 0.005318770185112953
2018-10-24 12:52:03.362640:	Training iteration: 523400, Loss: 0.0059545040130615234
2018-10-24 12:53:24.853869:	Training iteration: 523600, Loss: 0.003959252033382654
2018-10-24 12:54:45.814965:	Training iteration: 523800, Loss: 0.006399177014827728
2018-10-24 12:56:06.766458:	Training iteration: 524000, Loss: 0.007438338827341795
2018-10-24 12:57:27.486846:	Training iteration: 524200, Loss: 0.004737417679280043
2018-10-24 12:58:48.372212:	Training iteration: 524400, Loss: 0.005215618759393692
2018-10-24 13:00:09.116063:	Training iteration: 524600, Loss: 0.004745126701891422
2018-10-24 13:01:30.147609:	Training iteration: 524800, Loss: 0.004782583098858595
2018-10-24 13:02:50.296267:	Training iteration: 525000, Loss: 0.008890933357179165
2018-10-24 13:04:10.847042:	Training iteration: 525200, Loss: 0.007830657996237278
2018-10-24 13:05:32.308275:	Training iteration: 525400, Loss: 0.005403804127126932
2018-10-24 13:06:52.984517:	Training iteration: 525600, Loss: 0.008266757242381573
2018-10-24 13:08:13.517297:	Training iteration: 525800, Loss: 0.006212564185261726
2018-10-24 13:09:33.670880:	Training iteration: 526000, Loss: 0.008644129149615765
2018-10-24 13:10:53.484446:	Training iteration: 526200, Loss: 0.0062456331215798855
2018-10-24 13:12:14.049012:	Training iteration: 526400, Loss: 0.003919916693121195
2018-10-24 13:13:34.392417:	Training iteration: 526600, Loss: 0.005862012505531311
2018-10-24 13:14:54.264616:	Training iteration: 526800, Loss: 0.0035761746112257242
2018-10-24 13:16:14.394142:	Training iteration: 527000, Loss: 0.005685862153768539
2018-10-24 13:17:34.401268:	Training iteration: 527200, Loss: 0.006897251587361097
2018-10-24 13:18:54.481012:	Training iteration: 527400, Loss: 0.0037758375983685255
2018-10-24 13:20:14.562511:	Training iteration: 527600, Loss: 0.004985848441720009
2018-10-24 13:21:34.897445:	Training iteration: 527800, Loss: 0.0043622893281280994
2018-10-24 13:22:55.368756:	Training iteration: 528000, Loss: 0.0066810534335672855
2018-10-24 13:24:16.191639:	Training iteration: 528200, Loss: 0.007442530710250139
2018-10-24 13:25:37.056290:	Training iteration: 528400, Loss: 0.0039328886196017265
2018-10-24 13:26:58.042999:	Training iteration: 528600, Loss: 0.006841898430138826
2018-10-24 13:28:18.993281:	Training iteration: 528800, Loss: 0.006015647668391466
2018-10-24 13:29:39.813928:	Training iteration: 529000, Loss: 0.004669656977057457
2018-10-24 13:31:00.809508:	Training iteration: 529200, Loss: 0.005424397066235542
2018-10-24 13:32:21.677593:	Training iteration: 529400, Loss: 0.0069845933467149734
2018-10-24 13:33:42.711965:	Training iteration: 529600, Loss: 0.003924321848899126
2018-10-24 13:35:03.438079:	Training iteration: 529800, Loss: 0.004425585735589266
2018-10-24 13:36:24.139678:	Training iteration: 530000, Loss: 0.004039753694087267
Checkpoint
2018-10-24 13:37:47.006145:	Training iteration: 530200, Loss: 0.003612685250118375
2018-10-24 13:39:08.022639:	Training iteration: 530400, Loss: 0.005329973995685577
2018-10-24 13:40:29.723437:	Training iteration: 530600, Loss: 0.007044011261314154
2018-10-24 13:41:50.945402:	Training iteration: 530800, Loss: 0.0081757428124547
2018-10-24 13:43:12.875198:	Training iteration: 531000, Loss: 0.007827728055417538
2018-10-24 13:44:34.627413:	Training iteration: 531200, Loss: 0.006183704361319542
2018-10-24 13:45:56.062536:	Training iteration: 531400, Loss: 0.0069902329705655575
2018-10-24 13:47:18.069625:	Training iteration: 531600, Loss: 0.007802394684404135
2018-10-24 13:48:39.393807:	Training iteration: 531800, Loss: 0.004572820384055376
2018-10-24 13:50:01.149875:	Training iteration: 532000, Loss: 0.00614760210737586
2018-10-24 13:51:23.035648:	Training iteration: 532200, Loss: 0.00575617840513587
2018-10-24 13:52:46.187756:	Training iteration: 532400, Loss: 0.0037644151598215103
2018-10-24 13:54:08.154503:	Training iteration: 532600, Loss: 0.005985694006085396
2018-10-24 13:55:30.097578:	Training iteration: 532800, Loss: 0.0067156278528273106
2018-10-24 13:56:51.952835:	Training iteration: 533000, Loss: 0.005604837089776993
2018-10-24 13:58:14.114188:	Training iteration: 533200, Loss: 0.005521387327462435
2018-10-24 13:59:35.991784:	Training iteration: 533400, Loss: 0.006469451356679201
2018-10-24 14:00:58.556447:	Training iteration: 533600, Loss: 0.006279063876718283
2018-10-24 14:02:20.619594:	Training iteration: 533800, Loss: 0.004359155893325806
2018-10-24 14:03:42.208770:	Training iteration: 534000, Loss: 0.002300696447491646
2018-10-24 14:05:03.914417:	Training iteration: 534200, Loss: 0.0038652457296848297
2018-10-24 14:06:25.183487:	Training iteration: 534400, Loss: 0.0044561284594237804
2018-10-24 14:07:46.857288:	Training iteration: 534600, Loss: 0.004454873036593199
2018-10-24 14:09:08.612299:	Training iteration: 534800, Loss: 0.004362500738352537
2018-10-24 14:10:30.600581:	Training iteration: 535000, Loss: 0.005473180208355188
2018-10-24 14:11:52.087688:	Training iteration: 535200, Loss: 0.010017379187047482
2018-10-24 14:13:14.015735:	Training iteration: 535400, Loss: 0.00855393335223198
2018-10-24 14:14:35.840701:	Training iteration: 535600, Loss: 0.0035782905761152506
2018-10-24 14:15:57.763899:	Training iteration: 535800, Loss: 0.004105559084564447
2018-10-24 14:17:19.474275:	Training iteration: 536000, Loss: 0.0034624908585101366
2018-10-24 14:18:40.680256:	Training iteration: 536200, Loss: 0.007541249971836805
2018-10-24 14:20:02.229936:	Training iteration: 536400, Loss: 0.005750384647399187
2018-10-24 14:21:23.474755:	Training iteration: 536600, Loss: 0.006878392305225134
2018-10-24 14:22:45.181349:	Training iteration: 536800, Loss: 0.0041964915581047535
2018-10-24 14:24:06.949433:	Training iteration: 537000, Loss: 0.004158384632319212
2018-10-24 14:25:28.887660:	Training iteration: 537200, Loss: 0.003761001629754901
2018-10-24 14:26:50.732648:	Training iteration: 537400, Loss: 0.006939733866602182
2018-10-24 14:28:12.473220:	Training iteration: 537600, Loss: 0.0037540991324931383
2018-10-24 14:29:34.387791:	Training iteration: 537800, Loss: 0.003834665520116687
2018-10-24 14:30:56.735398:	Training iteration: 538000, Loss: 0.004008665680885315
2018-10-24 14:32:18.581890:	Training iteration: 538200, Loss: 0.004628386348485947
2018-10-24 14:33:40.233080:	Training iteration: 538400, Loss: 0.004656091798096895
2018-10-24 14:35:02.715981:	Training iteration: 538600, Loss: 0.004520599264651537
2018-10-24 14:36:24.425088:	Training iteration: 538800, Loss: 0.004652855452150106
2018-10-24 14:37:46.264494:	Training iteration: 539000, Loss: 0.004413909744471312
2018-10-24 14:39:07.913884:	Training iteration: 539200, Loss: 0.004026693757623434
2018-10-24 14:40:29.716563:	Training iteration: 539400, Loss: 0.003595382208004594
2018-10-24 14:41:51.586266:	Training iteration: 539600, Loss: 0.0050117396749556065
2018-10-24 14:43:13.374613:	Training iteration: 539800, Loss: 0.0037915955763310194
2018-10-24 14:44:35.323225:	Training iteration: 540000, Loss: 0.006568115204572678
Checkpoint
2018-10-24 14:45:59.928937:	Training iteration: 540200, Loss: 0.004498496185988188
2018-10-24 14:47:21.572376:	Training iteration: 540400, Loss: 0.004747555125504732
2018-10-24 14:48:43.350252:	Training iteration: 540600, Loss: 0.005804616957902908
2018-10-24 14:50:05.271500:	Training iteration: 540800, Loss: 0.00644877552986145
2018-10-24 14:51:27.186187:	Training iteration: 541000, Loss: 0.0044054132886230946
2018-10-24 14:52:49.080022:	Training iteration: 541200, Loss: 0.003849954577162862
2018-10-24 14:54:11.457252:	Training iteration: 541400, Loss: 0.00604109326377511
2018-10-24 14:55:33.356160:	Training iteration: 541600, Loss: 0.005869352724403143
2018-10-24 14:56:55.466685:	Training iteration: 541800, Loss: 0.006558211520314217
2018-10-24 14:58:17.463042:	Training iteration: 542000, Loss: 0.004934828262776136
2018-10-24 14:59:39.566434:	Training iteration: 542200, Loss: 0.007789270486682653
2018-10-24 15:01:01.459920:	Training iteration: 542400, Loss: 0.005776035133749247
2018-10-24 15:02:23.259618:	Training iteration: 542600, Loss: 0.0035897300112992525
2018-10-24 15:03:45.250488:	Training iteration: 542800, Loss: 0.010729820467531681
2018-10-24 15:05:06.994034:	Training iteration: 543000, Loss: 0.00754587771371007
2018-10-24 15:06:28.882547:	Training iteration: 543200, Loss: 0.007190490607172251
2018-10-24 15:07:50.625186:	Training iteration: 543400, Loss: 0.006347529590129852
2018-10-24 15:09:12.661343:	Training iteration: 543600, Loss: 0.005718776490539312
2018-10-24 15:10:34.449007:	Training iteration: 543800, Loss: 0.01062361802905798
2018-10-24 15:11:56.241370:	Training iteration: 544000, Loss: 0.0030188951641321182
2018-10-24 15:13:18.061538:	Training iteration: 544200, Loss: 0.005103662144392729
2018-10-24 15:14:39.255364:	Training iteration: 544400, Loss: 0.004653044510632753
2018-10-24 15:16:01.571235:	Training iteration: 544600, Loss: 0.006804723292589188
2018-10-24 15:17:23.453454:	Training iteration: 544800, Loss: 0.006050387863069773
2018-10-24 15:18:45.376378:	Training iteration: 545000, Loss: 0.003932930529117584
2018-10-24 15:20:07.549235:	Training iteration: 545200, Loss: 0.003567800624296069
2018-10-24 15:21:29.086799:	Training iteration: 545400, Loss: 0.005028339568525553
2018-10-24 15:22:50.490404:	Training iteration: 545600, Loss: 0.004748165607452393
2018-10-24 15:24:11.274999:	Training iteration: 545800, Loss: 0.006441479083150625
2018-10-24 15:25:33.385110:	Training iteration: 546000, Loss: 0.0033910523634403944
2018-10-24 15:26:55.133805:	Training iteration: 546200, Loss: 0.006556472275406122
2018-10-24 15:28:16.522864:	Training iteration: 546400, Loss: 0.006114255636930466
2018-10-24 15:29:38.290942:	Training iteration: 546600, Loss: 0.003531281603500247
2018-10-24 15:31:00.081531:	Training iteration: 546800, Loss: 0.004205536562949419
2018-10-24 15:32:22.210650:	Training iteration: 547000, Loss: 0.0037919096648693085
2018-10-24 15:33:44.345535:	Training iteration: 547200, Loss: 0.003920523915439844
2018-10-24 15:35:05.943042:	Training iteration: 547400, Loss: 0.004384756088256836
2018-10-24 15:36:27.802063:	Training iteration: 547600, Loss: 0.005759429186582565
2018-10-24 15:37:49.642427:	Training iteration: 547800, Loss: 0.004507861565798521
2018-10-24 15:39:11.788386:	Training iteration: 548000, Loss: 0.0062481495551764965
2018-10-24 15:40:34.137851:	Training iteration: 548200, Loss: 0.005023727659136057
2018-10-24 15:41:56.029062:	Training iteration: 548400, Loss: 0.006600172724574804
2018-10-24 15:43:17.723714:	Training iteration: 548600, Loss: 0.007387151475995779
2018-10-24 15:44:39.443412:	Training iteration: 548800, Loss: 0.004809368867427111
2018-10-24 15:46:01.627172:	Training iteration: 549000, Loss: 0.005229513626545668
2018-10-24 15:47:23.596665:	Training iteration: 549200, Loss: 0.005436706822365522
2018-10-24 15:48:45.991196:	Training iteration: 549400, Loss: 0.003737430786713958
2018-10-24 15:50:07.397810:	Training iteration: 549600, Loss: 0.006790002342313528
2018-10-24 15:51:29.304800:	Training iteration: 549800, Loss: 0.006947385147213936
2018-10-24 15:52:51.231693:	Training iteration: 550000, Loss: 0.0043527958914637566
Checkpoint
2018-10-24 15:54:15.791922:	Training iteration: 550200, Loss: 0.005154257174581289
2018-10-24 15:55:37.886850:	Training iteration: 550400, Loss: 0.006327816750854254
2018-10-24 15:56:59.633997:	Training iteration: 550600, Loss: 0.00495878467336297
2018-10-24 15:58:21.568856:	Training iteration: 550800, Loss: 0.007324586156755686
2018-10-24 15:59:43.708707:	Training iteration: 551000, Loss: 0.006427878979593515
2018-10-24 16:01:05.476300:	Training iteration: 551200, Loss: 0.005532041657716036
2018-10-24 16:02:27.112281:	Training iteration: 551400, Loss: 0.0077390181832015514
2018-10-24 16:03:49.481582:	Training iteration: 551600, Loss: 0.005096769891679287
2018-10-24 16:05:11.177305:	Training iteration: 551800, Loss: 0.004832560662180185
2018-10-24 16:06:33.266163:	Training iteration: 552000, Loss: 0.0041463058441877365
2018-10-24 16:07:55.375240:	Training iteration: 552200, Loss: 0.003213359974324703
2018-10-24 16:09:17.754800:	Training iteration: 552400, Loss: 0.008635523729026318
2018-10-24 16:10:39.053743:	Training iteration: 552600, Loss: 0.010194236412644386
2018-10-24 16:12:01.174597:	Training iteration: 552800, Loss: 0.004905751906335354
2018-10-24 16:13:22.665927:	Training iteration: 553000, Loss: 0.005273497197777033
2018-10-24 16:14:44.784209:	Training iteration: 553200, Loss: 0.005975514650344849
2018-10-24 16:16:06.786467:	Training iteration: 553400, Loss: 0.00660618906840682
2018-10-24 16:17:29.350615:	Training iteration: 553600, Loss: 0.005618898663669825
2018-10-24 16:18:51.542486:	Training iteration: 553800, Loss: 0.004877667874097824
2018-10-24 16:20:13.478246:	Training iteration: 554000, Loss: 0.0034040699247270823
2018-10-24 16:21:35.495391:	Training iteration: 554200, Loss: 0.004079793114215136
2018-10-24 16:22:57.186009:	Training iteration: 554400, Loss: 0.005006194114685059
2018-10-24 16:24:19.484219:	Training iteration: 554600, Loss: 0.004408104345202446
2018-10-24 16:25:41.654113:	Training iteration: 554800, Loss: 0.0035348127130419016
2018-10-24 16:27:03.881512:	Training iteration: 555000, Loss: 0.005666257347911596
2018-10-24 16:28:26.541778:	Training iteration: 555200, Loss: 0.006557755637913942
2018-10-24 16:29:49.680742:	Training iteration: 555400, Loss: 0.007142206188291311
2018-10-24 16:31:11.936861:	Training iteration: 555600, Loss: 0.005632761400192976
2018-10-24 16:32:34.650814:	Training iteration: 555800, Loss: 0.0030842076521366835
2018-10-24 16:33:57.084173:	Training iteration: 556000, Loss: 0.005426952149719
2018-10-24 16:35:19.795197:	Training iteration: 556200, Loss: 0.004812342580407858
2018-10-24 16:36:42.737009:	Training iteration: 556400, Loss: 0.005098563153296709
2018-10-24 16:38:05.419462:	Training iteration: 556600, Loss: 0.004719010554254055
2018-10-24 16:39:28.262731:	Training iteration: 556800, Loss: 0.005148505326360464
2018-10-24 16:40:51.229678:	Training iteration: 557000, Loss: 0.003935204353183508
2018-10-24 16:42:14.145959:	Training iteration: 557200, Loss: 0.006402391940355301
2018-10-24 16:43:37.180401:	Training iteration: 557400, Loss: 0.004581734072417021
2018-10-24 16:45:00.211722:	Training iteration: 557600, Loss: 0.006502542644739151
2018-10-24 16:46:23.393873:	Training iteration: 557800, Loss: 0.005220222752541304
2018-10-24 16:47:46.316524:	Training iteration: 558000, Loss: 0.0067238896153867245
2018-10-24 16:49:09.556991:	Training iteration: 558200, Loss: 0.003945561591535807
2018-10-24 16:50:32.814368:	Training iteration: 558400, Loss: 0.004019701387733221
2018-10-24 16:51:55.385936:	Training iteration: 558600, Loss: 0.0054733362048864365
2018-10-24 16:53:18.312946:	Training iteration: 558800, Loss: 0.0039301118813455105
2018-10-24 16:54:41.258597:	Training iteration: 559000, Loss: 0.0036986451596021652
2018-10-24 16:56:04.451905:	Training iteration: 559200, Loss: 0.006258626934140921
2018-10-24 16:57:27.358427:	Training iteration: 559400, Loss: 0.006404460873454809
2018-10-24 16:58:50.298240:	Training iteration: 559600, Loss: 0.00464726472273469
2018-10-24 17:00:02.516140:	Epoch 6 finished after 559777 iterations.
Validating
2018-10-24 17:00:02.532086:	Entering validation loop
2018-10-24 17:00:12.535762: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 608 of 1000
2018-10-24 17:00:18.003365: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 17:00:53.425067:	Validation iteration: 200, Loss: 0.005805096123367548
2018-10-24 17:01:29.766088:	Validation iteration: 400, Loss: 0.008509465493261814
2018-10-24 17:02:06.038642:	Validation iteration: 600, Loss: 0.007672691252082586
2018-10-24 17:02:42.255388:	Validation iteration: 800, Loss: 0.004729254636913538
2018-10-24 17:03:19.776468:	Validation iteration: 1000, Loss: 0.00704136723652482
2018-10-24 17:03:55.941770:	Validation iteration: 1200, Loss: 0.006746232975274324
2018-10-24 17:04:40.534126: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 545 of 1000
2018-10-24 17:04:46.682492: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 17:04:48.031882:	Validation iteration: 1400, Loss: 0.007110385689884424
2018-10-24 17:05:22.105124:	Validation iteration: 1600, Loss: 0.006533023435622454
2018-10-24 17:05:57.704824:	Validation iteration: 1800, Loss: 0.0056749009527266026
2018-10-24 17:06:36.778418:	Validation iteration: 2000, Loss: 0.0073417373932898045
2018-10-24 17:07:12.690848:	Validation iteration: 2200, Loss: 0.006185735110193491
2018-10-24 17:07:48.521593:	Validation iteration: 2400, Loss: 0.008380469866096973
2018-10-24 17:08:24.574462:	Validation iteration: 2600, Loss: 0.007908530533313751
2018-10-24 17:09:09.132130: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 498 of 1000
2018-10-24 17:09:16.692925: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 17:09:19.353928:	Validation iteration: 2800, Loss: 0.005497831851243973
2018-10-24 17:09:53.076010:	Validation iteration: 3000, Loss: 0.0048835561610758305
2018-10-24 17:10:28.661226:	Validation iteration: 3200, Loss: 0.004201032686978579
2018-10-24 17:11:04.170727:	Validation iteration: 3400, Loss: 0.0054658339358866215
2018-10-24 17:11:42.256454:	Validation iteration: 3600, Loss: 0.006114290561527014
2018-10-24 17:12:18.061204:	Validation iteration: 3800, Loss: 0.006032489240169525
2018-10-24 17:12:53.965163:	Validation iteration: 4000, Loss: 0.006836449261754751
2018-10-24 17:13:35.624221: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 688 of 1000
2018-10-24 17:13:39.784440: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 17:13:43.725695:	Validation iteration: 4200, Loss: 0.006347023416310549
2018-10-24 17:14:17.632025:	Validation iteration: 4400, Loss: 0.007175335194915533
2018-10-24 17:14:53.270875:	Validation iteration: 4600, Loss: 0.006700494792312384
2018-10-24 17:15:28.613078:	Validation iteration: 4800, Loss: 0.005439841654151678
2018-10-24 17:16:04.584405:	Validation iteration: 5000, Loss: 0.006122334394603968
2018-10-24 17:16:39.713693:	Validation iteration: 5200, Loss: 0.006325503811240196
2018-10-24 17:17:15.563159:	Validation iteration: 5400, Loss: 0.00581427663564682
2018-10-24 17:17:56.081049: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 651 of 1000
2018-10-24 17:18:00.973730: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 17:18:06.187692:	Validation iteration: 5600, Loss: 0.005866388324648142
2018-10-24 17:18:40.089945:	Validation iteration: 5800, Loss: 0.0037624258548021317
2018-10-24 17:19:15.060043:	Validation iteration: 6000, Loss: 0.004447691608220339
2018-10-24 17:19:50.358748:	Validation iteration: 6200, Loss: 0.00488949054852128
2018-10-24 17:20:25.999596:	Validation iteration: 6400, Loss: 0.004516576416790485
2018-10-24 17:21:01.247731:	Validation iteration: 6600, Loss: 0.005471915006637573
2018-10-24 17:21:37.001326:	Validation iteration: 6800, Loss: 0.004677035380154848
2018-10-24 17:22:11.820856:	Validation iteration: 7000, Loss: 0.006018519401550293
2018-10-24 17:22:47.806460:	Validation iteration: 7200, Loss: 0.005914626177400351
2018-10-24 17:23:23.880726:	Validation iteration: 7400, Loss: 0.008823983371257782
Validation check mean loss: 0.005918579525438481
Validation loss has improved!
New best validation cost!
Training complete after 7 epochs.
Finished requested number of epochs.
Final validation loss: 0.005918579525438481
This was the best validation loss achieved
Starting testing
2018-10-24 17:23:41.231791:	Entering test loop
2018-10-24 17:23:55.824316: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 662 of 1000
2018-10-24 17:24:00.597028: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 17:24:00.926469:	Testing iteration: 0, Loss: 0.005836583208292723
2018-10-24 17:27:16.188917:	Testing iteration: 200, Loss: 0.005282942205667496
2018-10-24 17:30:34.941301:	Testing iteration: 400, Loss: 0.006346432026475668
2018-10-24 17:34:02.293008:	Testing iteration: 600, Loss: 0.006499212235212326
2018-10-24 17:37:28.070726:	Testing iteration: 800, Loss: 0.006072739604860544
2018-10-24 17:40:59.739459:	Testing iteration: 1000, Loss: 0.005694112274795771
2018-10-24 17:42:30.590558: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 692 of 1000
2018-10-24 17:42:34.917860: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 17:44:45.742689:	Testing iteration: 1200, Loss: 0.006419042125344276
2018-10-24 17:48:20.458860:	Testing iteration: 1400, Loss: 0.004919011611491442
2018-10-24 17:51:57.174867:	Testing iteration: 1600, Loss: 0.005089698825031519
2018-10-24 17:55:34.711011:	Testing iteration: 1800, Loss: 0.0046226452104747295
2018-10-24 17:59:16.289126:	Testing iteration: 2000, Loss: 0.004631089512258768
2018-10-24 18:02:52.181152: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 691 of 1000
2018-10-24 18:02:56.493561: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 18:04:00.309726:	Testing iteration: 2200, Loss: 0.00509341387078166
2018-10-24 18:08:26.259721:	Testing iteration: 2400, Loss: 0.004553255159407854
2018-10-24 18:13:14.351861:	Testing iteration: 2600, Loss: 0.004856871906667948
2018-10-24 18:17:14.667865:	Testing iteration: 2800, Loss: 0.005746760871261358
2018-10-24 18:21:10.166807:	Testing iteration: 3000, Loss: 0.004847347736358643
2018-10-24 18:25:08.202801:	Testing iteration: 3200, Loss: 0.0048890807665884495
2018-10-24 18:25:56.388702: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 691 of 1000
2018-10-24 18:26:00.521815: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 18:29:32.870705:	Testing iteration: 3400, Loss: 0.006240823771804571
2018-10-24 18:33:36.667196:	Testing iteration: 3600, Loss: 0.005736900959163904
2018-10-24 18:37:44.600664:	Testing iteration: 3800, Loss: 0.004550773650407791
2018-10-24 18:41:57.368798:	Testing iteration: 4000, Loss: 0.0038829613476991653
2018-10-24 18:46:18.179245:	Testing iteration: 4200, Loss: 0.0060121905989944935
2018-10-24 18:48:43.320021: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 659 of 1000
2018-10-24 18:48:48.452076: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-24 18:50:52.734511:	Testing iteration: 4400, Loss: 0.006428290158510208
2018-10-24 18:55:14.050073:	Testing iteration: 4600, Loss: 0.00406008493155241
2018-10-24 18:59:35.817765:	Testing iteration: 4800, Loss: 0.004919558763504028
2018-10-24 19:04:41.600471:	Testing iteration: 5000, Loss: 0.005450535099953413
2018-10-24 19:09:54.476191:	Testing iteration: 5200, Loss: 0.006446206476539373
2018-10-24 19:14:56.164475:	Testing iteration: 5400, Loss: 0.0064565264619886875
2018-10-24 19:19:38.838282:	Testing iteration: 5600, Loss: 0.005131733603775501
2018-10-24 19:24:22.888712:	Testing iteration: 5800, Loss: 0.005051939748227596
2018-10-24 19:29:03.908440:	Testing iteration: 6000, Loss: 0.005566891748458147
2018-10-24 19:33:49.694228:	Testing iteration: 6200, Loss: 0.007023240905255079
Test pass complete
Mean loss over test set: 0.00553865391913986
Data saved to dumps/149 for later audio metric calculation
2018-10-24 19:35:41.006981:
	All done with experiment 149!
	Initial test loss: 0.006722370703282082
	Final test loss: 0.00553865391913986
