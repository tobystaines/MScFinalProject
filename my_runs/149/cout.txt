INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "149"
Experiment ID: 149
Preparing dataset
Dataset ready
2018-10-21 12:14:35.671933: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-10-21 12:14:35.942835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-21 12:14:35.944391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:26:00.0
totalMemory: 10.91GiB freeMemory: 10.76GiB
2018-10-21 12:14:35.944412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:26:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Running initialisation test
Starting testing
2018-10-21 12:14:48.391711:	Entering test loop
2018-10-21 12:14:58.922376: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 469 of 1000
2018-10-21 12:15:08.019252: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 12:15:16.731448:	Testing iteration: 0, Loss: 0.0067664943635463715
2018-10-21 12:17:10.747443:	Testing iteration: 200, Loss: 0.0048614260740578175
2018-10-21 12:19:05.567822:	Testing iteration: 400, Loss: 0.005713187158107758
2018-10-21 12:21:05.916506:	Testing iteration: 600, Loss: 0.007568668108433485
2018-10-21 12:23:08.580985:	Testing iteration: 800, Loss: 0.003867474151775241
2018-10-21 12:25:15.186852:	Testing iteration: 1000, Loss: 0.004866656381636858
2018-10-21 12:26:14.237378: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 586 of 1000
2018-10-21 12:26:20.700023: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 12:27:45.771777:	Testing iteration: 1200, Loss: 0.004695537965744734
2018-10-21 12:30:00.034911:	Testing iteration: 1400, Loss: 0.006452595815062523
2018-10-21 12:32:15.929225:	Testing iteration: 1600, Loss: 0.005985687952488661
2018-10-21 12:34:37.727806:	Testing iteration: 1800, Loss: 0.005522201303392649
2018-10-21 12:37:04.175368:	Testing iteration: 2000, Loss: 0.0063097551465034485
2018-10-21 12:39:04.275037: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 603 of 1000
2018-10-21 12:39:10.462029: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 12:39:46.463969:	Testing iteration: 2200, Loss: 0.0067658028565347195
2018-10-21 12:42:15.951047:	Testing iteration: 2400, Loss: 0.009105042554438114
2018-10-21 12:44:47.889483:	Testing iteration: 2600, Loss: 0.010166799649596214
2018-10-21 12:47:23.332191:	Testing iteration: 2800, Loss: 0.007524014916270971
2018-10-21 12:50:01.176643:	Testing iteration: 3000, Loss: 0.007472439203411341
2018-10-21 12:52:44.563422:	Testing iteration: 3200, Loss: 0.004548377823084593
2018-10-21 12:53:17.577103: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 606 of 1000
2018-10-21 12:53:23.446316: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 12:55:44.357811:	Testing iteration: 3400, Loss: 0.010156634263694286
2018-10-21 12:58:34.138067:	Testing iteration: 3600, Loss: 0.0041009485721588135
2018-10-21 13:01:38.682377:	Testing iteration: 3800, Loss: 0.0069780475459992886
2018-10-21 13:05:00.094384:	Testing iteration: 4000, Loss: 0.005777793470770121
2018-10-21 13:08:22.914739:	Testing iteration: 4200, Loss: 0.0068930648267269135
2018-10-21 13:10:21.267283: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 618 of 1000
2018-10-21 13:10:27.615506: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 13:12:02.825152:	Testing iteration: 4400, Loss: 0.006802053656429052
2018-10-21 13:15:14.416328:	Testing iteration: 4600, Loss: 0.009296963922679424
2018-10-21 13:18:22.872658:	Testing iteration: 4800, Loss: 0.012321126647293568
2018-10-21 13:21:32.716080:	Testing iteration: 5000, Loss: 0.006777389440685511
2018-10-21 13:24:45.477632:	Testing iteration: 5200, Loss: 0.009041144512593746
2018-10-21 13:28:02.278496:	Testing iteration: 5400, Loss: 0.007462214212864637
2018-10-21 13:31:19.869434:	Testing iteration: 5600, Loss: 0.0066007934510707855
2018-10-21 13:34:41.569262:	Testing iteration: 5800, Loss: 0.008079192601144314
2018-10-21 13:38:06.825837:	Testing iteration: 6000, Loss: 0.009128127247095108
2018-10-21 13:41:34.191175:	Testing iteration: 6200, Loss: 0.004442439414560795
Test pass complete
Mean loss over test set: 0.006722370703282082
Data saved to dumps/149 for later audio metric calculation
Starting training
2018-10-21 13:43:10.516080: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 692 of 1000
2018-10-21 13:43:14.507791: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 13:44:42.705171:	Training iteration: 200, Loss: 0.0037726934533566236
2018-10-21 13:46:18.734494:	Training iteration: 400, Loss: 0.004040947183966637
2018-10-21 13:48:01.059356:	Training iteration: 600, Loss: 0.0041722399182617664
2018-10-21 13:49:45.051175:	Training iteration: 800, Loss: 0.005352379288524389
2018-10-21 13:51:30.012897:	Training iteration: 1000, Loss: 0.0031208230648189783
2018-10-21 13:53:18.196084:	Training iteration: 1200, Loss: 0.0037995281163603067
2018-10-21 13:55:06.110586:	Training iteration: 1400, Loss: 0.004604616668075323
2018-10-21 13:56:54.032369:	Training iteration: 1600, Loss: 0.004598718602210283
2018-10-21 13:58:40.685123:	Training iteration: 1800, Loss: 0.003837287425994873
2018-10-21 14:00:28.058159:	Training iteration: 2000, Loss: 0.004872603341937065
2018-10-21 14:02:14.847446:	Training iteration: 2200, Loss: 0.002542734844610095
2018-10-21 14:04:03.736764:	Training iteration: 2400, Loss: 0.006266981363296509
2018-10-21 14:05:53.204489:	Training iteration: 2600, Loss: 0.004347190726548433
2018-10-21 14:07:43.584004:	Training iteration: 2800, Loss: 0.004915768280625343
2018-10-21 14:09:27.815328:	Training iteration: 3000, Loss: 0.005650512408465147
2018-10-21 14:11:13.144621:	Training iteration: 3200, Loss: 0.003979028668254614
2018-10-21 14:12:58.923009:	Training iteration: 3400, Loss: 0.003765036351978779
2018-10-21 14:14:44.395628:	Training iteration: 3600, Loss: 0.0040371776558458805
2018-10-21 14:16:29.852519:	Training iteration: 3800, Loss: 0.004439252428710461
2018-10-21 14:18:17.041870:	Training iteration: 4000, Loss: 0.005553100723773241
2018-10-21 14:20:04.432413:	Training iteration: 4200, Loss: 0.002881592372432351
2018-10-21 14:21:51.686121:	Training iteration: 4400, Loss: 0.0034376420080661774
2018-10-21 14:23:38.179938:	Training iteration: 4600, Loss: 0.004397931043058634
2018-10-21 14:25:24.365100:	Training iteration: 4800, Loss: 0.005932917352765799
2018-10-21 14:27:09.538034:	Training iteration: 5000, Loss: 0.005023652222007513
2018-10-21 14:28:55.893225:	Training iteration: 5200, Loss: 0.004208942409604788
2018-10-21 14:30:42.569083:	Training iteration: 5400, Loss: 0.0034269343595951796
2018-10-21 14:32:30.491559:	Training iteration: 5600, Loss: 0.005241965409368277
2018-10-21 14:34:17.875079:	Training iteration: 5800, Loss: 0.0036563079338520765
2018-10-21 14:36:04.372649:	Training iteration: 6000, Loss: 0.003989153541624546
2018-10-21 14:37:49.826573:	Training iteration: 6200, Loss: 0.0046133059076964855
2018-10-21 14:39:38.120326:	Training iteration: 6400, Loss: 0.004190834239125252
2018-10-21 14:41:26.610189:	Training iteration: 6600, Loss: 0.0034785978496074677
2018-10-21 14:43:15.860765:	Training iteration: 6800, Loss: 0.004301533568650484
2018-10-21 14:45:03.964146:	Training iteration: 7000, Loss: 0.004884280730038881
2018-10-21 14:46:53.450525:	Training iteration: 7200, Loss: 0.004045052453875542
2018-10-21 14:48:42.176476:	Training iteration: 7400, Loss: 0.004717858508229256
2018-10-21 14:50:30.438927:	Training iteration: 7600, Loss: 0.0040185716934502125
2018-10-21 14:52:17.896466:	Training iteration: 7800, Loss: 0.004369428846985102
2018-10-21 14:54:03.657399:	Training iteration: 8000, Loss: 0.005783951375633478
2018-10-21 14:55:51.746180:	Training iteration: 8200, Loss: 0.004078024532645941
2018-10-21 14:56:35.048587: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 642 of 1000
2018-10-21 14:56:39.950467: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 14:57:47.895101:	Training iteration: 8400, Loss: 0.006537502631545067
2018-10-21 14:59:34.319926:	Training iteration: 8600, Loss: 0.00686138728633523
2018-10-21 15:01:24.916625:	Training iteration: 8800, Loss: 0.004093351308256388
2018-10-21 15:03:11.164668:	Training iteration: 9000, Loss: 0.004184307996183634
2018-10-21 15:04:57.962730:	Training iteration: 9200, Loss: 0.003728957613930106
2018-10-21 15:06:47.812309:	Training iteration: 9400, Loss: 0.004763621371239424
2018-10-21 15:08:35.342489:	Training iteration: 9600, Loss: 0.004674328025430441
2018-10-21 15:10:24.075431:	Training iteration: 9800, Loss: 0.005813965108245611
2018-10-21 15:12:14.430022:	Training iteration: 10000, Loss: 0.004321030806750059
Checkpoint
2018-10-21 15:14:18.754365:	Training iteration: 10200, Loss: 0.005622869823127985
2018-10-21 15:16:02.065809:	Training iteration: 10400, Loss: 0.004407168831676245
2018-10-21 15:17:48.856836:	Training iteration: 10600, Loss: 0.004608117043972015
2018-10-21 15:19:35.239700:	Training iteration: 10800, Loss: 0.004551224410533905
2018-10-21 15:21:21.443409:	Training iteration: 11000, Loss: 0.005580951925367117
2018-10-21 15:23:07.663243:	Training iteration: 11200, Loss: 0.004218424204736948
2018-10-21 15:24:53.954434:	Training iteration: 11400, Loss: 0.0055897594429552555
2018-10-21 15:26:38.478281:	Training iteration: 11600, Loss: 0.00651344982907176
2018-10-21 15:28:24.189356:	Training iteration: 11800, Loss: 0.004975136369466782
2018-10-21 15:30:11.520507:	Training iteration: 12000, Loss: 0.005701734218746424
2018-10-21 15:31:56.948342:	Training iteration: 12200, Loss: 0.004095959011465311
2018-10-21 15:33:43.401207:	Training iteration: 12400, Loss: 0.003544008359313011
2018-10-21 15:35:29.623278:	Training iteration: 12600, Loss: 0.005294140428304672
2018-10-21 15:37:19.424392:	Training iteration: 12800, Loss: 0.003959173336625099
2018-10-21 15:38:51.865793:	Training iteration: 13000, Loss: 0.004244996700435877
2018-10-21 15:40:19.446772:	Training iteration: 13200, Loss: 0.0038694695103913546
2018-10-21 15:41:46.582434:	Training iteration: 13400, Loss: 0.005783988628536463
2018-10-21 15:43:12.582497:	Training iteration: 13600, Loss: 0.005817478056997061
2018-10-21 15:44:37.649067:	Training iteration: 13800, Loss: 0.00456068804487586
2018-10-21 15:46:05.099871:	Training iteration: 14000, Loss: 0.0047765509225428104
2018-10-21 15:47:38.657402:	Training iteration: 14200, Loss: 0.0044247801415622234
2018-10-21 15:49:19.961366:	Training iteration: 14400, Loss: 0.005338374059647322
2018-10-21 15:51:04.353967:	Training iteration: 14600, Loss: 0.004980985075235367
2018-10-21 15:53:05.113451:	Training iteration: 14800, Loss: 0.004015162121504545
2018-10-21 15:54:47.690795:	Training iteration: 15000, Loss: 0.004945915192365646
2018-10-21 15:56:36.191905:	Training iteration: 15200, Loss: 0.0036298104096204042
2018-10-21 15:58:27.340586:	Training iteration: 15400, Loss: 0.005385936703532934
2018-10-21 16:00:20.801355:	Training iteration: 15600, Loss: 0.0037600763607770205
2018-10-21 16:02:19.387355:	Training iteration: 15800, Loss: 0.0045217047445476055
2018-10-21 16:04:19.866428:	Training iteration: 16000, Loss: 0.0048203556798398495
2018-10-21 16:06:20.698215:	Training iteration: 16200, Loss: 0.0050347899086773396
2018-10-21 16:08:20.279528:	Training iteration: 16400, Loss: 0.005184689536690712
2018-10-21 16:10:13.089956: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 631 of 1000
2018-10-21 16:10:17.933958: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 16:10:28.916511:	Training iteration: 16600, Loss: 0.006448822561651468
2018-10-21 16:12:19.916481:	Training iteration: 16800, Loss: 0.004867831710726023
2018-10-21 16:14:12.702258:	Training iteration: 17000, Loss: 0.005336580332368612
2018-10-21 16:15:59.775076:	Training iteration: 17200, Loss: 0.005315958056598902
2018-10-21 16:17:47.150681:	Training iteration: 17400, Loss: 0.008662131614983082
2018-10-21 16:19:34.630007:	Training iteration: 17600, Loss: 0.0060807145200669765
2018-10-21 16:21:22.873845:	Training iteration: 17800, Loss: 0.004506553988903761
2018-10-21 16:23:11.351942:	Training iteration: 18000, Loss: 0.005689407233148813
2018-10-21 16:25:01.864518:	Training iteration: 18200, Loss: 0.004799462854862213
2018-10-21 16:26:51.311924:	Training iteration: 18400, Loss: 0.0050361971370875835
2018-10-21 16:28:39.637904:	Training iteration: 18600, Loss: 0.005273330956697464
2018-10-21 16:30:27.044609:	Training iteration: 18800, Loss: 0.005923168733716011
2018-10-21 16:32:13.707545:	Training iteration: 19000, Loss: 0.005755996331572533
2018-10-21 16:34:03.608357:	Training iteration: 19200, Loss: 0.004024003632366657
2018-10-21 16:35:53.890201:	Training iteration: 19400, Loss: 0.005356708075851202
2018-10-21 16:37:42.381874:	Training iteration: 19600, Loss: 0.004822164308279753
2018-10-21 16:39:31.567287:	Training iteration: 19800, Loss: 0.004129207693040371
2018-10-21 16:41:18.732259:	Training iteration: 20000, Loss: 0.004732923582196236
Checkpoint
2018-10-21 16:43:49.232100:	Training iteration: 20200, Loss: 0.005631677806377411
2018-10-21 16:45:29.798003:	Training iteration: 20400, Loss: 0.004930447321385145
2018-10-21 16:47:13.274050:	Training iteration: 20600, Loss: 0.006695280317217112
2018-10-21 16:48:56.900395:	Training iteration: 20800, Loss: 0.0048722680658102036
2018-10-21 16:50:43.127346:	Training iteration: 21000, Loss: 0.004515289794653654
2018-10-21 16:52:30.038543:	Training iteration: 21200, Loss: 0.006878615822643042
2018-10-21 16:54:18.130137:	Training iteration: 21400, Loss: 0.005369042977690697
2018-10-21 16:56:05.168008:	Training iteration: 21600, Loss: 0.0032893826719373465
2018-10-21 16:57:52.073184:	Training iteration: 21800, Loss: 0.00567741459235549
2018-10-21 16:59:39.319099:	Training iteration: 22000, Loss: 0.003762936219573021
2018-10-21 17:01:25.217605:	Training iteration: 22200, Loss: 0.00709480931982398
2018-10-21 17:03:13.750735:	Training iteration: 22400, Loss: 0.004565655253827572
2018-10-21 17:05:01.974266:	Training iteration: 22600, Loss: 0.004134416114538908
2018-10-21 17:06:48.959794:	Training iteration: 22800, Loss: 0.00445933360606432
2018-10-21 17:08:37.102621:	Training iteration: 23000, Loss: 0.0047761257737874985
2018-10-21 17:10:26.503938:	Training iteration: 23200, Loss: 0.006490966770797968
2018-10-21 17:12:17.668040:	Training iteration: 23400, Loss: 0.005479301791638136
2018-10-21 17:14:09.857462:	Training iteration: 23600, Loss: 0.004304150119423866
2018-10-21 17:16:01.892162:	Training iteration: 23800, Loss: 0.005655287299305201
2018-10-21 17:17:52.463087:	Training iteration: 24000, Loss: 0.0054082684218883514
2018-10-21 17:19:39.997575:	Training iteration: 24200, Loss: 0.006090777460485697
2018-10-21 17:21:26.586942:	Training iteration: 24400, Loss: 0.00369894877076149
2018-10-21 17:23:13.746373:	Training iteration: 24600, Loss: 0.004281382542103529
2018-10-21 17:25:02.814095:	Training iteration: 24800, Loss: 0.006234075874090195
2018-10-21 17:25:57.698491: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 661 of 1000
2018-10-21 17:26:02.183969: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 17:27:00.817918:	Training iteration: 25000, Loss: 0.004215410444885492
2018-10-21 17:28:48.354826:	Training iteration: 25200, Loss: 0.003845197381451726
2018-10-21 17:30:36.433575:	Training iteration: 25400, Loss: 0.003920992370694876
2018-10-21 17:32:24.945351:	Training iteration: 25600, Loss: 0.0044617061503231525
2018-10-21 17:34:11.977530:	Training iteration: 25800, Loss: 0.003642231924459338
2018-10-21 17:36:00.893938:	Training iteration: 26000, Loss: 0.006205336656421423
2018-10-21 17:37:49.996472:	Training iteration: 26200, Loss: 0.0042540766298770905
2018-10-21 17:39:36.673851:	Training iteration: 26400, Loss: 0.004998540971428156
2018-10-21 17:41:21.722012:	Training iteration: 26600, Loss: 0.0040438175201416016
2018-10-21 17:43:12.527214:	Training iteration: 26800, Loss: 0.006615743041038513
2018-10-21 17:45:00.132862:	Training iteration: 27000, Loss: 0.004113791044801474
2018-10-21 17:46:48.820223:	Training iteration: 27200, Loss: 0.00529438117519021
2018-10-21 17:48:36.402742:	Training iteration: 27400, Loss: 0.006298985332250595
2018-10-21 17:50:23.992535:	Training iteration: 27600, Loss: 0.0037615057080984116
2018-10-21 17:52:12.154586:	Training iteration: 27800, Loss: 0.003439035266637802
2018-10-21 17:54:02.311487:	Training iteration: 28000, Loss: 0.005555763375014067
2018-10-21 17:55:51.687642:	Training iteration: 28200, Loss: 0.0050086588598787785
2018-10-21 17:57:41.264807:	Training iteration: 28400, Loss: 0.005677816923707724
2018-10-21 17:59:31.512824:	Training iteration: 28600, Loss: 0.0038973602931946516
2018-10-21 18:01:21.112813:	Training iteration: 28800, Loss: 0.0036103276070207357
2018-10-21 18:03:12.733046:	Training iteration: 29000, Loss: 0.0027827646117657423
2018-10-21 18:05:02.849546:	Training iteration: 29200, Loss: 0.004931362811475992
2018-10-21 18:06:50.370787:	Training iteration: 29400, Loss: 0.005077969282865524
2018-10-21 18:08:38.718541:	Training iteration: 29600, Loss: 0.003868143307045102
2018-10-21 18:10:27.813376:	Training iteration: 29800, Loss: 0.004204252269119024
2018-10-21 18:12:17.611475:	Training iteration: 30000, Loss: 0.00598873570561409
Checkpoint
2018-10-21 18:14:07.844306:	Training iteration: 30200, Loss: 0.005401221569627523
2018-10-21 18:15:56.489219:	Training iteration: 30400, Loss: 0.0028752803336828947
2018-10-21 18:17:44.391083:	Training iteration: 30600, Loss: 0.004662145860493183
2018-10-21 18:19:34.397654:	Training iteration: 30800, Loss: 0.005600946489721537
2018-10-21 18:21:27.141674:	Training iteration: 31000, Loss: 0.003795548574998975
2018-10-21 18:23:18.743779:	Training iteration: 31200, Loss: 0.003380751935765147
2018-10-21 18:25:11.622190:	Training iteration: 31400, Loss: 0.004735768307000399
2018-10-21 18:27:03.809690:	Training iteration: 31600, Loss: 0.004456364084035158
2018-10-21 18:28:54.880417:	Training iteration: 31800, Loss: 0.004860505927354097
2018-10-21 18:30:44.750552:	Training iteration: 32000, Loss: 0.004325216170400381
2018-10-21 18:32:34.373465:	Training iteration: 32200, Loss: 0.0036490755155682564
2018-10-21 18:34:25.684988:	Training iteration: 32400, Loss: 0.004856365267187357
2018-10-21 18:36:17.377821:	Training iteration: 32600, Loss: 0.005225112196058035
2018-10-21 18:38:08.771215:	Training iteration: 32800, Loss: 0.005779445171356201
2018-10-21 18:39:57.824869:	Training iteration: 33000, Loss: 0.003518257988616824
2018-10-21 18:41:49.510369:	Training iteration: 33200, Loss: 0.004583823960274458
2018-10-21 18:43:41.634450:	Training iteration: 33400, Loss: 0.004115989897400141
2018-10-21 18:45:23.176103: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 720 of 1000
2018-10-21 18:45:26.302523: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 18:45:43.920517:	Training iteration: 33600, Loss: 0.007016906514763832
2018-10-21 18:47:31.724022:	Training iteration: 33800, Loss: 0.006462448742240667
2018-10-21 18:49:20.988146:	Training iteration: 34000, Loss: 0.008300799876451492
2018-10-21 18:51:10.366501:	Training iteration: 34200, Loss: 0.004849076736718416
2018-10-21 18:53:01.189127:	Training iteration: 34400, Loss: 0.006728440057486296
2018-10-21 18:54:51.495967:	Training iteration: 34600, Loss: 0.0066247135400772095
2018-10-21 18:56:41.164599:	Training iteration: 34800, Loss: 0.004763510078191757
2018-10-21 18:58:30.868513:	Training iteration: 35000, Loss: 0.003498772857710719
2018-10-21 19:00:20.668272:	Training iteration: 35200, Loss: 0.003877554088830948
2018-10-21 19:02:10.010893:	Training iteration: 35400, Loss: 0.005445197224617004
2018-10-21 19:03:59.363413:	Training iteration: 35600, Loss: 0.00542322127148509
2018-10-21 19:05:47.424124:	Training iteration: 35800, Loss: 0.00835817214101553
2018-10-21 19:07:35.573690:	Training iteration: 36000, Loss: 0.009067405946552753
2018-10-21 19:09:23.006396:	Training iteration: 36200, Loss: 0.004740412812680006
2018-10-21 19:11:11.312857:	Training iteration: 36400, Loss: 0.004996582865715027
2018-10-21 19:12:59.738065:	Training iteration: 36600, Loss: 0.003388046519830823
2018-10-21 19:14:49.178671:	Training iteration: 36800, Loss: 0.004853413440287113
2018-10-21 19:16:37.975754:	Training iteration: 37000, Loss: 0.004671166185289621
2018-10-21 19:18:28.340955:	Training iteration: 37200, Loss: 0.0032426349353045225
2018-10-21 19:20:18.544840:	Training iteration: 37400, Loss: 0.005789484828710556
2018-10-21 19:22:10.097244:	Training iteration: 37600, Loss: 0.005435949191451073
2018-10-21 19:23:57.518116:	Training iteration: 37800, Loss: 0.005204298533499241
2018-10-21 19:25:45.697186:	Training iteration: 38000, Loss: 0.005562978330999613
2018-10-21 19:27:35.648752:	Training iteration: 38200, Loss: 0.006859338376671076
2018-10-21 19:29:26.197090:	Training iteration: 38400, Loss: 0.007833237759768963
2018-10-21 19:31:16.693010:	Training iteration: 38600, Loss: 0.004051674623042345
2018-10-21 19:33:05.780081:	Training iteration: 38800, Loss: 0.004250612575560808
2018-10-21 19:34:55.057271:	Training iteration: 39000, Loss: 0.005702211055904627
2018-10-21 19:36:45.005570:	Training iteration: 39200, Loss: 0.004606613889336586
2018-10-21 19:38:34.474302:	Training iteration: 39400, Loss: 0.003071095561608672
2018-10-21 19:40:22.188439:	Training iteration: 39600, Loss: 0.0034194334875792265
2018-10-21 19:42:09.985496:	Training iteration: 39800, Loss: 0.003488769056275487
2018-10-21 19:44:00.765473:	Training iteration: 40000, Loss: 0.004917967598885298
Checkpoint
2018-10-21 19:45:52.219644:	Training iteration: 40200, Loss: 0.008124075829982758
2018-10-21 19:47:40.073883:	Training iteration: 40400, Loss: 0.003511110320687294
2018-10-21 19:49:29.823503:	Training iteration: 40600, Loss: 0.00648132711648941
2018-10-21 19:51:22.287258:	Training iteration: 40800, Loss: 0.005947371479123831
2018-10-21 19:53:14.524159:	Training iteration: 41000, Loss: 0.00721761817112565
2018-10-21 19:55:09.435771:	Training iteration: 41200, Loss: 0.003917214926332235
2018-10-21 19:57:03.676043:	Training iteration: 41400, Loss: 0.006819050759077072
2018-10-21 19:58:56.238330:	Training iteration: 41600, Loss: 0.005684154573827982
2018-10-21 20:00:52.105677:	Training iteration: 41800, Loss: 0.0035674944519996643
2018-10-21 20:02:46.062738:	Training iteration: 42000, Loss: 0.00548192672431469
2018-10-21 20:04:42.280955:	Training iteration: 42200, Loss: 0.006606787908822298
2018-10-21 20:06:34.962933:	Training iteration: 42400, Loss: 0.006742683704942465
2018-10-21 20:08:29.608815:	Training iteration: 42600, Loss: 0.00912133976817131
2018-10-21 20:10:24.447003:	Training iteration: 42800, Loss: 0.0047606793232262135
2018-10-21 20:12:17.576260:	Training iteration: 43000, Loss: 0.006663425359874964
2018-10-21 20:14:08.894814:	Training iteration: 43200, Loss: 0.00766144460067153
2018-10-21 20:16:00.127203:	Training iteration: 43400, Loss: 0.0063780671916902065
2018-10-21 20:17:49.599204:	Training iteration: 43600, Loss: 0.004957374185323715
2018-10-21 20:19:39.860029:	Training iteration: 43800, Loss: 0.005125824827700853
2018-10-21 20:21:30.747317:	Training iteration: 44000, Loss: 0.005597698036581278
2018-10-21 20:23:21.269289:	Training iteration: 44200, Loss: 0.00700734369456768
2018-10-21 20:25:11.189444:	Training iteration: 44400, Loss: 0.0038762844633311033
2018-10-21 20:27:01.840648:	Training iteration: 44600, Loss: 0.007153382059186697
2018-10-21 20:28:50.572578:	Training iteration: 44800, Loss: 0.004779379814863205
2018-10-21 20:30:38.247130:	Training iteration: 45000, Loss: 0.010891617275774479
2018-10-21 20:32:27.399927:	Training iteration: 45200, Loss: 0.00746935373172164
2018-10-21 20:34:15.342283:	Training iteration: 45400, Loss: 0.0066754757426679134
2018-10-21 20:36:04.501287:	Training iteration: 45600, Loss: 0.00799452792853117
2018-10-21 20:37:53.077259:	Training iteration: 45800, Loss: 0.006988395005464554
2018-10-21 20:39:44.206365:	Training iteration: 46000, Loss: 0.004932987969368696
2018-10-21 20:41:33.460869:	Training iteration: 46200, Loss: 0.004527607467025518
2018-10-21 20:43:21.611722:	Training iteration: 46400, Loss: 0.007987109944224358
2018-10-21 20:45:07.416030:	Training iteration: 46600, Loss: 0.006446583662182093
2018-10-21 20:46:54.331150:	Training iteration: 46800, Loss: 0.00825469195842743
2018-10-21 20:48:40.917495:	Training iteration: 47000, Loss: 0.0041452632285654545
2018-10-21 20:50:27.378832:	Training iteration: 47200, Loss: 0.006689500063657761
2018-10-21 20:52:15.236420:	Training iteration: 47400, Loss: 0.006478801369667053
2018-10-21 20:54:04.183832:	Training iteration: 47600, Loss: 0.0053203110583126545
2018-10-21 20:55:51.572881:	Training iteration: 47800, Loss: 0.0042138188146054745
2018-10-21 20:57:40.869157:	Training iteration: 48000, Loss: 0.004259031731635332
2018-10-21 20:59:14.883371:	Training iteration: 48200, Loss: 0.005187679547816515
2018-10-21 21:00:55.215061:	Training iteration: 48400, Loss: 0.005395069252699614
2018-10-21 21:02:39.635977:	Training iteration: 48600, Loss: 0.00831468403339386
2018-10-21 21:04:25.635311:	Training iteration: 48800, Loss: 0.0069135078229010105
2018-10-21 21:06:12.438078:	Training iteration: 49000, Loss: 0.008034073747694492
2018-10-21 21:07:58.635610:	Training iteration: 49200, Loss: 0.005280064418911934
2018-10-21 21:09:44.917984:	Training iteration: 49400, Loss: 0.00743850925937295
2018-10-21 21:11:31.984862:	Training iteration: 49600, Loss: 0.00570099800825119
2018-10-21 21:13:19.256390:	Training iteration: 49800, Loss: 0.007264683023095131
2018-10-21 21:15:08.060132:	Training iteration: 50000, Loss: 0.006581742316484451
Checkpoint
2018-10-21 21:16:57.898114:	Training iteration: 50200, Loss: 0.0052603743970394135
2018-10-21 21:18:45.017888:	Training iteration: 50400, Loss: 0.007216181606054306
2018-10-21 21:20:31.391430:	Training iteration: 50600, Loss: 0.005863734986633062
2018-10-21 21:22:20.200229:	Training iteration: 50800, Loss: 0.003995532635599375
2018-10-21 21:24:08.440480:	Training iteration: 51000, Loss: 0.008589275181293488
2018-10-21 21:25:57.128139:	Training iteration: 51200, Loss: 0.006887430790811777
2018-10-21 21:27:45.852798:	Training iteration: 51400, Loss: 0.004514607135206461
2018-10-21 21:29:33.993947:	Training iteration: 51600, Loss: 0.011661347001791
2018-10-21 21:31:22.332120:	Training iteration: 51800, Loss: 0.007009891327470541
2018-10-21 21:33:12.293536:	Training iteration: 52000, Loss: 0.004554403480142355
2018-10-21 21:35:02.769896:	Training iteration: 52200, Loss: 0.004638811107724905
2018-10-21 21:36:50.880417:	Training iteration: 52400, Loss: 0.007954242639243603
2018-10-21 21:38:38.809190:	Training iteration: 52600, Loss: 0.009285062551498413
2018-10-21 21:40:27.526409:	Training iteration: 52800, Loss: 0.0061895283870399
2018-10-21 21:42:15.141852:	Training iteration: 53000, Loss: 0.004750473890453577
2018-10-21 21:44:02.986071:	Training iteration: 53200, Loss: 0.0037820052821189165
2018-10-21 21:45:51.364109:	Training iteration: 53400, Loss: 0.004379295278340578
2018-10-21 21:47:39.609356:	Training iteration: 53600, Loss: 0.004329513292759657
2018-10-21 21:49:27.160589:	Training iteration: 53800, Loss: 0.007002662867307663
2018-10-21 21:51:14.249440:	Training iteration: 54000, Loss: 0.005219649523496628
2018-10-21 21:53:00.943336:	Training iteration: 54200, Loss: 0.0033481114078313112
2018-10-21 21:54:49.631433:	Training iteration: 54400, Loss: 0.0051173013634979725
2018-10-21 21:56:38.254795:	Training iteration: 54600, Loss: 0.0031820200383663177
2018-10-21 21:58:27.852030:	Training iteration: 54800, Loss: 0.006685107480734587
2018-10-21 22:00:15.160931:	Training iteration: 55000, Loss: 0.004308927338570356
2018-10-21 22:02:00.791062:	Training iteration: 55200, Loss: 0.007068139035254717
2018-10-21 22:03:47.827020:	Training iteration: 55400, Loss: 0.0038737349677830935
2018-10-21 22:05:34.592266:	Training iteration: 55600, Loss: 0.005362195428460836
2018-10-21 22:07:23.321101:	Training iteration: 55800, Loss: 0.007059136405587196
2018-10-21 22:09:13.243848:	Training iteration: 56000, Loss: 0.008466645143926144
2018-10-21 22:11:01.753710:	Training iteration: 56200, Loss: 0.004992831964045763
2018-10-21 22:12:53.752650:	Training iteration: 56400, Loss: 0.008902009576559067
2018-10-21 22:14:44.077182:	Training iteration: 56600, Loss: 0.004504029173403978
2018-10-21 22:16:33.426825:	Training iteration: 56800, Loss: 0.0053314934484660625
2018-10-21 22:18:21.467687:	Training iteration: 57000, Loss: 0.008844410069286823
2018-10-21 22:20:10.424271:	Training iteration: 57200, Loss: 0.00563775934278965
2018-10-21 22:22:00.173255:	Training iteration: 57400, Loss: 0.0036998651921749115
2018-10-21 22:23:51.368391:	Training iteration: 57600, Loss: 0.006746351253241301
2018-10-21 22:25:42.595893:	Training iteration: 57800, Loss: 0.005356110632419586
2018-10-21 22:27:32.861515:	Training iteration: 58000, Loss: 0.0028189278673380613
2018-10-21 22:29:20.867874:	Training iteration: 58200, Loss: 0.005045974627137184
2018-10-21 22:31:10.355391:	Training iteration: 58400, Loss: 0.0041547599248588085
2018-10-21 22:32:56.111448:	Training iteration: 58600, Loss: 0.0054999166168272495
2018-10-21 22:34:44.778063:	Training iteration: 58800, Loss: 0.007408348377794027
2018-10-21 22:36:34.171631:	Training iteration: 59000, Loss: 0.003856225870549679
2018-10-21 22:38:21.638179:	Training iteration: 59200, Loss: 0.004190923646092415
2018-10-21 22:40:09.706513:	Training iteration: 59400, Loss: 0.0050157043151557446
2018-10-21 22:41:58.428474:	Training iteration: 59600, Loss: 0.0045067667961120605
2018-10-21 22:43:48.160321:	Training iteration: 59800, Loss: 0.004989513661712408
2018-10-21 22:45:39.453804:	Training iteration: 60000, Loss: 0.004545015748590231
Checkpoint
2018-10-21 22:47:31.208618:	Training iteration: 60200, Loss: 0.007426382973790169
2018-10-21 22:49:19.993147:	Training iteration: 60400, Loss: 0.005902457982301712
2018-10-21 22:51:07.831673:	Training iteration: 60600, Loss: 0.005054414737969637
2018-10-21 22:52:56.402820:	Training iteration: 60800, Loss: 0.008741769939661026
2018-10-21 22:54:44.803997:	Training iteration: 61000, Loss: 0.004400686826556921
2018-10-21 22:56:34.106083:	Training iteration: 61200, Loss: 0.006048053037375212
2018-10-21 22:58:21.477343:	Training iteration: 61400, Loss: 0.004618094768375158
2018-10-21 23:00:09.325263:	Training iteration: 61600, Loss: 0.008037727326154709
2018-10-21 23:01:58.611285:	Training iteration: 61800, Loss: 0.005752407014369965
2018-10-21 23:03:45.351454:	Training iteration: 62000, Loss: 0.005562891718000174
2018-10-21 23:05:30.660493:	Training iteration: 62200, Loss: 0.0061959158629179
2018-10-21 23:07:16.174994:	Training iteration: 62400, Loss: 0.0035639237612485886
2018-10-21 23:09:03.560804:	Training iteration: 62600, Loss: 0.009478184394538403
2018-10-21 23:10:51.882877:	Training iteration: 62800, Loss: 0.0060341390781104565
2018-10-21 23:12:38.673544:	Training iteration: 63000, Loss: 0.009359347634017467
2018-10-21 23:14:24.778845:	Training iteration: 63200, Loss: 0.004075673408806324
2018-10-21 23:16:11.465206:	Training iteration: 63400, Loss: 0.00820866134017706
2018-10-21 23:17:59.283124:	Training iteration: 63600, Loss: 0.006848569493740797
2018-10-21 23:19:46.240971:	Training iteration: 63800, Loss: 0.008320939727127552
2018-10-21 23:21:35.062506:	Training iteration: 64000, Loss: 0.00840615201741457
2018-10-21 23:23:23.364405:	Training iteration: 64200, Loss: 0.012011236511170864
2018-10-21 23:25:10.773357:	Training iteration: 64400, Loss: 0.0038216973189264536
2018-10-21 23:26:58.994904:	Training iteration: 64600, Loss: 0.003594551933929324
2018-10-21 23:28:46.461470:	Training iteration: 64800, Loss: 0.004863489884883165
2018-10-21 23:30:34.624518:	Training iteration: 65000, Loss: 0.004581726621836424
2018-10-21 23:32:22.209738:	Training iteration: 65200, Loss: 0.0053322226740419865
2018-10-21 23:34:10.749343:	Training iteration: 65400, Loss: 0.005856575444340706
2018-10-21 23:35:58.999867:	Training iteration: 65600, Loss: 0.005004414822906256
2018-10-21 23:37:46.100889:	Training iteration: 65800, Loss: 0.005387600511312485
2018-10-21 23:39:33.891490:	Training iteration: 66000, Loss: 0.00997703056782484
2018-10-21 23:41:21.992682:	Training iteration: 66200, Loss: 0.004865328315645456
2018-10-21 23:43:10.760331:	Training iteration: 66400, Loss: 0.004437791649252176
2018-10-21 23:44:59.972627:	Training iteration: 66600, Loss: 0.004555757623165846
2018-10-21 23:46:48.585754:	Training iteration: 66800, Loss: 0.004114571958780289
2018-10-21 23:48:38.277391:	Training iteration: 67000, Loss: 0.007015701849013567
2018-10-21 23:50:30.142972:	Training iteration: 67200, Loss: 0.00414067879319191
2018-10-21 23:52:21.559679:	Training iteration: 67400, Loss: 0.004924467299133539
2018-10-21 23:54:13.089070:	Training iteration: 67600, Loss: 0.004344699438661337
2018-10-21 23:56:05.349687:	Training iteration: 67800, Loss: 0.005461327265948057
2018-10-21 23:57:58.901996:	Training iteration: 68000, Loss: 0.004373180214315653
2018-10-21 23:59:49.101198:	Training iteration: 68200, Loss: 0.003957963082939386
2018-10-22 00:01:40.730905:	Training iteration: 68400, Loss: 0.007538292091339827
2018-10-22 00:03:34.218588:	Training iteration: 68600, Loss: 0.005127046722918749
2018-10-22 00:05:27.364788:	Training iteration: 68800, Loss: 0.005450190510600805
2018-10-22 00:07:22.356099:	Training iteration: 69000, Loss: 0.006613302510231733
2018-10-22 00:09:15.757678:	Training iteration: 69200, Loss: 0.004688641522079706
2018-10-22 00:11:07.255345:	Training iteration: 69400, Loss: 0.004088318906724453
2018-10-22 00:12:56.182496:	Training iteration: 69600, Loss: 0.005764196161180735
2018-10-22 00:14:44.712046:	Training iteration: 69800, Loss: 0.0049310773611068726
2018-10-22 00:16:32.473672:	Training iteration: 70000, Loss: 0.005547078792005777
Checkpoint
2018-10-22 00:18:24.087618:	Training iteration: 70200, Loss: 0.003518499666824937
2018-10-22 00:20:14.748895:	Training iteration: 70400, Loss: 0.005047349724918604
2018-10-22 00:22:06.491155:	Training iteration: 70600, Loss: 0.006914117839187384
2018-10-22 00:23:57.217854:	Training iteration: 70800, Loss: 0.006084675434976816
2018-10-22 00:25:47.994722:	Training iteration: 71000, Loss: 0.006733367685228586
2018-10-22 00:27:38.360075:	Training iteration: 71200, Loss: 0.006653787102550268
2018-10-22 00:29:26.352988:	Training iteration: 71400, Loss: 0.005024620797485113
2018-10-22 00:31:14.151102:	Training iteration: 71600, Loss: 0.005913719534873962
2018-10-22 00:33:02.539809:	Training iteration: 71800, Loss: 0.006393488496541977
2018-10-22 00:34:50.854726:	Training iteration: 72000, Loss: 0.005049120634794235
2018-10-22 00:36:37.653963:	Training iteration: 72200, Loss: 0.007887756451964378
2018-10-22 00:38:24.390701:	Training iteration: 72400, Loss: 0.004456155002117157
2018-10-22 00:40:11.695095:	Training iteration: 72600, Loss: 0.004428191110491753
2018-10-22 00:41:58.966039:	Training iteration: 72800, Loss: 0.005547558888792992
2018-10-22 00:43:47.356157:	Training iteration: 73000, Loss: 0.004498774651437998
2018-10-22 00:45:34.486972:	Training iteration: 73200, Loss: 0.005369298625737429
2018-10-22 00:47:20.726384:	Training iteration: 73400, Loss: 0.0058089750818908215
2018-10-22 00:49:08.547620:	Training iteration: 73600, Loss: 0.004876564722508192
2018-10-22 00:50:58.036551:	Training iteration: 73800, Loss: 0.005642857402563095
2018-10-22 00:52:48.455860:	Training iteration: 74000, Loss: 0.006148507818579674
2018-10-22 00:54:36.105494:	Training iteration: 74200, Loss: 0.004900874104350805
2018-10-22 00:56:22.628673:	Training iteration: 74400, Loss: 0.004755750764161348
2018-10-22 00:58:13.445720:	Training iteration: 74600, Loss: 0.0061731901951134205
2018-10-22 01:00:06.493917:	Training iteration: 74800, Loss: 0.005568691994994879
2018-10-22 01:01:57.464860:	Training iteration: 75000, Loss: 0.0044552721083164215
2018-10-22 01:03:49.043836:	Training iteration: 75200, Loss: 0.008381289429962635
2018-10-22 01:05:39.696876:	Training iteration: 75400, Loss: 0.0072968690656125546
2018-10-22 01:07:30.890112:	Training iteration: 75600, Loss: 0.005308844149112701
2018-10-22 01:09:21.540035:	Training iteration: 75800, Loss: 0.0073171635158360004
2018-10-22 01:11:11.011505:	Training iteration: 76000, Loss: 0.008531083352863789
2018-10-22 01:12:58.310878:	Training iteration: 76200, Loss: 0.005357093643397093
2018-10-22 01:14:47.690269:	Training iteration: 76400, Loss: 0.006265776231884956
2018-10-22 01:16:33.486351:	Training iteration: 76600, Loss: 0.004960687831044197
2018-10-22 01:18:21.616221:	Training iteration: 76800, Loss: 0.006640932057052851
2018-10-22 01:20:11.018634:	Training iteration: 77000, Loss: 0.005018012132495642
2018-10-22 01:22:02.616237:	Training iteration: 77200, Loss: 0.005124275106936693
2018-10-22 01:23:52.845806:	Training iteration: 77400, Loss: 0.005454763770103455
2018-10-22 01:25:42.252749:	Training iteration: 77600, Loss: 0.0072085014544427395
2018-10-22 01:27:31.819117:	Training iteration: 77800, Loss: 0.0035268592182546854
2018-10-22 01:29:21.933872:	Training iteration: 78000, Loss: 0.004741842392832041
2018-10-22 01:31:11.803937:	Training iteration: 78200, Loss: 0.005787333473563194
2018-10-22 01:33:01.907300:	Training iteration: 78400, Loss: 0.004705002065747976
2018-10-22 01:34:49.960490:	Training iteration: 78600, Loss: 0.003663487732410431
2018-10-22 01:36:37.882122:	Training iteration: 78800, Loss: 0.005464873742312193
2018-10-22 01:38:25.101016:	Training iteration: 79000, Loss: 0.006244182121008635
2018-10-22 01:40:11.554209:	Training iteration: 79200, Loss: 0.008034499362111092
2018-10-22 01:41:59.975251:	Training iteration: 79400, Loss: 0.011049437336623669
2018-10-22 01:43:49.626353:	Training iteration: 79600, Loss: 0.006899021565914154
2018-10-22 01:45:38.012794:	Training iteration: 79800, Loss: 0.007506217807531357
2018-10-22 01:47:09.548374:	Epoch 0 finished after 79969 iterations.
Validating
2018-10-22 01:47:13.299487:	Entering validation loop
2018-10-22 01:47:23.919931: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 597 of 1000
2018-10-22 01:47:30.008100: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 01:48:09.762692:	Validation iteration: 200, Loss: 0.006744939833879471
2018-10-22 01:48:53.783434:	Validation iteration: 400, Loss: 0.006857837084680796
2018-10-22 01:49:40.925241:	Validation iteration: 600, Loss: 0.004964159335941076
2018-10-22 01:50:26.217427:	Validation iteration: 800, Loss: 0.00623699463903904
2018-10-22 01:51:11.689760:	Validation iteration: 1000, Loss: 0.003957616165280342
2018-10-22 01:51:56.740695:	Validation iteration: 1200, Loss: 0.0068263523280620575
2018-10-22 01:52:50.167412: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 611 of 1000
2018-10-22 01:52:56.055189: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 01:52:57.687178:	Validation iteration: 1400, Loss: 0.00614334037527442
2018-10-22 01:53:38.197247:	Validation iteration: 1600, Loss: 0.005921881180256605
2018-10-22 01:54:22.680850:	Validation iteration: 1800, Loss: 0.006983477622270584
2018-10-22 01:55:07.492375:	Validation iteration: 2000, Loss: 0.0057160635478794575
2018-10-22 01:55:54.590205:	Validation iteration: 2200, Loss: 0.0065560657531023026
2018-10-22 01:56:39.595265:	Validation iteration: 2400, Loss: 0.006138049066066742
2018-10-22 01:57:23.298692:	Validation iteration: 2600, Loss: 0.004462870769202709
2018-10-22 01:58:15.007474: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 456 of 1000
2018-10-22 01:58:22.425667: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 01:58:25.523322:	Validation iteration: 2800, Loss: 0.0054214331321418285
2018-10-22 01:59:06.171369:	Validation iteration: 3000, Loss: 0.004878150764852762
2018-10-22 01:59:50.488332:	Validation iteration: 3200, Loss: 0.005133924074470997
2018-10-22 02:00:35.349649:	Validation iteration: 3400, Loss: 0.005993615835905075
2018-10-22 02:01:19.767274:	Validation iteration: 3600, Loss: 0.006654147524386644
2018-10-22 02:02:05.121289:	Validation iteration: 3800, Loss: 0.007160215172916651
2018-10-22 02:02:50.052880:	Validation iteration: 4000, Loss: 0.005284463986754417
2018-10-22 02:03:39.363570: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 672 of 1000
2018-10-22 02:03:43.783349: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:03:48.476098:	Validation iteration: 4200, Loss: 0.006545318756252527
2018-10-22 02:04:30.624627:	Validation iteration: 4400, Loss: 0.009286249987781048
2018-10-22 02:05:16.526972:	Validation iteration: 4600, Loss: 0.0065985447727143764
2018-10-22 02:06:00.914055:	Validation iteration: 4800, Loss: 0.006089319940656424
2018-10-22 02:06:45.041873:	Validation iteration: 5000, Loss: 0.006759652402251959
2018-10-22 02:07:29.744450:	Validation iteration: 5200, Loss: 0.006282883230596781
2018-10-22 02:08:14.313662:	Validation iteration: 5400, Loss: 0.007638797163963318
2018-10-22 02:09:02.273598: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 492 of 1000
2018-10-22 02:09:10.567899: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:09:16.719831:	Validation iteration: 5600, Loss: 0.004355830606073141
2018-10-22 02:09:56.898582:	Validation iteration: 5800, Loss: 0.004534932319074869
2018-10-22 02:10:40.449500:	Validation iteration: 6000, Loss: 0.007900687865912914
2018-10-22 02:11:23.992870:	Validation iteration: 6200, Loss: 0.0079395342618227
2018-10-22 02:12:07.305728:	Validation iteration: 6400, Loss: 0.0065101576037704945
2018-10-22 02:12:51.296825:	Validation iteration: 6600, Loss: 0.006383089814335108
2018-10-22 02:13:36.572165:	Validation iteration: 6800, Loss: 0.005180991720408201
2018-10-22 02:14:20.825836:	Validation iteration: 7000, Loss: 0.00549321947619319
2018-10-22 02:15:04.694761:	Validation iteration: 7200, Loss: 0.008476425893604755
2018-10-22 02:15:49.052903:	Validation iteration: 7400, Loss: 0.006481265649199486
Validation check mean loss: 0.005970435526692621
Validation loss has improved!
New best validation cost!
2018-10-22 02:16:21.407784: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 689 of 1000
2018-10-22 02:16:25.479268: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:16:39.745625:	Training iteration: 80000, Loss: 0.004513969179242849
Checkpoint
2018-10-22 02:18:33.759464:	Training iteration: 80200, Loss: 0.0033721725922077894
2018-10-22 02:20:20.846084:	Training iteration: 80400, Loss: 0.0046802847646176815
2018-10-22 02:22:09.005031:	Training iteration: 80600, Loss: 0.00429380452260375
2018-10-22 02:23:56.410231:	Training iteration: 80800, Loss: 0.0037243098486214876
2018-10-22 02:25:44.247832:	Training iteration: 81000, Loss: 0.0034300757106393576
2018-10-22 02:27:32.786949:	Training iteration: 81200, Loss: 0.004624588880687952
2018-10-22 02:29:24.020271:	Training iteration: 81400, Loss: 0.005282364785671234
2018-10-22 02:31:15.176748:	Training iteration: 81600, Loss: 0.006241717841476202
2018-10-22 02:33:05.836107:	Training iteration: 81800, Loss: 0.004056263715028763
2018-10-22 02:34:55.751232:	Training iteration: 82000, Loss: 0.005353828426450491
2018-10-22 02:36:44.199677:	Training iteration: 82200, Loss: 0.006928803864866495
2018-10-22 02:38:32.398335:	Training iteration: 82400, Loss: 0.004018602892756462
2018-10-22 02:40:21.429006:	Training iteration: 82600, Loss: 0.005182372871786356
2018-10-22 02:42:08.942714:	Training iteration: 82800, Loss: 0.004154888913035393
2018-10-22 02:43:56.326585:	Training iteration: 83000, Loss: 0.004561448935419321
2018-10-22 02:45:43.458044:	Training iteration: 83200, Loss: 0.0034662168473005295
2018-10-22 02:47:30.046955:	Training iteration: 83400, Loss: 0.004353388678282499
2018-10-22 02:49:19.288756:	Training iteration: 83600, Loss: 0.00579808047041297
2018-10-22 02:51:08.596062:	Training iteration: 83800, Loss: 0.003977720160037279
2018-10-22 02:52:58.645850:	Training iteration: 84000, Loss: 0.0030321229714900255
2018-10-22 02:54:48.200831:	Training iteration: 84200, Loss: 0.004620676394551992
2018-10-22 02:56:38.288233:	Training iteration: 84400, Loss: 0.004294427577406168
2018-10-22 02:58:27.173066:	Training iteration: 84600, Loss: 0.003742433153092861
2018-10-22 03:00:16.450961:	Training iteration: 84800, Loss: 0.004211239982396364
2018-10-22 03:02:07.641250:	Training iteration: 85000, Loss: 0.004115115385502577
2018-10-22 03:03:58.734187:	Training iteration: 85200, Loss: 0.005387384910136461
2018-10-22 03:05:50.304695:	Training iteration: 85400, Loss: 0.005924752447754145
2018-10-22 03:07:41.362909:	Training iteration: 85600, Loss: 0.005862133577466011
2018-10-22 03:09:32.568729:	Training iteration: 85800, Loss: 0.0034369148779660463
2018-10-22 03:11:22.835202:	Training iteration: 86000, Loss: 0.005681672599166632
2018-10-22 03:13:13.791767:	Training iteration: 86200, Loss: 0.003869185922667384
2018-10-22 03:15:01.108773:	Training iteration: 86400, Loss: 0.0047508967109024525
2018-10-22 03:16:49.001285:	Training iteration: 86600, Loss: 0.004217421170324087
2018-10-22 03:18:36.503675:	Training iteration: 86800, Loss: 0.0033454063814133406
2018-10-22 03:20:25.327544:	Training iteration: 87000, Loss: 0.0029771907720714808
2018-10-22 03:22:13.786132:	Training iteration: 87200, Loss: 0.005623176693916321
2018-10-22 03:24:03.496921:	Training iteration: 87400, Loss: 0.006185585167258978
2018-10-22 03:25:53.468788:	Training iteration: 87600, Loss: 0.005335379391908646
2018-10-22 03:27:41.943314:	Training iteration: 87800, Loss: 0.0033585724886506796
2018-10-22 03:29:30.857434:	Training iteration: 88000, Loss: 0.004075656179338694
2018-10-22 03:31:19.551428:	Training iteration: 88200, Loss: 0.003616020083427429
2018-10-22 03:31:46.720739: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 652 of 1000
2018-10-22 03:31:51.519242: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 03:33:16.824945:	Training iteration: 88400, Loss: 0.005502550397068262
2018-10-22 03:35:04.882163:	Training iteration: 88600, Loss: 0.004255661275237799
2018-10-22 03:36:56.029463:	Training iteration: 88800, Loss: 0.0036543256137520075
2018-10-22 03:38:46.312699:	Training iteration: 89000, Loss: 0.004760124254971743
2018-10-22 03:40:36.832683:	Training iteration: 89200, Loss: 0.005277858581393957
2018-10-22 03:42:28.361203:	Training iteration: 89400, Loss: 0.005699096713215113
2018-10-22 03:44:20.194888:	Training iteration: 89600, Loss: 0.004906842950731516
2018-10-22 03:46:08.212503:	Training iteration: 89800, Loss: 0.004471533466130495
2018-10-22 03:47:57.333337:	Training iteration: 90000, Loss: 0.003849926171824336
Checkpoint
2018-10-22 03:49:48.979894:	Training iteration: 90200, Loss: 0.003888505743816495
2018-10-22 03:51:39.316342:	Training iteration: 90400, Loss: 0.00403545331209898
2018-10-22 03:53:27.880031:	Training iteration: 90600, Loss: 0.00447881780564785
2018-10-22 03:55:14.036728:	Training iteration: 90800, Loss: 0.005128644406795502
2018-10-22 03:57:00.629553:	Training iteration: 91000, Loss: 0.004030976444482803
2018-10-22 03:58:47.001148:	Training iteration: 91200, Loss: 0.006510943174362183
2018-10-22 04:00:35.421086:	Training iteration: 91400, Loss: 0.007629450410604477
2018-10-22 04:02:23.802617:	Training iteration: 91600, Loss: 0.0064164274372160435
2018-10-22 04:04:11.378148:	Training iteration: 91800, Loss: 0.004908181726932526
2018-10-22 04:05:59.480917:	Training iteration: 92000, Loss: 0.0035498712677508593
2018-10-22 04:07:47.299674:	Training iteration: 92200, Loss: 0.004521768540143967
2018-10-22 04:09:35.264685:	Training iteration: 92400, Loss: 0.004944149404764175
2018-10-22 04:11:23.775788:	Training iteration: 92600, Loss: 0.0045512765645980835
2018-10-22 04:13:11.910275:	Training iteration: 92800, Loss: 0.004095533397048712
2018-10-22 04:14:59.049840:	Training iteration: 93000, Loss: 0.007226273883134127
2018-10-22 04:16:46.982673:	Training iteration: 93200, Loss: 0.007038322743028402
2018-10-22 04:18:33.153611:	Training iteration: 93400, Loss: 0.0057656108401715755
2018-10-22 04:20:21.034395:	Training iteration: 93600, Loss: 0.003914929926395416
2018-10-22 04:22:08.044229:	Training iteration: 93800, Loss: 0.008004301227629185
2018-10-22 04:23:54.309517:	Training iteration: 94000, Loss: 0.0047346726059913635
2018-10-22 04:25:41.266653:	Training iteration: 94200, Loss: 0.005000115837901831
2018-10-22 04:27:28.900122:	Training iteration: 94400, Loss: 0.004547791555523872
2018-10-22 04:29:17.743914:	Training iteration: 94600, Loss: 0.004911277908831835
2018-10-22 04:31:07.918868:	Training iteration: 94800, Loss: 0.004984027240425348
2018-10-22 04:32:58.241099:	Training iteration: 95000, Loss: 0.005180466920137405
2018-10-22 04:34:47.190115:	Training iteration: 95200, Loss: 0.005267970263957977
2018-10-22 04:36:36.482670:	Training iteration: 95400, Loss: 0.005576420575380325
2018-10-22 04:38:25.061815:	Training iteration: 95600, Loss: 0.004910005256533623
2018-10-22 04:40:11.749924:	Training iteration: 95800, Loss: 0.004368361551314592
2018-10-22 04:42:01.082015:	Training iteration: 96000, Loss: 0.004147535655647516
2018-10-22 04:43:49.277594:	Training iteration: 96200, Loss: 0.00413426011800766
2018-10-22 04:45:37.862545:	Training iteration: 96400, Loss: 0.004802224691957235
2018-10-22 04:47:04.804731: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 680 of 1000
2018-10-22 04:47:08.926942: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 04:47:35.326910:	Training iteration: 96600, Loss: 0.006995069328695536
2018-10-22 04:49:20.044367:	Training iteration: 96800, Loss: 0.0056195869110524654
2018-10-22 04:51:07.640635:	Training iteration: 97000, Loss: 0.0056176818907260895
2018-10-22 04:52:55.440357:	Training iteration: 97200, Loss: 0.0042433603666722775
2018-10-22 04:54:43.884827:	Training iteration: 97400, Loss: 0.0053368923254311085
2018-10-22 04:56:30.273985:	Training iteration: 97600, Loss: 0.005202579777687788
2018-10-22 04:58:15.278425:	Training iteration: 97800, Loss: 0.004559702705591917
2018-10-22 05:00:02.187071:	Training iteration: 98000, Loss: 0.004518516361713409
2018-10-22 05:01:49.268010:	Training iteration: 98200, Loss: 0.005338850896805525
2018-10-22 05:03:36.485493:	Training iteration: 98400, Loss: 0.0030584298074245453
2018-10-22 05:05:23.061430:	Training iteration: 98600, Loss: 0.004868345335125923
2018-10-22 05:07:09.942342:	Training iteration: 98800, Loss: 0.006842608097940683
2018-10-22 05:08:56.794789:	Training iteration: 99000, Loss: 0.004499603062868118
2018-10-22 05:10:45.224351:	Training iteration: 99200, Loss: 0.005513206589967012
2018-10-22 05:12:30.011580:	Training iteration: 99400, Loss: 0.004855745937675238
2018-10-22 05:14:16.752935:	Training iteration: 99600, Loss: 0.006751274690032005
2018-10-22 05:16:02.609448:	Training iteration: 99800, Loss: 0.005532457958906889
2018-10-22 05:17:49.097942:	Training iteration: 100000, Loss: 0.004392996896058321
Checkpoint
2018-10-22 05:19:37.310654:	Training iteration: 100200, Loss: 0.004305216949433088
2018-10-22 05:21:23.329228:	Training iteration: 100400, Loss: 0.005346307065337896
2018-10-22 05:23:10.070817:	Training iteration: 100600, Loss: 0.0037687362637370825
2018-10-22 05:24:58.404757:	Training iteration: 100800, Loss: 0.0042801303789019585
2018-10-22 05:26:44.736983:	Training iteration: 101000, Loss: 0.0036649403627961874
2018-10-22 05:28:31.968634:	Training iteration: 101200, Loss: 0.0037330768536776304
2018-10-22 05:30:18.840428:	Training iteration: 101400, Loss: 0.006149856373667717
2018-10-22 05:32:06.282770:	Training iteration: 101600, Loss: 0.004757680464535952
2018-10-22 05:33:53.370816:	Training iteration: 101800, Loss: 0.006766524165868759
2018-10-22 05:35:40.309326:	Training iteration: 102000, Loss: 0.004174368921667337
2018-10-22 05:37:27.965492:	Training iteration: 102200, Loss: 0.005580151919275522
2018-10-22 05:39:15.594329:	Training iteration: 102400, Loss: 0.005615388974547386
2018-10-22 05:41:02.518200:	Training iteration: 102600, Loss: 0.0071727982722222805
2018-10-22 05:42:49.470147:	Training iteration: 102800, Loss: 0.004713414702564478
2018-10-22 05:44:35.374065:	Training iteration: 103000, Loss: 0.005937869194895029
2018-10-22 05:46:21.859368:	Training iteration: 103200, Loss: 0.004458706360310316
2018-10-22 05:48:10.105254:	Training iteration: 103400, Loss: 0.003939331043511629
2018-10-22 05:49:56.162265:	Training iteration: 103600, Loss: 0.004405206069350243
2018-10-22 05:51:43.095230:	Training iteration: 103800, Loss: 0.004450791049748659
2018-10-22 05:53:28.069646:	Training iteration: 104000, Loss: 0.0049581523053348064
2018-10-22 05:55:15.014543:	Training iteration: 104200, Loss: 0.008623114787042141
2018-10-22 05:57:02.504669:	Training iteration: 104400, Loss: 0.005669110920280218
2018-10-22 05:58:51.370650:	Training iteration: 104600, Loss: 0.00495051359757781
2018-10-22 06:00:40.183196:	Training iteration: 104800, Loss: 0.004266046918928623
2018-10-22 06:01:18.335500: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 659 of 1000
2018-10-22 06:01:22.654435: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 06:02:36.591557:	Training iteration: 105000, Loss: 0.004728769883513451
2018-10-22 06:04:22.078094:	Training iteration: 105200, Loss: 0.003960566129535437
2018-10-22 06:06:08.680611:	Training iteration: 105400, Loss: 0.003891815198585391
2018-10-22 06:07:57.505044:	Training iteration: 105600, Loss: 0.0024866655003279448
2018-10-22 06:09:46.203740:	Training iteration: 105800, Loss: 0.0038869641721248627
2018-10-22 06:11:35.192862:	Training iteration: 106000, Loss: 0.0035817541647702456
2018-10-22 06:13:24.782247:	Training iteration: 106200, Loss: 0.004953681956976652
2018-10-22 06:15:10.978405:	Training iteration: 106400, Loss: 0.004675381351262331
2018-10-22 06:16:58.427879:	Training iteration: 106600, Loss: 0.0033278700429946184
2018-10-22 06:18:45.555943:	Training iteration: 106800, Loss: 0.003496613586321473
2018-10-22 06:20:31.503856:	Training iteration: 107000, Loss: 0.0038886733818799257
2018-10-22 06:22:16.776402:	Training iteration: 107200, Loss: 0.006040047388523817
2018-10-22 06:24:02.811740:	Training iteration: 107400, Loss: 0.0030365868005901575
2018-10-22 06:25:49.708232:	Training iteration: 107600, Loss: 0.004690275061875582
2018-10-22 06:27:34.712359:	Training iteration: 107800, Loss: 0.003464776324108243
2018-10-22 06:29:19.386066:	Training iteration: 108000, Loss: 0.0038504283875226974
2018-10-22 06:31:03.697941:	Training iteration: 108200, Loss: 0.004785103723406792
2018-10-22 06:32:48.898180:	Training iteration: 108400, Loss: 0.004935434553772211
2018-10-22 06:34:33.615268:	Training iteration: 108600, Loss: 0.003959326539188623
2018-10-22 06:36:19.094741:	Training iteration: 108800, Loss: 0.006327961105853319
2018-10-22 06:38:04.478956:	Training iteration: 109000, Loss: 0.0054171145893633366
2018-10-22 06:39:49.785569:	Training iteration: 109200, Loss: 0.0033446263987571
2018-10-22 06:41:34.763011:	Training iteration: 109400, Loss: 0.005261907819658518
2018-10-22 06:43:19.391760:	Training iteration: 109600, Loss: 0.0048383441753685474
2018-10-22 06:45:03.847028:	Training iteration: 109800, Loss: 0.0034317236859351397
2018-10-22 06:46:48.040355:	Training iteration: 110000, Loss: 0.0038076138589531183
Checkpoint
2018-10-22 06:48:32.982805:	Training iteration: 110200, Loss: 0.005393424537032843
2018-10-22 06:50:17.849151:	Training iteration: 110400, Loss: 0.003392326645553112
2018-10-22 06:52:03.701790:	Training iteration: 110600, Loss: 0.00459284009411931
2018-10-22 06:53:49.088613:	Training iteration: 110800, Loss: 0.002900158753618598
2018-10-22 06:55:33.920595:	Training iteration: 111000, Loss: 0.005844354163855314
2018-10-22 06:57:18.651650:	Training iteration: 111200, Loss: 0.003356743371114135
2018-10-22 06:59:04.247483:	Training iteration: 111400, Loss: 0.004678503144532442
2018-10-22 07:00:51.038223:	Training iteration: 111600, Loss: 0.004345212131738663
2018-10-22 07:02:37.969304:	Training iteration: 111800, Loss: 0.004115859512239695
2018-10-22 07:04:23.814878:	Training iteration: 112000, Loss: 0.004918075632303953
2018-10-22 07:06:09.227592:	Training iteration: 112200, Loss: 0.004537621047347784
2018-10-22 07:07:54.554335:	Training iteration: 112400, Loss: 0.006059771403670311
2018-10-22 07:09:41.258814:	Training iteration: 112600, Loss: 0.0033611140679568052
2018-10-22 07:11:24.640068:	Training iteration: 112800, Loss: 0.0038506004493683577
2018-10-22 07:13:09.922387:	Training iteration: 113000, Loss: 0.003550683381035924
2018-10-22 07:14:53.996140:	Training iteration: 113200, Loss: 0.0048821368254721165
2018-10-22 07:16:37.033294:	Training iteration: 113400, Loss: 0.004032615572214127
2018-10-22 07:17:54.832818: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 664 of 1000
2018-10-22 07:17:59.040776: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 07:18:31.717076:	Training iteration: 113600, Loss: 0.006049970630556345
2018-10-22 07:20:14.526329:	Training iteration: 113800, Loss: 0.003951527178287506
2018-10-22 07:21:57.681963:	Training iteration: 114000, Loss: 0.004927827510982752
2018-10-22 07:23:42.405108:	Training iteration: 114200, Loss: 0.0056079961359500885
2018-10-22 07:25:23.233499:	Training iteration: 114400, Loss: 0.005927730817347765
2018-10-22 07:27:05.949353:	Training iteration: 114600, Loss: 0.0031341174617409706
2018-10-22 07:28:49.590132:	Training iteration: 114800, Loss: 0.003887988394126296
2018-10-22 07:30:33.569678:	Training iteration: 115000, Loss: 0.005063104908913374
2018-10-22 07:32:20.421135:	Training iteration: 115200, Loss: 0.00401883153244853
2018-10-22 07:34:05.812124:	Training iteration: 115400, Loss: 0.005655964370816946
2018-10-22 07:35:51.981918:	Training iteration: 115600, Loss: 0.008883458562195301
2018-10-22 07:37:37.740489:	Training iteration: 115800, Loss: 0.008090081624686718
2018-10-22 07:39:22.703326:	Training iteration: 116000, Loss: 0.007497405167669058
2018-10-22 07:41:07.447868:	Training iteration: 116200, Loss: 0.004937028978019953
2018-10-22 07:42:52.659372:	Training iteration: 116400, Loss: 0.005406104493886232
2018-10-22 07:44:36.678629:	Training iteration: 116600, Loss: 0.005402179900556803
2018-10-22 07:46:21.723529:	Training iteration: 116800, Loss: 0.004320499487221241
2018-10-22 07:48:06.186710:	Training iteration: 117000, Loss: 0.005083128344267607
2018-10-22 07:49:50.052087:	Training iteration: 117200, Loss: 0.008899698965251446
2018-10-22 07:51:34.115267:	Training iteration: 117400, Loss: 0.00578699866309762
2018-10-22 07:53:17.940411:	Training iteration: 117600, Loss: 0.004489387851208448
2018-10-22 07:55:01.299308:	Training iteration: 117800, Loss: 0.005913118366152048
2018-10-22 07:56:42.847415:	Training iteration: 118000, Loss: 0.00586478179320693
2018-10-22 07:58:26.117020:	Training iteration: 118200, Loss: 0.005648352205753326
2018-10-22 08:00:09.037429:	Training iteration: 118400, Loss: 0.005824262276291847
2018-10-22 08:01:51.346127:	Training iteration: 118600, Loss: 0.004400497768074274
2018-10-22 08:03:33.049962:	Training iteration: 118800, Loss: 0.0047450014390051365
2018-10-22 08:05:17.568321:	Training iteration: 119000, Loss: 0.004945374559611082
2018-10-22 08:07:00.963525:	Training iteration: 119200, Loss: 0.0035586438607424498
2018-10-22 08:08:46.301070:	Training iteration: 119400, Loss: 0.00418520625680685
2018-10-22 08:10:30.589944:	Training iteration: 119600, Loss: 0.004280091729015112
2018-10-22 08:12:14.883984:	Training iteration: 119800, Loss: 0.0050271302461624146
2018-10-22 08:13:58.901885:	Training iteration: 120000, Loss: 0.00617217319086194
Checkpoint
2018-10-22 08:15:45.035455:	Training iteration: 120200, Loss: 0.004805454518646002
2018-10-22 08:17:29.081336:	Training iteration: 120400, Loss: 0.003651060163974762
2018-10-22 08:19:12.964762:	Training iteration: 120600, Loss: 0.004905844572931528
2018-10-22 08:20:55.443064:	Training iteration: 120800, Loss: 0.006375855300575495
2018-10-22 08:22:40.026772:	Training iteration: 121000, Loss: 0.004266895819455385
2018-10-22 08:24:23.938816:	Training iteration: 121200, Loss: 0.004772070795297623
2018-10-22 08:26:07.670651:	Training iteration: 121400, Loss: 0.004916600417345762
2018-10-22 08:27:50.304119:	Training iteration: 121600, Loss: 0.006192445755004883
2018-10-22 08:29:32.392145:	Training iteration: 121800, Loss: 0.006274659186601639
2018-10-22 08:31:16.431321:	Training iteration: 122000, Loss: 0.004559928551316261
2018-10-22 08:33:01.042183:	Training iteration: 122200, Loss: 0.00903030950576067
2018-10-22 08:34:45.591228:	Training iteration: 122400, Loss: 0.005325356964021921
2018-10-22 08:36:31.282309:	Training iteration: 122600, Loss: 0.005823708605021238
2018-10-22 08:38:18.033228:	Training iteration: 122800, Loss: 0.006784610450267792
2018-10-22 08:40:03.979214:	Training iteration: 123000, Loss: 0.009988218545913696
2018-10-22 08:41:51.419843:	Training iteration: 123200, Loss: 0.005057962145656347
2018-10-22 08:43:37.424033:	Training iteration: 123400, Loss: 0.003773221978917718
2018-10-22 08:45:23.249969:	Training iteration: 123600, Loss: 0.005259152036160231
2018-10-22 08:47:08.011069:	Training iteration: 123800, Loss: 0.003366057528182864
2018-10-22 08:48:52.664150:	Training iteration: 124000, Loss: 0.00500241806730628
2018-10-22 08:50:38.011014:	Training iteration: 124200, Loss: 0.006040697451680899
2018-10-22 08:52:21.427345:	Training iteration: 124400, Loss: 0.008804370649158955
2018-10-22 08:54:06.521430:	Training iteration: 124600, Loss: 0.004736247938126326
2018-10-22 08:55:50.684707:	Training iteration: 124800, Loss: 0.007046742830425501
2018-10-22 08:57:33.251256:	Training iteration: 125000, Loss: 0.007278007920831442
2018-10-22 08:59:17.306997:	Training iteration: 125200, Loss: 0.005028955172747374
2018-10-22 09:01:01.066479:	Training iteration: 125400, Loss: 0.006963036954402924
2018-10-22 09:02:46.215975:	Training iteration: 125600, Loss: 0.008279585279524326
2018-10-22 09:04:30.088017:	Training iteration: 125800, Loss: 0.004262290429323912
2018-10-22 09:06:13.704927:	Training iteration: 126000, Loss: 0.0066383820958435535
2018-10-22 09:07:57.108689:	Training iteration: 126200, Loss: 0.003969002980738878
2018-10-22 09:09:42.241468:	Training iteration: 126400, Loss: 0.006385814864188433
2018-10-22 09:11:24.797902:	Training iteration: 126600, Loss: 0.005933528300374746
2018-10-22 09:13:08.611825:	Training iteration: 126800, Loss: 0.0063968501053750515
2018-10-22 09:14:53.475673:	Training iteration: 127000, Loss: 0.0044780634343624115
2018-10-22 09:16:37.743233:	Training iteration: 127200, Loss: 0.006385338958352804
2018-10-22 09:18:22.260263:	Training iteration: 127400, Loss: 0.005762232001870871
2018-10-22 09:20:06.038617:	Training iteration: 127600, Loss: 0.0032692484091967344
2018-10-22 09:21:50.148029:	Training iteration: 127800, Loss: 0.004718191921710968
2018-10-22 09:23:34.781737:	Training iteration: 128000, Loss: 0.0066087860614061356
2018-10-22 09:25:18.088840:	Training iteration: 128200, Loss: 0.006654039025306702
2018-10-22 09:27:01.099401:	Training iteration: 128400, Loss: 0.005403528455644846
2018-10-22 09:28:44.876369:	Training iteration: 128600, Loss: 0.00831462349742651
2018-10-22 09:30:27.406490:	Training iteration: 128800, Loss: 0.005537562072277069
2018-10-22 09:32:10.067366:	Training iteration: 129000, Loss: 0.006795156747102737
2018-10-22 09:33:53.052893:	Training iteration: 129200, Loss: 0.00880294106900692
2018-10-22 09:35:36.871021:	Training iteration: 129400, Loss: 0.005420377943664789
2018-10-22 09:37:20.379565:	Training iteration: 129600, Loss: 0.006630655378103256
2018-10-22 09:39:03.158124:	Training iteration: 129800, Loss: 0.006929128896445036
2018-10-22 09:40:48.097909:	Training iteration: 130000, Loss: 0.007750466000288725
Checkpoint
2018-10-22 09:42:32.266035:	Training iteration: 130200, Loss: 0.0052608405239880085
2018-10-22 09:44:16.541003:	Training iteration: 130400, Loss: 0.0048646326176822186
2018-10-22 09:46:01.696452:	Training iteration: 130600, Loss: 0.006494118366390467
2018-10-22 09:47:46.985180:	Training iteration: 130800, Loss: 0.005817183759063482
2018-10-22 09:49:31.219008:	Training iteration: 131000, Loss: 0.008822176605463028
2018-10-22 09:51:15.849974:	Training iteration: 131200, Loss: 0.006352003663778305
2018-10-22 09:52:59.492549:	Training iteration: 131400, Loss: 0.0051276725716888905
2018-10-22 09:54:43.112534:	Training iteration: 131600, Loss: 0.004994450602680445
2018-10-22 09:56:28.271450:	Training iteration: 131800, Loss: 0.005926227662712336
2018-10-22 09:58:12.752824:	Training iteration: 132000, Loss: 0.008313721977174282
2018-10-22 09:59:56.173790:	Training iteration: 132200, Loss: 0.0053155794739723206
2018-10-22 10:01:41.709244:	Training iteration: 132400, Loss: 0.0053652264177799225
2018-10-22 10:03:26.341756:	Training iteration: 132600, Loss: 0.005383639130741358
2018-10-22 10:05:10.342447:	Training iteration: 132800, Loss: 0.004017689730972052
2018-10-22 10:06:55.404878:	Training iteration: 133000, Loss: 0.006021826062351465
2018-10-22 10:08:39.012244:	Training iteration: 133200, Loss: 0.004002690780907869
2018-10-22 10:10:22.839038:	Training iteration: 133400, Loss: 0.005556137766689062
2018-10-22 10:12:08.215119:	Training iteration: 133600, Loss: 0.007086575031280518
2018-10-22 10:13:51.065595:	Training iteration: 133800, Loss: 0.007682472001761198
2018-10-22 10:15:36.215171:	Training iteration: 134000, Loss: 0.005815097596496344
2018-10-22 10:17:20.489190:	Training iteration: 134200, Loss: 0.004478906746953726
2018-10-22 10:19:04.417802:	Training iteration: 134400, Loss: 0.005382988601922989
2018-10-22 10:20:46.876588:	Training iteration: 134600, Loss: 0.003318268805742264
2018-10-22 10:22:31.223436:	Training iteration: 134800, Loss: 0.004567671567201614
2018-10-22 10:24:14.781045:	Training iteration: 135000, Loss: 0.004162488039582968
2018-10-22 10:25:59.100861:	Training iteration: 135200, Loss: 0.004290225449949503
2018-10-22 10:27:41.929774:	Training iteration: 135400, Loss: 0.006878294050693512
2018-10-22 10:29:25.197817:	Training iteration: 135600, Loss: 0.006677133496850729
2018-10-22 10:31:08.706304:	Training iteration: 135800, Loss: 0.0056318421848118305
2018-10-22 10:32:51.748303:	Training iteration: 136000, Loss: 0.005651236977428198
2018-10-22 10:34:34.568389:	Training iteration: 136200, Loss: 0.004689743276685476
2018-10-22 10:36:17.567973:	Training iteration: 136400, Loss: 0.00823794025927782
2018-10-22 10:37:59.810430:	Training iteration: 136600, Loss: 0.008672752417623997
2018-10-22 10:39:42.919019:	Training iteration: 136800, Loss: 0.006982378661632538
2018-10-22 10:41:26.436709:	Training iteration: 137000, Loss: 0.004518297966569662
2018-10-22 10:43:09.363500:	Training iteration: 137200, Loss: 0.00631724065169692
2018-10-22 10:44:52.949198:	Training iteration: 137400, Loss: 0.005579243879765272
2018-10-22 10:46:36.586536:	Training iteration: 137600, Loss: 0.005427466705441475
2018-10-22 10:48:20.617104:	Training iteration: 137800, Loss: 0.0073791202157735825
2018-10-22 10:50:04.080408:	Training iteration: 138000, Loss: 0.004998130723834038
2018-10-22 10:51:48.025852:	Training iteration: 138200, Loss: 0.004579869564622641
2018-10-22 10:53:29.750587:	Training iteration: 138400, Loss: 0.004151348490267992
2018-10-22 10:55:13.886163:	Training iteration: 138600, Loss: 0.0031683091074228287
2018-10-22 10:56:56.589208:	Training iteration: 138800, Loss: 0.0050485869869589806
2018-10-22 10:58:39.645276:	Training iteration: 139000, Loss: 0.0057209753431379795
2018-10-22 11:00:22.634611:	Training iteration: 139200, Loss: 0.004467849154025316
2018-10-22 11:02:05.392602:	Training iteration: 139400, Loss: 0.005883061792701483
2018-10-22 11:03:47.323910:	Training iteration: 139600, Loss: 0.005334984511137009
2018-10-22 11:05:30.359967:	Training iteration: 139800, Loss: 0.004753293469548225
2018-10-22 11:07:12.012088:	Training iteration: 140000, Loss: 0.0036317892372608185
Checkpoint
2018-10-22 11:08:56.897812:	Training iteration: 140200, Loss: 0.004155741073191166
2018-10-22 11:10:39.917419:	Training iteration: 140400, Loss: 0.007689972873777151
2018-10-22 11:12:24.091164:	Training iteration: 140600, Loss: 0.0038016820326447487
2018-10-22 11:14:06.271949:	Training iteration: 140800, Loss: 0.003479684004560113
2018-10-22 11:15:49.360277:	Training iteration: 141000, Loss: 0.004479629453271627
2018-10-22 11:17:32.756811:	Training iteration: 141200, Loss: 0.004708310589194298
2018-10-22 11:19:16.586465:	Training iteration: 141400, Loss: 0.004054963123053312
2018-10-22 11:21:00.452608:	Training iteration: 141600, Loss: 0.008609510958194733
2018-10-22 11:22:43.144759:	Training iteration: 141800, Loss: 0.005221670959144831
2018-10-22 11:24:26.189402:	Training iteration: 142000, Loss: 0.003971954341977835
2018-10-22 11:26:09.632688:	Training iteration: 142200, Loss: 0.006251148413866758
2018-10-22 11:27:52.494354:	Training iteration: 142400, Loss: 0.007085777353495359
2018-10-22 11:29:35.828655:	Training iteration: 142600, Loss: 0.010655130259692669
2018-10-22 11:31:20.247532:	Training iteration: 142800, Loss: 0.006905729416757822
2018-10-22 11:33:04.561252:	Training iteration: 143000, Loss: 0.00589715363457799
2018-10-22 11:34:48.333982:	Training iteration: 143200, Loss: 0.00924630556255579
2018-10-22 11:36:33.622514:	Training iteration: 143400, Loss: 0.007214836776256561
2018-10-22 11:38:16.695110:	Training iteration: 143600, Loss: 0.0044238450936973095
2018-10-22 11:40:00.274382:	Training iteration: 143800, Loss: 0.007095595821738243
2018-10-22 11:41:43.611145:	Training iteration: 144000, Loss: 0.008293875493109226
2018-10-22 11:43:27.950953:	Training iteration: 144200, Loss: 0.009901865385472775
2018-10-22 11:45:13.101115:	Training iteration: 144400, Loss: 0.00608416460454464
2018-10-22 11:46:56.532728:	Training iteration: 144600, Loss: 0.005537820979952812
2018-10-22 11:48:41.293090:	Training iteration: 144800, Loss: 0.003950537648051977
2018-10-22 11:50:24.719682:	Training iteration: 145000, Loss: 0.005033465567976236
2018-10-22 11:52:08.378924:	Training iteration: 145200, Loss: 0.004737401846796274
2018-10-22 11:53:52.217545:	Training iteration: 145400, Loss: 0.003208161098882556
2018-10-22 11:55:36.155397:	Training iteration: 145600, Loss: 0.0037093565333634615
2018-10-22 11:57:20.133070:	Training iteration: 145800, Loss: 0.005290175322443247
2018-10-22 11:59:03.229083:	Training iteration: 146000, Loss: 0.005679893773049116
2018-10-22 12:00:47.722388:	Training iteration: 146200, Loss: 0.006183698773384094
2018-10-22 12:02:36.279117:	Training iteration: 146400, Loss: 0.004964565858244896
2018-10-22 12:04:21.102587:	Training iteration: 146600, Loss: 0.0052217659540474415
2018-10-22 12:06:04.543475:	Training iteration: 146800, Loss: 0.0062857321463525295
2018-10-22 12:07:46.622220:	Training iteration: 147000, Loss: 0.004865508992224932
2018-10-22 12:09:31.307956:	Training iteration: 147200, Loss: 0.005135819781571627
2018-10-22 12:11:14.135557:	Training iteration: 147400, Loss: 0.004055052530020475
2018-10-22 12:12:56.828206:	Training iteration: 147600, Loss: 0.008193482644855976
2018-10-22 12:14:39.629371:	Training iteration: 147800, Loss: 0.006268370896577835
2018-10-22 12:16:21.973986:	Training iteration: 148000, Loss: 0.005877370480448008
2018-10-22 12:18:05.435535:	Training iteration: 148200, Loss: 0.0034111745189875364
2018-10-22 12:19:46.786224:	Training iteration: 148400, Loss: 0.007131345570087433
2018-10-22 12:21:29.667779:	Training iteration: 148600, Loss: 0.004902899265289307
2018-10-22 12:23:13.031244:	Training iteration: 148800, Loss: 0.007940378971397877
2018-10-22 12:24:56.176926:	Training iteration: 149000, Loss: 0.005221080034971237
2018-10-22 12:26:38.552146:	Training iteration: 149200, Loss: 0.005780769977718592
2018-10-22 12:28:19.701719:	Training iteration: 149400, Loss: 0.006527113262563944
2018-10-22 12:30:02.187155:	Training iteration: 149600, Loss: 0.00617734482511878
2018-10-22 12:31:45.436119:	Training iteration: 149800, Loss: 0.004344369750469923
2018-10-22 12:33:27.758214:	Training iteration: 150000, Loss: 0.004308001603931189
Checkpoint
2018-10-22 12:35:11.739045:	Training iteration: 150200, Loss: 0.003861998440697789
2018-10-22 12:37:11.132831:	Training iteration: 150400, Loss: 0.004731586202979088
2018-10-22 12:38:46.820920:	Training iteration: 150600, Loss: 0.005563138518482447
2018-10-22 12:40:27.217634:	Training iteration: 150800, Loss: 0.004688514396548271
2018-10-22 12:42:08.246002:	Training iteration: 151000, Loss: 0.009119492955505848
2018-10-22 12:43:50.633284:	Training iteration: 151200, Loss: 0.0043808273039758205
2018-10-22 12:45:34.136078:	Training iteration: 151400, Loss: 0.006336860358715057
2018-10-22 12:47:16.585678:	Training iteration: 151600, Loss: 0.008823097683489323
2018-10-22 12:49:00.024560:	Training iteration: 151800, Loss: 0.005570759531110525
2018-10-22 12:50:45.996371:	Training iteration: 152000, Loss: 0.006029097363352776
2018-10-22 12:52:29.032241:	Training iteration: 152200, Loss: 0.004009409807622433
2018-10-22 12:54:12.631895:	Training iteration: 152400, Loss: 0.004380297381430864
2018-10-22 12:55:57.281494:	Training iteration: 152600, Loss: 0.0046631949953734875
2018-10-22 12:57:40.575243:	Training iteration: 152800, Loss: 0.00571015989407897
2018-10-22 12:59:25.038107:	Training iteration: 153000, Loss: 0.002624066546559334
2018-10-22 13:01:08.060062:	Training iteration: 153200, Loss: 0.007178951054811478
2018-10-22 13:02:50.961156:	Training iteration: 153400, Loss: 0.004721019417047501
2018-10-22 13:04:36.242920:	Training iteration: 153600, Loss: 0.004889692645519972
2018-10-22 13:06:18.769936:	Training iteration: 153800, Loss: 0.006830174475908279
2018-10-22 13:08:01.923568:	Training iteration: 154000, Loss: 0.005730328615754843
2018-10-22 13:09:44.666490:	Training iteration: 154200, Loss: 0.0046921526081860065
2018-10-22 13:11:27.312358:	Training iteration: 154400, Loss: 0.004369496833533049
2018-10-22 13:13:12.052204:	Training iteration: 154600, Loss: 0.005547253414988518
2018-10-22 13:14:56.320243:	Training iteration: 154800, Loss: 0.005661800038069487
2018-10-22 13:16:40.517164:	Training iteration: 155000, Loss: 0.005307493265718222
2018-10-22 13:18:24.752990:	Training iteration: 155200, Loss: 0.005426220595836639
2018-10-22 13:20:08.411679:	Training iteration: 155400, Loss: 0.008104448206722736
2018-10-22 13:21:53.574331:	Training iteration: 155600, Loss: 0.00744851678609848
2018-10-22 13:23:36.737423:	Training iteration: 155800, Loss: 0.005309304688125849
2018-10-22 13:25:19.938668:	Training iteration: 156000, Loss: 0.0059441071934998035
2018-10-22 13:27:04.803828:	Training iteration: 156200, Loss: 0.005153157282620668
2018-10-22 13:28:49.862420:	Training iteration: 156400, Loss: 0.00713927811011672
2018-10-22 13:30:34.568283:	Training iteration: 156600, Loss: 0.005019382108002901
2018-10-22 13:32:20.603927:	Training iteration: 156800, Loss: 0.004014423117041588
2018-10-22 13:34:04.414510:	Training iteration: 157000, Loss: 0.004024841357022524
2018-10-22 13:35:49.622181:	Training iteration: 157200, Loss: 0.004497996065765619
2018-10-22 13:37:34.095119:	Training iteration: 157400, Loss: 0.0063239592127501965
2018-10-22 13:39:15.816964:	Training iteration: 157600, Loss: 0.005705614108592272
2018-10-22 13:40:59.263772:	Training iteration: 157800, Loss: 0.006166770588606596
2018-10-22 13:42:43.100969:	Training iteration: 158000, Loss: 0.0033097295090556145
2018-10-22 13:44:26.516031:	Training iteration: 158200, Loss: 0.006149932276457548
2018-10-22 13:46:10.051551:	Training iteration: 158400, Loss: 0.004841126501560211
2018-10-22 13:47:53.339211:	Training iteration: 158600, Loss: 0.006565111223608255
2018-10-22 13:49:36.733980:	Training iteration: 158800, Loss: 0.005997361149638891
2018-10-22 13:51:18.641012:	Training iteration: 159000, Loss: 0.003821229562163353
2018-10-22 13:53:02.454192:	Training iteration: 159200, Loss: 0.004304928705096245
2018-10-22 13:54:45.109341:	Training iteration: 159400, Loss: 0.005719762295484543
2018-10-22 13:56:28.298919:	Training iteration: 159600, Loss: 0.004351674113422632
2018-10-22 13:58:12.340183:	Training iteration: 159800, Loss: 0.006702200975269079
2018-10-22 13:59:21.127486:	Epoch 1 finished after 159937 iterations.
Validating
2018-10-22 13:59:21.309205:	Entering validation loop
2018-10-22 13:59:31.343088: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 604 of 1000
2018-10-22 13:59:37.363961: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:00:16.236390:	Validation iteration: 200, Loss: 0.00789600145071745
2018-10-22 14:00:58.656180:	Validation iteration: 400, Loss: 0.006316151935607195
2018-10-22 14:01:42.325768:	Validation iteration: 600, Loss: 0.005670974496752024
2018-10-22 14:02:25.692602:	Validation iteration: 800, Loss: 0.005041914060711861
2018-10-22 14:03:11.601406:	Validation iteration: 1000, Loss: 0.008029120974242687
2018-10-22 14:03:54.865407:	Validation iteration: 1200, Loss: 0.006902220193296671
2018-10-22 14:04:46.355181: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 631 of 1000
2018-10-22 14:04:51.989474: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:04:53.611703:	Validation iteration: 1400, Loss: 0.006992841139435768
2018-10-22 14:05:32.395296:	Validation iteration: 1600, Loss: 0.007428698241710663
2018-10-22 14:06:14.612632:	Validation iteration: 1800, Loss: 0.007410941179841757
2018-10-22 14:06:57.398967:	Validation iteration: 2000, Loss: 0.007254500407725573
2018-10-22 14:07:41.464217:	Validation iteration: 2200, Loss: 0.005472511053085327
2018-10-22 14:08:25.357269:	Validation iteration: 2400, Loss: 0.006418462842702866
2018-10-22 14:09:08.361255:	Validation iteration: 2600, Loss: 0.006230778526514769
2018-10-22 14:09:58.076074: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 596 of 1000
2018-10-22 14:10:04.023486: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:10:07.155290:	Validation iteration: 2800, Loss: 0.006298007443547249
2018-10-22 14:10:46.127885:	Validation iteration: 3000, Loss: 0.007080326322466135
2018-10-22 14:11:28.704281:	Validation iteration: 3200, Loss: 0.006176235619932413
2018-10-22 14:12:12.251317:	Validation iteration: 3400, Loss: 0.007394677493721247
2018-10-22 14:12:55.832781:	Validation iteration: 3600, Loss: 0.005533245857805014
2018-10-22 14:13:39.555109:	Validation iteration: 3800, Loss: 0.0044950442388653755
2018-10-22 14:14:23.253040:	Validation iteration: 4000, Loss: 0.006061470601707697
2018-10-22 14:15:12.015329: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 603 of 1000
2018-10-22 14:15:17.657056: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:15:22.396618:	Validation iteration: 4200, Loss: 0.007193078752607107
2018-10-22 14:16:01.674773:	Validation iteration: 4400, Loss: 0.006727028638124466
2018-10-22 14:16:45.242986:	Validation iteration: 4600, Loss: 0.004572769161313772
2018-10-22 14:17:29.007164:	Validation iteration: 4800, Loss: 0.0076669566333293915
2018-10-22 14:18:12.980207:	Validation iteration: 5000, Loss: 0.005871823523193598
2018-10-22 14:18:57.022178:	Validation iteration: 5200, Loss: 0.008370761759579182
2018-10-22 14:19:41.964789:	Validation iteration: 5400, Loss: 0.007698278408497572
2018-10-22 14:20:28.352348: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 561 of 1000
2018-10-22 14:20:35.008847: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:20:41.118612:	Validation iteration: 5600, Loss: 0.007864672690629959
2018-10-22 14:21:20.772900:	Validation iteration: 5800, Loss: 0.006457535084336996
2018-10-22 14:22:03.621203:	Validation iteration: 6000, Loss: 0.005662186071276665
2018-10-22 14:22:46.670012:	Validation iteration: 6200, Loss: 0.0033240194898098707
2018-10-22 14:23:29.031627:	Validation iteration: 6400, Loss: 0.00432561943307519
2018-10-22 14:24:13.206153:	Validation iteration: 6600, Loss: 0.004781050607562065
2018-10-22 14:24:56.710703:	Validation iteration: 6800, Loss: 0.005963955540210009
2018-10-22 14:25:41.493406:	Validation iteration: 7000, Loss: 0.004778286907821894
2018-10-22 14:26:24.960231:	Validation iteration: 7200, Loss: 0.004841184243559837
2018-10-22 14:27:08.871169:	Validation iteration: 7400, Loss: 0.003809058340266347
Validation check mean loss: 0.005959838641531444
Validation loss has improved!
New best validation cost!
2018-10-22 14:27:40.540500: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 679 of 1000
2018-10-22 14:27:44.744681: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:28:13.749042:	Training iteration: 160000, Loss: 0.004967920016497374
Checkpoint
2018-10-22 14:29:57.098449:	Training iteration: 160200, Loss: 0.0038224151358008385
2018-10-22 14:31:41.003385:	Training iteration: 160400, Loss: 0.0036995727568864822
2018-10-22 14:33:26.189107:	Training iteration: 160600, Loss: 0.005197481717914343
2018-10-22 14:35:09.634974:	Training iteration: 160800, Loss: 0.0038199082482606173
2018-10-22 14:36:53.977346:	Training iteration: 161000, Loss: 0.005632750224322081
2018-10-22 14:38:37.721572:	Training iteration: 161200, Loss: 0.004574358928948641
2018-10-22 14:40:22.820207:	Training iteration: 161400, Loss: 0.0039780656807124615
2018-10-22 14:42:05.012691:	Training iteration: 161600, Loss: 0.005054083187133074
2018-10-22 14:43:48.926913:	Training iteration: 161800, Loss: 0.004156568553298712
2018-10-22 14:45:33.455706:	Training iteration: 162000, Loss: 0.00452291639521718
2018-10-22 14:47:18.322761:	Training iteration: 162200, Loss: 0.005430700723081827
2018-10-22 14:49:02.600503:	Training iteration: 162400, Loss: 0.005176343489438295
2018-10-22 14:50:46.639333:	Training iteration: 162600, Loss: 0.00515827676281333
2018-10-22 14:52:31.939971:	Training iteration: 162800, Loss: 0.002843550406396389
2018-10-22 14:54:16.219253:	Training iteration: 163000, Loss: 0.0038279565051198006
2018-10-22 14:56:01.592540:	Training iteration: 163200, Loss: 0.005409928038716316
2018-10-22 14:57:44.405819:	Training iteration: 163400, Loss: 0.005109114572405815
2018-10-22 14:59:29.745958:	Training iteration: 163600, Loss: 0.0035168828908354044
2018-10-22 15:01:15.529889:	Training iteration: 163800, Loss: 0.003992476966232061
2018-10-22 15:03:00.553034:	Training iteration: 164000, Loss: 0.0033974095713347197
2018-10-22 15:04:44.301910:	Training iteration: 164200, Loss: 0.0038649237249046564
2018-10-22 15:06:28.850317:	Training iteration: 164400, Loss: 0.0061081149615347385
2018-10-22 15:08:13.616490:	Training iteration: 164600, Loss: 0.003027735510841012
2018-10-22 15:09:56.362299:	Training iteration: 164800, Loss: 0.002359120175242424
2018-10-22 15:11:39.495516:	Training iteration: 165000, Loss: 0.004739950876682997
2018-10-22 15:13:22.001303:	Training iteration: 165200, Loss: 0.006050227675586939
2018-10-22 15:15:04.872391:	Training iteration: 165400, Loss: 0.006415870040655136
2018-10-22 15:16:48.064793:	Training iteration: 165600, Loss: 0.0038327977526932955
2018-10-22 15:18:30.997602:	Training iteration: 165800, Loss: 0.005216527730226517
2018-10-22 15:20:13.073305:	Training iteration: 166000, Loss: 0.0038992695044726133
2018-10-22 15:21:56.196761:	Training iteration: 166200, Loss: 0.0043610683642327785
2018-10-22 15:23:39.512757:	Training iteration: 166400, Loss: 0.005529490765184164
2018-10-22 15:25:22.415161:	Training iteration: 166600, Loss: 0.0027498577255755663
2018-10-22 15:27:06.255769:	Training iteration: 166800, Loss: 0.004772060550749302
2018-10-22 15:28:49.348003:	Training iteration: 167000, Loss: 0.00567970983684063
2018-10-22 15:30:33.106213:	Training iteration: 167200, Loss: 0.00474562868475914
2018-10-22 15:32:15.800769:	Training iteration: 167400, Loss: 0.006245551165193319
2018-10-22 15:33:58.159095:	Training iteration: 167600, Loss: 0.004054997116327286
2018-10-22 15:35:43.579085:	Training iteration: 167800, Loss: 0.004686631262302399
2018-10-22 15:37:29.508469:	Training iteration: 168000, Loss: 0.004886115901172161
2018-10-22 15:39:15.284472:	Training iteration: 168200, Loss: 0.004758866038173437
2018-10-22 15:39:25.344074: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 656 of 1000
2018-10-22 15:39:29.983026: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 15:41:09.346561:	Training iteration: 168400, Loss: 0.006943332497030497
2018-10-22 15:42:54.431550:	Training iteration: 168600, Loss: 0.006024142261594534
2018-10-22 15:44:39.963061:	Training iteration: 168800, Loss: 0.005139817018061876
2018-10-22 15:46:24.074273:	Training iteration: 169000, Loss: 0.004888537805527449
2018-10-22 15:48:08.267445:	Training iteration: 169200, Loss: 0.004275248385965824
2018-10-22 15:49:52.242453:	Training iteration: 169400, Loss: 0.0037531505804508924
2018-10-22 15:51:40.713918:	Training iteration: 169600, Loss: 0.004363650921732187
2018-10-22 15:53:28.283925:	Training iteration: 169800, Loss: 0.005162922665476799
2018-10-22 15:55:15.412924:	Training iteration: 170000, Loss: 0.00597212603315711
Checkpoint
2018-10-22 15:57:02.190704:	Training iteration: 170200, Loss: 0.00684186490252614
2018-10-22 15:58:47.815916:	Training iteration: 170400, Loss: 0.00415662070736289
2018-10-22 16:00:33.100854:	Training iteration: 170600, Loss: 0.0058450959622859955
2018-10-22 16:02:17.960439:	Training iteration: 170800, Loss: 0.004312989767640829
2018-10-22 16:04:02.405404:	Training iteration: 171000, Loss: 0.004182890523225069
2018-10-22 16:05:46.362737:	Training iteration: 171200, Loss: 0.004034759011119604
2018-10-22 16:07:30.921954:	Training iteration: 171400, Loss: 0.006417388562113047
2018-10-22 16:09:17.934329:	Training iteration: 171600, Loss: 0.004506519064307213
2018-10-22 16:11:02.948454:	Training iteration: 171800, Loss: 0.006430823355913162
2018-10-22 16:12:49.426067:	Training iteration: 172000, Loss: 0.007324788719415665
2018-10-22 16:14:34.964947:	Training iteration: 172200, Loss: 0.0049799284897744656
2018-10-22 16:16:19.088089:	Training iteration: 172400, Loss: 0.005005190148949623
2018-10-22 16:18:04.628881:	Training iteration: 172600, Loss: 0.004410557448863983
2018-10-22 16:19:47.798301:	Training iteration: 172800, Loss: 0.005784634500741959
2018-10-22 16:21:31.894680:	Training iteration: 173000, Loss: 0.005819657351821661
2018-10-22 16:23:16.332193:	Training iteration: 173200, Loss: 0.004650356713682413
2018-10-22 16:25:02.185406:	Training iteration: 173400, Loss: 0.007141027599573135
2018-10-22 16:26:47.720298:	Training iteration: 173600, Loss: 0.005495724733918905
2018-10-22 16:28:33.199592:	Training iteration: 173800, Loss: 0.0039004564750939608
2018-10-22 16:30:19.273045:	Training iteration: 174000, Loss: 0.0047957440838217735
2018-10-22 16:32:03.970020:	Training iteration: 174200, Loss: 0.004804177209734917
2018-10-22 16:33:48.950158:	Training iteration: 174400, Loss: 0.004501855932176113
2018-10-22 16:35:33.937709:	Training iteration: 174600, Loss: 0.00560313044115901
2018-10-22 16:37:17.878813:	Training iteration: 174800, Loss: 0.004381267819553614
2018-10-22 16:39:02.786550:	Training iteration: 175000, Loss: 0.0044662863947451115
2018-10-22 16:40:47.257378:	Training iteration: 175200, Loss: 0.005151951219886541
2018-10-22 16:42:30.425334:	Training iteration: 175400, Loss: 0.003443480236455798
2018-10-22 16:44:14.717640:	Training iteration: 175600, Loss: 0.006352428812533617
2018-10-22 16:45:58.409511:	Training iteration: 175800, Loss: 0.004635939374566078
2018-10-22 16:47:42.712699:	Training iteration: 176000, Loss: 0.005060413386672735
2018-10-22 16:49:24.760010:	Training iteration: 176200, Loss: 0.0034135766327381134
2018-10-22 16:51:06.930240:	Training iteration: 176400, Loss: 0.0032247246708720922
2018-10-22 16:52:14.782659: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 644 of 1000
2018-10-22 16:52:19.394314: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 16:53:00.121276:	Training iteration: 176600, Loss: 0.004422538448125124
2018-10-22 16:54:42.776213:	Training iteration: 176800, Loss: 0.004635240416973829
2018-10-22 16:56:24.964173:	Training iteration: 177000, Loss: 0.0038374552968889475
2018-10-22 16:58:07.819203:	Training iteration: 177200, Loss: 0.005641559604555368
2018-10-22 16:59:50.944573:	Training iteration: 177400, Loss: 0.003835557261481881
2018-10-22 17:01:34.819388:	Training iteration: 177600, Loss: 0.006157842930406332
2018-10-22 17:03:19.326549:	Training iteration: 177800, Loss: 0.004848645534366369
2018-10-22 17:05:02.467880:	Training iteration: 178000, Loss: 0.004954085219651461
2018-10-22 17:06:46.147974:	Training iteration: 178200, Loss: 0.005639944225549698
2018-10-22 17:08:31.291599:	Training iteration: 178400, Loss: 0.0048674121499061584
2018-10-22 17:10:16.778600:	Training iteration: 178600, Loss: 0.004809423815459013
2018-10-22 17:12:01.755317:	Training iteration: 178800, Loss: 0.003602653741836548
2018-10-22 17:13:48.170022:	Training iteration: 179000, Loss: 0.004415441304445267
2018-10-22 17:15:33.099073:	Training iteration: 179200, Loss: 0.004182992968708277
2018-10-22 17:17:17.446368:	Training iteration: 179400, Loss: 0.004394222982227802
2018-10-22 17:19:00.306475:	Training iteration: 179600, Loss: 0.004109052941203117
2018-10-22 17:20:46.955585:	Training iteration: 179800, Loss: 0.005876276642084122
2018-10-22 17:22:32.831051:	Training iteration: 180000, Loss: 0.0044904653914272785
Checkpoint
2018-10-22 17:24:19.412153:	Training iteration: 180200, Loss: 0.0046472144313156605
2018-10-22 17:26:05.377653:	Training iteration: 180400, Loss: 0.00444156676530838
2018-10-22 17:27:48.662311:	Training iteration: 180600, Loss: 0.0046587237156927586
2018-10-22 17:29:33.185029:	Training iteration: 180800, Loss: 0.007131517399102449
2018-10-22 17:31:17.363636:	Training iteration: 181000, Loss: 0.005821188446134329
2018-10-22 17:33:02.388516:	Training iteration: 181200, Loss: 0.004820871166884899
2018-10-22 17:34:48.027604:	Training iteration: 181400, Loss: 0.007044267375022173
2018-10-22 17:36:30.967043:	Training iteration: 181600, Loss: 0.004639497492462397
2018-10-22 17:38:14.396822:	Training iteration: 181800, Loss: 0.006547898519784212
2018-10-22 17:39:59.265729:	Training iteration: 182000, Loss: 0.004084499087184668
2018-10-22 17:41:50.307710:	Training iteration: 182200, Loss: 0.0036520594730973244
2018-10-22 17:43:34.680609:	Training iteration: 182400, Loss: 0.006483849138021469
2018-10-22 17:45:21.031014:	Training iteration: 182600, Loss: 0.005006775725632906
2018-10-22 17:47:04.878508:	Training iteration: 182800, Loss: 0.005234507378190756
2018-10-22 17:48:50.820343:	Training iteration: 183000, Loss: 0.005678211804479361
2018-10-22 17:50:33.961268:	Training iteration: 183200, Loss: 0.005161674227565527
2018-10-22 17:52:00.598971:	Training iteration: 183400, Loss: 0.004656398668885231
2018-10-22 17:53:26.421830:	Training iteration: 183600, Loss: 0.004843438975512981
2018-10-22 17:54:51.978394:	Training iteration: 183800, Loss: 0.0049076019786298275
2018-10-22 17:56:17.292069:	Training iteration: 184000, Loss: 0.006588019896298647
2018-10-22 17:57:43.282857:	Training iteration: 184200, Loss: 0.00442681647837162
2018-10-22 17:59:09.081035:	Training iteration: 184400, Loss: 0.0050370884127914906
2018-10-22 18:00:35.468326:	Training iteration: 184600, Loss: 0.003113768994808197
2018-10-22 18:02:01.643893:	Training iteration: 184800, Loss: 0.0045522525906562805
2018-10-22 18:02:19.420605: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 582 of 1000
2018-10-22 18:02:24.627921: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 18:03:42.982064:	Training iteration: 185000, Loss: 0.004982894752174616
2018-10-22 18:05:09.038910:	Training iteration: 185200, Loss: 0.0047037131153047085
2018-10-22 18:06:34.369976:	Training iteration: 185400, Loss: 0.0029467802960425615
2018-10-22 18:08:00.432037:	Training iteration: 185600, Loss: 0.0031843383330851793
2018-10-22 18:09:26.341377:	Training iteration: 185800, Loss: 0.004527072887867689
2018-10-22 18:10:51.698420:	Training iteration: 186000, Loss: 0.003193162614479661
2018-10-22 18:12:17.737566:	Training iteration: 186200, Loss: 0.003977564629167318
2018-10-22 18:13:44.181632:	Training iteration: 186400, Loss: 0.004444799851626158
2018-10-22 18:15:09.520786:	Training iteration: 186600, Loss: 0.004038509447127581
2018-10-22 18:16:35.586999:	Training iteration: 186800, Loss: 0.0034721686970442533
2018-10-22 18:18:01.182684:	Training iteration: 187000, Loss: 0.0037624321412295103
2018-10-22 18:19:26.632953:	Training iteration: 187200, Loss: 0.003873566398397088
2018-10-22 18:20:52.228765:	Training iteration: 187400, Loss: 0.005945775657892227
2018-10-22 18:22:18.030857:	Training iteration: 187600, Loss: 0.004709774162620306
2018-10-22 18:23:42.759261:	Training iteration: 187800, Loss: 0.0030847780872136354
2018-10-22 18:25:08.120327:	Training iteration: 188000, Loss: 0.005908682476729155
2018-10-22 18:26:33.334187:	Training iteration: 188200, Loss: 0.0030522493179887533
2018-10-22 18:27:58.491227:	Training iteration: 188400, Loss: 0.0033419046085327864
2018-10-22 18:29:23.462955:	Training iteration: 188600, Loss: 0.0029307848308235407
2018-10-22 18:30:48.832073:	Training iteration: 188800, Loss: 0.0029367816168814898
2018-10-22 18:32:14.427589:	Training iteration: 189000, Loss: 0.004067862406373024
2018-10-22 18:33:39.541747:	Training iteration: 189200, Loss: 0.004509581718593836
2018-10-22 18:35:04.401515:	Training iteration: 189400, Loss: 0.004257263150066137
2018-10-22 18:36:29.494051:	Training iteration: 189600, Loss: 0.004909955430775881
2018-10-22 18:37:54.803699:	Training iteration: 189800, Loss: 0.00466722110286355
2018-10-22 18:39:19.522849:	Training iteration: 190000, Loss: 0.0045973458327353
Checkpoint
2018-10-22 18:40:48.903636:	Training iteration: 190200, Loss: 0.0028304271399974823
2018-10-22 18:42:14.002665:	Training iteration: 190400, Loss: 0.00280401180498302
2018-10-22 18:43:38.933099:	Training iteration: 190600, Loss: 0.004181666299700737
2018-10-22 18:45:03.148031:	Training iteration: 190800, Loss: 0.0021356248762458563
2018-10-22 18:46:28.876369:	Training iteration: 191000, Loss: 0.003698676824569702
2018-10-22 18:47:54.103643:	Training iteration: 191200, Loss: 0.004406111780554056
2018-10-22 18:49:18.728541:	Training iteration: 191400, Loss: 0.004204389173537493
2018-10-22 18:50:43.539804:	Training iteration: 191600, Loss: 0.004134702030569315
2018-10-22 18:52:08.852216:	Training iteration: 191800, Loss: 0.003924528602510691
2018-10-22 18:53:34.980819:	Training iteration: 192000, Loss: 0.004685206804424524
2018-10-22 18:55:00.456100:	Training iteration: 192200, Loss: 0.004014583304524422
2018-10-22 18:56:25.873496:	Training iteration: 192400, Loss: 0.0033836814109236
2018-10-22 18:57:50.885696:	Training iteration: 192600, Loss: 0.004023750312626362
2018-10-22 18:59:16.611119:	Training iteration: 192800, Loss: 0.003744391957297921
2018-10-22 19:00:41.580904:	Training iteration: 193000, Loss: 0.003227242035791278
2018-10-22 19:02:09.776069:	Training iteration: 193200, Loss: 0.0032958395313471556
2018-10-22 19:03:34.276830:	Training iteration: 193400, Loss: 0.004884313326328993
2018-10-22 19:04:26.075370: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 831 of 1000
2018-10-22 19:04:27.969259: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 19:05:10.297232:	Training iteration: 193600, Loss: 0.008501417003571987
2018-10-22 19:06:34.535549:	Training iteration: 193800, Loss: 0.005680564325302839
2018-10-22 19:07:59.502013:	Training iteration: 194000, Loss: 0.006030000746250153
2018-10-22 19:09:24.234159:	Training iteration: 194200, Loss: 0.004535766784101725
2018-10-22 19:10:48.601995:	Training iteration: 194400, Loss: 0.005717841442674398
2018-10-22 19:12:12.617771:	Training iteration: 194600, Loss: 0.0035005996469408274
2018-10-22 19:13:38.263694:	Training iteration: 194800, Loss: 0.005093633662909269
2018-10-22 19:15:02.773890:	Training iteration: 195000, Loss: 0.003312427317723632
2018-10-22 19:16:27.362450:	Training iteration: 195200, Loss: 0.0031308706384152174
2018-10-22 19:17:52.401859:	Training iteration: 195400, Loss: 0.00720421364530921
2018-10-22 19:19:16.462408:	Training iteration: 195600, Loss: 0.004773878492414951
2018-10-22 19:20:40.150615:	Training iteration: 195800, Loss: 0.006910009775310755
2018-10-22 19:22:04.275290:	Training iteration: 196000, Loss: 0.009199744090437889
2018-10-22 19:23:28.170525:	Training iteration: 196200, Loss: 0.0038376152515411377
2018-10-22 19:24:52.124618:	Training iteration: 196400, Loss: 0.0031473066192120314
2018-10-22 19:26:15.554785:	Training iteration: 196600, Loss: 0.003526482032611966
2018-10-22 19:27:39.426165:	Training iteration: 196800, Loss: 0.004641483072191477
2018-10-22 19:29:02.975916:	Training iteration: 197000, Loss: 0.004401260521262884
2018-10-22 19:30:26.842508:	Training iteration: 197200, Loss: 0.004530655685812235
2018-10-22 19:31:50.900172:	Training iteration: 197400, Loss: 0.007565113250166178
2018-10-22 19:33:14.735206:	Training iteration: 197600, Loss: 0.005066162906587124
2018-10-22 19:34:37.900792:	Training iteration: 197800, Loss: 0.007169836666435003
2018-10-22 19:36:01.886435:	Training iteration: 198000, Loss: 0.004777607973664999
2018-10-22 19:37:25.574714:	Training iteration: 198200, Loss: 0.004868000280112028
2018-10-22 19:38:49.457764:	Training iteration: 198400, Loss: 0.004060814622789621
2018-10-22 19:40:13.166883:	Training iteration: 198600, Loss: 0.00323843932710588
2018-10-22 19:41:38.221607:	Training iteration: 198800, Loss: 0.008457023650407791
2018-10-22 19:43:02.948289:	Training iteration: 199000, Loss: 0.004426749888807535
2018-10-22 19:44:27.591429:	Training iteration: 199200, Loss: 0.005043426528573036
2018-10-22 19:45:52.378851:	Training iteration: 199400, Loss: 0.0036659620236605406
2018-10-22 19:47:17.340940:	Training iteration: 199600, Loss: 0.004168665502220392
2018-10-22 19:48:41.884749:	Training iteration: 199800, Loss: 0.004198736045509577
2018-10-22 19:50:06.729460:	Training iteration: 200000, Loss: 0.005620906595140696
Checkpoint
2018-10-22 19:51:33.785499:	Training iteration: 200200, Loss: 0.002796148881316185
2018-10-22 19:52:58.751274:	Training iteration: 200400, Loss: 0.00456504849717021
2018-10-22 19:54:23.524317:	Training iteration: 200600, Loss: 0.00459279352799058
2018-10-22 19:55:48.447188:	Training iteration: 200800, Loss: 0.006232539191842079
2018-10-22 19:57:12.995578:	Training iteration: 201000, Loss: 0.004626626614481211
2018-10-22 19:58:38.113813:	Training iteration: 201200, Loss: 0.0048145572654902935
2018-10-22 20:00:02.341259:	Training iteration: 201400, Loss: 0.004930260591208935
2018-10-22 20:01:26.534643:	Training iteration: 201600, Loss: 0.0040248967707157135
2018-10-22 20:02:51.301555:	Training iteration: 201800, Loss: 0.003629007376730442
2018-10-22 20:04:15.937877:	Training iteration: 202000, Loss: 0.004909526091068983
2018-10-22 20:05:40.428641:	Training iteration: 202200, Loss: 0.003225088119506836
2018-10-22 20:07:05.275852:	Training iteration: 202400, Loss: 0.0036345943808555603
2018-10-22 20:08:29.417970:	Training iteration: 202600, Loss: 0.006463551428169012
2018-10-22 20:09:53.953533:	Training iteration: 202800, Loss: 0.004938914440572262
2018-10-22 20:11:19.351915:	Training iteration: 203000, Loss: 0.007254272699356079
2018-10-22 20:12:43.566296:	Training iteration: 203200, Loss: 0.007768627256155014
2018-10-22 20:14:07.993372:	Training iteration: 203400, Loss: 0.004260613117367029
2018-10-22 20:15:32.219110:	Training iteration: 203600, Loss: 0.0036840650718659163
2018-10-22 20:16:56.458028:	Training iteration: 203800, Loss: 0.005483461078256369
2018-10-22 20:18:20.642897:	Training iteration: 204000, Loss: 0.0086913937702775
2018-10-22 20:19:44.414008:	Training iteration: 204200, Loss: 0.007501245941966772
2018-10-22 20:21:08.907634:	Training iteration: 204400, Loss: 0.006057223305106163
2018-10-22 20:22:33.161491:	Training iteration: 204600, Loss: 0.008356347680091858
2018-10-22 20:23:57.385777:	Training iteration: 204800, Loss: 0.007785780355334282
2018-10-22 20:25:21.677502:	Training iteration: 205000, Loss: 0.005926682148128748
2018-10-22 20:26:45.331362:	Training iteration: 205200, Loss: 0.0059516276232898235
2018-10-22 20:28:10.052048:	Training iteration: 205400, Loss: 0.006105313543230295
2018-10-22 20:29:34.234013:	Training iteration: 205600, Loss: 0.006301999092102051
2018-10-22 20:30:58.100001:	Training iteration: 205800, Loss: 0.0042399014346301556
2018-10-22 20:32:21.540379:	Training iteration: 206000, Loss: 0.0034585464745759964
2018-10-22 20:33:45.925098:	Training iteration: 206200, Loss: 0.0036996507551521063
2018-10-22 20:35:10.411430:	Training iteration: 206400, Loss: 0.007961983792483807
2018-10-22 20:36:34.912956:	Training iteration: 206600, Loss: 0.006784370634704828
2018-10-22 20:37:59.168841:	Training iteration: 206800, Loss: 0.007691498380154371
2018-10-22 20:39:23.773417:	Training iteration: 207000, Loss: 0.005524723324924707
2018-10-22 20:40:48.553698:	Training iteration: 207200, Loss: 0.0064461734145879745
2018-10-22 20:42:12.343242:	Training iteration: 207400, Loss: 0.00505093252286315
2018-10-22 20:43:37.170893:	Training iteration: 207600, Loss: 0.006437980104237795
2018-10-22 20:45:02.311561:	Training iteration: 207800, Loss: 0.0046639940701425076
2018-10-22 20:46:26.854906:	Training iteration: 208000, Loss: 0.0064167059026658535
2018-10-22 20:47:51.281009:	Training iteration: 208200, Loss: 0.0067402333952486515
2018-10-22 20:49:16.506216:	Training iteration: 208400, Loss: 0.0038938287179917097
2018-10-22 20:50:40.856048:	Training iteration: 208600, Loss: 0.008214162662625313
2018-10-22 20:52:05.826340:	Training iteration: 208800, Loss: 0.004318188410252333
2018-10-22 20:53:30.574771:	Training iteration: 209000, Loss: 0.007385519798845053
2018-10-22 20:54:54.752462:	Training iteration: 209200, Loss: 0.007022510748356581
2018-10-22 20:56:19.717464:	Training iteration: 209400, Loss: 0.0096202427521348
2018-10-22 20:57:44.801596:	Training iteration: 209600, Loss: 0.005939988419413567
2018-10-22 20:59:09.526949:	Training iteration: 209800, Loss: 0.0038912321906536818
2018-10-22 21:00:34.305806:	Training iteration: 210000, Loss: 0.005042810458689928
Checkpoint
2018-10-22 21:02:02.535776:	Training iteration: 210200, Loss: 0.005715858191251755
2018-10-22 21:03:27.444151:	Training iteration: 210400, Loss: 0.0048684109933674335
2018-10-22 21:04:51.698872:	Training iteration: 210600, Loss: 0.004625402856618166
2018-10-22 21:06:17.984791:	Training iteration: 210800, Loss: 0.006950384471565485
2018-10-22 21:07:42.463277:	Training iteration: 211000, Loss: 0.004859786480665207
2018-10-22 21:09:06.925150:	Training iteration: 211200, Loss: 0.006422135513275862
2018-10-22 21:10:30.819157:	Training iteration: 211400, Loss: 0.004775234032422304
2018-10-22 21:11:55.021964:	Training iteration: 211600, Loss: 0.006379814352840185
2018-10-22 21:13:19.200562:	Training iteration: 211800, Loss: 0.009187125600874424
2018-10-22 21:14:43.384721:	Training iteration: 212000, Loss: 0.008048065938055515
2018-10-22 21:16:07.332317:	Training iteration: 212200, Loss: 0.003798219608142972
2018-10-22 21:17:31.085812:	Training iteration: 212400, Loss: 0.006714186165481806
2018-10-22 21:18:55.120395:	Training iteration: 212600, Loss: 0.00397301884368062
2018-10-22 21:20:18.543882:	Training iteration: 212800, Loss: 0.005537411663681269
2018-10-22 21:21:42.636397:	Training iteration: 213000, Loss: 0.003910867962986231
2018-10-22 21:23:06.367492:	Training iteration: 213200, Loss: 0.0043647377751767635
2018-10-22 21:24:30.573993:	Training iteration: 213400, Loss: 0.004468109458684921
2018-10-22 21:25:55.033580:	Training iteration: 213600, Loss: 0.0050553516484797
2018-10-22 21:27:18.639747:	Training iteration: 213800, Loss: 0.005022482480853796
2018-10-22 21:28:42.466349:	Training iteration: 214000, Loss: 0.0038046122062951326
2018-10-22 21:30:06.349242:	Training iteration: 214200, Loss: 0.0042158085852861404
2018-10-22 21:31:30.441061:	Training iteration: 214400, Loss: 0.00552491145208478
2018-10-22 21:32:54.764143:	Training iteration: 214600, Loss: 0.0031064434442669153
2018-10-22 21:34:19.156919:	Training iteration: 214800, Loss: 0.005863219499588013
2018-10-22 21:35:43.349398:	Training iteration: 215000, Loss: 0.006567607168108225
2018-10-22 21:37:07.730455:	Training iteration: 215200, Loss: 0.004575433675199747
2018-10-22 21:38:32.880491:	Training iteration: 215400, Loss: 0.0052959248423576355
2018-10-22 21:39:57.274802:	Training iteration: 215600, Loss: 0.006136032287031412
2018-10-22 21:41:20.866960:	Training iteration: 215800, Loss: 0.004808933008462191
2018-10-22 21:42:45.606186:	Training iteration: 216000, Loss: 0.004629557020962238
2018-10-22 21:44:10.469302:	Training iteration: 216200, Loss: 0.005968574434518814
2018-10-22 21:45:35.146375:	Training iteration: 216400, Loss: 0.005399256944656372
2018-10-22 21:46:59.560666:	Training iteration: 216600, Loss: 0.004379883408546448
2018-10-22 21:48:24.248783:	Training iteration: 216800, Loss: 0.004560787696391344
2018-10-22 21:49:48.348502:	Training iteration: 217000, Loss: 0.004637678619474173
2018-10-22 21:51:13.188778:	Training iteration: 217200, Loss: 0.005031564738601446
2018-10-22 21:52:37.711285:	Training iteration: 217400, Loss: 0.005742558743804693
2018-10-22 21:54:02.498249:	Training iteration: 217600, Loss: 0.00431009940803051
2018-10-22 21:55:27.423887:	Training iteration: 217800, Loss: 0.0037559110205620527
2018-10-22 21:56:51.796853:	Training iteration: 218000, Loss: 0.0036029843613505363
2018-10-22 21:58:15.981195:	Training iteration: 218200, Loss: 0.005018456373363733
2018-10-22 21:59:40.621787:	Training iteration: 218400, Loss: 0.005333427339792252
2018-10-22 22:01:05.193099:	Training iteration: 218600, Loss: 0.0062921177595853806
2018-10-22 22:02:29.187433:	Training iteration: 218800, Loss: 0.006866893265396357
2018-10-22 22:03:53.782386:	Training iteration: 219000, Loss: 0.00362461362965405
2018-10-22 22:05:17.543013:	Training iteration: 219200, Loss: 0.0052233911119401455
2018-10-22 22:06:41.482017:	Training iteration: 219400, Loss: 0.004871805664151907
2018-10-22 22:08:05.830408:	Training iteration: 219600, Loss: 0.004689177963882685
2018-10-22 22:09:29.630387:	Training iteration: 219800, Loss: 0.005559112410992384
2018-10-22 22:10:53.690598:	Training iteration: 220000, Loss: 0.003987676464021206
Checkpoint
2018-10-22 22:12:20.774423:	Training iteration: 220200, Loss: 0.009549219161272049
2018-10-22 22:13:44.292998:	Training iteration: 220400, Loss: 0.009385941550135612
2018-10-22 22:15:07.990963:	Training iteration: 220600, Loss: 0.0036765504628419876
2018-10-22 22:16:32.341641:	Training iteration: 220800, Loss: 0.00740438885986805
2018-10-22 22:17:55.766566:	Training iteration: 221000, Loss: 0.0064142742194235325
2018-10-22 22:19:19.503617:	Training iteration: 221200, Loss: 0.004213748499751091
2018-10-22 22:20:42.996037:	Training iteration: 221400, Loss: 0.008393974974751472
2018-10-22 22:22:07.273333:	Training iteration: 221600, Loss: 0.006134297698736191
2018-10-22 22:23:31.593766:	Training iteration: 221800, Loss: 0.004021437373012304
2018-10-22 22:24:55.659382:	Training iteration: 222000, Loss: 0.004853275138884783
2018-10-22 22:26:19.985416:	Training iteration: 222200, Loss: 0.003932794090360403
2018-10-22 22:27:43.759415:	Training iteration: 222400, Loss: 0.004601737949997187
2018-10-22 22:29:08.550120:	Training iteration: 222600, Loss: 0.004873621743172407
2018-10-22 22:30:33.535463:	Training iteration: 222800, Loss: 0.012196180410683155
2018-10-22 22:31:58.504005:	Training iteration: 223000, Loss: 0.009855369105935097
2018-10-22 22:33:23.450946:	Training iteration: 223200, Loss: 0.0061146654188632965
2018-10-22 22:34:47.478396:	Training iteration: 223400, Loss: 0.006123846862465143
2018-10-22 22:36:11.919743:	Training iteration: 223600, Loss: 0.0049938359297811985
2018-10-22 22:37:37.437216:	Training iteration: 223800, Loss: 0.0076985121704638
2018-10-22 22:39:01.730899:	Training iteration: 224000, Loss: 0.0073726363480091095
2018-10-22 22:40:26.398866:	Training iteration: 224200, Loss: 0.013930746354162693
2018-10-22 22:41:51.245886:	Training iteration: 224400, Loss: 0.00932862889021635
2018-10-22 22:43:15.632748:	Training iteration: 224600, Loss: 0.007201289292424917
2018-10-22 22:44:39.736425:	Training iteration: 224800, Loss: 0.0050419350154697895
2018-10-22 22:46:04.844262:	Training iteration: 225000, Loss: 0.0031426374334841967
2018-10-22 22:47:29.301945:	Training iteration: 225200, Loss: 0.005649862810969353
2018-10-22 22:48:53.879348:	Training iteration: 225400, Loss: 0.002947154687717557
2018-10-22 22:50:19.174260:	Training iteration: 225600, Loss: 0.005529545713216066
2018-10-22 22:51:43.942385:	Training iteration: 225800, Loss: 0.005103823728859425
2018-10-22 22:53:08.051534:	Training iteration: 226000, Loss: 0.006531921681016684
2018-10-22 22:54:32.454948:	Training iteration: 226200, Loss: 0.0055668237619102
2018-10-22 22:55:56.564286:	Training iteration: 226400, Loss: 0.004953465890139341
2018-10-22 22:57:21.039509:	Training iteration: 226600, Loss: 0.006525496486574411
2018-10-22 22:58:45.431038:	Training iteration: 226800, Loss: 0.0035738309379667044
2018-10-22 23:00:09.433958:	Training iteration: 227000, Loss: 0.00524519756436348
2018-10-22 23:01:33.630393:	Training iteration: 227200, Loss: 0.004435041453689337
2018-10-22 23:02:57.205895:	Training iteration: 227400, Loss: 0.003557957476004958
2018-10-22 23:04:21.224968:	Training iteration: 227600, Loss: 0.003645359305664897
2018-10-22 23:05:45.626544:	Training iteration: 227800, Loss: 0.004528445191681385
2018-10-22 23:07:09.448243:	Training iteration: 228000, Loss: 0.004174458794295788
2018-10-22 23:08:33.019724:	Training iteration: 228200, Loss: 0.006954304873943329
2018-10-22 23:09:57.485856:	Training iteration: 228400, Loss: 0.0076424796134233475
2018-10-22 23:11:21.288673:	Training iteration: 228600, Loss: 0.006396008189767599
2018-10-22 23:12:44.880745:	Training iteration: 228800, Loss: 0.009110352955758572
2018-10-22 23:14:09.232920:	Training iteration: 229000, Loss: 0.007149409037083387
2018-10-22 23:15:32.733191:	Training iteration: 229200, Loss: 0.006062536034733057
2018-10-22 23:16:56.372950:	Training iteration: 229400, Loss: 0.007178860250860453
2018-10-22 23:18:20.378311:	Training iteration: 229600, Loss: 0.005817104130983353
2018-10-22 23:19:43.985224:	Training iteration: 229800, Loss: 0.005667110905051231
2018-10-22 23:21:08.236512:	Training iteration: 230000, Loss: 0.004909558687359095
Checkpoint
2018-10-22 23:22:37.216812:	Training iteration: 230200, Loss: 0.004564179573208094
2018-10-22 23:24:01.348198:	Training iteration: 230400, Loss: 0.0045365639962255955
2018-10-22 23:25:25.913136:	Training iteration: 230600, Loss: 0.003406414994969964
2018-10-22 23:26:50.115416:	Training iteration: 230800, Loss: 0.004139144439250231
2018-10-22 23:28:14.491641:	Training iteration: 231000, Loss: 0.005171814933419228
2018-10-22 23:29:39.360652:	Training iteration: 231200, Loss: 0.007664510514587164
2018-10-22 23:31:03.450479:	Training iteration: 231400, Loss: 0.0043127890676259995
2018-10-22 23:32:28.030339:	Training iteration: 231600, Loss: 0.004846247378736734
2018-10-22 23:33:52.209979:	Training iteration: 231800, Loss: 0.009217528626322746
2018-10-22 23:35:17.108924:	Training iteration: 232000, Loss: 0.004582629073411226
2018-10-22 23:36:41.403387:	Training iteration: 232200, Loss: 0.0047288513742387295
2018-10-22 23:38:06.168122:	Training iteration: 232400, Loss: 0.004067708272486925
2018-10-22 23:39:30.542525:	Training iteration: 232600, Loss: 0.007399767637252808
2018-10-22 23:40:54.406707:	Training iteration: 232800, Loss: 0.008894109167158604
2018-10-22 23:42:19.458202:	Training iteration: 233000, Loss: 0.005225088447332382
2018-10-22 23:43:43.879412:	Training iteration: 233200, Loss: 0.004565884824842215
2018-10-22 23:45:09.124007:	Training iteration: 233400, Loss: 0.00582518195733428
2018-10-22 23:46:34.205835:	Training iteration: 233600, Loss: 0.007697677705436945
2018-10-22 23:47:58.274720:	Training iteration: 233800, Loss: 0.006870826240628958
2018-10-22 23:49:23.527347:	Training iteration: 234000, Loss: 0.005416309926658869
2018-10-22 23:50:48.072258:	Training iteration: 234200, Loss: 0.005643242504447699
2018-10-22 23:52:12.185026:	Training iteration: 234400, Loss: 0.0037777533289045095
2018-10-22 23:53:36.216827:	Training iteration: 234600, Loss: 0.004425215069204569
2018-10-22 23:55:00.864720:	Training iteration: 234800, Loss: 0.0042557124979794025
2018-10-22 23:56:25.146346:	Training iteration: 235000, Loss: 0.007077626883983612
2018-10-22 23:57:49.111394:	Training iteration: 235200, Loss: 0.0055069890804588795
2018-10-22 23:59:13.735107:	Training iteration: 235400, Loss: 0.006596285849809647
2018-10-23 00:00:37.285728:	Training iteration: 235600, Loss: 0.004514913074672222
2018-10-23 00:02:01.229143:	Training iteration: 235800, Loss: 0.006431614514440298
2018-10-23 00:03:25.236995:	Training iteration: 236000, Loss: 0.0067510176450014114
2018-10-23 00:04:48.918170:	Training iteration: 236200, Loss: 0.004141532350331545
2018-10-23 00:06:12.372611:	Training iteration: 236400, Loss: 0.006023753900080919
2018-10-23 00:07:36.116434:	Training iteration: 236600, Loss: 0.004199750255793333
2018-10-23 00:09:00.156081:	Training iteration: 236800, Loss: 0.0059135048650205135
2018-10-23 00:10:24.092377:	Training iteration: 237000, Loss: 0.006544014904648066
2018-10-23 00:11:48.018219:	Training iteration: 237200, Loss: 0.005091797560453415
2018-10-23 00:13:11.555000:	Training iteration: 237400, Loss: 0.003178957151249051
2018-10-23 00:14:35.961746:	Training iteration: 237600, Loss: 0.005527853965759277
2018-10-23 00:16:00.100563:	Training iteration: 237800, Loss: 0.006769208237528801
2018-10-23 00:17:24.675286:	Training iteration: 238000, Loss: 0.005090213846415281
2018-10-23 00:18:48.737872:	Training iteration: 238200, Loss: 0.003674668027088046
2018-10-23 00:20:13.500613:	Training iteration: 238400, Loss: 0.005448728799819946
2018-10-23 00:21:38.047128:	Training iteration: 238600, Loss: 0.005326293874531984
2018-10-23 00:23:02.835205:	Training iteration: 238800, Loss: 0.0052327862940728664
2018-10-23 00:24:27.522709:	Training iteration: 239000, Loss: 0.00393105112016201
2018-10-23 00:25:52.657101:	Training iteration: 239200, Loss: 0.009043306112289429
2018-10-23 00:27:17.184115:	Training iteration: 239400, Loss: 0.00617242231965065
2018-10-23 00:28:41.603010:	Training iteration: 239600, Loss: 0.0051211039535701275
2018-10-23 00:30:05.918975:	Training iteration: 239800, Loss: 0.004222618415951729
2018-10-23 00:30:49.875169:	Epoch 2 finished after 239905 iterations.
Validating
2018-10-23 00:30:49.983344:	Entering validation loop
2018-10-23 00:31:00.007352: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 649 of 1000
2018-10-23 00:31:04.846580: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:31:40.614331:	Validation iteration: 200, Loss: 0.004943173844367266
2018-10-23 00:32:16.662456:	Validation iteration: 400, Loss: 0.004927926696836948
2018-10-23 00:32:53.188324:	Validation iteration: 600, Loss: 0.007887603715062141
2018-10-23 00:33:30.383534:	Validation iteration: 800, Loss: 0.003496601479128003
2018-10-23 00:34:06.860372:	Validation iteration: 1000, Loss: 0.00602959468960762
2018-10-23 00:34:43.979492:	Validation iteration: 1200, Loss: 0.006292422767728567
2018-10-23 00:35:30.056814: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 669 of 1000
2018-10-23 00:35:34.582824: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:35:35.961068:	Validation iteration: 1400, Loss: 0.005200291518121958
2018-10-23 00:36:11.814365:	Validation iteration: 1600, Loss: 0.006542359013110399
2018-10-23 00:36:47.875957:	Validation iteration: 1800, Loss: 0.006999887526035309
2018-10-23 00:37:24.510898:	Validation iteration: 2000, Loss: 0.005528055131435394
2018-10-23 00:38:01.665235:	Validation iteration: 2200, Loss: 0.007565019186586142
2018-10-23 00:38:38.028970:	Validation iteration: 2400, Loss: 0.008825051598250866
2018-10-23 00:39:14.867030:	Validation iteration: 2600, Loss: 0.007205257657915354
2018-10-23 00:39:59.149754: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 529 of 1000
2018-10-23 00:40:05.753423: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:40:08.458477:	Validation iteration: 2800, Loss: 0.007542847190052271
2018-10-23 00:40:44.689934:	Validation iteration: 3000, Loss: 0.006372521165758371
2018-10-23 00:41:21.414808:	Validation iteration: 3200, Loss: 0.004947233945131302
2018-10-23 00:41:58.393814:	Validation iteration: 3400, Loss: 0.005190922878682613
2018-10-23 00:42:34.620010:	Validation iteration: 3600, Loss: 0.005888741463422775
2018-10-23 00:43:11.533670:	Validation iteration: 3800, Loss: 0.004539630841463804
2018-10-23 00:43:48.130098:	Validation iteration: 4000, Loss: 0.0048099178820848465
2018-10-23 00:44:31.357774: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 650 of 1000
2018-10-23 00:44:36.010543: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:44:40.012730:	Validation iteration: 4200, Loss: 0.0063221328891813755
2018-10-23 00:45:15.496044:	Validation iteration: 4400, Loss: 0.0063096764497458935
2018-10-23 00:45:51.085159:	Validation iteration: 4600, Loss: 0.007123095449060202
2018-10-23 00:46:27.431671:	Validation iteration: 4800, Loss: 0.006741993594914675
2018-10-23 00:47:04.350537:	Validation iteration: 5000, Loss: 0.006478708237409592
2018-10-23 00:47:40.689855:	Validation iteration: 5200, Loss: 0.007115040440112352
2018-10-23 00:48:17.054441:	Validation iteration: 5400, Loss: 0.005508077796548605
2018-10-23 00:48:58.327324: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 614 of 1000
2018-10-23 00:49:04.125565: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:49:09.501742:	Validation iteration: 5600, Loss: 0.004498508293181658
2018-10-23 00:49:45.316535:	Validation iteration: 5800, Loss: 0.00619850680232048
2018-10-23 00:50:21.668219:	Validation iteration: 6000, Loss: 0.006168274208903313
2018-10-23 00:50:57.329894:	Validation iteration: 6200, Loss: 0.0041971332393586636
2018-10-23 00:51:33.428628:	Validation iteration: 6400, Loss: 0.005319857969880104
2018-10-23 00:52:12.570435:	Validation iteration: 6600, Loss: 0.004390560556203127
2018-10-23 00:52:48.784001:	Validation iteration: 6800, Loss: 0.005994038190692663
2018-10-23 00:53:26.061078:	Validation iteration: 7000, Loss: 0.0037116315215826035
2018-10-23 00:54:02.933829:	Validation iteration: 7200, Loss: 0.00591307645663619
2018-10-23 00:54:39.425351:	Validation iteration: 7400, Loss: 0.006434659007936716
Validation check mean loss: 0.005929216462873478
Validation loss has improved!
New best validation cost!
2018-10-23 00:55:06.773639: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 735 of 1000
2018-10-23 00:55:10.117226: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 00:55:49.560013:	Training iteration: 240000, Loss: 0.004540411289781332
Checkpoint
2018-10-23 00:57:16.952358:	Training iteration: 240200, Loss: 0.0061303251422941685
2018-10-23 00:58:41.326326:	Training iteration: 240400, Loss: 0.0049539715982973576
2018-10-23 01:00:06.052687:	Training iteration: 240600, Loss: 0.003411757992580533
2018-10-23 01:01:30.348142:	Training iteration: 240800, Loss: 0.0045048207975924015
2018-10-23 01:02:54.731360:	Training iteration: 241000, Loss: 0.003208782523870468
2018-10-23 01:04:19.584032:	Training iteration: 241200, Loss: 0.003896136535331607
2018-10-23 01:05:43.905379:	Training iteration: 241400, Loss: 0.004024892579764128
2018-10-23 01:07:07.825750:	Training iteration: 241600, Loss: 0.005019893404096365
2018-10-23 01:08:32.619104:	Training iteration: 241800, Loss: 0.002660207450389862
2018-10-23 01:09:57.551139:	Training iteration: 242000, Loss: 0.0034264184068888426
2018-10-23 01:11:22.703736:	Training iteration: 242200, Loss: 0.0026216537225991488
2018-10-23 01:12:47.280332:	Training iteration: 242400, Loss: 0.006379773374646902
2018-10-23 01:14:12.385002:	Training iteration: 242600, Loss: 0.006015724036842585
2018-10-23 01:15:37.551216:	Training iteration: 242800, Loss: 0.004443930462002754
2018-10-23 01:17:02.305231:	Training iteration: 243000, Loss: 0.003603390185162425
2018-10-23 01:18:27.413383:	Training iteration: 243200, Loss: 0.003477308200672269
2018-10-23 01:19:52.758353:	Training iteration: 243400, Loss: 0.004605141934007406
2018-10-23 01:21:17.660216:	Training iteration: 243600, Loss: 0.0043185134418308735
2018-10-23 01:22:42.825021:	Training iteration: 243800, Loss: 0.0037175510078668594
2018-10-23 01:24:07.237055:	Training iteration: 244000, Loss: 0.004659172613173723
2018-10-23 01:25:32.952982:	Training iteration: 244200, Loss: 0.0029986214358359575
2018-10-23 01:26:57.989005:	Training iteration: 244400, Loss: 0.004206677433103323
2018-10-23 01:28:23.311314:	Training iteration: 244600, Loss: 0.004584969952702522
2018-10-23 01:29:48.328783:	Training iteration: 244800, Loss: 0.0033829074818640947
2018-10-23 01:31:13.527794:	Training iteration: 245000, Loss: 0.0047049145214259624
2018-10-23 01:32:38.593139:	Training iteration: 245200, Loss: 0.003681482747197151
2018-10-23 01:34:04.165624:	Training iteration: 245400, Loss: 0.003974221646785736
2018-10-23 01:35:29.277609:	Training iteration: 245600, Loss: 0.005046279635280371
2018-10-23 01:36:54.514586:	Training iteration: 245800, Loss: 0.003644282231107354
2018-10-23 01:38:19.928882:	Training iteration: 246000, Loss: 0.004453785251826048
2018-10-23 01:39:44.463128:	Training iteration: 246200, Loss: 0.0034940491896122694
2018-10-23 01:41:09.649839:	Training iteration: 246400, Loss: 0.004894180689007044
2018-10-23 01:42:34.661851:	Training iteration: 246600, Loss: 0.004784644115716219
2018-10-23 01:43:59.572781:	Training iteration: 246800, Loss: 0.004080293700098991
2018-10-23 01:45:24.374171:	Training iteration: 247000, Loss: 0.005031624808907509
2018-10-23 01:46:49.249276:	Training iteration: 247200, Loss: 0.0032118104863911867
2018-10-23 01:48:13.625913:	Training iteration: 247400, Loss: 0.004394873045384884
2018-10-23 01:49:37.968441:	Training iteration: 247600, Loss: 0.006182258483022451
2018-10-23 01:51:02.655333:	Training iteration: 247800, Loss: 0.003991845995187759
2018-10-23 01:52:27.156551:	Training iteration: 248000, Loss: 0.006261270027607679
2018-10-23 01:53:48.247450: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 594 of 1000
2018-10-23 01:53:53.508338: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 01:54:06.333023:	Training iteration: 248200, Loss: 0.0051005203276872635
2018-10-23 01:55:29.175993:	Training iteration: 248400, Loss: 0.004746283870190382
2018-10-23 01:56:53.129518:	Training iteration: 248600, Loss: 0.0037760932464152575
2018-10-23 01:58:17.068681:	Training iteration: 248800, Loss: 0.004447161685675383
2018-10-23 01:59:41.310060:	Training iteration: 249000, Loss: 0.005396615713834763
2018-10-23 02:01:06.289120:	Training iteration: 249200, Loss: 0.004357778932899237
2018-10-23 02:02:30.240920:	Training iteration: 249400, Loss: 0.0047706072218716145
2018-10-23 02:03:54.577304:	Training iteration: 249600, Loss: 0.004579664207994938
2018-10-23 02:05:19.688680:	Training iteration: 249800, Loss: 0.0043820771388709545
2018-10-23 02:06:44.169195:	Training iteration: 250000, Loss: 0.0041297911666333675
Checkpoint
2018-10-23 02:08:11.677781:	Training iteration: 250200, Loss: 0.004413563758134842
2018-10-23 02:09:36.267044:	Training iteration: 250400, Loss: 0.00565386563539505
2018-10-23 02:11:00.915797:	Training iteration: 250600, Loss: 0.005835283547639847
2018-10-23 02:12:25.686714:	Training iteration: 250800, Loss: 0.005219269543886185
2018-10-23 02:13:50.813409:	Training iteration: 251000, Loss: 0.004072295036166906
2018-10-23 02:15:15.898029:	Training iteration: 251200, Loss: 0.0052909343503415585
2018-10-23 02:16:41.045752:	Training iteration: 251400, Loss: 0.0038588764145970345
2018-10-23 02:18:05.527158:	Training iteration: 251600, Loss: 0.0041675325483083725
2018-10-23 02:19:30.946951:	Training iteration: 251800, Loss: 0.005774704273790121
2018-10-23 02:20:55.838599:	Training iteration: 252000, Loss: 0.0035849381238222122
2018-10-23 02:22:20.908044:	Training iteration: 252200, Loss: 0.003776178928092122
2018-10-23 02:23:45.556288:	Training iteration: 252400, Loss: 0.0045724003575742245
2018-10-23 02:25:10.450435:	Training iteration: 252600, Loss: 0.006627222057431936
2018-10-23 02:26:35.869313:	Training iteration: 252800, Loss: 0.004584502428770065
2018-10-23 02:28:01.018384:	Training iteration: 253000, Loss: 0.0038384057115763426
2018-10-23 02:29:26.325074:	Training iteration: 253200, Loss: 0.00438601104542613
2018-10-23 02:30:50.742668:	Training iteration: 253400, Loss: 0.004346322733908892
2018-10-23 02:32:16.252497:	Training iteration: 253600, Loss: 0.005627525504678488
2018-10-23 02:33:41.147604:	Training iteration: 253800, Loss: 0.004422375932335854
2018-10-23 02:35:05.937608:	Training iteration: 254000, Loss: 0.005588797386735678
2018-10-23 02:36:30.915007:	Training iteration: 254200, Loss: 0.006139291916042566
2018-10-23 02:37:55.858118:	Training iteration: 254400, Loss: 0.004901979584246874
2018-10-23 02:39:20.462327:	Training iteration: 254600, Loss: 0.004534274805337191
2018-10-23 02:40:45.515262:	Training iteration: 254800, Loss: 0.004761762451380491
2018-10-23 02:42:10.080406:	Training iteration: 255000, Loss: 0.004871793556958437
2018-10-23 02:43:35.199695:	Training iteration: 255200, Loss: 0.005767903756350279
2018-10-23 02:44:59.533154:	Training iteration: 255400, Loss: 0.004035928752273321
2018-10-23 02:46:24.008707:	Training iteration: 255600, Loss: 0.0037443230394273996
2018-10-23 02:47:48.376649:	Training iteration: 255800, Loss: 0.002811051206663251
2018-10-23 02:49:12.647904:	Training iteration: 256000, Loss: 0.004061064682900906
2018-10-23 02:50:36.681305:	Training iteration: 256200, Loss: 0.004769949708133936
2018-10-23 02:52:01.168149:	Training iteration: 256400, Loss: 0.0034815678372979164
2018-10-23 02:52:44.630569: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 718 of 1000
2018-10-23 02:52:48.060344: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 02:53:37.172988:	Training iteration: 256600, Loss: 0.004321616608649492
2018-10-23 02:55:01.602191:	Training iteration: 256800, Loss: 0.004496304783970118
2018-10-23 02:56:26.369901:	Training iteration: 257000, Loss: 0.006105508655309677
2018-10-23 02:57:50.581943:	Training iteration: 257200, Loss: 0.00509141618385911
2018-10-23 02:59:15.146988:	Training iteration: 257400, Loss: 0.005342879798263311
2018-10-23 03:00:39.242347:	Training iteration: 257600, Loss: 0.005900456104427576
2018-10-23 03:02:03.896946:	Training iteration: 257800, Loss: 0.0032049568835645914
2018-10-23 03:03:29.637938:	Training iteration: 258000, Loss: 0.0059088715352118015
2018-10-23 03:04:54.479955:	Training iteration: 258200, Loss: 0.004692342132329941
2018-10-23 03:06:20.929276:	Training iteration: 258400, Loss: 0.00391189381480217
2018-10-23 03:07:45.863364:	Training iteration: 258600, Loss: 0.006075885146856308
2018-10-23 03:09:13.307421:	Training iteration: 258800, Loss: 0.0036282241344451904
2018-10-23 03:10:37.923635:	Training iteration: 259000, Loss: 0.004732516128569841
2018-10-23 03:12:02.982239:	Training iteration: 259200, Loss: 0.005432553123682737
2018-10-23 03:13:27.821813:	Training iteration: 259400, Loss: 0.00518996873870492
2018-10-23 03:14:55.391391:	Training iteration: 259600, Loss: 0.0034204116091132164
2018-10-23 03:16:20.130125:	Training iteration: 259800, Loss: 0.005942333955317736
2018-10-23 03:17:46.038443:	Training iteration: 260000, Loss: 0.005419857334345579
Checkpoint
2018-10-23 03:19:14.435218:	Training iteration: 260200, Loss: 0.006976487580686808
2018-10-23 03:20:38.992693:	Training iteration: 260400, Loss: 0.004731399472802877
2018-10-23 03:22:04.584585:	Training iteration: 260600, Loss: 0.004231275990605354
2018-10-23 03:23:29.216729:	Training iteration: 260800, Loss: 0.0048077767714858055
2018-10-23 03:24:54.118513:	Training iteration: 261000, Loss: 0.0031574645545333624
2018-10-23 03:26:19.016080:	Training iteration: 261200, Loss: 0.004259979818016291
2018-10-23 03:27:45.394619:	Training iteration: 261400, Loss: 0.00474320026114583
2018-10-23 03:29:10.502855:	Training iteration: 261600, Loss: 0.0048961094580590725
2018-10-23 03:30:35.451125:	Training iteration: 261800, Loss: 0.005210084840655327
2018-10-23 03:31:59.910140:	Training iteration: 262000, Loss: 0.005416788626462221
2018-10-23 03:33:24.843234:	Training iteration: 262200, Loss: 0.0061333361081779
2018-10-23 03:34:48.935082:	Training iteration: 262400, Loss: 0.0042057037353515625
2018-10-23 03:36:13.715232:	Training iteration: 262600, Loss: 0.003883672645315528
2018-10-23 03:37:38.459222:	Training iteration: 262800, Loss: 0.005404740571975708
2018-10-23 03:39:02.457516:	Training iteration: 263000, Loss: 0.006221145391464233
2018-10-23 03:40:26.881242:	Training iteration: 263200, Loss: 0.0073546445928514
2018-10-23 03:41:51.302805:	Training iteration: 263400, Loss: 0.0039239302277565
2018-10-23 03:43:16.034734:	Training iteration: 263600, Loss: 0.004619855899363756
2018-10-23 03:44:40.472341:	Training iteration: 263800, Loss: 0.004476919770240784
2018-10-23 03:46:05.306471:	Training iteration: 264000, Loss: 0.005753235425800085
2018-10-23 03:47:29.686321:	Training iteration: 264200, Loss: 0.005506306421011686
2018-10-23 03:48:54.005999:	Training iteration: 264400, Loss: 0.004194668959826231
2018-10-23 03:50:17.953914:	Training iteration: 264600, Loss: 0.006044598761945963
2018-10-23 03:51:46.254989: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 689 of 1000
2018-10-23 03:51:50.301611: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 03:51:56.060867:	Training iteration: 264800, Loss: 0.005070238374173641
2018-10-23 03:53:19.619229:	Training iteration: 265000, Loss: 0.005766888614743948
2018-10-23 03:54:44.296404:	Training iteration: 265200, Loss: 0.003114132909104228
2018-10-23 03:56:08.639708:	Training iteration: 265400, Loss: 0.006478667724877596
2018-10-23 03:57:34.019698:	Training iteration: 265600, Loss: 0.005774026736617088
2018-10-23 03:58:58.457193:	Training iteration: 265800, Loss: 0.0029264118056744337
2018-10-23 04:00:24.129718:	Training iteration: 266000, Loss: 0.00340641220100224
2018-10-23 04:01:48.656565:	Training iteration: 266200, Loss: 0.0036856967490166426
2018-10-23 04:03:13.652798:	Training iteration: 266400, Loss: 0.0036442112177610397
2018-10-23 04:04:38.756668:	Training iteration: 266600, Loss: 0.00587495556101203
2018-10-23 04:06:03.849152:	Training iteration: 266800, Loss: 0.004070552531629801
2018-10-23 04:07:29.577310:	Training iteration: 267000, Loss: 0.005491414573043585
2018-10-23 04:08:53.487161:	Training iteration: 267200, Loss: 0.004796633496880531
2018-10-23 04:10:18.597146:	Training iteration: 267400, Loss: 0.004315605387091637
2018-10-23 04:11:44.085351:	Training iteration: 267600, Loss: 0.005503110587596893
2018-10-23 04:13:08.896915:	Training iteration: 267800, Loss: 0.00533019145950675
2018-10-23 04:14:33.953854:	Training iteration: 268000, Loss: 0.0031001169700175524
2018-10-23 04:15:59.477419:	Training iteration: 268200, Loss: 0.002993215573951602
2018-10-23 04:17:24.273370:	Training iteration: 268400, Loss: 0.0037783216685056686
2018-10-23 04:18:49.962858:	Training iteration: 268600, Loss: 0.0033713001757860184
2018-10-23 04:20:15.374656:	Training iteration: 268800, Loss: 0.0031945195514708757
2018-10-23 04:21:40.664591:	Training iteration: 269000, Loss: 0.0040938411839306355
2018-10-23 04:23:05.526853:	Training iteration: 269200, Loss: 0.004034589510411024
2018-10-23 04:24:30.849172:	Training iteration: 269400, Loss: 0.004889327567070723
2018-10-23 04:25:55.555862:	Training iteration: 269600, Loss: 0.003574036294594407
2018-10-23 04:27:20.888201:	Training iteration: 269800, Loss: 0.003550758585333824
2018-10-23 04:28:45.727012:	Training iteration: 270000, Loss: 0.004688102286309004
Checkpoint
2018-10-23 04:30:12.793538:	Training iteration: 270200, Loss: 0.003587904619053006
2018-10-23 04:31:37.121265:	Training iteration: 270400, Loss: 0.0043927631340920925
2018-10-23 04:33:02.055460:	Training iteration: 270600, Loss: 0.0042841569520533085
2018-10-23 04:34:26.395725:	Training iteration: 270800, Loss: 0.00599574064835906
2018-10-23 04:35:51.079982:	Training iteration: 271000, Loss: 0.0038585152942687273
2018-10-23 04:37:15.390223:	Training iteration: 271200, Loss: 0.0037569766864180565
2018-10-23 04:38:39.859295:	Training iteration: 271400, Loss: 0.005221343133598566
2018-10-23 04:40:04.385658:	Training iteration: 271600, Loss: 0.0029114081989973783
2018-10-23 04:41:28.897415:	Training iteration: 271800, Loss: 0.0038135144859552383
2018-10-23 04:42:53.627025:	Training iteration: 272000, Loss: 0.004291930701583624
2018-10-23 04:44:18.998370:	Training iteration: 272200, Loss: 0.0032733974512666464
2018-10-23 04:45:43.483630:	Training iteration: 272400, Loss: 0.005481796804815531
2018-10-23 04:47:08.148848:	Training iteration: 272600, Loss: 0.004975773394107819
2018-10-23 04:48:32.971922:	Training iteration: 272800, Loss: 0.0050745438784360886
2018-10-23 04:49:57.762922:	Training iteration: 273000, Loss: 0.005458307918161154
2018-10-23 04:51:22.506774:	Training iteration: 273200, Loss: 0.0035751918330788612
2018-10-23 04:52:47.150951:	Training iteration: 273400, Loss: 0.0036308381240814924
2018-10-23 04:53:25.541899: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 645 of 1000
2018-10-23 04:53:29.457064: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 04:54:24.583277:	Training iteration: 273600, Loss: 0.004971932619810104
2018-10-23 04:55:49.043542:	Training iteration: 273800, Loss: 0.00436142273247242
2018-10-23 04:57:12.489151:	Training iteration: 274000, Loss: 0.007456956896930933
2018-10-23 04:58:37.524470:	Training iteration: 274200, Loss: 0.0049247960560023785
2018-10-23 05:00:02.826175:	Training iteration: 274400, Loss: 0.006852210033684969
2018-10-23 05:01:27.474855:	Training iteration: 274600, Loss: 0.005436751991510391
2018-10-23 05:02:52.066728:	Training iteration: 274800, Loss: 0.003520337864756584
2018-10-23 05:04:16.960966:	Training iteration: 275000, Loss: 0.0029206902254372835
2018-10-23 05:05:42.452099:	Training iteration: 275200, Loss: 0.006909344810992479
2018-10-23 05:07:07.709673:	Training iteration: 275400, Loss: 0.010017328895628452
2018-10-23 05:08:31.303546:	Training iteration: 275600, Loss: 0.007224878761917353
2018-10-23 05:09:55.628203:	Training iteration: 275800, Loss: 0.007641410920768976
2018-10-23 05:11:20.480659:	Training iteration: 276000, Loss: 0.0038308333605527878
2018-10-23 05:12:44.983032:	Training iteration: 276200, Loss: 0.004732434172183275
2018-10-23 05:14:09.591638:	Training iteration: 276400, Loss: 0.004481828305870295
2018-10-23 05:15:34.125254:	Training iteration: 276600, Loss: 0.0036644740030169487
2018-10-23 05:16:58.809361:	Training iteration: 276800, Loss: 0.004089090507477522
2018-10-23 05:18:23.959530:	Training iteration: 277000, Loss: 0.004919794853776693
2018-10-23 05:19:48.344493:	Training iteration: 277200, Loss: 0.006487933453172445
2018-10-23 05:21:13.314652:	Training iteration: 277400, Loss: 0.009249995462596416
2018-10-23 05:22:37.906826:	Training iteration: 277600, Loss: 0.004287257324904203
2018-10-23 05:24:02.509051:	Training iteration: 277800, Loss: 0.006078364793211222
2018-10-23 05:25:27.080430:	Training iteration: 278000, Loss: 0.0061220452189445496
2018-10-23 05:26:51.203684:	Training iteration: 278200, Loss: 0.006708489265292883
2018-10-23 05:28:15.624891:	Training iteration: 278400, Loss: 0.004527457524091005
2018-10-23 05:29:40.420622:	Training iteration: 278600, Loss: 0.0038837685715407133
2018-10-23 05:31:04.392083:	Training iteration: 278800, Loss: 0.009271170012652874
2018-10-23 05:32:28.157663:	Training iteration: 279000, Loss: 0.006072232499718666
2018-10-23 05:33:52.727948:	Training iteration: 279200, Loss: 0.0034369423519819975
2018-10-23 05:35:17.132563:	Training iteration: 279400, Loss: 0.004106924403458834
2018-10-23 05:36:41.332304:	Training iteration: 279600, Loss: 0.003491326468065381
2018-10-23 05:38:05.094039:	Training iteration: 279800, Loss: 0.0037504853680729866
2018-10-23 05:39:29.700932:	Training iteration: 280000, Loss: 0.005015298258513212
Checkpoint
2018-10-23 05:40:56.423972:	Training iteration: 280200, Loss: 0.004540794063359499
2018-10-23 05:42:20.426394:	Training iteration: 280400, Loss: 0.00465208338573575
2018-10-23 05:43:45.185633:	Training iteration: 280600, Loss: 0.004924119915813208
2018-10-23 05:45:09.399793:	Training iteration: 280800, Loss: 0.003679730696603656
2018-10-23 05:46:32.786129:	Training iteration: 281000, Loss: 0.005943296942859888
2018-10-23 05:47:57.545496:	Training iteration: 281200, Loss: 0.003948007710278034
2018-10-23 05:49:22.049546:	Training iteration: 281400, Loss: 0.006294913589954376
2018-10-23 05:50:47.312520:	Training iteration: 281600, Loss: 0.00474024610593915
2018-10-23 05:52:11.512912:	Training iteration: 281800, Loss: 0.004122509155422449
2018-10-23 05:53:35.693517:	Training iteration: 282000, Loss: 0.004200128838419914
2018-10-23 05:54:59.994253:	Training iteration: 282200, Loss: 0.00349438120611012
2018-10-23 05:56:24.716663:	Training iteration: 282400, Loss: 0.005062159616500139
2018-10-23 05:57:48.902203:	Training iteration: 282600, Loss: 0.0038826807867735624
2018-10-23 05:59:14.124781:	Training iteration: 282800, Loss: 0.0055005974136292934
2018-10-23 06:00:38.883058:	Training iteration: 283000, Loss: 0.006792211439460516
2018-10-23 06:02:02.726172:	Training iteration: 283200, Loss: 0.004407473374158144
2018-10-23 06:03:28.049573:	Training iteration: 283400, Loss: 0.005831323564052582
2018-10-23 06:04:54.567139:	Training iteration: 283600, Loss: 0.0067749000154435635
2018-10-23 06:06:19.012969:	Training iteration: 283800, Loss: 0.005311686545610428
2018-10-23 06:07:43.648663:	Training iteration: 284000, Loss: 0.005356306675821543
2018-10-23 06:09:08.247374:	Training iteration: 284200, Loss: 0.006478739436715841
2018-10-23 06:10:32.774065:	Training iteration: 284400, Loss: 0.004519290756434202
2018-10-23 06:11:57.733101:	Training iteration: 284600, Loss: 0.005554328206926584
2018-10-23 06:13:22.333197:	Training iteration: 284800, Loss: 0.0068352906964719296
2018-10-23 06:14:47.105795:	Training iteration: 285000, Loss: 0.005328273866325617
2018-10-23 06:16:11.307825:	Training iteration: 285200, Loss: 0.009875424206256866
2018-10-23 06:17:36.202934:	Training iteration: 285400, Loss: 0.004370806273072958
2018-10-23 06:19:00.401666:	Training iteration: 285600, Loss: 0.007253685500472784
2018-10-23 06:20:24.749877:	Training iteration: 285800, Loss: 0.005997943226248026
2018-10-23 06:21:49.161884:	Training iteration: 286000, Loss: 0.005564613733440638
2018-10-23 06:23:13.187903:	Training iteration: 286200, Loss: 0.002712169662117958
2018-10-23 06:24:37.036609:	Training iteration: 286400, Loss: 0.010096156969666481
2018-10-23 06:26:02.312445:	Training iteration: 286600, Loss: 0.006727123633027077
2018-10-23 06:28:00.187703:	Training iteration: 286800, Loss: 0.00707256281748414
2018-10-23 06:39:58.749533:	Training iteration: 287000, Loss: 0.006696542724967003
2018-10-23 06:57:59.706960:	Training iteration: 287200, Loss: 0.003348207101225853
2018-10-23 06:59:13.933084:	Training iteration: 287400, Loss: 0.004841626156121492
2018-10-23 07:00:30.425550:	Training iteration: 287600, Loss: 0.0044386484660208225
2018-10-23 07:01:49.088378:	Training iteration: 287800, Loss: 0.004373843315988779
2018-10-23 07:03:09.394585:	Training iteration: 288000, Loss: 0.005088191479444504
2018-10-23 07:04:30.425837:	Training iteration: 288200, Loss: 0.004398609045892954
2018-10-23 07:05:55.060418:	Training iteration: 288400, Loss: 0.0082034170627594
2018-10-23 07:19:25.745096:	Training iteration: 288600, Loss: 0.008080695755779743
2018-10-23 07:20:39.858121:	Training iteration: 288800, Loss: 0.0049512190744280815
2018-10-23 07:21:54.689016:	Training iteration: 289000, Loss: 0.005395233165472746
2018-10-23 07:23:12.956264:	Training iteration: 289200, Loss: 0.007070703897625208
2018-10-23 07:24:35.223365:	Training iteration: 289400, Loss: 0.005305187311023474
2018-10-23 07:25:58.450219:	Training iteration: 289600, Loss: 0.009071165695786476
2018-10-23 07:40:25.012709:	Training iteration: 289800, Loss: 0.004667004104703665
2018-10-23 07:41:44.925472:	Training iteration: 290000, Loss: 0.0038396550808101892
Checkpoint
2018-10-23 08:02:30.656282:	Training iteration: 290200, Loss: 0.004636072088032961
