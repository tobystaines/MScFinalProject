INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "148"
Experiment ID: 148
Preparing dataset
Dataset ready
2018-10-20 21:09:08.004750: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-10-20 21:09:08.141720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-20 21:09:08.156579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:27:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-10-20 21:09:08.156623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:27:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Running initialisation test
Starting testing
2018-10-20 21:09:13.399151:	Entering test loop
2018-10-20 21:09:23.709723: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 609 of 1000
2018-10-20 21:09:29.348075: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 21:09:30.304304:	Testing iteration: 0, Loss: 0.2584666311740875
2018-10-20 21:11:06.655038:	Testing iteration: 200, Loss: 0.35236865282058716
2018-10-20 21:13:06.576854:	Testing iteration: 400, Loss: 0.17463159561157227
2018-10-20 21:15:31.916083:	Testing iteration: 600, Loss: 0.19756612181663513
2018-10-20 21:18:19.849227:	Testing iteration: 800, Loss: 0.43594229221343994
2018-10-20 21:21:34.637201:	Testing iteration: 1000, Loss: 0.4084331691265106
2018-10-20 21:25:16.106131:	Testing iteration: 1200, Loss: 0.19151359796524048
2018-10-20 21:29:23.258157:	Testing iteration: 1400, Loss: 0.21892684698104858
2018-10-20 21:33:53.752983:	Testing iteration: 1600, Loss: 0.17358669638633728
2018-10-20 21:34:23.727193: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 525 of 1000
2018-10-20 21:34:30.369103: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 21:39:07.191245:	Testing iteration: 1800, Loss: 0.23300819098949432
2018-10-20 21:44:27.925199:	Testing iteration: 2000, Loss: 0.26893895864486694
2018-10-20 21:50:15.403835:	Testing iteration: 2200, Loss: 0.26706430315971375
2018-10-20 21:56:28.254670:	Testing iteration: 2400, Loss: 0.49986693263053894
2018-10-20 22:03:16.659852:	Testing iteration: 2600, Loss: 0.26598137617111206
2018-10-20 22:11:07.468978:	Testing iteration: 2800, Loss: 0.31968826055526733
2018-10-20 22:18:53.706304:	Testing iteration: 3000, Loss: 0.29920893907546997
2018-10-20 22:26:49.332904:	Testing iteration: 3200, Loss: 0.4115212559700012
2018-10-20 22:28:08.057959: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 680 of 1000
2018-10-20 22:28:12.308997: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 22:35:25.404736:	Testing iteration: 3400, Loss: 0.2831465005874634
2018-10-20 22:44:16.776402:	Testing iteration: 3600, Loss: 0.40030062198638916
2018-10-20 22:53:33.172603:	Testing iteration: 3800, Loss: 0.20456837117671967
2018-10-20 23:03:36.335305:	Testing iteration: 4000, Loss: 0.30739113688468933
2018-10-20 23:14:20.150061:	Testing iteration: 4200, Loss: 0.23805704712867737
2018-10-20 23:24:55.805317:	Testing iteration: 4400, Loss: 0.18137609958648682
2018-10-20 23:36:01.176687:	Testing iteration: 4600, Loss: 0.3323960304260254
2018-10-20 23:47:33.648378:	Testing iteration: 4800, Loss: 0.3064291477203369
2018-10-20 23:50:12.040679: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 698 of 1000
2018-10-20 23:50:16.207522: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 23:59:45.120678:	Testing iteration: 5000, Loss: 0.15517136454582214
2018-10-21 00:12:38.127956:	Testing iteration: 5200, Loss: 0.3063785433769226
2018-10-21 00:25:29.395845:	Testing iteration: 5400, Loss: 0.23009797930717468
2018-10-21 00:38:48.543820:	Testing iteration: 5600, Loss: 0.23583734035491943
2018-10-21 00:52:30.841932:	Testing iteration: 5800, Loss: 0.28511953353881836
2018-10-21 01:06:55.282309:	Testing iteration: 6000, Loss: 0.27564072608947754
2018-10-21 01:21:37.467277:	Testing iteration: 6200, Loss: 0.20897743105888367
2018-10-21 01:37:02.463376:	Testing iteration: 6400, Loss: 0.3762286305427551
2018-10-21 01:41:10.385793: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 607 of 1000
2018-10-21 01:41:16.331650: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 01:51:32.111094:	Testing iteration: 6600, Loss: 0.19712238013744354
2018-10-21 02:06:24.474940:	Testing iteration: 6800, Loss: 0.11457601934671402
2018-10-21 02:21:30.250371:	Testing iteration: 7000, Loss: 0.3137941360473633
2018-10-21 02:36:54.755241:	Testing iteration: 7200, Loss: 0.1933240443468094
2018-10-21 02:52:49.357593:	Testing iteration: 7400, Loss: 0.20052799582481384
2018-10-21 03:10:35.909101:	Testing iteration: 7600, Loss: 0.24109528958797455
2018-10-21 03:27:22.611392:	Testing iteration: 7800, Loss: 0.2977597117424011
2018-10-21 03:44:26.533196:	Testing iteration: 8000, Loss: 0.3005787134170532
2018-10-21 04:02:05.475946:	Testing iteration: 8200, Loss: 0.3033231794834137
2018-10-21 04:20:22.593891:	Testing iteration: 8400, Loss: 0.18622562289237976
2018-10-21 04:38:40.218548:	Testing iteration: 8600, Loss: 0.22702699899673462
2018-10-21 04:57:28.530392:	Testing iteration: 8800, Loss: 0.19741038978099823
2018-10-21 05:17:03.447876:	Testing iteration: 9000, Loss: 0.3022823631763458
2018-10-21 05:36:41.790992:	Testing iteration: 9200, Loss: 0.19890469312667847
2018-10-21 05:56:51.250352:	Testing iteration: 9400, Loss: 0.27569031715393066
Test pass complete
Mean loss over test set: 0.2682450266733128
Data saved to dumps/148 for later audio metric calculation
Starting training
2018-10-21 05:59:05.977082: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 614 of 1000
2018-10-21 05:59:10.704112: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 05:59:34.478249:	Training iteration: 200, Loss: 0.23657497763633728
2018-10-21 05:59:58.118817:	Training iteration: 400, Loss: 0.23348908126354218
2018-10-21 06:00:21.869464:	Training iteration: 600, Loss: 0.14186978340148926
2018-10-21 06:00:45.597959:	Training iteration: 800, Loss: 0.1518852710723877
2018-10-21 06:01:09.297803:	Training iteration: 1000, Loss: 0.2037995308637619
2018-10-21 06:01:33.060280:	Training iteration: 1200, Loss: 0.33486610651016235
2018-10-21 06:01:56.737436:	Training iteration: 1400, Loss: 0.283541202545166
2018-10-21 06:02:28.817076:	Training iteration: 1600, Loss: 0.12305489182472229
2018-10-21 06:02:52.574993:	Training iteration: 1800, Loss: 0.20242935419082642
2018-10-21 06:03:16.607827:	Training iteration: 2000, Loss: 0.22235527634620667
2018-10-21 06:03:41.666196:	Training iteration: 2200, Loss: 0.15255744755268097
2018-10-21 06:04:07.195842:	Training iteration: 2400, Loss: 0.18285992741584778
2018-10-21 06:04:32.678131:	Training iteration: 2600, Loss: 0.16881580650806427
2018-10-21 06:04:57.988961:	Training iteration: 2800, Loss: 0.1921704113483429
2018-10-21 06:05:23.854236:	Training iteration: 3000, Loss: 0.16980582475662231
2018-10-21 06:05:49.698578:	Training iteration: 3200, Loss: 0.2618377208709717
2018-10-21 06:06:15.444445:	Training iteration: 3400, Loss: 0.1267564296722412
2018-10-21 06:06:41.344678:	Training iteration: 3600, Loss: 0.25795042514801025
2018-10-21 06:07:08.313657:	Training iteration: 3800, Loss: 0.2994891405105591
2018-10-21 06:07:33.815916:	Training iteration: 4000, Loss: 0.23918801546096802
2018-10-21 06:08:00.418970:	Training iteration: 4200, Loss: 0.19350066781044006
2018-10-21 06:08:26.446578:	Training iteration: 4400, Loss: 0.17714151740074158
2018-10-21 06:08:52.556657:	Training iteration: 4600, Loss: 0.15009164810180664
2018-10-21 06:09:19.115565:	Training iteration: 4800, Loss: 0.1114514023065567
2018-10-21 06:09:45.004597:	Training iteration: 5000, Loss: 0.2186104953289032
2018-10-21 06:10:11.255178:	Training iteration: 5200, Loss: 0.203346386551857
2018-10-21 06:10:37.485154:	Training iteration: 5400, Loss: 0.1384221911430359
2018-10-21 06:11:03.763047:	Training iteration: 5600, Loss: 0.2916020154953003
2018-10-21 06:11:30.035395:	Training iteration: 5800, Loss: 0.15342462062835693
2018-10-21 06:11:56.578085:	Training iteration: 6000, Loss: 0.14261171221733093
2018-10-21 06:12:24.054923:	Training iteration: 6200, Loss: 0.14130763709545135
2018-10-21 06:12:50.322500:	Training iteration: 6400, Loss: 0.07879269868135452
2018-10-21 06:13:16.610669:	Training iteration: 6600, Loss: 0.11536656320095062
2018-10-21 06:13:42.974123:	Training iteration: 6800, Loss: 0.17031827569007874
2018-10-21 06:14:09.639014:	Training iteration: 7000, Loss: 0.14001432061195374
2018-10-21 06:14:35.488236:	Training iteration: 7200, Loss: 0.20406824350357056
2018-10-21 06:15:01.783395:	Training iteration: 7400, Loss: 0.14490966498851776
2018-10-21 06:15:28.505205:	Training iteration: 7600, Loss: 0.11297338455915451
2018-10-21 06:15:55.119884:	Training iteration: 7800, Loss: 0.1995915025472641
2018-10-21 06:16:23.088438:	Training iteration: 8000, Loss: 0.11954429000616074
2018-10-21 06:16:49.687856:	Training iteration: 8200, Loss: 0.1985853612422943
2018-10-21 06:17:15.411951:	Training iteration: 8400, Loss: 0.17276884615421295
2018-10-21 06:17:42.249413:	Training iteration: 8600, Loss: 0.15432946383953094
2018-10-21 06:18:08.821116:	Training iteration: 8800, Loss: 0.1037624180316925
2018-10-21 06:18:35.076958:	Training iteration: 9000, Loss: 0.21469876170158386
2018-10-21 06:19:02.359074:	Training iteration: 9200, Loss: 0.17579829692840576
2018-10-21 06:19:28.871404:	Training iteration: 9400, Loss: 0.12879648804664612
2018-10-21 06:19:55.333003:	Training iteration: 9600, Loss: 0.11694639921188354
2018-10-21 06:20:22.115250:	Training iteration: 9800, Loss: 0.14815665781497955
2018-10-21 06:20:48.353739:	Training iteration: 10000, Loss: 0.1435011625289917
2018-10-21 06:21:15.381630:	Training iteration: 10200, Loss: 0.19103378057479858
2018-10-21 06:21:42.043065:	Training iteration: 10400, Loss: 0.10914258658885956
2018-10-21 06:22:08.768780:	Training iteration: 10600, Loss: 0.12168818712234497
2018-10-21 06:22:35.788594:	Training iteration: 10800, Loss: 0.1066097766160965
2018-10-21 06:23:02.485076:	Training iteration: 11000, Loss: 0.14510880410671234
2018-10-21 06:23:29.184864:	Training iteration: 11200, Loss: 0.12398897856473923
2018-10-21 06:23:55.209027:	Training iteration: 11400, Loss: 0.15366196632385254
2018-10-21 06:24:21.782399:	Training iteration: 11600, Loss: 0.11902280151844025
2018-10-21 06:24:49.184336:	Training iteration: 11800, Loss: 0.11878030002117157
2018-10-21 06:25:16.123765:	Training iteration: 12000, Loss: 0.12283959984779358
2018-10-21 06:25:43.087217:	Training iteration: 12200, Loss: 0.09585043787956238
2018-10-21 06:26:19.580296: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 767 of 1000
2018-10-21 06:26:22.378881: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 06:26:23.011684:	Training iteration: 12400, Loss: 0.08822490274906158
2018-10-21 06:26:47.912617:	Training iteration: 12600, Loss: 0.13247525691986084
2018-10-21 06:27:14.277741:	Training iteration: 12800, Loss: 0.14165553450584412
2018-10-21 06:27:40.976532:	Training iteration: 13000, Loss: 0.13325434923171997
2018-10-21 06:28:06.363580:	Training iteration: 13200, Loss: 0.1008923351764679
2018-10-21 06:28:32.786772:	Training iteration: 13400, Loss: 0.08547432720661163
2018-10-21 06:28:58.519707:	Training iteration: 13600, Loss: 0.12166159600019455
2018-10-21 06:29:26.329262:	Training iteration: 13800, Loss: 0.18005730211734772
2018-10-21 06:29:54.203679:	Training iteration: 14000, Loss: 0.13580967485904694
2018-10-21 06:30:21.223837:	Training iteration: 14200, Loss: 0.1393139362335205
2018-10-21 06:30:47.987239:	Training iteration: 14400, Loss: 0.1434783637523651
2018-10-21 06:31:15.089385:	Training iteration: 14600, Loss: 0.11308223754167557
2018-10-21 06:31:41.872725:	Training iteration: 14800, Loss: 0.09789521247148514
2018-10-21 06:32:08.897939:	Training iteration: 15000, Loss: 0.157161682844162
2018-10-21 06:32:35.798949:	Training iteration: 15200, Loss: 0.18247704207897186
2018-10-21 06:33:02.486356:	Training iteration: 15400, Loss: 0.1506153792142868
2018-10-21 06:33:28.726032:	Training iteration: 15600, Loss: 0.08969302475452423
2018-10-21 06:33:54.844862:	Training iteration: 15800, Loss: 0.08799801766872406
2018-10-21 06:34:21.571434:	Training iteration: 16000, Loss: 0.1677878201007843
2018-10-21 06:34:48.114284:	Training iteration: 16200, Loss: 0.1443306803703308
2018-10-21 06:35:14.097396:	Training iteration: 16400, Loss: 0.08423379063606262
2018-10-21 06:35:40.815914:	Training iteration: 16600, Loss: 0.11591748893260956
2018-10-21 06:36:07.481823:	Training iteration: 16800, Loss: 0.11571106314659119
2018-10-21 06:36:33.813701:	Training iteration: 17000, Loss: 0.1427672952413559
2018-10-21 06:37:00.354961:	Training iteration: 17200, Loss: 0.1681060791015625
2018-10-21 06:37:26.905317:	Training iteration: 17400, Loss: 0.10911676287651062
2018-10-21 06:37:54.270976:	Training iteration: 17600, Loss: 0.09416660666465759
2018-10-21 06:38:20.298551:	Training iteration: 17800, Loss: 0.1865828037261963
2018-10-21 06:38:48.219445:	Training iteration: 18000, Loss: 0.1461385190486908
2018-10-21 06:39:14.254252:	Training iteration: 18200, Loss: 0.13065367937088013
2018-10-21 06:39:40.856634:	Training iteration: 18400, Loss: 0.11880812793970108
2018-10-21 06:40:08.193765:	Training iteration: 18600, Loss: 0.08965658396482468
2018-10-21 06:40:35.425413:	Training iteration: 18800, Loss: 0.08875386416912079
2018-10-21 06:41:01.876686:	Training iteration: 19000, Loss: 0.12283137440681458
2018-10-21 06:41:28.691486:	Training iteration: 19200, Loss: 0.07921847701072693
2018-10-21 06:41:56.027092:	Training iteration: 19400, Loss: 0.10344729572534561
2018-10-21 06:42:22.735586:	Training iteration: 19600, Loss: 0.20563042163848877
2018-10-21 06:42:49.742294:	Training iteration: 19800, Loss: 0.18067187070846558
2018-10-21 06:43:16.485578:	Training iteration: 20000, Loss: 0.1447916179895401
2018-10-21 06:43:43.276404:	Training iteration: 20200, Loss: 0.11964668333530426
2018-10-21 06:44:09.766385:	Training iteration: 20400, Loss: 0.12173473089933395
2018-10-21 06:44:36.082309:	Training iteration: 20600, Loss: 0.14589470624923706
2018-10-21 06:45:03.432101:	Training iteration: 20800, Loss: 0.17450012266635895
2018-10-21 06:45:30.726056:	Training iteration: 21000, Loss: 0.16177807748317719
2018-10-21 06:45:56.558683:	Training iteration: 21200, Loss: 0.12357316166162491
2018-10-21 06:46:23.448592:	Training iteration: 21400, Loss: 0.12905265390872955
2018-10-21 06:46:50.500665:	Training iteration: 21600, Loss: 0.11720284819602966
2018-10-21 06:47:17.076356:	Training iteration: 21800, Loss: 0.11170252412557602
2018-10-21 06:47:43.308053:	Training iteration: 22000, Loss: 0.11404496431350708
2018-10-21 06:48:10.474428:	Training iteration: 22200, Loss: 0.11329768598079681
2018-10-21 06:48:36.852427:	Training iteration: 22400, Loss: 0.13194778561592102
2018-10-21 06:49:04.347340:	Training iteration: 22600, Loss: 0.13926109671592712
2018-10-21 06:49:31.902437:	Training iteration: 22800, Loss: 0.11501958221197128
2018-10-21 06:49:59.099582:	Training iteration: 23000, Loss: 0.156609445810318
2018-10-21 06:50:26.011649:	Training iteration: 23200, Loss: 0.07937317341566086
2018-10-21 06:50:53.080114:	Training iteration: 23400, Loss: 0.08089078962802887
2018-10-21 06:51:19.732077:	Training iteration: 23600, Loss: 0.1316399872303009
2018-10-21 06:51:46.711195:	Training iteration: 23800, Loss: 0.11523202061653137
2018-10-21 06:52:13.283026:	Training iteration: 24000, Loss: 0.08543148636817932
2018-10-21 06:52:40.132116:	Training iteration: 24200, Loss: 0.11299454420804977
2018-10-21 06:53:07.196535:	Training iteration: 24400, Loss: 0.09208158403635025
2018-10-21 06:53:34.378928:	Training iteration: 24600, Loss: 0.09405724704265594
2018-10-21 06:54:01.494545:	Training iteration: 24800, Loss: 0.1411924660205841
2018-10-21 06:54:20.113998: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 792 of 1000
2018-10-21 06:54:22.504658: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 06:54:39.359121:	Training iteration: 25000, Loss: 0.08517341315746307
2018-10-21 06:55:05.881140:	Training iteration: 25200, Loss: 0.11867222189903259
2018-10-21 06:55:32.841572:	Training iteration: 25400, Loss: 0.11768423020839691
2018-10-21 06:55:59.518650:	Training iteration: 25600, Loss: 0.07786650955677032
2018-10-21 06:56:26.109251:	Training iteration: 25800, Loss: 0.06624587625265121
2018-10-21 06:56:52.610680:	Training iteration: 26000, Loss: 0.0771208107471466
2018-10-21 06:57:19.947683:	Training iteration: 26200, Loss: 0.07454288005828857
2018-10-21 06:57:45.861032:	Training iteration: 26400, Loss: 0.07079369574785233
2018-10-21 06:58:12.494352:	Training iteration: 26600, Loss: 0.12329989671707153
2018-10-21 06:58:38.652992:	Training iteration: 26800, Loss: 0.08271881937980652
2018-10-21 06:59:06.006500:	Training iteration: 27000, Loss: 0.07649730890989304
2018-10-21 06:59:33.825566:	Training iteration: 27200, Loss: 0.05256516486406326
2018-10-21 07:00:01.472974:	Training iteration: 27400, Loss: 0.07626021653413773
2018-10-21 07:00:27.701698:	Training iteration: 27600, Loss: 0.0939813032746315
2018-10-21 07:00:55.146305:	Training iteration: 27800, Loss: 0.08493070304393768
2018-10-21 07:01:21.768736:	Training iteration: 28000, Loss: 0.07177552580833435
2018-10-21 07:01:49.604574:	Training iteration: 28200, Loss: 0.06754803657531738
2018-10-21 07:02:15.619647:	Training iteration: 28400, Loss: 0.0672549232840538
2018-10-21 07:02:42.507932:	Training iteration: 28600, Loss: 0.06969501078128815
2018-10-21 07:03:09.306812:	Training iteration: 28800, Loss: 0.1035786122083664
2018-10-21 07:03:36.173059:	Training iteration: 29000, Loss: 0.09159724414348602
2018-10-21 07:04:02.916055:	Training iteration: 29200, Loss: 0.07183365523815155
2018-10-21 07:04:29.106119:	Training iteration: 29400, Loss: 0.07952467352151871
2018-10-21 07:04:55.771413:	Training iteration: 29600, Loss: 0.11928023397922516
2018-10-21 07:05:24.714685:	Training iteration: 29800, Loss: 0.0645144134759903
2018-10-21 07:05:51.547545:	Training iteration: 30000, Loss: 0.10571631044149399
2018-10-21 07:06:18.235790:	Training iteration: 30200, Loss: 0.0730988085269928
2018-10-21 07:06:44.167683:	Training iteration: 30400, Loss: 0.09630725532770157
2018-10-21 07:07:11.316747:	Training iteration: 30600, Loss: 0.11444105207920074
2018-10-21 07:07:37.990311:	Training iteration: 30800, Loss: 0.14601659774780273
2018-10-21 07:08:04.343553:	Training iteration: 31000, Loss: 0.07831704616546631
2018-10-21 07:08:31.507671:	Training iteration: 31200, Loss: 0.10215239971876144
2018-10-21 07:08:58.138576:	Training iteration: 31400, Loss: 0.09228172898292542
2018-10-21 07:09:24.642720:	Training iteration: 31600, Loss: 0.0881103128194809
2018-10-21 07:09:51.251239:	Training iteration: 31800, Loss: 0.0805654376745224
2018-10-21 07:10:17.733828:	Training iteration: 32000, Loss: 0.08878567814826965
2018-10-21 07:10:44.629403:	Training iteration: 32200, Loss: 0.09575412422418594
2018-10-21 07:11:10.304757:	Training iteration: 32400, Loss: 0.08437451720237732
2018-10-21 07:11:36.776021:	Training iteration: 32600, Loss: 0.0971250832080841
2018-10-21 07:12:04.276509:	Training iteration: 32800, Loss: 0.06958018243312836
2018-10-21 07:12:31.196728:	Training iteration: 33000, Loss: 0.08077020198106766
2018-10-21 07:12:57.842194:	Training iteration: 33200, Loss: 0.11870225518941879
2018-10-21 07:13:24.527592:	Training iteration: 33400, Loss: 0.08022066205739975
2018-10-21 07:13:52.309751:	Training iteration: 33600, Loss: 0.1603553295135498
2018-10-21 07:14:18.954045:	Training iteration: 33800, Loss: 0.09384682029485703
2018-10-21 07:14:44.945476:	Training iteration: 34000, Loss: 0.058908507227897644
2018-10-21 07:15:11.418384:	Training iteration: 34200, Loss: 0.06994335353374481
2018-10-21 07:15:38.292127:	Training iteration: 34400, Loss: 0.1179443746805191
2018-10-21 07:16:05.163502:	Training iteration: 34600, Loss: 0.10242149233818054
2018-10-21 07:16:32.088959:	Training iteration: 34800, Loss: 0.08528022468090057
2018-10-21 07:16:58.796118:	Training iteration: 35000, Loss: 0.10522440820932388
2018-10-21 07:17:25.262814:	Training iteration: 35200, Loss: 0.05178489536046982
2018-10-21 07:17:51.251894:	Training iteration: 35400, Loss: 0.12509410083293915
2018-10-21 07:18:18.292866:	Training iteration: 35600, Loss: 0.06505417823791504
2018-10-21 07:18:44.644126:	Training iteration: 35800, Loss: 0.1389792412519455
2018-10-21 07:19:11.200215:	Training iteration: 36000, Loss: 0.1025388315320015
2018-10-21 07:19:37.738510:	Training iteration: 36200, Loss: 0.10286325216293335
2018-10-21 07:20:03.920094:	Training iteration: 36400, Loss: 0.08460161089897156
2018-10-21 07:20:30.618455:	Training iteration: 36600, Loss: 0.0813719779253006
2018-10-21 07:20:59.032393:	Training iteration: 36800, Loss: 0.09384208917617798
2018-10-21 07:21:25.625857:	Training iteration: 37000, Loss: 0.1042892187833786
2018-10-21 07:21:51.927401:	Training iteration: 37200, Loss: 0.08415648341178894
2018-10-21 07:22:19.357847: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 789 of 1000
2018-10-21 07:22:21.920073: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 07:22:31.221536:	Training iteration: 37400, Loss: 0.15927720069885254
2018-10-21 07:22:56.959463:	Training iteration: 37600, Loss: 0.08898692578077316
2018-10-21 07:23:23.494731:	Training iteration: 37800, Loss: 0.139888733625412
2018-10-21 07:23:50.142069:	Training iteration: 38000, Loss: 0.09836572408676147
2018-10-21 07:24:16.985626:	Training iteration: 38200, Loss: 0.16580148041248322
2018-10-21 07:24:43.414925:	Training iteration: 38400, Loss: 0.08603742718696594
2018-10-21 07:25:09.781816:	Training iteration: 38600, Loss: 0.1378268003463745
2018-10-21 07:25:36.833739:	Training iteration: 38800, Loss: 0.18850502371788025
2018-10-21 07:26:03.476863:	Training iteration: 39000, Loss: 0.18298596143722534
2018-10-21 07:26:29.306356:	Training iteration: 39200, Loss: 0.12551994621753693
2018-10-21 07:26:56.079551:	Training iteration: 39400, Loss: 0.22053278982639313
2018-10-21 07:27:22.689660:	Training iteration: 39600, Loss: 0.1461719125509262
2018-10-21 07:27:48.279032:	Training iteration: 39800, Loss: 0.12407456338405609
2018-10-21 07:28:15.504898:	Training iteration: 40000, Loss: 0.15295520424842834
2018-10-21 07:28:42.022921:	Training iteration: 40200, Loss: 0.10845623910427094
2018-10-21 07:29:09.448002:	Training iteration: 40400, Loss: 0.15188124775886536
2018-10-21 07:29:35.956025:	Training iteration: 40600, Loss: 0.10084015876054764
2018-10-21 07:30:03.497194:	Training iteration: 40800, Loss: 0.10198354721069336
2018-10-21 07:30:30.405813:	Training iteration: 41000, Loss: 0.23115086555480957
2018-10-21 07:30:57.258606:	Training iteration: 41200, Loss: 0.10063618421554565
2018-10-21 07:31:23.976648:	Training iteration: 41400, Loss: 0.11937159299850464
2018-10-21 07:31:50.807620:	Training iteration: 41600, Loss: 0.07490109652280807
2018-10-21 07:32:17.444816:	Training iteration: 41800, Loss: 0.091281957924366
2018-10-21 07:32:44.977183:	Training iteration: 42000, Loss: 0.13838444650173187
2018-10-21 07:33:10.964365:	Training iteration: 42200, Loss: 0.16747929155826569
2018-10-21 07:33:37.871250:	Training iteration: 42400, Loss: 0.10937166213989258
2018-10-21 07:34:04.173614:	Training iteration: 42600, Loss: 0.20862439274787903
2018-10-21 07:34:31.367690:	Training iteration: 42800, Loss: 0.1859912872314453
2018-10-21 07:34:58.488864:	Training iteration: 43000, Loss: 0.17534279823303223
2018-10-21 07:35:24.903635:	Training iteration: 43200, Loss: 0.10617317259311676
2018-10-21 07:35:51.417606:	Training iteration: 43400, Loss: 0.13226845860481262
2018-10-21 07:36:18.682580:	Training iteration: 43600, Loss: 0.08656905591487885
2018-10-21 07:36:44.990144:	Training iteration: 43800, Loss: 0.08943764120340347
2018-10-21 07:37:11.271308:	Training iteration: 44000, Loss: 0.14851722121238708
2018-10-21 07:37:37.730273:	Training iteration: 44200, Loss: 0.1796574741601944
2018-10-21 07:38:04.521601:	Training iteration: 44400, Loss: 0.13533276319503784
2018-10-21 07:38:32.107223:	Training iteration: 44600, Loss: 0.132709801197052
2018-10-21 07:38:58.903530:	Training iteration: 44800, Loss: 0.1609218716621399
2018-10-21 07:39:25.401417:	Training iteration: 45000, Loss: 0.13850842416286469
2018-10-21 07:39:51.678130:	Training iteration: 45200, Loss: 0.10626465082168579
2018-10-21 07:40:18.973434:	Training iteration: 45400, Loss: 0.1915959119796753
2018-10-21 07:40:46.274591:	Training iteration: 45600, Loss: 0.13748107850551605
2018-10-21 07:41:12.815840:	Training iteration: 45800, Loss: 0.1840382069349289
2018-10-21 07:41:39.159496:	Training iteration: 46000, Loss: 0.12208780646324158
2018-10-21 07:42:06.136041:	Training iteration: 46200, Loss: 0.1231536865234375
2018-10-21 07:42:33.102048:	Training iteration: 46400, Loss: 0.1170395165681839
2018-10-21 07:42:59.092756:	Training iteration: 46600, Loss: 0.10943780094385147
2018-10-21 07:43:25.883528:	Training iteration: 46800, Loss: 0.19687172770500183
2018-10-21 07:43:51.957784:	Training iteration: 47000, Loss: 0.10378250479698181
2018-10-21 07:44:18.990257:	Training iteration: 47200, Loss: 0.15605580806732178
2018-10-21 07:44:46.108206:	Training iteration: 47400, Loss: 0.11434680223464966
2018-10-21 07:45:13.126985:	Training iteration: 47600, Loss: 0.16570745408535004
2018-10-21 07:45:39.951228:	Training iteration: 47800, Loss: 0.13742578029632568
2018-10-21 07:46:07.001143:	Training iteration: 48000, Loss: 0.22339937090873718
2018-10-21 07:46:34.762122:	Training iteration: 48200, Loss: 0.12695591151714325
2018-10-21 07:47:01.695936:	Training iteration: 48400, Loss: 0.13772746920585632
2018-10-21 07:47:28.299043:	Training iteration: 48600, Loss: 0.20976826548576355
2018-10-21 07:47:55.730107:	Training iteration: 48800, Loss: 0.09481239318847656
2018-10-21 07:48:22.755943:	Training iteration: 49000, Loss: 0.07152576744556427
2018-10-21 07:48:49.132719:	Training iteration: 49200, Loss: 0.14569811522960663
2018-10-21 07:49:16.027801:	Training iteration: 49400, Loss: 0.13717786967754364
2018-10-21 07:49:42.547007:	Training iteration: 49600, Loss: 0.09228824824094772
2018-10-21 07:50:09.483182:	Training iteration: 49800, Loss: 0.07902997732162476
2018-10-21 07:50:37.501117:	Training iteration: 50000, Loss: 0.1236228421330452
2018-10-21 07:51:04.574371:	Training iteration: 50200, Loss: 0.0930822417140007
2018-10-21 07:51:33.576775: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 657 of 1000
2018-10-21 07:51:37.144757: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 07:51:44.040699:	Training iteration: 50400, Loss: 0.22167916595935822
2018-10-21 07:52:10.198893:	Training iteration: 50600, Loss: 0.1579388827085495
2018-10-21 07:52:36.549721:	Training iteration: 50800, Loss: 0.16348788142204285
2018-10-21 07:53:03.315794:	Training iteration: 51000, Loss: 0.14297086000442505
2018-10-21 07:53:29.729041:	Training iteration: 51200, Loss: 0.14208535850048065
2018-10-21 07:53:57.098812:	Training iteration: 51400, Loss: 0.28679412603378296
2018-10-21 07:54:23.694589:	Training iteration: 51600, Loss: 0.16608625650405884
2018-10-21 07:54:50.626077:	Training iteration: 51800, Loss: 0.15461237728595734
2018-10-21 07:55:17.360482:	Training iteration: 52000, Loss: 0.34201309084892273
2018-10-21 07:55:43.998511:	Training iteration: 52200, Loss: 0.16134794056415558
2018-10-21 07:56:10.721257:	Training iteration: 52400, Loss: 0.15596209466457367
2018-10-21 07:56:37.454808:	Training iteration: 52600, Loss: 0.13658387959003448
2018-10-21 07:57:03.441939:	Training iteration: 52800, Loss: 0.152972012758255
2018-10-21 07:57:29.400318:	Training iteration: 53000, Loss: 0.14016155898571014
2018-10-21 07:57:56.874856:	Training iteration: 53200, Loss: 0.16271467506885529
2018-10-21 07:58:23.606050:	Training iteration: 53400, Loss: 0.15208950638771057
2018-10-21 07:58:50.460107:	Training iteration: 53600, Loss: 0.20927909016609192
2018-10-21 07:59:16.808281:	Training iteration: 53800, Loss: 0.20206701755523682
2018-10-21 07:59:43.886378:	Training iteration: 54000, Loss: 0.115826316177845
2018-10-21 08:00:10.293631:	Training iteration: 54200, Loss: 0.11090724915266037
2018-10-21 08:00:37.457331:	Training iteration: 54400, Loss: 0.16069070994853973
2018-10-21 08:01:03.939760:	Training iteration: 54600, Loss: 0.16937623918056488
2018-10-21 08:01:30.783441:	Training iteration: 54800, Loss: 0.15294137597084045
2018-10-21 08:01:57.472002:	Training iteration: 55000, Loss: 0.12604816257953644
2018-10-21 08:02:24.338201:	Training iteration: 55200, Loss: 0.1906358003616333
2018-10-21 08:02:50.863003:	Training iteration: 55400, Loss: 0.13492897152900696
2018-10-21 08:03:17.279254:	Training iteration: 55600, Loss: 0.15739423036575317
2018-10-21 08:03:44.665794:	Training iteration: 55800, Loss: 0.20138168334960938
2018-10-21 08:04:11.189513:	Training iteration: 56000, Loss: 0.21453535556793213
2018-10-21 08:04:37.976071:	Training iteration: 56200, Loss: 0.10589022934436798
2018-10-21 08:05:04.036439:	Training iteration: 56400, Loss: 0.10799821466207504
2018-10-21 08:05:31.091522:	Training iteration: 56600, Loss: 0.23287338018417358
2018-10-21 08:05:57.907211:	Training iteration: 56800, Loss: 0.18287521600723267
2018-10-21 08:06:24.653686:	Training iteration: 57000, Loss: 0.21233516931533813
2018-10-21 08:06:51.322237:	Training iteration: 57200, Loss: 0.15194354951381683
2018-10-21 08:07:18.354782:	Training iteration: 57400, Loss: 0.14696261286735535
2018-10-21 08:07:45.309540:	Training iteration: 57600, Loss: 0.12336893379688263
2018-10-21 08:08:11.458503:	Training iteration: 57800, Loss: 0.20211176574230194
2018-10-21 08:08:37.922807:	Training iteration: 58000, Loss: 0.17968705296516418
2018-10-21 08:09:04.804495:	Training iteration: 58200, Loss: 0.18566060066223145
2018-10-21 08:09:32.100116:	Training iteration: 58400, Loss: 0.17578241229057312
2018-10-21 08:09:58.588142:	Training iteration: 58600, Loss: 0.17505896091461182
2018-10-21 08:10:24.981445:	Training iteration: 58800, Loss: 0.14227789640426636
2018-10-21 08:10:51.480199:	Training iteration: 59000, Loss: 0.16556397080421448
2018-10-21 08:11:18.563516:	Training iteration: 59200, Loss: 0.1680801510810852
2018-10-21 08:11:45.475854:	Training iteration: 59400, Loss: 0.2335560917854309
2018-10-21 08:12:12.110082:	Training iteration: 59600, Loss: 0.1650562584400177
2018-10-21 08:12:38.567334:	Training iteration: 59800, Loss: 0.1651674211025238
2018-10-21 08:13:05.694217:	Training iteration: 60000, Loss: 0.18383745849132538
2018-10-21 08:13:32.077016:	Training iteration: 60200, Loss: 0.20719745755195618
2018-10-21 08:13:58.130834:	Training iteration: 60400, Loss: 0.15835580229759216
2018-10-21 08:14:24.333284:	Training iteration: 60600, Loss: 0.20858687162399292
2018-10-21 08:14:50.982020:	Training iteration: 60800, Loss: 0.19524800777435303
2018-10-21 08:15:17.774219:	Training iteration: 61000, Loss: 0.13805338740348816
2018-10-21 08:15:43.938003:	Training iteration: 61200, Loss: 0.1252867579460144
2018-10-21 08:16:10.741632:	Training iteration: 61400, Loss: 0.15343807637691498
2018-10-21 08:16:38.135025:	Training iteration: 61600, Loss: 0.14676415920257568
2018-10-21 08:17:05.206528:	Training iteration: 61800, Loss: 0.19014835357666016
2018-10-21 08:17:31.405702:	Training iteration: 62000, Loss: 0.19842185080051422
2018-10-21 08:17:58.598334:	Training iteration: 62200, Loss: 0.17800559103488922
2018-10-21 08:18:24.360317:	Training iteration: 62400, Loss: 0.20718683302402496
2018-10-21 08:18:51.136354:	Training iteration: 62600, Loss: 0.18597091734409332
2018-10-21 08:19:17.407706:	Training iteration: 62800, Loss: 0.10991640388965607
2018-10-21 08:19:45.205365:	Training iteration: 63000, Loss: 0.15308532118797302
2018-10-21 08:20:12.340855:	Training iteration: 63200, Loss: 0.1539798378944397
2018-10-21 08:20:39.671206:	Training iteration: 63400, Loss: 0.18964195251464844
2018-10-21 08:21:06.761807:	Training iteration: 63600, Loss: 0.15165886282920837
2018-10-21 08:21:33.794872:	Training iteration: 63800, Loss: 0.17686273157596588
2018-10-21 08:21:59.669323:	Training iteration: 64000, Loss: 0.15400967001914978
2018-10-21 08:22:26.762526:	Training iteration: 64200, Loss: 0.20579794049263
2018-10-21 08:22:53.133297:	Training iteration: 64400, Loss: 0.18858814239501953
2018-10-21 08:23:19.703214:	Training iteration: 64600, Loss: 0.1668461561203003
2018-10-21 08:23:46.412645:	Training iteration: 64800, Loss: 0.15525972843170166
2018-10-21 08:24:13.494223:	Training iteration: 65000, Loss: 0.14294010400772095
2018-10-21 08:24:40.092554:	Training iteration: 65200, Loss: 0.1284571886062622
2018-10-21 08:25:07.333209:	Training iteration: 65400, Loss: 0.15836794674396515
2018-10-21 08:25:34.965026:	Training iteration: 65600, Loss: 0.11735247820615768
2018-10-21 08:26:00.931015:	Training iteration: 65800, Loss: 0.15143054723739624
2018-10-21 08:26:28.341950:	Training iteration: 66000, Loss: 0.17530858516693115
2018-10-21 08:26:55.078168:	Training iteration: 66200, Loss: 0.1380842626094818
2018-10-21 08:27:21.889242:	Training iteration: 66400, Loss: 0.14081093668937683
2018-10-21 08:27:48.054858:	Training iteration: 66600, Loss: 0.2127177119255066
2018-10-21 08:28:14.804295:	Training iteration: 66800, Loss: 0.1685052514076233
2018-10-21 08:28:42.091603:	Training iteration: 67000, Loss: 0.25382199883461
2018-10-21 08:29:09.270858:	Training iteration: 67200, Loss: 0.29888808727264404
2018-10-21 08:29:35.768849:	Training iteration: 67400, Loss: 0.2702113389968872
2018-10-21 08:30:02.217925:	Training iteration: 67600, Loss: 0.17454934120178223
2018-10-21 08:30:29.579687:	Training iteration: 67800, Loss: 0.14079387485980988
2018-10-21 08:30:56.512641:	Training iteration: 68000, Loss: 0.1325308382511139
2018-10-21 08:31:23.384592:	Training iteration: 68200, Loss: 0.20627561211585999
2018-10-21 08:31:50.388251:	Training iteration: 68400, Loss: 0.12365853786468506
2018-10-21 08:32:17.195956:	Training iteration: 68600, Loss: 0.14417168498039246
2018-10-21 08:32:44.086985:	Training iteration: 68800, Loss: 0.1366642415523529
2018-10-21 08:33:10.975958:	Training iteration: 69000, Loss: 0.21123725175857544
2018-10-21 08:33:37.719463:	Training iteration: 69200, Loss: 0.14972171187400818
2018-10-21 08:34:04.037396:	Training iteration: 69400, Loss: 0.07012701034545898
2018-10-21 08:34:30.388515:	Training iteration: 69600, Loss: 0.15131691098213196
2018-10-21 08:34:57.955740:	Training iteration: 69800, Loss: 0.19174307584762573
2018-10-21 08:35:24.408881:	Training iteration: 70000, Loss: 0.18312618136405945
2018-10-21 08:35:51.694083:	Training iteration: 70200, Loss: 0.14970004558563232
2018-10-21 08:36:18.296728:	Training iteration: 70400, Loss: 0.20344693958759308
2018-10-21 08:36:44.819074:	Training iteration: 70600, Loss: 0.1692913919687271
2018-10-21 08:37:12.101550:	Training iteration: 70800, Loss: 0.195286363363266
2018-10-21 08:37:39.514636:	Training iteration: 71000, Loss: 0.15572980046272278
2018-10-21 08:38:05.425157:	Training iteration: 71200, Loss: 0.18493446707725525
2018-10-21 08:38:31.727258:	Training iteration: 71400, Loss: 0.13481754064559937
2018-10-21 08:38:58.628165:	Training iteration: 71600, Loss: 0.1393667757511139
2018-10-21 08:39:25.464498:	Training iteration: 71800, Loss: 0.14734452962875366
2018-10-21 08:39:51.420922:	Training iteration: 72000, Loss: 0.12208376079797745
2018-10-21 08:40:18.249485:	Training iteration: 72200, Loss: 0.19483238458633423
2018-10-21 08:40:45.101428:	Training iteration: 72400, Loss: 0.1615179032087326
2018-10-21 08:41:10.848140:	Training iteration: 72600, Loss: 0.18145567178726196
2018-10-21 08:41:38.158358:	Training iteration: 72800, Loss: 0.19229090213775635
2018-10-21 08:42:05.190158:	Training iteration: 73000, Loss: 0.12632553279399872
2018-10-21 08:42:31.314180:	Training iteration: 73200, Loss: 0.162236750125885
2018-10-21 08:42:57.799358:	Training iteration: 73400, Loss: 0.1481950581073761
2018-10-21 08:43:25.427026:	Training iteration: 73600, Loss: 0.12986396253108978
2018-10-21 08:43:52.476123:	Training iteration: 73800, Loss: 0.16451658308506012
2018-10-21 08:44:18.808197:	Training iteration: 74000, Loss: 0.16537559032440186
2018-10-21 08:44:46.286423:	Training iteration: 74200, Loss: 0.16390913724899292
2018-10-21 08:45:13.114343:	Training iteration: 74400, Loss: 0.1333591789007187
2018-10-21 08:45:40.521067:	Training iteration: 74600, Loss: 0.14884307980537415
2018-10-21 08:46:07.792672:	Training iteration: 74800, Loss: 0.14242982864379883
2018-10-21 08:46:33.906961:	Training iteration: 75000, Loss: 0.0986335501074791
2018-10-21 08:47:01.155187:	Training iteration: 75200, Loss: 0.1681358367204666
2018-10-21 08:47:28.057476:	Training iteration: 75400, Loss: 0.16489869356155396
2018-10-21 08:47:54.725193:	Training iteration: 75600, Loss: 0.16189700365066528
2018-10-21 08:48:20.589166:	Training iteration: 75800, Loss: 0.1352352499961853
2018-10-21 08:48:47.253397:	Training iteration: 76000, Loss: 0.17301931977272034
2018-10-21 08:49:13.852521:	Training iteration: 76200, Loss: 0.1335572898387909
2018-10-21 08:49:41.111749:	Training iteration: 76400, Loss: 0.1676015853881836
2018-10-21 08:50:08.589156:	Training iteration: 76600, Loss: 0.19834071397781372
2018-10-21 08:50:35.922264:	Training iteration: 76800, Loss: 0.14153960347175598
2018-10-21 08:51:02.048539:	Training iteration: 77000, Loss: 0.1349189579486847
2018-10-21 08:51:28.289908:	Training iteration: 77200, Loss: 0.10581609606742859
2018-10-21 08:51:54.428561:	Training iteration: 77400, Loss: 0.14313948154449463
2018-10-21 08:52:21.274248:	Training iteration: 77600, Loss: 0.11859562993049622
2018-10-21 08:52:48.120558:	Training iteration: 77800, Loss: 0.16211619973182678
2018-10-21 08:53:15.556007:	Training iteration: 78000, Loss: 0.1384032517671585
2018-10-21 08:53:43.179465:	Training iteration: 78200, Loss: 0.15696319937705994
2018-10-21 08:54:10.077654:	Training iteration: 78400, Loss: 0.13837149739265442
2018-10-21 08:54:37.178249:	Training iteration: 78600, Loss: 0.18471397459506989
2018-10-21 08:55:04.103032:	Training iteration: 78800, Loss: 0.19385801255702972
2018-10-21 08:55:30.312649:	Training iteration: 79000, Loss: 0.12219860404729843
2018-10-21 08:55:57.454591:	Training iteration: 79200, Loss: 0.15254813432693481
2018-10-21 08:56:24.420454:	Training iteration: 79400, Loss: 0.1331888735294342
2018-10-21 08:56:50.833848:	Training iteration: 79600, Loss: 0.1324060708284378
2018-10-21 08:57:17.034453:	Training iteration: 79800, Loss: 0.16724762320518494
2018-10-21 08:57:43.706381:	Training iteration: 80000, Loss: 0.14419814944267273
2018-10-21 08:58:10.601293:	Training iteration: 80200, Loss: 0.1333746314048767
2018-10-21 08:58:38.494947:	Training iteration: 80400, Loss: 0.1340765506029129
2018-10-21 08:59:04.943756:	Training iteration: 80600, Loss: 0.19606494903564453
2018-10-21 08:59:31.450836:	Training iteration: 80800, Loss: 0.15307798981666565
2018-10-21 08:59:58.530270:	Training iteration: 81000, Loss: 0.1525798738002777
2018-10-21 09:00:24.854150:	Training iteration: 81200, Loss: 0.15131482481956482
2018-10-21 09:00:51.478643:	Training iteration: 81400, Loss: 0.14565733075141907
2018-10-21 09:01:18.171213:	Training iteration: 81600, Loss: 0.15865227580070496
2018-10-21 09:01:44.970956:	Training iteration: 81800, Loss: 0.16449373960494995
2018-10-21 09:02:10.754131:	Training iteration: 82000, Loss: 0.16573747992515564
2018-10-21 09:02:37.287576:	Training iteration: 82200, Loss: 0.1348963975906372
2018-10-21 09:03:04.901454:	Training iteration: 82400, Loss: 0.16216284036636353
2018-10-21 09:03:32.211152:	Training iteration: 82600, Loss: 0.1709056794643402
2018-10-21 09:03:59.202743:	Training iteration: 82800, Loss: 0.13317041099071503
2018-10-21 09:04:25.597080:	Training iteration: 83000, Loss: 0.1696392297744751
2018-10-21 09:04:52.778547:	Training iteration: 83200, Loss: 0.17481833696365356
2018-10-21 09:05:18.689827:	Training iteration: 83400, Loss: 0.17479200661182404
2018-10-21 09:05:46.221627:	Training iteration: 83600, Loss: 0.21191023290157318
2018-10-21 09:06:13.181591:	Training iteration: 83800, Loss: 0.23394611477851868
2018-10-21 09:06:40.851232:	Training iteration: 84000, Loss: 0.19146224856376648
2018-10-21 09:07:08.235894:	Training iteration: 84200, Loss: 0.1380656659603119
2018-10-21 09:07:35.039030:	Training iteration: 84400, Loss: 0.20649147033691406
2018-10-21 09:08:01.607192:	Training iteration: 84600, Loss: 0.1561427265405655
2018-10-21 09:08:27.585007:	Training iteration: 84800, Loss: 0.16079623997211456
2018-10-21 09:08:53.542836:	Training iteration: 85000, Loss: 0.16486573219299316
2018-10-21 09:09:20.817813:	Training iteration: 85200, Loss: 0.10237890481948853
2018-10-21 09:09:47.669142:	Training iteration: 85400, Loss: 0.18142575025558472
2018-10-21 09:10:14.779101:	Training iteration: 85600, Loss: 0.1442895233631134
2018-10-21 09:10:41.450265:	Training iteration: 85800, Loss: 0.19047784805297852
2018-10-21 09:11:07.691684:	Training iteration: 86000, Loss: 0.16822072863578796
2018-10-21 09:11:34.611924:	Training iteration: 86200, Loss: 0.15166553854942322
2018-10-21 09:12:01.488495:	Training iteration: 86400, Loss: 0.2075902670621872
2018-10-21 09:12:27.690390:	Training iteration: 86600, Loss: 0.16228435933589935
2018-10-21 09:12:54.393206:	Training iteration: 86800, Loss: 0.19264405965805054
2018-10-21 09:13:21.756880:	Training iteration: 87000, Loss: 0.12184427678585052
2018-10-21 09:13:49.044865:	Training iteration: 87200, Loss: 0.12870906293392181
2018-10-21 09:14:15.841055:	Training iteration: 87400, Loss: 0.13400903344154358
2018-10-21 09:14:42.614403:	Training iteration: 87600, Loss: 0.1525820791721344
2018-10-21 09:15:09.728400:	Training iteration: 87800, Loss: 0.1293797791004181
2018-10-21 09:15:36.303627:	Training iteration: 88000, Loss: 0.21564942598342896
2018-10-21 09:16:03.236461:	Training iteration: 88200, Loss: 0.15401601791381836
2018-10-21 09:16:29.200864:	Training iteration: 88400, Loss: 0.1461488902568817
2018-10-21 09:16:55.866457:	Training iteration: 88600, Loss: 0.13778838515281677
2018-10-21 09:17:22.396421:	Training iteration: 88800, Loss: 0.15642966330051422
2018-10-21 09:17:49.341836:	Training iteration: 89000, Loss: 0.1969374418258667
2018-10-21 09:18:15.730357:	Training iteration: 89200, Loss: 0.13859587907791138
2018-10-21 09:18:43.081191:	Training iteration: 89400, Loss: 0.1372908651828766
2018-10-21 09:19:10.497512:	Training iteration: 89600, Loss: 0.2267000377178192
2018-10-21 09:19:36.454926:	Training iteration: 89800, Loss: 0.12023825943470001
2018-10-21 09:20:03.163571:	Training iteration: 90000, Loss: 0.12837067246437073
2018-10-21 09:20:30.526302:	Training iteration: 90200, Loss: 0.115847148001194
2018-10-21 09:20:57.378510:	Training iteration: 90400, Loss: 0.18295101821422577
2018-10-21 09:21:24.821834:	Training iteration: 90600, Loss: 0.15744377672672272
2018-10-21 09:21:51.758616:	Training iteration: 90800, Loss: 0.16815480589866638
2018-10-21 09:22:18.883738:	Training iteration: 91000, Loss: 0.12650030851364136
2018-10-21 09:22:45.349851:	Training iteration: 91200, Loss: 0.12113891541957855
2018-10-21 09:23:11.965582:	Training iteration: 91400, Loss: 0.20518501102924347
2018-10-21 09:23:39.513749:	Training iteration: 91600, Loss: 0.22167488932609558
2018-10-21 09:24:06.302006:	Training iteration: 91800, Loss: 0.16172818839550018
2018-10-21 09:24:33.367246:	Training iteration: 92000, Loss: 0.143609881401062
2018-10-21 09:24:59.778607:	Training iteration: 92200, Loss: 0.11851920187473297
2018-10-21 09:25:26.408469:	Training iteration: 92400, Loss: 0.12272130697965622
2018-10-21 09:25:53.423760:	Training iteration: 92600, Loss: 0.14673206210136414
2018-10-21 09:26:20.863314:	Training iteration: 92800, Loss: 0.13526153564453125
2018-10-21 09:26:47.634252:	Training iteration: 93000, Loss: 0.13814018666744232
2018-10-21 09:27:14.886106:	Training iteration: 93200, Loss: 0.12580016255378723
2018-10-21 09:27:42.292089:	Training iteration: 93400, Loss: 0.1825263500213623
2018-10-21 09:28:09.559072:	Training iteration: 93600, Loss: 0.15127140283584595
2018-10-21 09:28:36.748040:	Training iteration: 93800, Loss: 0.14742887020111084
2018-10-21 09:29:03.407705:	Training iteration: 94000, Loss: 0.167409747838974
2018-10-21 09:29:29.747878:	Training iteration: 94200, Loss: 0.34256988763809204
2018-10-21 09:29:56.327831:	Training iteration: 94400, Loss: 0.17049655318260193
2018-10-21 09:30:22.768606:	Training iteration: 94600, Loss: 0.17019140720367432
2018-10-21 09:30:49.269017:	Training iteration: 94800, Loss: 0.1373070776462555
2018-10-21 09:31:16.275510:	Training iteration: 95000, Loss: 0.1371031105518341
2018-10-21 09:31:43.174878:	Training iteration: 95200, Loss: 0.19600927829742432
2018-10-21 09:32:10.053884:	Training iteration: 95400, Loss: 0.15140369534492493
2018-10-21 09:32:36.880015:	Training iteration: 95600, Loss: 0.1772746443748474
2018-10-21 09:33:03.374900:	Training iteration: 95800, Loss: 0.12580204010009766
2018-10-21 09:33:29.454295:	Training iteration: 96000, Loss: 0.11141470074653625
2018-10-21 09:33:55.751562:	Training iteration: 96200, Loss: 0.16898921132087708
2018-10-21 09:34:21.806316:	Training iteration: 96400, Loss: 0.14947086572647095
2018-10-21 09:34:48.717712:	Training iteration: 96600, Loss: 0.15951353311538696
2018-10-21 09:35:15.034667:	Training iteration: 96800, Loss: 0.15638640522956848
2018-10-21 09:35:41.677279:	Training iteration: 97000, Loss: 0.14342929422855377
2018-10-21 09:36:08.333882:	Training iteration: 97200, Loss: 0.08089715242385864
2018-10-21 09:36:35.217592:	Training iteration: 97400, Loss: 0.15582603216171265
2018-10-21 09:37:02.462559:	Training iteration: 97600, Loss: 0.12472054362297058
2018-10-21 09:37:29.073557:	Training iteration: 97800, Loss: 0.13392864167690277
2018-10-21 09:37:55.750154:	Training iteration: 98000, Loss: 0.09984272718429565
2018-10-21 09:38:22.764118:	Training iteration: 98200, Loss: 0.14637570083141327
2018-10-21 09:38:49.577362:	Training iteration: 98400, Loss: 0.15693244338035583
2018-10-21 09:39:16.613457:	Training iteration: 98600, Loss: 0.1823178082704544
2018-10-21 09:39:44.080736:	Training iteration: 98800, Loss: 0.14902250468730927
2018-10-21 09:40:10.615454:	Training iteration: 99000, Loss: 0.1793517768383026
2018-10-21 09:40:37.071127:	Training iteration: 99200, Loss: 0.1468169391155243
2018-10-21 09:41:03.693640:	Training iteration: 99400, Loss: 0.1519053429365158
2018-10-21 09:41:30.912755:	Training iteration: 99600, Loss: 0.19746874272823334
2018-10-21 09:41:57.972272:	Training iteration: 99800, Loss: 0.12367136031389236
2018-10-21 09:42:24.143262:	Training iteration: 100000, Loss: 0.1290372610092163
2018-10-21 09:42:50.591708:	Training iteration: 100200, Loss: 0.1291392743587494
2018-10-21 09:43:17.835323:	Training iteration: 100400, Loss: 0.14054575562477112
2018-10-21 09:43:44.462377:	Training iteration: 100600, Loss: 0.1369287073612213
2018-10-21 09:44:10.910120:	Training iteration: 100800, Loss: 0.14677132666110992
2018-10-21 09:44:37.569702:	Training iteration: 101000, Loss: 0.12905806303024292
2018-10-21 09:45:03.789367:	Training iteration: 101200, Loss: 0.09262719750404358
2018-10-21 09:45:31.428258:	Training iteration: 101400, Loss: 0.14152802526950836
2018-10-21 09:45:58.036783:	Training iteration: 101600, Loss: 0.12260168790817261
2018-10-21 09:46:23.882209:	Training iteration: 101800, Loss: 0.10473006963729858
2018-10-21 09:46:51.262673:	Training iteration: 102000, Loss: 0.20388263463974
2018-10-21 09:47:18.168504:	Training iteration: 102200, Loss: 0.1203370988368988
2018-10-21 09:47:44.994807:	Training iteration: 102400, Loss: 0.12488807737827301
2018-10-21 09:48:12.186034:	Training iteration: 102600, Loss: 0.22083497047424316
2018-10-21 09:48:38.508152:	Training iteration: 102800, Loss: 0.19083765149116516
2018-10-21 09:49:05.858693:	Training iteration: 103000, Loss: 0.14260989427566528
2018-10-21 09:49:32.026505:	Training iteration: 103200, Loss: 0.1939077526330948
2018-10-21 09:49:59.328090:	Training iteration: 103400, Loss: 0.1603618562221527
2018-10-21 09:50:25.428989:	Training iteration: 103600, Loss: 0.10942575335502625
2018-10-21 09:50:52.106878:	Training iteration: 103800, Loss: 0.149396151304245
2018-10-21 09:51:18.557824:	Training iteration: 104000, Loss: 0.18324582278728485
2018-10-21 09:51:44.497261:	Training iteration: 104200, Loss: 0.17160876095294952
2018-10-21 09:52:11.003179:	Training iteration: 104400, Loss: 0.1646997183561325
2018-10-21 09:52:38.519664:	Training iteration: 104600, Loss: 0.17470821738243103
2018-10-21 09:53:05.547575:	Training iteration: 104800, Loss: 0.15653511881828308
2018-10-21 09:53:32.748572:	Training iteration: 105000, Loss: 0.17393392324447632
2018-10-21 09:53:58.415311:	Training iteration: 105200, Loss: 0.20134031772613525
2018-10-21 09:54:26.749388:	Training iteration: 105400, Loss: 0.13080823421478271
2018-10-21 09:54:53.279291:	Training iteration: 105600, Loss: 0.1601329892873764
2018-10-21 09:55:19.710129:	Training iteration: 105800, Loss: 0.14728054404258728
2018-10-21 09:55:46.667816:	Training iteration: 106000, Loss: 0.1407054364681244
2018-10-21 09:56:12.868492:	Training iteration: 106200, Loss: 0.14699319005012512
2018-10-21 09:56:40.881025:	Training iteration: 106400, Loss: 0.19392022490501404
2018-10-21 09:57:07.543710:	Training iteration: 106600, Loss: 0.24052147567272186
2018-10-21 09:57:34.209270:	Training iteration: 106800, Loss: 0.16453513503074646
2018-10-21 09:58:01.640102:	Training iteration: 107000, Loss: 0.15368328988552094
2018-10-21 09:58:28.134084:	Training iteration: 107200, Loss: 0.09870481491088867
2018-10-21 09:58:55.239428:	Training iteration: 107400, Loss: 0.1338256299495697
2018-10-21 09:59:20.936992:	Training iteration: 107600, Loss: 0.10833697021007538
2018-10-21 09:59:47.297328:	Training iteration: 107800, Loss: 0.19786956906318665
2018-10-21 10:00:14.138434:	Training iteration: 108000, Loss: 0.17098328471183777
2018-10-21 10:00:41.472512:	Training iteration: 108200, Loss: 0.1361437439918518
2018-10-21 10:01:09.012514:	Training iteration: 108400, Loss: 0.14286455512046814
2018-10-21 10:01:34.699140:	Training iteration: 108600, Loss: 0.13744112849235535
2018-10-21 10:02:01.814538:	Training iteration: 108800, Loss: 0.2396652102470398
2018-10-21 10:02:28.479059:	Training iteration: 109000, Loss: 0.1882132589817047
2018-10-21 10:02:55.077763:	Training iteration: 109200, Loss: 0.17515327036380768
2018-10-21 10:03:23.483412:	Training iteration: 109400, Loss: 0.1582474708557129
2018-10-21 10:03:49.542026:	Training iteration: 109600, Loss: 0.1318083554506302
2018-10-21 10:04:15.958022:	Training iteration: 109800, Loss: 0.16812169551849365
2018-10-21 10:04:42.482580:	Training iteration: 110000, Loss: 0.164456307888031
2018-10-21 10:05:08.819876:	Training iteration: 110200, Loss: 0.13462096452713013
2018-10-21 10:05:36.229578:	Training iteration: 110400, Loss: 0.1362350732088089
2018-10-21 10:06:02.068446:	Training iteration: 110600, Loss: 0.1422574818134308
2018-10-21 10:06:28.860453:	Training iteration: 110800, Loss: 0.1290055811405182
2018-10-21 10:06:55.952551:	Training iteration: 111000, Loss: 0.17792166769504547
2018-10-21 10:07:22.499675:	Training iteration: 111200, Loss: 0.1630435287952423
2018-10-21 10:07:49.025949:	Training iteration: 111400, Loss: 0.17667034268379211
2018-10-21 10:08:15.865035:	Training iteration: 111600, Loss: 0.15891362726688385
2018-10-21 10:08:42.567090:	Training iteration: 111800, Loss: 0.19620400667190552
2018-10-21 10:09:09.487529:	Training iteration: 112000, Loss: 0.1407012641429901
2018-10-21 10:09:35.632780:	Training iteration: 112200, Loss: 0.19793549180030823
2018-10-21 10:10:01.675397:	Training iteration: 112400, Loss: 0.1821606457233429
2018-10-21 10:10:28.652649:	Training iteration: 112600, Loss: 0.15486860275268555
2018-10-21 10:10:56.050266:	Training iteration: 112800, Loss: 0.14008069038391113
2018-10-21 10:11:22.136336:	Training iteration: 113000, Loss: 0.18479397892951965
2018-10-21 10:11:48.775133:	Training iteration: 113200, Loss: 0.1281559318304062
2018-10-21 10:12:15.075458:	Training iteration: 113400, Loss: 0.16161638498306274
2018-10-21 10:12:41.841222:	Training iteration: 113600, Loss: 0.14298492670059204
2018-10-21 10:13:08.407185:	Training iteration: 113800, Loss: 0.12997564673423767
2018-10-21 10:13:35.804290:	Training iteration: 114000, Loss: 0.13917042315006256
2018-10-21 10:14:02.309796:	Training iteration: 114200, Loss: 0.18171416223049164
2018-10-21 10:14:29.532289:	Training iteration: 114400, Loss: 0.1445717215538025
2018-10-21 10:14:56.953209:	Training iteration: 114600, Loss: 0.21510660648345947
2018-10-21 10:15:23.225088:	Training iteration: 114800, Loss: 0.16602376103401184
2018-10-21 10:15:49.406239:	Training iteration: 115000, Loss: 0.19965650141239166
2018-10-21 10:16:16.125851:	Training iteration: 115200, Loss: 0.09956883639097214
2018-10-21 10:16:42.187521:	Training iteration: 115400, Loss: 0.1491514891386032
2018-10-21 10:17:09.070509:	Training iteration: 115600, Loss: 0.15442758798599243
2018-10-21 10:17:36.019045:	Training iteration: 115800, Loss: 0.18367639183998108
2018-10-21 10:18:02.240850:	Training iteration: 116000, Loss: 0.1947411298751831
2018-10-21 10:18:28.893774:	Training iteration: 116200, Loss: 0.1632835865020752
2018-10-21 10:18:55.526374:	Training iteration: 116400, Loss: 0.13039708137512207
2018-10-21 10:19:22.967834:	Training iteration: 116600, Loss: 0.15756523609161377
2018-10-21 10:19:49.714674:	Training iteration: 116800, Loss: 0.16267919540405273
2018-10-21 10:20:16.059326:	Training iteration: 117000, Loss: 0.12777230143547058
2018-10-21 10:20:42.639701:	Training iteration: 117200, Loss: 0.1520794928073883
2018-10-21 10:21:08.774962:	Training iteration: 117400, Loss: 0.13580696284770966
2018-10-21 10:21:35.614731:	Training iteration: 117600, Loss: 0.13412317633628845
2018-10-21 10:22:02.207862:	Training iteration: 117800, Loss: 0.122057244181633
2018-10-21 10:22:28.622470:	Training iteration: 118000, Loss: 0.1409418284893036
2018-10-21 10:22:55.637336:	Training iteration: 118200, Loss: 0.1150682270526886
2018-10-21 10:23:22.726213:	Training iteration: 118400, Loss: 0.1327298879623413
2018-10-21 10:23:50.124211:	Training iteration: 118600, Loss: 0.15625926852226257
2018-10-21 10:24:16.415012:	Training iteration: 118800, Loss: 0.16688774526119232
2018-10-21 10:24:44.422065:	Training iteration: 119000, Loss: 0.2365354597568512
2018-10-21 10:25:10.396302:	Training iteration: 119200, Loss: 0.18806225061416626
2018-10-21 10:25:36.941602:	Training iteration: 119400, Loss: 0.20162537693977356
2018-10-21 10:26:04.361674:	Training iteration: 119600, Loss: 0.17247521877288818
2018-10-21 10:26:32.007956:	Training iteration: 119800, Loss: 0.16472946107387543
2018-10-21 10:26:52.549808:	Epoch 0 finished after 119952 iterations.
No images to record
Validating
2018-10-21 10:26:54.177180:	Entering validation loop
2018-10-21 10:27:05.808462: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 708 of 1000
2018-10-21 10:27:10.048778: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:27:21.374849:	Validation iteration: 200, Loss: 0.26909154653549194
2018-10-21 10:27:33.410157:	Validation iteration: 400, Loss: 0.18982237577438354
2018-10-21 10:27:45.828610:	Validation iteration: 600, Loss: 0.22602002322673798
2018-10-21 10:27:57.834885:	Validation iteration: 800, Loss: 0.20373296737670898
2018-10-21 10:28:10.037210:	Validation iteration: 1000, Loss: 0.13355031609535217
2018-10-21 10:28:23.022708:	Validation iteration: 1200, Loss: 0.1491011381149292
2018-10-21 10:28:35.659701:	Validation iteration: 1400, Loss: 0.15063366293907166
2018-10-21 10:28:49.462842:	Validation iteration: 1600, Loss: 0.16878722608089447
2018-10-21 10:29:02.636859:	Validation iteration: 1800, Loss: 0.1490117609500885
2018-10-21 10:29:15.791934:	Validation iteration: 2000, Loss: 0.1718689203262329
2018-10-21 10:29:31.642559: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 729 of 1000
2018-10-21 10:29:35.083959: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:29:41.891647:	Validation iteration: 2200, Loss: 0.19775426387786865
2018-10-21 10:29:53.438700:	Validation iteration: 2400, Loss: 0.23680463433265686
2018-10-21 10:30:05.767397:	Validation iteration: 2600, Loss: 0.18738524615764618
2018-10-21 10:30:18.477363:	Validation iteration: 2800, Loss: 0.19305330514907837
2018-10-21 10:30:31.323735:	Validation iteration: 3000, Loss: 0.2421514391899109
2018-10-21 10:30:43.844017:	Validation iteration: 3200, Loss: 0.29864734411239624
2018-10-21 10:30:56.644774:	Validation iteration: 3400, Loss: 0.19973860681056976
2018-10-21 10:31:09.192279:	Validation iteration: 3600, Loss: 0.268449604511261
2018-10-21 10:31:22.030834:	Validation iteration: 3800, Loss: 0.19653338193893433
2018-10-21 10:31:35.049940:	Validation iteration: 4000, Loss: 0.18260493874549866
2018-10-21 10:31:56.624670: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 724 of 1000
2018-10-21 10:32:00.236199: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:32:01.682600:	Validation iteration: 4200, Loss: 0.19289496541023254
2018-10-21 10:32:12.891537:	Validation iteration: 4400, Loss: 0.1681879162788391
2018-10-21 10:32:24.773044:	Validation iteration: 4600, Loss: 0.1669195294380188
2018-10-21 10:32:37.237495:	Validation iteration: 4800, Loss: 0.1970503181219101
2018-10-21 10:32:50.103994:	Validation iteration: 5000, Loss: 0.14684641361236572
2018-10-21 10:33:03.102033:	Validation iteration: 5200, Loss: 0.14976081252098083
2018-10-21 10:33:15.935053:	Validation iteration: 5400, Loss: 0.19792440533638
2018-10-21 10:33:28.384511:	Validation iteration: 5600, Loss: 0.1660611480474472
2018-10-21 10:33:41.092659:	Validation iteration: 5800, Loss: 0.20066478848457336
2018-10-21 10:33:53.950398:	Validation iteration: 6000, Loss: 0.11820381879806519
2018-10-21 10:34:06.843790:	Validation iteration: 6200, Loss: 0.1877514123916626
2018-10-21 10:34:21.018328: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 735 of 1000
2018-10-21 10:34:24.477660: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:34:32.071711:	Validation iteration: 6400, Loss: 0.24217219650745392
2018-10-21 10:34:43.736086:	Validation iteration: 6600, Loss: 0.2358994036912918
2018-10-21 10:34:56.046433:	Validation iteration: 6800, Loss: 0.298628032207489
2018-10-21 10:35:08.370996:	Validation iteration: 7000, Loss: 0.1543826162815094
2018-10-21 10:35:21.032412:	Validation iteration: 7200, Loss: 0.23410142958164215
2018-10-21 10:35:33.532582:	Validation iteration: 7400, Loss: 0.2553701102733612
2018-10-21 10:35:46.197184:	Validation iteration: 7600, Loss: 0.19498735666275024
2018-10-21 10:35:58.642070:	Validation iteration: 7800, Loss: 0.21493561565876007
2018-10-21 10:36:11.708527:	Validation iteration: 8000, Loss: 0.16551174223423004
2018-10-21 10:36:24.774156:	Validation iteration: 8200, Loss: 0.2121756374835968
2018-10-21 10:36:44.648812: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 666 of 1000
2018-10-21 10:36:49.144240: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:36:51.887519:	Validation iteration: 8400, Loss: 0.22584253549575806
2018-10-21 10:37:03.221088:	Validation iteration: 8600, Loss: 0.1650453507900238
2018-10-21 10:37:15.163106:	Validation iteration: 8800, Loss: 0.13662537932395935
2018-10-21 10:37:27.710108:	Validation iteration: 9000, Loss: 0.15678304433822632
2018-10-21 10:37:40.246498:	Validation iteration: 9200, Loss: 0.14064478874206543
2018-10-21 10:37:52.925911:	Validation iteration: 9400, Loss: 0.1116405799984932
2018-10-21 10:38:05.412810:	Validation iteration: 9600, Loss: 0.19803917407989502
2018-10-21 10:38:18.143652:	Validation iteration: 9800, Loss: 0.21702177822589874
2018-10-21 10:38:30.615112:	Validation iteration: 10000, Loss: 0.1559019386768341
2018-10-21 10:38:43.793132:	Validation iteration: 10200, Loss: 0.147318035364151
2018-10-21 10:38:56.064180:	Validation iteration: 10400, Loss: 0.15754559636116028
2018-10-21 10:39:08.726767:	Validation iteration: 10600, Loss: 0.22099602222442627
2018-10-21 10:39:21.304336:	Validation iteration: 10800, Loss: 0.17777585983276367
2018-10-21 10:39:34.581163:	Validation iteration: 11000, Loss: 0.1603681445121765
2018-10-21 10:39:47.576056:	Validation iteration: 11200, Loss: 0.18252426385879517
Validation check mean loss: 0.19444050375195282
Validation loss has improved!
New best validation cost!
Checkpoint
2018-10-21 10:40:26.312072: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 798 of 1000
2018-10-21 10:40:28.659521: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:40:34.577318:	Training iteration: 120000, Loss: 0.14191322028636932
2018-10-21 10:40:58.369177:	Training iteration: 120200, Loss: 0.12505438923835754
2018-10-21 10:41:23.425301:	Training iteration: 120400, Loss: 0.15745869278907776
2018-10-21 10:41:49.639903:	Training iteration: 120600, Loss: 0.08608725666999817
2018-10-21 10:42:16.838957:	Training iteration: 120800, Loss: 0.12382084131240845
2018-10-21 10:42:43.720029:	Training iteration: 121000, Loss: 0.09797757863998413
2018-10-21 10:43:10.303064:	Training iteration: 121200, Loss: 0.15235362946987152
2018-10-21 10:43:36.909591:	Training iteration: 121400, Loss: 0.10142262279987335
2018-10-21 10:44:04.234256:	Training iteration: 121600, Loss: 0.12214824557304382
2018-10-21 10:44:30.047088:	Training iteration: 121800, Loss: 0.07594669610261917
2018-10-21 10:44:57.238496:	Training iteration: 122000, Loss: 0.11849276721477509
2018-10-21 10:45:22.836256:	Training iteration: 122200, Loss: 0.0863318145275116
2018-10-21 10:45:49.944551:	Training iteration: 122400, Loss: 0.09998547285795212
