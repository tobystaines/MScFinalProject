INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "148"
Experiment ID: 148
Preparing dataset
Dataset ready
2018-10-20 21:09:08.004750: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-10-20 21:09:08.141720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-20 21:09:08.156579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:27:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-10-20 21:09:08.156623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:27:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Running initialisation test
Starting testing
2018-10-20 21:09:13.399151:	Entering test loop
2018-10-20 21:09:23.709723: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 609 of 1000
2018-10-20 21:09:29.348075: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 21:09:30.304304:	Testing iteration: 0, Loss: 0.2584666311740875
2018-10-20 21:11:06.655038:	Testing iteration: 200, Loss: 0.35236865282058716
2018-10-20 21:13:06.576854:	Testing iteration: 400, Loss: 0.17463159561157227
2018-10-20 21:15:31.916083:	Testing iteration: 600, Loss: 0.19756612181663513
2018-10-20 21:18:19.849227:	Testing iteration: 800, Loss: 0.43594229221343994
2018-10-20 21:21:34.637201:	Testing iteration: 1000, Loss: 0.4084331691265106
2018-10-20 21:25:16.106131:	Testing iteration: 1200, Loss: 0.19151359796524048
2018-10-20 21:29:23.258157:	Testing iteration: 1400, Loss: 0.21892684698104858
2018-10-20 21:33:53.752983:	Testing iteration: 1600, Loss: 0.17358669638633728
2018-10-20 21:34:23.727193: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 525 of 1000
2018-10-20 21:34:30.369103: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 21:39:07.191245:	Testing iteration: 1800, Loss: 0.23300819098949432
2018-10-20 21:44:27.925199:	Testing iteration: 2000, Loss: 0.26893895864486694
2018-10-20 21:50:15.403835:	Testing iteration: 2200, Loss: 0.26706430315971375
2018-10-20 21:56:28.254670:	Testing iteration: 2400, Loss: 0.49986693263053894
2018-10-20 22:03:16.659852:	Testing iteration: 2600, Loss: 0.26598137617111206
2018-10-20 22:11:07.468978:	Testing iteration: 2800, Loss: 0.31968826055526733
2018-10-20 22:18:53.706304:	Testing iteration: 3000, Loss: 0.29920893907546997
2018-10-20 22:26:49.332904:	Testing iteration: 3200, Loss: 0.4115212559700012
2018-10-20 22:28:08.057959: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 680 of 1000
2018-10-20 22:28:12.308997: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 22:35:25.404736:	Testing iteration: 3400, Loss: 0.2831465005874634
2018-10-20 22:44:16.776402:	Testing iteration: 3600, Loss: 0.40030062198638916
2018-10-20 22:53:33.172603:	Testing iteration: 3800, Loss: 0.20456837117671967
2018-10-20 23:03:36.335305:	Testing iteration: 4000, Loss: 0.30739113688468933
2018-10-20 23:14:20.150061:	Testing iteration: 4200, Loss: 0.23805704712867737
2018-10-20 23:24:55.805317:	Testing iteration: 4400, Loss: 0.18137609958648682
2018-10-20 23:36:01.176687:	Testing iteration: 4600, Loss: 0.3323960304260254
2018-10-20 23:47:33.648378:	Testing iteration: 4800, Loss: 0.3064291477203369
2018-10-20 23:50:12.040679: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 698 of 1000
2018-10-20 23:50:16.207522: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-20 23:59:45.120678:	Testing iteration: 5000, Loss: 0.15517136454582214
2018-10-21 00:12:38.127956:	Testing iteration: 5200, Loss: 0.3063785433769226
2018-10-21 00:25:29.395845:	Testing iteration: 5400, Loss: 0.23009797930717468
2018-10-21 00:38:48.543820:	Testing iteration: 5600, Loss: 0.23583734035491943
2018-10-21 00:52:30.841932:	Testing iteration: 5800, Loss: 0.28511953353881836
2018-10-21 01:06:55.282309:	Testing iteration: 6000, Loss: 0.27564072608947754
2018-10-21 01:21:37.467277:	Testing iteration: 6200, Loss: 0.20897743105888367
2018-10-21 01:37:02.463376:	Testing iteration: 6400, Loss: 0.3762286305427551
2018-10-21 01:41:10.385793: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 607 of 1000
2018-10-21 01:41:16.331650: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 01:51:32.111094:	Testing iteration: 6600, Loss: 0.19712238013744354
2018-10-21 02:06:24.474940:	Testing iteration: 6800, Loss: 0.11457601934671402
2018-10-21 02:21:30.250371:	Testing iteration: 7000, Loss: 0.3137941360473633
2018-10-21 02:36:54.755241:	Testing iteration: 7200, Loss: 0.1933240443468094
2018-10-21 02:52:49.357593:	Testing iteration: 7400, Loss: 0.20052799582481384
2018-10-21 03:10:35.909101:	Testing iteration: 7600, Loss: 0.24109528958797455
2018-10-21 03:27:22.611392:	Testing iteration: 7800, Loss: 0.2977597117424011
2018-10-21 03:44:26.533196:	Testing iteration: 8000, Loss: 0.3005787134170532
2018-10-21 04:02:05.475946:	Testing iteration: 8200, Loss: 0.3033231794834137
2018-10-21 04:20:22.593891:	Testing iteration: 8400, Loss: 0.18622562289237976
2018-10-21 04:38:40.218548:	Testing iteration: 8600, Loss: 0.22702699899673462
2018-10-21 04:57:28.530392:	Testing iteration: 8800, Loss: 0.19741038978099823
2018-10-21 05:17:03.447876:	Testing iteration: 9000, Loss: 0.3022823631763458
2018-10-21 05:36:41.790992:	Testing iteration: 9200, Loss: 0.19890469312667847
2018-10-21 05:56:51.250352:	Testing iteration: 9400, Loss: 0.27569031715393066
Test pass complete
Mean loss over test set: 0.2682450266733128
Data saved to dumps/148 for later audio metric calculation
Starting training
2018-10-21 05:59:05.977082: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 614 of 1000
2018-10-21 05:59:10.704112: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 05:59:34.478249:	Training iteration: 200, Loss: 0.23657497763633728
2018-10-21 05:59:58.118817:	Training iteration: 400, Loss: 0.23348908126354218
2018-10-21 06:00:21.869464:	Training iteration: 600, Loss: 0.14186978340148926
2018-10-21 06:00:45.597959:	Training iteration: 800, Loss: 0.1518852710723877
2018-10-21 06:01:09.297803:	Training iteration: 1000, Loss: 0.2037995308637619
2018-10-21 06:01:33.060280:	Training iteration: 1200, Loss: 0.33486610651016235
2018-10-21 06:01:56.737436:	Training iteration: 1400, Loss: 0.283541202545166
2018-10-21 06:02:28.817076:	Training iteration: 1600, Loss: 0.12305489182472229
2018-10-21 06:02:52.574993:	Training iteration: 1800, Loss: 0.20242935419082642
2018-10-21 06:03:16.607827:	Training iteration: 2000, Loss: 0.22235527634620667
2018-10-21 06:03:41.666196:	Training iteration: 2200, Loss: 0.15255744755268097
2018-10-21 06:04:07.195842:	Training iteration: 2400, Loss: 0.18285992741584778
2018-10-21 06:04:32.678131:	Training iteration: 2600, Loss: 0.16881580650806427
2018-10-21 06:04:57.988961:	Training iteration: 2800, Loss: 0.1921704113483429
2018-10-21 06:05:23.854236:	Training iteration: 3000, Loss: 0.16980582475662231
2018-10-21 06:05:49.698578:	Training iteration: 3200, Loss: 0.2618377208709717
2018-10-21 06:06:15.444445:	Training iteration: 3400, Loss: 0.1267564296722412
2018-10-21 06:06:41.344678:	Training iteration: 3600, Loss: 0.25795042514801025
2018-10-21 06:07:08.313657:	Training iteration: 3800, Loss: 0.2994891405105591
2018-10-21 06:07:33.815916:	Training iteration: 4000, Loss: 0.23918801546096802
2018-10-21 06:08:00.418970:	Training iteration: 4200, Loss: 0.19350066781044006
2018-10-21 06:08:26.446578:	Training iteration: 4400, Loss: 0.17714151740074158
2018-10-21 06:08:52.556657:	Training iteration: 4600, Loss: 0.15009164810180664
2018-10-21 06:09:19.115565:	Training iteration: 4800, Loss: 0.1114514023065567
2018-10-21 06:09:45.004597:	Training iteration: 5000, Loss: 0.2186104953289032
2018-10-21 06:10:11.255178:	Training iteration: 5200, Loss: 0.203346386551857
2018-10-21 06:10:37.485154:	Training iteration: 5400, Loss: 0.1384221911430359
2018-10-21 06:11:03.763047:	Training iteration: 5600, Loss: 0.2916020154953003
2018-10-21 06:11:30.035395:	Training iteration: 5800, Loss: 0.15342462062835693
2018-10-21 06:11:56.578085:	Training iteration: 6000, Loss: 0.14261171221733093
2018-10-21 06:12:24.054923:	Training iteration: 6200, Loss: 0.14130763709545135
2018-10-21 06:12:50.322500:	Training iteration: 6400, Loss: 0.07879269868135452
2018-10-21 06:13:16.610669:	Training iteration: 6600, Loss: 0.11536656320095062
2018-10-21 06:13:42.974123:	Training iteration: 6800, Loss: 0.17031827569007874
2018-10-21 06:14:09.639014:	Training iteration: 7000, Loss: 0.14001432061195374
2018-10-21 06:14:35.488236:	Training iteration: 7200, Loss: 0.20406824350357056
2018-10-21 06:15:01.783395:	Training iteration: 7400, Loss: 0.14490966498851776
2018-10-21 06:15:28.505205:	Training iteration: 7600, Loss: 0.11297338455915451
2018-10-21 06:15:55.119884:	Training iteration: 7800, Loss: 0.1995915025472641
2018-10-21 06:16:23.088438:	Training iteration: 8000, Loss: 0.11954429000616074
2018-10-21 06:16:49.687856:	Training iteration: 8200, Loss: 0.1985853612422943
2018-10-21 06:17:15.411951:	Training iteration: 8400, Loss: 0.17276884615421295
2018-10-21 06:17:42.249413:	Training iteration: 8600, Loss: 0.15432946383953094
2018-10-21 06:18:08.821116:	Training iteration: 8800, Loss: 0.1037624180316925
2018-10-21 06:18:35.076958:	Training iteration: 9000, Loss: 0.21469876170158386
2018-10-21 06:19:02.359074:	Training iteration: 9200, Loss: 0.17579829692840576
2018-10-21 06:19:28.871404:	Training iteration: 9400, Loss: 0.12879648804664612
2018-10-21 06:19:55.333003:	Training iteration: 9600, Loss: 0.11694639921188354
2018-10-21 06:20:22.115250:	Training iteration: 9800, Loss: 0.14815665781497955
2018-10-21 06:20:48.353739:	Training iteration: 10000, Loss: 0.1435011625289917
2018-10-21 06:21:15.381630:	Training iteration: 10200, Loss: 0.19103378057479858
2018-10-21 06:21:42.043065:	Training iteration: 10400, Loss: 0.10914258658885956
2018-10-21 06:22:08.768780:	Training iteration: 10600, Loss: 0.12168818712234497
2018-10-21 06:22:35.788594:	Training iteration: 10800, Loss: 0.1066097766160965
2018-10-21 06:23:02.485076:	Training iteration: 11000, Loss: 0.14510880410671234
2018-10-21 06:23:29.184864:	Training iteration: 11200, Loss: 0.12398897856473923
2018-10-21 06:23:55.209027:	Training iteration: 11400, Loss: 0.15366196632385254
2018-10-21 06:24:21.782399:	Training iteration: 11600, Loss: 0.11902280151844025
2018-10-21 06:24:49.184336:	Training iteration: 11800, Loss: 0.11878030002117157
2018-10-21 06:25:16.123765:	Training iteration: 12000, Loss: 0.12283959984779358
2018-10-21 06:25:43.087217:	Training iteration: 12200, Loss: 0.09585043787956238
2018-10-21 06:26:19.580296: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 767 of 1000
2018-10-21 06:26:22.378881: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 06:26:23.011684:	Training iteration: 12400, Loss: 0.08822490274906158
2018-10-21 06:26:47.912617:	Training iteration: 12600, Loss: 0.13247525691986084
2018-10-21 06:27:14.277741:	Training iteration: 12800, Loss: 0.14165553450584412
2018-10-21 06:27:40.976532:	Training iteration: 13000, Loss: 0.13325434923171997
2018-10-21 06:28:06.363580:	Training iteration: 13200, Loss: 0.1008923351764679
2018-10-21 06:28:32.786772:	Training iteration: 13400, Loss: 0.08547432720661163
2018-10-21 06:28:58.519707:	Training iteration: 13600, Loss: 0.12166159600019455
2018-10-21 06:29:26.329262:	Training iteration: 13800, Loss: 0.18005730211734772
2018-10-21 06:29:54.203679:	Training iteration: 14000, Loss: 0.13580967485904694
2018-10-21 06:30:21.223837:	Training iteration: 14200, Loss: 0.1393139362335205
2018-10-21 06:30:47.987239:	Training iteration: 14400, Loss: 0.1434783637523651
2018-10-21 06:31:15.089385:	Training iteration: 14600, Loss: 0.11308223754167557
2018-10-21 06:31:41.872725:	Training iteration: 14800, Loss: 0.09789521247148514
2018-10-21 06:32:08.897939:	Training iteration: 15000, Loss: 0.157161682844162
2018-10-21 06:32:35.798949:	Training iteration: 15200, Loss: 0.18247704207897186
2018-10-21 06:33:02.486356:	Training iteration: 15400, Loss: 0.1506153792142868
2018-10-21 06:33:28.726032:	Training iteration: 15600, Loss: 0.08969302475452423
2018-10-21 06:33:54.844862:	Training iteration: 15800, Loss: 0.08799801766872406
2018-10-21 06:34:21.571434:	Training iteration: 16000, Loss: 0.1677878201007843
2018-10-21 06:34:48.114284:	Training iteration: 16200, Loss: 0.1443306803703308
2018-10-21 06:35:14.097396:	Training iteration: 16400, Loss: 0.08423379063606262
2018-10-21 06:35:40.815914:	Training iteration: 16600, Loss: 0.11591748893260956
2018-10-21 06:36:07.481823:	Training iteration: 16800, Loss: 0.11571106314659119
2018-10-21 06:36:33.813701:	Training iteration: 17000, Loss: 0.1427672952413559
2018-10-21 06:37:00.354961:	Training iteration: 17200, Loss: 0.1681060791015625
2018-10-21 06:37:26.905317:	Training iteration: 17400, Loss: 0.10911676287651062
2018-10-21 06:37:54.270976:	Training iteration: 17600, Loss: 0.09416660666465759
2018-10-21 06:38:20.298551:	Training iteration: 17800, Loss: 0.1865828037261963
2018-10-21 06:38:48.219445:	Training iteration: 18000, Loss: 0.1461385190486908
2018-10-21 06:39:14.254252:	Training iteration: 18200, Loss: 0.13065367937088013
2018-10-21 06:39:40.856634:	Training iteration: 18400, Loss: 0.11880812793970108
2018-10-21 06:40:08.193765:	Training iteration: 18600, Loss: 0.08965658396482468
2018-10-21 06:40:35.425413:	Training iteration: 18800, Loss: 0.08875386416912079
2018-10-21 06:41:01.876686:	Training iteration: 19000, Loss: 0.12283137440681458
2018-10-21 06:41:28.691486:	Training iteration: 19200, Loss: 0.07921847701072693
2018-10-21 06:41:56.027092:	Training iteration: 19400, Loss: 0.10344729572534561
2018-10-21 06:42:22.735586:	Training iteration: 19600, Loss: 0.20563042163848877
2018-10-21 06:42:49.742294:	Training iteration: 19800, Loss: 0.18067187070846558
2018-10-21 06:43:16.485578:	Training iteration: 20000, Loss: 0.1447916179895401
2018-10-21 06:43:43.276404:	Training iteration: 20200, Loss: 0.11964668333530426
2018-10-21 06:44:09.766385:	Training iteration: 20400, Loss: 0.12173473089933395
2018-10-21 06:44:36.082309:	Training iteration: 20600, Loss: 0.14589470624923706
2018-10-21 06:45:03.432101:	Training iteration: 20800, Loss: 0.17450012266635895
2018-10-21 06:45:30.726056:	Training iteration: 21000, Loss: 0.16177807748317719
2018-10-21 06:45:56.558683:	Training iteration: 21200, Loss: 0.12357316166162491
2018-10-21 06:46:23.448592:	Training iteration: 21400, Loss: 0.12905265390872955
2018-10-21 06:46:50.500665:	Training iteration: 21600, Loss: 0.11720284819602966
2018-10-21 06:47:17.076356:	Training iteration: 21800, Loss: 0.11170252412557602
2018-10-21 06:47:43.308053:	Training iteration: 22000, Loss: 0.11404496431350708
2018-10-21 06:48:10.474428:	Training iteration: 22200, Loss: 0.11329768598079681
2018-10-21 06:48:36.852427:	Training iteration: 22400, Loss: 0.13194778561592102
2018-10-21 06:49:04.347340:	Training iteration: 22600, Loss: 0.13926109671592712
2018-10-21 06:49:31.902437:	Training iteration: 22800, Loss: 0.11501958221197128
2018-10-21 06:49:59.099582:	Training iteration: 23000, Loss: 0.156609445810318
2018-10-21 06:50:26.011649:	Training iteration: 23200, Loss: 0.07937317341566086
2018-10-21 06:50:53.080114:	Training iteration: 23400, Loss: 0.08089078962802887
2018-10-21 06:51:19.732077:	Training iteration: 23600, Loss: 0.1316399872303009
2018-10-21 06:51:46.711195:	Training iteration: 23800, Loss: 0.11523202061653137
2018-10-21 06:52:13.283026:	Training iteration: 24000, Loss: 0.08543148636817932
2018-10-21 06:52:40.132116:	Training iteration: 24200, Loss: 0.11299454420804977
2018-10-21 06:53:07.196535:	Training iteration: 24400, Loss: 0.09208158403635025
2018-10-21 06:53:34.378928:	Training iteration: 24600, Loss: 0.09405724704265594
2018-10-21 06:54:01.494545:	Training iteration: 24800, Loss: 0.1411924660205841
2018-10-21 06:54:20.113998: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 792 of 1000
2018-10-21 06:54:22.504658: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 06:54:39.359121:	Training iteration: 25000, Loss: 0.08517341315746307
2018-10-21 06:55:05.881140:	Training iteration: 25200, Loss: 0.11867222189903259
2018-10-21 06:55:32.841572:	Training iteration: 25400, Loss: 0.11768423020839691
2018-10-21 06:55:59.518650:	Training iteration: 25600, Loss: 0.07786650955677032
2018-10-21 06:56:26.109251:	Training iteration: 25800, Loss: 0.06624587625265121
2018-10-21 06:56:52.610680:	Training iteration: 26000, Loss: 0.0771208107471466
2018-10-21 06:57:19.947683:	Training iteration: 26200, Loss: 0.07454288005828857
2018-10-21 06:57:45.861032:	Training iteration: 26400, Loss: 0.07079369574785233
2018-10-21 06:58:12.494352:	Training iteration: 26600, Loss: 0.12329989671707153
2018-10-21 06:58:38.652992:	Training iteration: 26800, Loss: 0.08271881937980652
2018-10-21 06:59:06.006500:	Training iteration: 27000, Loss: 0.07649730890989304
2018-10-21 06:59:33.825566:	Training iteration: 27200, Loss: 0.05256516486406326
2018-10-21 07:00:01.472974:	Training iteration: 27400, Loss: 0.07626021653413773
2018-10-21 07:00:27.701698:	Training iteration: 27600, Loss: 0.0939813032746315
2018-10-21 07:00:55.146305:	Training iteration: 27800, Loss: 0.08493070304393768
2018-10-21 07:01:21.768736:	Training iteration: 28000, Loss: 0.07177552580833435
2018-10-21 07:01:49.604574:	Training iteration: 28200, Loss: 0.06754803657531738
2018-10-21 07:02:15.619647:	Training iteration: 28400, Loss: 0.0672549232840538
2018-10-21 07:02:42.507932:	Training iteration: 28600, Loss: 0.06969501078128815
2018-10-21 07:03:09.306812:	Training iteration: 28800, Loss: 0.1035786122083664
2018-10-21 07:03:36.173059:	Training iteration: 29000, Loss: 0.09159724414348602
2018-10-21 07:04:02.916055:	Training iteration: 29200, Loss: 0.07183365523815155
2018-10-21 07:04:29.106119:	Training iteration: 29400, Loss: 0.07952467352151871
2018-10-21 07:04:55.771413:	Training iteration: 29600, Loss: 0.11928023397922516
2018-10-21 07:05:24.714685:	Training iteration: 29800, Loss: 0.0645144134759903
2018-10-21 07:05:51.547545:	Training iteration: 30000, Loss: 0.10571631044149399
2018-10-21 07:06:18.235790:	Training iteration: 30200, Loss: 0.0730988085269928
2018-10-21 07:06:44.167683:	Training iteration: 30400, Loss: 0.09630725532770157
2018-10-21 07:07:11.316747:	Training iteration: 30600, Loss: 0.11444105207920074
2018-10-21 07:07:37.990311:	Training iteration: 30800, Loss: 0.14601659774780273
2018-10-21 07:08:04.343553:	Training iteration: 31000, Loss: 0.07831704616546631
2018-10-21 07:08:31.507671:	Training iteration: 31200, Loss: 0.10215239971876144
2018-10-21 07:08:58.138576:	Training iteration: 31400, Loss: 0.09228172898292542
2018-10-21 07:09:24.642720:	Training iteration: 31600, Loss: 0.0881103128194809
2018-10-21 07:09:51.251239:	Training iteration: 31800, Loss: 0.0805654376745224
2018-10-21 07:10:17.733828:	Training iteration: 32000, Loss: 0.08878567814826965
2018-10-21 07:10:44.629403:	Training iteration: 32200, Loss: 0.09575412422418594
2018-10-21 07:11:10.304757:	Training iteration: 32400, Loss: 0.08437451720237732
2018-10-21 07:11:36.776021:	Training iteration: 32600, Loss: 0.0971250832080841
2018-10-21 07:12:04.276509:	Training iteration: 32800, Loss: 0.06958018243312836
2018-10-21 07:12:31.196728:	Training iteration: 33000, Loss: 0.08077020198106766
2018-10-21 07:12:57.842194:	Training iteration: 33200, Loss: 0.11870225518941879
2018-10-21 07:13:24.527592:	Training iteration: 33400, Loss: 0.08022066205739975
2018-10-21 07:13:52.309751:	Training iteration: 33600, Loss: 0.1603553295135498
2018-10-21 07:14:18.954045:	Training iteration: 33800, Loss: 0.09384682029485703
2018-10-21 07:14:44.945476:	Training iteration: 34000, Loss: 0.058908507227897644
2018-10-21 07:15:11.418384:	Training iteration: 34200, Loss: 0.06994335353374481
2018-10-21 07:15:38.292127:	Training iteration: 34400, Loss: 0.1179443746805191
2018-10-21 07:16:05.163502:	Training iteration: 34600, Loss: 0.10242149233818054
2018-10-21 07:16:32.088959:	Training iteration: 34800, Loss: 0.08528022468090057
2018-10-21 07:16:58.796118:	Training iteration: 35000, Loss: 0.10522440820932388
2018-10-21 07:17:25.262814:	Training iteration: 35200, Loss: 0.05178489536046982
2018-10-21 07:17:51.251894:	Training iteration: 35400, Loss: 0.12509410083293915
2018-10-21 07:18:18.292866:	Training iteration: 35600, Loss: 0.06505417823791504
2018-10-21 07:18:44.644126:	Training iteration: 35800, Loss: 0.1389792412519455
2018-10-21 07:19:11.200215:	Training iteration: 36000, Loss: 0.1025388315320015
2018-10-21 07:19:37.738510:	Training iteration: 36200, Loss: 0.10286325216293335
2018-10-21 07:20:03.920094:	Training iteration: 36400, Loss: 0.08460161089897156
2018-10-21 07:20:30.618455:	Training iteration: 36600, Loss: 0.0813719779253006
2018-10-21 07:20:59.032393:	Training iteration: 36800, Loss: 0.09384208917617798
2018-10-21 07:21:25.625857:	Training iteration: 37000, Loss: 0.1042892187833786
2018-10-21 07:21:51.927401:	Training iteration: 37200, Loss: 0.08415648341178894
2018-10-21 07:22:19.357847: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 789 of 1000
2018-10-21 07:22:21.920073: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 07:22:31.221536:	Training iteration: 37400, Loss: 0.15927720069885254
2018-10-21 07:22:56.959463:	Training iteration: 37600, Loss: 0.08898692578077316
2018-10-21 07:23:23.494731:	Training iteration: 37800, Loss: 0.139888733625412
2018-10-21 07:23:50.142069:	Training iteration: 38000, Loss: 0.09836572408676147
2018-10-21 07:24:16.985626:	Training iteration: 38200, Loss: 0.16580148041248322
2018-10-21 07:24:43.414925:	Training iteration: 38400, Loss: 0.08603742718696594
2018-10-21 07:25:09.781816:	Training iteration: 38600, Loss: 0.1378268003463745
2018-10-21 07:25:36.833739:	Training iteration: 38800, Loss: 0.18850502371788025
2018-10-21 07:26:03.476863:	Training iteration: 39000, Loss: 0.18298596143722534
2018-10-21 07:26:29.306356:	Training iteration: 39200, Loss: 0.12551994621753693
2018-10-21 07:26:56.079551:	Training iteration: 39400, Loss: 0.22053278982639313
2018-10-21 07:27:22.689660:	Training iteration: 39600, Loss: 0.1461719125509262
2018-10-21 07:27:48.279032:	Training iteration: 39800, Loss: 0.12407456338405609
2018-10-21 07:28:15.504898:	Training iteration: 40000, Loss: 0.15295520424842834
2018-10-21 07:28:42.022921:	Training iteration: 40200, Loss: 0.10845623910427094
2018-10-21 07:29:09.448002:	Training iteration: 40400, Loss: 0.15188124775886536
2018-10-21 07:29:35.956025:	Training iteration: 40600, Loss: 0.10084015876054764
2018-10-21 07:30:03.497194:	Training iteration: 40800, Loss: 0.10198354721069336
2018-10-21 07:30:30.405813:	Training iteration: 41000, Loss: 0.23115086555480957
2018-10-21 07:30:57.258606:	Training iteration: 41200, Loss: 0.10063618421554565
2018-10-21 07:31:23.976648:	Training iteration: 41400, Loss: 0.11937159299850464
2018-10-21 07:31:50.807620:	Training iteration: 41600, Loss: 0.07490109652280807
2018-10-21 07:32:17.444816:	Training iteration: 41800, Loss: 0.091281957924366
2018-10-21 07:32:44.977183:	Training iteration: 42000, Loss: 0.13838444650173187
2018-10-21 07:33:10.964365:	Training iteration: 42200, Loss: 0.16747929155826569
2018-10-21 07:33:37.871250:	Training iteration: 42400, Loss: 0.10937166213989258
2018-10-21 07:34:04.173614:	Training iteration: 42600, Loss: 0.20862439274787903
2018-10-21 07:34:31.367690:	Training iteration: 42800, Loss: 0.1859912872314453
2018-10-21 07:34:58.488864:	Training iteration: 43000, Loss: 0.17534279823303223
2018-10-21 07:35:24.903635:	Training iteration: 43200, Loss: 0.10617317259311676
2018-10-21 07:35:51.417606:	Training iteration: 43400, Loss: 0.13226845860481262
2018-10-21 07:36:18.682580:	Training iteration: 43600, Loss: 0.08656905591487885
2018-10-21 07:36:44.990144:	Training iteration: 43800, Loss: 0.08943764120340347
2018-10-21 07:37:11.271308:	Training iteration: 44000, Loss: 0.14851722121238708
2018-10-21 07:37:37.730273:	Training iteration: 44200, Loss: 0.1796574741601944
2018-10-21 07:38:04.521601:	Training iteration: 44400, Loss: 0.13533276319503784
2018-10-21 07:38:32.107223:	Training iteration: 44600, Loss: 0.132709801197052
2018-10-21 07:38:58.903530:	Training iteration: 44800, Loss: 0.1609218716621399
2018-10-21 07:39:25.401417:	Training iteration: 45000, Loss: 0.13850842416286469
2018-10-21 07:39:51.678130:	Training iteration: 45200, Loss: 0.10626465082168579
2018-10-21 07:40:18.973434:	Training iteration: 45400, Loss: 0.1915959119796753
2018-10-21 07:40:46.274591:	Training iteration: 45600, Loss: 0.13748107850551605
2018-10-21 07:41:12.815840:	Training iteration: 45800, Loss: 0.1840382069349289
2018-10-21 07:41:39.159496:	Training iteration: 46000, Loss: 0.12208780646324158
2018-10-21 07:42:06.136041:	Training iteration: 46200, Loss: 0.1231536865234375
2018-10-21 07:42:33.102048:	Training iteration: 46400, Loss: 0.1170395165681839
2018-10-21 07:42:59.092756:	Training iteration: 46600, Loss: 0.10943780094385147
2018-10-21 07:43:25.883528:	Training iteration: 46800, Loss: 0.19687172770500183
2018-10-21 07:43:51.957784:	Training iteration: 47000, Loss: 0.10378250479698181
2018-10-21 07:44:18.990257:	Training iteration: 47200, Loss: 0.15605580806732178
2018-10-21 07:44:46.108206:	Training iteration: 47400, Loss: 0.11434680223464966
2018-10-21 07:45:13.126985:	Training iteration: 47600, Loss: 0.16570745408535004
2018-10-21 07:45:39.951228:	Training iteration: 47800, Loss: 0.13742578029632568
2018-10-21 07:46:07.001143:	Training iteration: 48000, Loss: 0.22339937090873718
2018-10-21 07:46:34.762122:	Training iteration: 48200, Loss: 0.12695591151714325
2018-10-21 07:47:01.695936:	Training iteration: 48400, Loss: 0.13772746920585632
2018-10-21 07:47:28.299043:	Training iteration: 48600, Loss: 0.20976826548576355
2018-10-21 07:47:55.730107:	Training iteration: 48800, Loss: 0.09481239318847656
2018-10-21 07:48:22.755943:	Training iteration: 49000, Loss: 0.07152576744556427
2018-10-21 07:48:49.132719:	Training iteration: 49200, Loss: 0.14569811522960663
2018-10-21 07:49:16.027801:	Training iteration: 49400, Loss: 0.13717786967754364
2018-10-21 07:49:42.547007:	Training iteration: 49600, Loss: 0.09228824824094772
2018-10-21 07:50:09.483182:	Training iteration: 49800, Loss: 0.07902997732162476
2018-10-21 07:50:37.501117:	Training iteration: 50000, Loss: 0.1236228421330452
2018-10-21 07:51:04.574371:	Training iteration: 50200, Loss: 0.0930822417140007
2018-10-21 07:51:33.576775: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 657 of 1000
2018-10-21 07:51:37.144757: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 07:51:44.040699:	Training iteration: 50400, Loss: 0.22167916595935822
2018-10-21 07:52:10.198893:	Training iteration: 50600, Loss: 0.1579388827085495
2018-10-21 07:52:36.549721:	Training iteration: 50800, Loss: 0.16348788142204285
2018-10-21 07:53:03.315794:	Training iteration: 51000, Loss: 0.14297086000442505
2018-10-21 07:53:29.729041:	Training iteration: 51200, Loss: 0.14208535850048065
2018-10-21 07:53:57.098812:	Training iteration: 51400, Loss: 0.28679412603378296
2018-10-21 07:54:23.694589:	Training iteration: 51600, Loss: 0.16608625650405884
2018-10-21 07:54:50.626077:	Training iteration: 51800, Loss: 0.15461237728595734
2018-10-21 07:55:17.360482:	Training iteration: 52000, Loss: 0.34201309084892273
2018-10-21 07:55:43.998511:	Training iteration: 52200, Loss: 0.16134794056415558
2018-10-21 07:56:10.721257:	Training iteration: 52400, Loss: 0.15596209466457367
2018-10-21 07:56:37.454808:	Training iteration: 52600, Loss: 0.13658387959003448
2018-10-21 07:57:03.441939:	Training iteration: 52800, Loss: 0.152972012758255
2018-10-21 07:57:29.400318:	Training iteration: 53000, Loss: 0.14016155898571014
2018-10-21 07:57:56.874856:	Training iteration: 53200, Loss: 0.16271467506885529
2018-10-21 07:58:23.606050:	Training iteration: 53400, Loss: 0.15208950638771057
2018-10-21 07:58:50.460107:	Training iteration: 53600, Loss: 0.20927909016609192
2018-10-21 07:59:16.808281:	Training iteration: 53800, Loss: 0.20206701755523682
2018-10-21 07:59:43.886378:	Training iteration: 54000, Loss: 0.115826316177845
2018-10-21 08:00:10.293631:	Training iteration: 54200, Loss: 0.11090724915266037
2018-10-21 08:00:37.457331:	Training iteration: 54400, Loss: 0.16069070994853973
2018-10-21 08:01:03.939760:	Training iteration: 54600, Loss: 0.16937623918056488
2018-10-21 08:01:30.783441:	Training iteration: 54800, Loss: 0.15294137597084045
2018-10-21 08:01:57.472002:	Training iteration: 55000, Loss: 0.12604816257953644
2018-10-21 08:02:24.338201:	Training iteration: 55200, Loss: 0.1906358003616333
2018-10-21 08:02:50.863003:	Training iteration: 55400, Loss: 0.13492897152900696
2018-10-21 08:03:17.279254:	Training iteration: 55600, Loss: 0.15739423036575317
2018-10-21 08:03:44.665794:	Training iteration: 55800, Loss: 0.20138168334960938
2018-10-21 08:04:11.189513:	Training iteration: 56000, Loss: 0.21453535556793213
2018-10-21 08:04:37.976071:	Training iteration: 56200, Loss: 0.10589022934436798
2018-10-21 08:05:04.036439:	Training iteration: 56400, Loss: 0.10799821466207504
2018-10-21 08:05:31.091522:	Training iteration: 56600, Loss: 0.23287338018417358
2018-10-21 08:05:57.907211:	Training iteration: 56800, Loss: 0.18287521600723267
2018-10-21 08:06:24.653686:	Training iteration: 57000, Loss: 0.21233516931533813
2018-10-21 08:06:51.322237:	Training iteration: 57200, Loss: 0.15194354951381683
2018-10-21 08:07:18.354782:	Training iteration: 57400, Loss: 0.14696261286735535
2018-10-21 08:07:45.309540:	Training iteration: 57600, Loss: 0.12336893379688263
2018-10-21 08:08:11.458503:	Training iteration: 57800, Loss: 0.20211176574230194
2018-10-21 08:08:37.922807:	Training iteration: 58000, Loss: 0.17968705296516418
2018-10-21 08:09:04.804495:	Training iteration: 58200, Loss: 0.18566060066223145
2018-10-21 08:09:32.100116:	Training iteration: 58400, Loss: 0.17578241229057312
2018-10-21 08:09:58.588142:	Training iteration: 58600, Loss: 0.17505896091461182
2018-10-21 08:10:24.981445:	Training iteration: 58800, Loss: 0.14227789640426636
2018-10-21 08:10:51.480199:	Training iteration: 59000, Loss: 0.16556397080421448
2018-10-21 08:11:18.563516:	Training iteration: 59200, Loss: 0.1680801510810852
2018-10-21 08:11:45.475854:	Training iteration: 59400, Loss: 0.2335560917854309
2018-10-21 08:12:12.110082:	Training iteration: 59600, Loss: 0.1650562584400177
2018-10-21 08:12:38.567334:	Training iteration: 59800, Loss: 0.1651674211025238
2018-10-21 08:13:05.694217:	Training iteration: 60000, Loss: 0.18383745849132538
2018-10-21 08:13:32.077016:	Training iteration: 60200, Loss: 0.20719745755195618
2018-10-21 08:13:58.130834:	Training iteration: 60400, Loss: 0.15835580229759216
2018-10-21 08:14:24.333284:	Training iteration: 60600, Loss: 0.20858687162399292
2018-10-21 08:14:50.982020:	Training iteration: 60800, Loss: 0.19524800777435303
2018-10-21 08:15:17.774219:	Training iteration: 61000, Loss: 0.13805338740348816
2018-10-21 08:15:43.938003:	Training iteration: 61200, Loss: 0.1252867579460144
2018-10-21 08:16:10.741632:	Training iteration: 61400, Loss: 0.15343807637691498
2018-10-21 08:16:38.135025:	Training iteration: 61600, Loss: 0.14676415920257568
2018-10-21 08:17:05.206528:	Training iteration: 61800, Loss: 0.19014835357666016
2018-10-21 08:17:31.405702:	Training iteration: 62000, Loss: 0.19842185080051422
2018-10-21 08:17:58.598334:	Training iteration: 62200, Loss: 0.17800559103488922
2018-10-21 08:18:24.360317:	Training iteration: 62400, Loss: 0.20718683302402496
2018-10-21 08:18:51.136354:	Training iteration: 62600, Loss: 0.18597091734409332
2018-10-21 08:19:17.407706:	Training iteration: 62800, Loss: 0.10991640388965607
2018-10-21 08:19:45.205365:	Training iteration: 63000, Loss: 0.15308532118797302
2018-10-21 08:20:12.340855:	Training iteration: 63200, Loss: 0.1539798378944397
2018-10-21 08:20:39.671206:	Training iteration: 63400, Loss: 0.18964195251464844
2018-10-21 08:21:06.761807:	Training iteration: 63600, Loss: 0.15165886282920837
2018-10-21 08:21:33.794872:	Training iteration: 63800, Loss: 0.17686273157596588
2018-10-21 08:21:59.669323:	Training iteration: 64000, Loss: 0.15400967001914978
2018-10-21 08:22:26.762526:	Training iteration: 64200, Loss: 0.20579794049263
2018-10-21 08:22:53.133297:	Training iteration: 64400, Loss: 0.18858814239501953
2018-10-21 08:23:19.703214:	Training iteration: 64600, Loss: 0.1668461561203003
2018-10-21 08:23:46.412645:	Training iteration: 64800, Loss: 0.15525972843170166
2018-10-21 08:24:13.494223:	Training iteration: 65000, Loss: 0.14294010400772095
2018-10-21 08:24:40.092554:	Training iteration: 65200, Loss: 0.1284571886062622
2018-10-21 08:25:07.333209:	Training iteration: 65400, Loss: 0.15836794674396515
2018-10-21 08:25:34.965026:	Training iteration: 65600, Loss: 0.11735247820615768
2018-10-21 08:26:00.931015:	Training iteration: 65800, Loss: 0.15143054723739624
2018-10-21 08:26:28.341950:	Training iteration: 66000, Loss: 0.17530858516693115
2018-10-21 08:26:55.078168:	Training iteration: 66200, Loss: 0.1380842626094818
2018-10-21 08:27:21.889242:	Training iteration: 66400, Loss: 0.14081093668937683
2018-10-21 08:27:48.054858:	Training iteration: 66600, Loss: 0.2127177119255066
2018-10-21 08:28:14.804295:	Training iteration: 66800, Loss: 0.1685052514076233
2018-10-21 08:28:42.091603:	Training iteration: 67000, Loss: 0.25382199883461
2018-10-21 08:29:09.270858:	Training iteration: 67200, Loss: 0.29888808727264404
2018-10-21 08:29:35.768849:	Training iteration: 67400, Loss: 0.2702113389968872
2018-10-21 08:30:02.217925:	Training iteration: 67600, Loss: 0.17454934120178223
2018-10-21 08:30:29.579687:	Training iteration: 67800, Loss: 0.14079387485980988
2018-10-21 08:30:56.512641:	Training iteration: 68000, Loss: 0.1325308382511139
2018-10-21 08:31:23.384592:	Training iteration: 68200, Loss: 0.20627561211585999
2018-10-21 08:31:50.388251:	Training iteration: 68400, Loss: 0.12365853786468506
2018-10-21 08:32:17.195956:	Training iteration: 68600, Loss: 0.14417168498039246
2018-10-21 08:32:44.086985:	Training iteration: 68800, Loss: 0.1366642415523529
2018-10-21 08:33:10.975958:	Training iteration: 69000, Loss: 0.21123725175857544
2018-10-21 08:33:37.719463:	Training iteration: 69200, Loss: 0.14972171187400818
2018-10-21 08:34:04.037396:	Training iteration: 69400, Loss: 0.07012701034545898
2018-10-21 08:34:30.388515:	Training iteration: 69600, Loss: 0.15131691098213196
2018-10-21 08:34:57.955740:	Training iteration: 69800, Loss: 0.19174307584762573
2018-10-21 08:35:24.408881:	Training iteration: 70000, Loss: 0.18312618136405945
2018-10-21 08:35:51.694083:	Training iteration: 70200, Loss: 0.14970004558563232
2018-10-21 08:36:18.296728:	Training iteration: 70400, Loss: 0.20344693958759308
2018-10-21 08:36:44.819074:	Training iteration: 70600, Loss: 0.1692913919687271
2018-10-21 08:37:12.101550:	Training iteration: 70800, Loss: 0.195286363363266
2018-10-21 08:37:39.514636:	Training iteration: 71000, Loss: 0.15572980046272278
2018-10-21 08:38:05.425157:	Training iteration: 71200, Loss: 0.18493446707725525
2018-10-21 08:38:31.727258:	Training iteration: 71400, Loss: 0.13481754064559937
2018-10-21 08:38:58.628165:	Training iteration: 71600, Loss: 0.1393667757511139
2018-10-21 08:39:25.464498:	Training iteration: 71800, Loss: 0.14734452962875366
2018-10-21 08:39:51.420922:	Training iteration: 72000, Loss: 0.12208376079797745
2018-10-21 08:40:18.249485:	Training iteration: 72200, Loss: 0.19483238458633423
2018-10-21 08:40:45.101428:	Training iteration: 72400, Loss: 0.1615179032087326
2018-10-21 08:41:10.848140:	Training iteration: 72600, Loss: 0.18145567178726196
2018-10-21 08:41:38.158358:	Training iteration: 72800, Loss: 0.19229090213775635
2018-10-21 08:42:05.190158:	Training iteration: 73000, Loss: 0.12632553279399872
2018-10-21 08:42:31.314180:	Training iteration: 73200, Loss: 0.162236750125885
2018-10-21 08:42:57.799358:	Training iteration: 73400, Loss: 0.1481950581073761
2018-10-21 08:43:25.427026:	Training iteration: 73600, Loss: 0.12986396253108978
2018-10-21 08:43:52.476123:	Training iteration: 73800, Loss: 0.16451658308506012
2018-10-21 08:44:18.808197:	Training iteration: 74000, Loss: 0.16537559032440186
2018-10-21 08:44:46.286423:	Training iteration: 74200, Loss: 0.16390913724899292
2018-10-21 08:45:13.114343:	Training iteration: 74400, Loss: 0.1333591789007187
2018-10-21 08:45:40.521067:	Training iteration: 74600, Loss: 0.14884307980537415
2018-10-21 08:46:07.792672:	Training iteration: 74800, Loss: 0.14242982864379883
2018-10-21 08:46:33.906961:	Training iteration: 75000, Loss: 0.0986335501074791
2018-10-21 08:47:01.155187:	Training iteration: 75200, Loss: 0.1681358367204666
2018-10-21 08:47:28.057476:	Training iteration: 75400, Loss: 0.16489869356155396
2018-10-21 08:47:54.725193:	Training iteration: 75600, Loss: 0.16189700365066528
2018-10-21 08:48:20.589166:	Training iteration: 75800, Loss: 0.1352352499961853
2018-10-21 08:48:47.253397:	Training iteration: 76000, Loss: 0.17301931977272034
2018-10-21 08:49:13.852521:	Training iteration: 76200, Loss: 0.1335572898387909
2018-10-21 08:49:41.111749:	Training iteration: 76400, Loss: 0.1676015853881836
2018-10-21 08:50:08.589156:	Training iteration: 76600, Loss: 0.19834071397781372
2018-10-21 08:50:35.922264:	Training iteration: 76800, Loss: 0.14153960347175598
2018-10-21 08:51:02.048539:	Training iteration: 77000, Loss: 0.1349189579486847
2018-10-21 08:51:28.289908:	Training iteration: 77200, Loss: 0.10581609606742859
2018-10-21 08:51:54.428561:	Training iteration: 77400, Loss: 0.14313948154449463
2018-10-21 08:52:21.274248:	Training iteration: 77600, Loss: 0.11859562993049622
2018-10-21 08:52:48.120558:	Training iteration: 77800, Loss: 0.16211619973182678
2018-10-21 08:53:15.556007:	Training iteration: 78000, Loss: 0.1384032517671585
2018-10-21 08:53:43.179465:	Training iteration: 78200, Loss: 0.15696319937705994
2018-10-21 08:54:10.077654:	Training iteration: 78400, Loss: 0.13837149739265442
2018-10-21 08:54:37.178249:	Training iteration: 78600, Loss: 0.18471397459506989
2018-10-21 08:55:04.103032:	Training iteration: 78800, Loss: 0.19385801255702972
2018-10-21 08:55:30.312649:	Training iteration: 79000, Loss: 0.12219860404729843
2018-10-21 08:55:57.454591:	Training iteration: 79200, Loss: 0.15254813432693481
2018-10-21 08:56:24.420454:	Training iteration: 79400, Loss: 0.1331888735294342
2018-10-21 08:56:50.833848:	Training iteration: 79600, Loss: 0.1324060708284378
2018-10-21 08:57:17.034453:	Training iteration: 79800, Loss: 0.16724762320518494
2018-10-21 08:57:43.706381:	Training iteration: 80000, Loss: 0.14419814944267273
2018-10-21 08:58:10.601293:	Training iteration: 80200, Loss: 0.1333746314048767
2018-10-21 08:58:38.494947:	Training iteration: 80400, Loss: 0.1340765506029129
2018-10-21 08:59:04.943756:	Training iteration: 80600, Loss: 0.19606494903564453
2018-10-21 08:59:31.450836:	Training iteration: 80800, Loss: 0.15307798981666565
2018-10-21 08:59:58.530270:	Training iteration: 81000, Loss: 0.1525798738002777
2018-10-21 09:00:24.854150:	Training iteration: 81200, Loss: 0.15131482481956482
2018-10-21 09:00:51.478643:	Training iteration: 81400, Loss: 0.14565733075141907
2018-10-21 09:01:18.171213:	Training iteration: 81600, Loss: 0.15865227580070496
2018-10-21 09:01:44.970956:	Training iteration: 81800, Loss: 0.16449373960494995
2018-10-21 09:02:10.754131:	Training iteration: 82000, Loss: 0.16573747992515564
2018-10-21 09:02:37.287576:	Training iteration: 82200, Loss: 0.1348963975906372
2018-10-21 09:03:04.901454:	Training iteration: 82400, Loss: 0.16216284036636353
2018-10-21 09:03:32.211152:	Training iteration: 82600, Loss: 0.1709056794643402
2018-10-21 09:03:59.202743:	Training iteration: 82800, Loss: 0.13317041099071503
2018-10-21 09:04:25.597080:	Training iteration: 83000, Loss: 0.1696392297744751
2018-10-21 09:04:52.778547:	Training iteration: 83200, Loss: 0.17481833696365356
2018-10-21 09:05:18.689827:	Training iteration: 83400, Loss: 0.17479200661182404
2018-10-21 09:05:46.221627:	Training iteration: 83600, Loss: 0.21191023290157318
2018-10-21 09:06:13.181591:	Training iteration: 83800, Loss: 0.23394611477851868
2018-10-21 09:06:40.851232:	Training iteration: 84000, Loss: 0.19146224856376648
2018-10-21 09:07:08.235894:	Training iteration: 84200, Loss: 0.1380656659603119
2018-10-21 09:07:35.039030:	Training iteration: 84400, Loss: 0.20649147033691406
2018-10-21 09:08:01.607192:	Training iteration: 84600, Loss: 0.1561427265405655
2018-10-21 09:08:27.585007:	Training iteration: 84800, Loss: 0.16079623997211456
2018-10-21 09:08:53.542836:	Training iteration: 85000, Loss: 0.16486573219299316
2018-10-21 09:09:20.817813:	Training iteration: 85200, Loss: 0.10237890481948853
2018-10-21 09:09:47.669142:	Training iteration: 85400, Loss: 0.18142575025558472
2018-10-21 09:10:14.779101:	Training iteration: 85600, Loss: 0.1442895233631134
2018-10-21 09:10:41.450265:	Training iteration: 85800, Loss: 0.19047784805297852
2018-10-21 09:11:07.691684:	Training iteration: 86000, Loss: 0.16822072863578796
2018-10-21 09:11:34.611924:	Training iteration: 86200, Loss: 0.15166553854942322
2018-10-21 09:12:01.488495:	Training iteration: 86400, Loss: 0.2075902670621872
2018-10-21 09:12:27.690390:	Training iteration: 86600, Loss: 0.16228435933589935
2018-10-21 09:12:54.393206:	Training iteration: 86800, Loss: 0.19264405965805054
2018-10-21 09:13:21.756880:	Training iteration: 87000, Loss: 0.12184427678585052
2018-10-21 09:13:49.044865:	Training iteration: 87200, Loss: 0.12870906293392181
2018-10-21 09:14:15.841055:	Training iteration: 87400, Loss: 0.13400903344154358
2018-10-21 09:14:42.614403:	Training iteration: 87600, Loss: 0.1525820791721344
2018-10-21 09:15:09.728400:	Training iteration: 87800, Loss: 0.1293797791004181
2018-10-21 09:15:36.303627:	Training iteration: 88000, Loss: 0.21564942598342896
2018-10-21 09:16:03.236461:	Training iteration: 88200, Loss: 0.15401601791381836
2018-10-21 09:16:29.200864:	Training iteration: 88400, Loss: 0.1461488902568817
2018-10-21 09:16:55.866457:	Training iteration: 88600, Loss: 0.13778838515281677
2018-10-21 09:17:22.396421:	Training iteration: 88800, Loss: 0.15642966330051422
2018-10-21 09:17:49.341836:	Training iteration: 89000, Loss: 0.1969374418258667
2018-10-21 09:18:15.730357:	Training iteration: 89200, Loss: 0.13859587907791138
2018-10-21 09:18:43.081191:	Training iteration: 89400, Loss: 0.1372908651828766
2018-10-21 09:19:10.497512:	Training iteration: 89600, Loss: 0.2267000377178192
2018-10-21 09:19:36.454926:	Training iteration: 89800, Loss: 0.12023825943470001
2018-10-21 09:20:03.163571:	Training iteration: 90000, Loss: 0.12837067246437073
2018-10-21 09:20:30.526302:	Training iteration: 90200, Loss: 0.115847148001194
2018-10-21 09:20:57.378510:	Training iteration: 90400, Loss: 0.18295101821422577
2018-10-21 09:21:24.821834:	Training iteration: 90600, Loss: 0.15744377672672272
2018-10-21 09:21:51.758616:	Training iteration: 90800, Loss: 0.16815480589866638
2018-10-21 09:22:18.883738:	Training iteration: 91000, Loss: 0.12650030851364136
2018-10-21 09:22:45.349851:	Training iteration: 91200, Loss: 0.12113891541957855
2018-10-21 09:23:11.965582:	Training iteration: 91400, Loss: 0.20518501102924347
2018-10-21 09:23:39.513749:	Training iteration: 91600, Loss: 0.22167488932609558
2018-10-21 09:24:06.302006:	Training iteration: 91800, Loss: 0.16172818839550018
2018-10-21 09:24:33.367246:	Training iteration: 92000, Loss: 0.143609881401062
2018-10-21 09:24:59.778607:	Training iteration: 92200, Loss: 0.11851920187473297
2018-10-21 09:25:26.408469:	Training iteration: 92400, Loss: 0.12272130697965622
2018-10-21 09:25:53.423760:	Training iteration: 92600, Loss: 0.14673206210136414
2018-10-21 09:26:20.863314:	Training iteration: 92800, Loss: 0.13526153564453125
2018-10-21 09:26:47.634252:	Training iteration: 93000, Loss: 0.13814018666744232
2018-10-21 09:27:14.886106:	Training iteration: 93200, Loss: 0.12580016255378723
2018-10-21 09:27:42.292089:	Training iteration: 93400, Loss: 0.1825263500213623
2018-10-21 09:28:09.559072:	Training iteration: 93600, Loss: 0.15127140283584595
2018-10-21 09:28:36.748040:	Training iteration: 93800, Loss: 0.14742887020111084
2018-10-21 09:29:03.407705:	Training iteration: 94000, Loss: 0.167409747838974
2018-10-21 09:29:29.747878:	Training iteration: 94200, Loss: 0.34256988763809204
2018-10-21 09:29:56.327831:	Training iteration: 94400, Loss: 0.17049655318260193
2018-10-21 09:30:22.768606:	Training iteration: 94600, Loss: 0.17019140720367432
2018-10-21 09:30:49.269017:	Training iteration: 94800, Loss: 0.1373070776462555
2018-10-21 09:31:16.275510:	Training iteration: 95000, Loss: 0.1371031105518341
2018-10-21 09:31:43.174878:	Training iteration: 95200, Loss: 0.19600927829742432
2018-10-21 09:32:10.053884:	Training iteration: 95400, Loss: 0.15140369534492493
2018-10-21 09:32:36.880015:	Training iteration: 95600, Loss: 0.1772746443748474
2018-10-21 09:33:03.374900:	Training iteration: 95800, Loss: 0.12580204010009766
2018-10-21 09:33:29.454295:	Training iteration: 96000, Loss: 0.11141470074653625
2018-10-21 09:33:55.751562:	Training iteration: 96200, Loss: 0.16898921132087708
2018-10-21 09:34:21.806316:	Training iteration: 96400, Loss: 0.14947086572647095
2018-10-21 09:34:48.717712:	Training iteration: 96600, Loss: 0.15951353311538696
2018-10-21 09:35:15.034667:	Training iteration: 96800, Loss: 0.15638640522956848
2018-10-21 09:35:41.677279:	Training iteration: 97000, Loss: 0.14342929422855377
2018-10-21 09:36:08.333882:	Training iteration: 97200, Loss: 0.08089715242385864
2018-10-21 09:36:35.217592:	Training iteration: 97400, Loss: 0.15582603216171265
2018-10-21 09:37:02.462559:	Training iteration: 97600, Loss: 0.12472054362297058
2018-10-21 09:37:29.073557:	Training iteration: 97800, Loss: 0.13392864167690277
2018-10-21 09:37:55.750154:	Training iteration: 98000, Loss: 0.09984272718429565
2018-10-21 09:38:22.764118:	Training iteration: 98200, Loss: 0.14637570083141327
2018-10-21 09:38:49.577362:	Training iteration: 98400, Loss: 0.15693244338035583
2018-10-21 09:39:16.613457:	Training iteration: 98600, Loss: 0.1823178082704544
2018-10-21 09:39:44.080736:	Training iteration: 98800, Loss: 0.14902250468730927
2018-10-21 09:40:10.615454:	Training iteration: 99000, Loss: 0.1793517768383026
2018-10-21 09:40:37.071127:	Training iteration: 99200, Loss: 0.1468169391155243
2018-10-21 09:41:03.693640:	Training iteration: 99400, Loss: 0.1519053429365158
2018-10-21 09:41:30.912755:	Training iteration: 99600, Loss: 0.19746874272823334
2018-10-21 09:41:57.972272:	Training iteration: 99800, Loss: 0.12367136031389236
2018-10-21 09:42:24.143262:	Training iteration: 100000, Loss: 0.1290372610092163
2018-10-21 09:42:50.591708:	Training iteration: 100200, Loss: 0.1291392743587494
2018-10-21 09:43:17.835323:	Training iteration: 100400, Loss: 0.14054575562477112
2018-10-21 09:43:44.462377:	Training iteration: 100600, Loss: 0.1369287073612213
2018-10-21 09:44:10.910120:	Training iteration: 100800, Loss: 0.14677132666110992
2018-10-21 09:44:37.569702:	Training iteration: 101000, Loss: 0.12905806303024292
2018-10-21 09:45:03.789367:	Training iteration: 101200, Loss: 0.09262719750404358
2018-10-21 09:45:31.428258:	Training iteration: 101400, Loss: 0.14152802526950836
2018-10-21 09:45:58.036783:	Training iteration: 101600, Loss: 0.12260168790817261
2018-10-21 09:46:23.882209:	Training iteration: 101800, Loss: 0.10473006963729858
2018-10-21 09:46:51.262673:	Training iteration: 102000, Loss: 0.20388263463974
2018-10-21 09:47:18.168504:	Training iteration: 102200, Loss: 0.1203370988368988
2018-10-21 09:47:44.994807:	Training iteration: 102400, Loss: 0.12488807737827301
2018-10-21 09:48:12.186034:	Training iteration: 102600, Loss: 0.22083497047424316
2018-10-21 09:48:38.508152:	Training iteration: 102800, Loss: 0.19083765149116516
2018-10-21 09:49:05.858693:	Training iteration: 103000, Loss: 0.14260989427566528
2018-10-21 09:49:32.026505:	Training iteration: 103200, Loss: 0.1939077526330948
2018-10-21 09:49:59.328090:	Training iteration: 103400, Loss: 0.1603618562221527
2018-10-21 09:50:25.428989:	Training iteration: 103600, Loss: 0.10942575335502625
2018-10-21 09:50:52.106878:	Training iteration: 103800, Loss: 0.149396151304245
2018-10-21 09:51:18.557824:	Training iteration: 104000, Loss: 0.18324582278728485
2018-10-21 09:51:44.497261:	Training iteration: 104200, Loss: 0.17160876095294952
2018-10-21 09:52:11.003179:	Training iteration: 104400, Loss: 0.1646997183561325
2018-10-21 09:52:38.519664:	Training iteration: 104600, Loss: 0.17470821738243103
2018-10-21 09:53:05.547575:	Training iteration: 104800, Loss: 0.15653511881828308
2018-10-21 09:53:32.748572:	Training iteration: 105000, Loss: 0.17393392324447632
2018-10-21 09:53:58.415311:	Training iteration: 105200, Loss: 0.20134031772613525
2018-10-21 09:54:26.749388:	Training iteration: 105400, Loss: 0.13080823421478271
2018-10-21 09:54:53.279291:	Training iteration: 105600, Loss: 0.1601329892873764
2018-10-21 09:55:19.710129:	Training iteration: 105800, Loss: 0.14728054404258728
2018-10-21 09:55:46.667816:	Training iteration: 106000, Loss: 0.1407054364681244
2018-10-21 09:56:12.868492:	Training iteration: 106200, Loss: 0.14699319005012512
2018-10-21 09:56:40.881025:	Training iteration: 106400, Loss: 0.19392022490501404
2018-10-21 09:57:07.543710:	Training iteration: 106600, Loss: 0.24052147567272186
2018-10-21 09:57:34.209270:	Training iteration: 106800, Loss: 0.16453513503074646
2018-10-21 09:58:01.640102:	Training iteration: 107000, Loss: 0.15368328988552094
2018-10-21 09:58:28.134084:	Training iteration: 107200, Loss: 0.09870481491088867
2018-10-21 09:58:55.239428:	Training iteration: 107400, Loss: 0.1338256299495697
2018-10-21 09:59:20.936992:	Training iteration: 107600, Loss: 0.10833697021007538
2018-10-21 09:59:47.297328:	Training iteration: 107800, Loss: 0.19786956906318665
2018-10-21 10:00:14.138434:	Training iteration: 108000, Loss: 0.17098328471183777
2018-10-21 10:00:41.472512:	Training iteration: 108200, Loss: 0.1361437439918518
2018-10-21 10:01:09.012514:	Training iteration: 108400, Loss: 0.14286455512046814
2018-10-21 10:01:34.699140:	Training iteration: 108600, Loss: 0.13744112849235535
2018-10-21 10:02:01.814538:	Training iteration: 108800, Loss: 0.2396652102470398
2018-10-21 10:02:28.479059:	Training iteration: 109000, Loss: 0.1882132589817047
2018-10-21 10:02:55.077763:	Training iteration: 109200, Loss: 0.17515327036380768
2018-10-21 10:03:23.483412:	Training iteration: 109400, Loss: 0.1582474708557129
2018-10-21 10:03:49.542026:	Training iteration: 109600, Loss: 0.1318083554506302
2018-10-21 10:04:15.958022:	Training iteration: 109800, Loss: 0.16812169551849365
2018-10-21 10:04:42.482580:	Training iteration: 110000, Loss: 0.164456307888031
2018-10-21 10:05:08.819876:	Training iteration: 110200, Loss: 0.13462096452713013
2018-10-21 10:05:36.229578:	Training iteration: 110400, Loss: 0.1362350732088089
2018-10-21 10:06:02.068446:	Training iteration: 110600, Loss: 0.1422574818134308
2018-10-21 10:06:28.860453:	Training iteration: 110800, Loss: 0.1290055811405182
2018-10-21 10:06:55.952551:	Training iteration: 111000, Loss: 0.17792166769504547
2018-10-21 10:07:22.499675:	Training iteration: 111200, Loss: 0.1630435287952423
2018-10-21 10:07:49.025949:	Training iteration: 111400, Loss: 0.17667034268379211
2018-10-21 10:08:15.865035:	Training iteration: 111600, Loss: 0.15891362726688385
2018-10-21 10:08:42.567090:	Training iteration: 111800, Loss: 0.19620400667190552
2018-10-21 10:09:09.487529:	Training iteration: 112000, Loss: 0.1407012641429901
2018-10-21 10:09:35.632780:	Training iteration: 112200, Loss: 0.19793549180030823
2018-10-21 10:10:01.675397:	Training iteration: 112400, Loss: 0.1821606457233429
2018-10-21 10:10:28.652649:	Training iteration: 112600, Loss: 0.15486860275268555
2018-10-21 10:10:56.050266:	Training iteration: 112800, Loss: 0.14008069038391113
2018-10-21 10:11:22.136336:	Training iteration: 113000, Loss: 0.18479397892951965
2018-10-21 10:11:48.775133:	Training iteration: 113200, Loss: 0.1281559318304062
2018-10-21 10:12:15.075458:	Training iteration: 113400, Loss: 0.16161638498306274
2018-10-21 10:12:41.841222:	Training iteration: 113600, Loss: 0.14298492670059204
2018-10-21 10:13:08.407185:	Training iteration: 113800, Loss: 0.12997564673423767
2018-10-21 10:13:35.804290:	Training iteration: 114000, Loss: 0.13917042315006256
2018-10-21 10:14:02.309796:	Training iteration: 114200, Loss: 0.18171416223049164
2018-10-21 10:14:29.532289:	Training iteration: 114400, Loss: 0.1445717215538025
2018-10-21 10:14:56.953209:	Training iteration: 114600, Loss: 0.21510660648345947
2018-10-21 10:15:23.225088:	Training iteration: 114800, Loss: 0.16602376103401184
2018-10-21 10:15:49.406239:	Training iteration: 115000, Loss: 0.19965650141239166
2018-10-21 10:16:16.125851:	Training iteration: 115200, Loss: 0.09956883639097214
2018-10-21 10:16:42.187521:	Training iteration: 115400, Loss: 0.1491514891386032
2018-10-21 10:17:09.070509:	Training iteration: 115600, Loss: 0.15442758798599243
2018-10-21 10:17:36.019045:	Training iteration: 115800, Loss: 0.18367639183998108
2018-10-21 10:18:02.240850:	Training iteration: 116000, Loss: 0.1947411298751831
2018-10-21 10:18:28.893774:	Training iteration: 116200, Loss: 0.1632835865020752
2018-10-21 10:18:55.526374:	Training iteration: 116400, Loss: 0.13039708137512207
2018-10-21 10:19:22.967834:	Training iteration: 116600, Loss: 0.15756523609161377
2018-10-21 10:19:49.714674:	Training iteration: 116800, Loss: 0.16267919540405273
2018-10-21 10:20:16.059326:	Training iteration: 117000, Loss: 0.12777230143547058
2018-10-21 10:20:42.639701:	Training iteration: 117200, Loss: 0.1520794928073883
2018-10-21 10:21:08.774962:	Training iteration: 117400, Loss: 0.13580696284770966
2018-10-21 10:21:35.614731:	Training iteration: 117600, Loss: 0.13412317633628845
2018-10-21 10:22:02.207862:	Training iteration: 117800, Loss: 0.122057244181633
2018-10-21 10:22:28.622470:	Training iteration: 118000, Loss: 0.1409418284893036
2018-10-21 10:22:55.637336:	Training iteration: 118200, Loss: 0.1150682270526886
2018-10-21 10:23:22.726213:	Training iteration: 118400, Loss: 0.1327298879623413
2018-10-21 10:23:50.124211:	Training iteration: 118600, Loss: 0.15625926852226257
2018-10-21 10:24:16.415012:	Training iteration: 118800, Loss: 0.16688774526119232
2018-10-21 10:24:44.422065:	Training iteration: 119000, Loss: 0.2365354597568512
2018-10-21 10:25:10.396302:	Training iteration: 119200, Loss: 0.18806225061416626
2018-10-21 10:25:36.941602:	Training iteration: 119400, Loss: 0.20162537693977356
2018-10-21 10:26:04.361674:	Training iteration: 119600, Loss: 0.17247521877288818
2018-10-21 10:26:32.007956:	Training iteration: 119800, Loss: 0.16472946107387543
2018-10-21 10:26:52.549808:	Epoch 0 finished after 119952 iterations.
No images to record
Validating
2018-10-21 10:26:54.177180:	Entering validation loop
2018-10-21 10:27:05.808462: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 708 of 1000
2018-10-21 10:27:10.048778: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:27:21.374849:	Validation iteration: 200, Loss: 0.26909154653549194
2018-10-21 10:27:33.410157:	Validation iteration: 400, Loss: 0.18982237577438354
2018-10-21 10:27:45.828610:	Validation iteration: 600, Loss: 0.22602002322673798
2018-10-21 10:27:57.834885:	Validation iteration: 800, Loss: 0.20373296737670898
2018-10-21 10:28:10.037210:	Validation iteration: 1000, Loss: 0.13355031609535217
2018-10-21 10:28:23.022708:	Validation iteration: 1200, Loss: 0.1491011381149292
2018-10-21 10:28:35.659701:	Validation iteration: 1400, Loss: 0.15063366293907166
2018-10-21 10:28:49.462842:	Validation iteration: 1600, Loss: 0.16878722608089447
2018-10-21 10:29:02.636859:	Validation iteration: 1800, Loss: 0.1490117609500885
2018-10-21 10:29:15.791934:	Validation iteration: 2000, Loss: 0.1718689203262329
2018-10-21 10:29:31.642559: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 729 of 1000
2018-10-21 10:29:35.083959: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:29:41.891647:	Validation iteration: 2200, Loss: 0.19775426387786865
2018-10-21 10:29:53.438700:	Validation iteration: 2400, Loss: 0.23680463433265686
2018-10-21 10:30:05.767397:	Validation iteration: 2600, Loss: 0.18738524615764618
2018-10-21 10:30:18.477363:	Validation iteration: 2800, Loss: 0.19305330514907837
2018-10-21 10:30:31.323735:	Validation iteration: 3000, Loss: 0.2421514391899109
2018-10-21 10:30:43.844017:	Validation iteration: 3200, Loss: 0.29864734411239624
2018-10-21 10:30:56.644774:	Validation iteration: 3400, Loss: 0.19973860681056976
2018-10-21 10:31:09.192279:	Validation iteration: 3600, Loss: 0.268449604511261
2018-10-21 10:31:22.030834:	Validation iteration: 3800, Loss: 0.19653338193893433
2018-10-21 10:31:35.049940:	Validation iteration: 4000, Loss: 0.18260493874549866
2018-10-21 10:31:56.624670: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 724 of 1000
2018-10-21 10:32:00.236199: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:32:01.682600:	Validation iteration: 4200, Loss: 0.19289496541023254
2018-10-21 10:32:12.891537:	Validation iteration: 4400, Loss: 0.1681879162788391
2018-10-21 10:32:24.773044:	Validation iteration: 4600, Loss: 0.1669195294380188
2018-10-21 10:32:37.237495:	Validation iteration: 4800, Loss: 0.1970503181219101
2018-10-21 10:32:50.103994:	Validation iteration: 5000, Loss: 0.14684641361236572
2018-10-21 10:33:03.102033:	Validation iteration: 5200, Loss: 0.14976081252098083
2018-10-21 10:33:15.935053:	Validation iteration: 5400, Loss: 0.19792440533638
2018-10-21 10:33:28.384511:	Validation iteration: 5600, Loss: 0.1660611480474472
2018-10-21 10:33:41.092659:	Validation iteration: 5800, Loss: 0.20066478848457336
2018-10-21 10:33:53.950398:	Validation iteration: 6000, Loss: 0.11820381879806519
2018-10-21 10:34:06.843790:	Validation iteration: 6200, Loss: 0.1877514123916626
2018-10-21 10:34:21.018328: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 735 of 1000
2018-10-21 10:34:24.477660: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:34:32.071711:	Validation iteration: 6400, Loss: 0.24217219650745392
2018-10-21 10:34:43.736086:	Validation iteration: 6600, Loss: 0.2358994036912918
2018-10-21 10:34:56.046433:	Validation iteration: 6800, Loss: 0.298628032207489
2018-10-21 10:35:08.370996:	Validation iteration: 7000, Loss: 0.1543826162815094
2018-10-21 10:35:21.032412:	Validation iteration: 7200, Loss: 0.23410142958164215
2018-10-21 10:35:33.532582:	Validation iteration: 7400, Loss: 0.2553701102733612
2018-10-21 10:35:46.197184:	Validation iteration: 7600, Loss: 0.19498735666275024
2018-10-21 10:35:58.642070:	Validation iteration: 7800, Loss: 0.21493561565876007
2018-10-21 10:36:11.708527:	Validation iteration: 8000, Loss: 0.16551174223423004
2018-10-21 10:36:24.774156:	Validation iteration: 8200, Loss: 0.2121756374835968
2018-10-21 10:36:44.648812: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 666 of 1000
2018-10-21 10:36:49.144240: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:36:51.887519:	Validation iteration: 8400, Loss: 0.22584253549575806
2018-10-21 10:37:03.221088:	Validation iteration: 8600, Loss: 0.1650453507900238
2018-10-21 10:37:15.163106:	Validation iteration: 8800, Loss: 0.13662537932395935
2018-10-21 10:37:27.710108:	Validation iteration: 9000, Loss: 0.15678304433822632
2018-10-21 10:37:40.246498:	Validation iteration: 9200, Loss: 0.14064478874206543
2018-10-21 10:37:52.925911:	Validation iteration: 9400, Loss: 0.1116405799984932
2018-10-21 10:38:05.412810:	Validation iteration: 9600, Loss: 0.19803917407989502
2018-10-21 10:38:18.143652:	Validation iteration: 9800, Loss: 0.21702177822589874
2018-10-21 10:38:30.615112:	Validation iteration: 10000, Loss: 0.1559019386768341
2018-10-21 10:38:43.793132:	Validation iteration: 10200, Loss: 0.147318035364151
2018-10-21 10:38:56.064180:	Validation iteration: 10400, Loss: 0.15754559636116028
2018-10-21 10:39:08.726767:	Validation iteration: 10600, Loss: 0.22099602222442627
2018-10-21 10:39:21.304336:	Validation iteration: 10800, Loss: 0.17777585983276367
2018-10-21 10:39:34.581163:	Validation iteration: 11000, Loss: 0.1603681445121765
2018-10-21 10:39:47.576056:	Validation iteration: 11200, Loss: 0.18252426385879517
Validation check mean loss: 0.19444050375195282
Validation loss has improved!
New best validation cost!
Checkpoint
2018-10-21 10:40:26.312072: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 798 of 1000
2018-10-21 10:40:28.659521: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 10:40:34.577318:	Training iteration: 120000, Loss: 0.14191322028636932
2018-10-21 10:40:58.369177:	Training iteration: 120200, Loss: 0.12505438923835754
2018-10-21 10:41:23.425301:	Training iteration: 120400, Loss: 0.15745869278907776
2018-10-21 10:41:49.639903:	Training iteration: 120600, Loss: 0.08608725666999817
2018-10-21 10:42:16.838957:	Training iteration: 120800, Loss: 0.12382084131240845
2018-10-21 10:42:43.720029:	Training iteration: 121000, Loss: 0.09797757863998413
2018-10-21 10:43:10.303064:	Training iteration: 121200, Loss: 0.15235362946987152
2018-10-21 10:43:36.909591:	Training iteration: 121400, Loss: 0.10142262279987335
2018-10-21 10:44:04.234256:	Training iteration: 121600, Loss: 0.12214824557304382
2018-10-21 10:44:30.047088:	Training iteration: 121800, Loss: 0.07594669610261917
2018-10-21 10:44:57.238496:	Training iteration: 122000, Loss: 0.11849276721477509
2018-10-21 10:45:22.836256:	Training iteration: 122200, Loss: 0.0863318145275116
2018-10-21 10:45:49.944551:	Training iteration: 122400, Loss: 0.09998547285795212
2018-10-21 10:46:16.907178:	Training iteration: 122600, Loss: 0.1046086847782135
2018-10-21 10:46:43.480796:	Training iteration: 122800, Loss: 0.1287350058555603
2018-10-21 10:47:09.603979:	Training iteration: 123000, Loss: 0.06091725826263428
2018-10-21 10:47:36.175096:	Training iteration: 123200, Loss: 0.13265198469161987
2018-10-21 10:48:02.990946:	Training iteration: 123400, Loss: 0.10926856100559235
2018-10-21 10:48:29.601884:	Training iteration: 123600, Loss: 0.09706106781959534
2018-10-21 10:48:55.232749:	Training iteration: 123800, Loss: 0.10697409510612488
2018-10-21 10:49:21.695052:	Training iteration: 124000, Loss: 0.14771133661270142
2018-10-21 10:49:48.167722:	Training iteration: 124200, Loss: 0.13328790664672852
2018-10-21 10:50:15.909540:	Training iteration: 124400, Loss: 0.09995651245117188
2018-10-21 10:50:42.338430:	Training iteration: 124600, Loss: 0.0833011269569397
2018-10-21 10:51:10.290590:	Training iteration: 124800, Loss: 0.12418503314256668
2018-10-21 10:51:36.592172:	Training iteration: 125000, Loss: 0.10682772845029831
2018-10-21 10:52:03.678074:	Training iteration: 125200, Loss: 0.13849247992038727
2018-10-21 10:52:30.377223:	Training iteration: 125400, Loss: 0.08468972146511078
2018-10-21 10:52:56.471095:	Training iteration: 125600, Loss: 0.14147858321666718
2018-10-21 10:53:22.688554:	Training iteration: 125800, Loss: 0.12757164239883423
2018-10-21 10:53:49.239840:	Training iteration: 126000, Loss: 0.10984550416469574
2018-10-21 10:54:16.845418:	Training iteration: 126200, Loss: 0.16831013560295105
2018-10-21 10:54:42.462625:	Training iteration: 126400, Loss: 0.0923338457942009
2018-10-21 10:55:09.924192:	Training iteration: 126600, Loss: 0.16217690706253052
2018-10-21 10:55:36.948840:	Training iteration: 126800, Loss: 0.1748799979686737
2018-10-21 10:56:04.176565:	Training iteration: 127000, Loss: 0.12202160060405731
2018-10-21 10:56:31.362889:	Training iteration: 127200, Loss: 0.10169678926467896
2018-10-21 10:56:57.869991:	Training iteration: 127400, Loss: 0.07854224741458893
2018-10-21 10:57:23.833019:	Training iteration: 127600, Loss: 0.09063677489757538
2018-10-21 10:57:50.939097:	Training iteration: 127800, Loss: 0.116789311170578
2018-10-21 10:58:17.797424:	Training iteration: 128000, Loss: 0.1448153257369995
2018-10-21 10:58:44.582275:	Training iteration: 128200, Loss: 0.1315312385559082
2018-10-21 10:59:11.870814:	Training iteration: 128400, Loss: 0.1472465991973877
2018-10-21 10:59:38.679466:	Training iteration: 128600, Loss: 0.09304242581129074
2018-10-21 11:00:04.962211:	Training iteration: 128800, Loss: 0.14484858512878418
2018-10-21 11:00:31.582907:	Training iteration: 129000, Loss: 0.12299950420856476
2018-10-21 11:00:57.790004:	Training iteration: 129200, Loss: 0.1360422968864441
2018-10-21 11:01:23.899054:	Training iteration: 129400, Loss: 0.13491006195545197
2018-10-21 11:01:50.331461:	Training iteration: 129600, Loss: 0.16676975786685944
2018-10-21 11:02:16.565892:	Training iteration: 129800, Loss: 0.12157967686653137
2018-10-21 11:02:43.278854:	Training iteration: 130000, Loss: 0.09751811623573303
2018-10-21 11:03:10.096263:	Training iteration: 130200, Loss: 0.10297150909900665
2018-10-21 11:03:37.506642:	Training iteration: 130400, Loss: 0.12243921309709549
2018-10-21 11:04:03.758579:	Training iteration: 130600, Loss: 0.08961350470781326
2018-10-21 11:04:30.676287:	Training iteration: 130800, Loss: 0.090207539498806
2018-10-21 11:04:57.550869:	Training iteration: 131000, Loss: 0.08946961164474487
2018-10-21 11:05:24.302560:	Training iteration: 131200, Loss: 0.13581231236457825
2018-10-21 11:05:52.490438:	Training iteration: 131400, Loss: 0.0740349143743515
2018-10-21 11:06:19.075189:	Training iteration: 131600, Loss: 0.10461662709712982
2018-10-21 11:06:45.999472:	Training iteration: 131800, Loss: 0.09041944891214371
2018-10-21 11:07:12.608070:	Training iteration: 132000, Loss: 0.14167936146259308
2018-10-21 11:07:39.968785:	Training iteration: 132200, Loss: 0.10129821300506592
2018-10-21 11:08:09.100267: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 722 of 1000
2018-10-21 11:08:12.415877: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 11:08:18.931671:	Training iteration: 132400, Loss: 0.11435920000076294
2018-10-21 11:08:45.197101:	Training iteration: 132600, Loss: 0.1298244595527649
2018-10-21 11:09:11.689923:	Training iteration: 132800, Loss: 0.10259313881397247
2018-10-21 11:09:37.919361:	Training iteration: 133000, Loss: 0.10922843217849731
2018-10-21 11:10:04.412793:	Training iteration: 133200, Loss: 0.1030736118555069
2018-10-21 11:10:31.319102:	Training iteration: 133400, Loss: 0.16258524358272552
2018-10-21 11:10:58.149656:	Training iteration: 133600, Loss: 0.11274667084217072
2018-10-21 11:11:24.567769:	Training iteration: 133800, Loss: 0.10202191770076752
2018-10-21 11:11:50.687571:	Training iteration: 134000, Loss: 0.12523247301578522
2018-10-21 11:12:16.464600:	Training iteration: 134200, Loss: 0.09311015903949738
2018-10-21 11:12:43.314872:	Training iteration: 134400, Loss: 0.11170069128274918
2018-10-21 11:13:10.541595:	Training iteration: 134600, Loss: 0.0823693722486496
2018-10-21 11:13:37.309553:	Training iteration: 134800, Loss: 0.1292003095149994
2018-10-21 11:14:04.381249:	Training iteration: 135000, Loss: 0.17376236617565155
2018-10-21 11:14:31.342515:	Training iteration: 135200, Loss: 0.09944476187229156
2018-10-21 11:14:58.153856:	Training iteration: 135400, Loss: 0.131953626871109
2018-10-21 11:15:25.086233:	Training iteration: 135600, Loss: 0.10760541260242462
2018-10-21 11:15:52.163612:	Training iteration: 135800, Loss: 0.14035023748874664
2018-10-21 11:16:18.675951:	Training iteration: 136000, Loss: 0.11583784222602844
2018-10-21 11:16:45.502633:	Training iteration: 136200, Loss: 0.10361391305923462
2018-10-21 11:17:12.684164:	Training iteration: 136400, Loss: 0.12950047850608826
2018-10-21 11:17:39.528800:	Training iteration: 136600, Loss: 0.10683520883321762
2018-10-21 11:18:05.986971:	Training iteration: 136800, Loss: 0.10380256175994873
2018-10-21 11:18:32.589923:	Training iteration: 137000, Loss: 0.14959977567195892
2018-10-21 11:18:59.119075:	Training iteration: 137200, Loss: 0.10238531231880188
2018-10-21 11:19:25.022573:	Training iteration: 137400, Loss: 0.10471352189779282
2018-10-21 11:19:51.754662:	Training iteration: 137600, Loss: 0.10217148065567017
2018-10-21 11:20:17.688321:	Training iteration: 137800, Loss: 0.10993475466966629
2018-10-21 11:20:45.349238:	Training iteration: 138000, Loss: 0.14086051285266876
2018-10-21 11:21:12.389895:	Training iteration: 138200, Loss: 0.10243183374404907
2018-10-21 11:21:39.480131:	Training iteration: 138400, Loss: 0.11652974784374237
2018-10-21 11:22:06.745854:	Training iteration: 138600, Loss: 0.08606752753257751
2018-10-21 11:22:33.560282:	Training iteration: 138800, Loss: 0.11471378803253174
2018-10-21 11:23:00.193849:	Training iteration: 139000, Loss: 0.12718060612678528
2018-10-21 11:23:27.680723:	Training iteration: 139200, Loss: 0.09632613509893417
2018-10-21 11:23:54.859904:	Training iteration: 139400, Loss: 0.15312345325946808
2018-10-21 11:24:21.155602:	Training iteration: 139600, Loss: 0.08955611288547516
2018-10-21 11:24:47.701915:	Training iteration: 139800, Loss: 0.08384151011705399
2018-10-21 11:25:14.670328:	Training iteration: 140000, Loss: 0.15087153017520905
2018-10-21 11:25:41.278395:	Training iteration: 140200, Loss: 0.09282469749450684
2018-10-21 11:26:08.358132:	Training iteration: 140400, Loss: 0.11899618059396744
2018-10-21 11:26:36.268470:	Training iteration: 140600, Loss: 0.13987517356872559
2018-10-21 11:27:02.149252:	Training iteration: 140800, Loss: 0.09616079926490784
2018-10-21 11:27:29.376794:	Training iteration: 141000, Loss: 0.13181047141551971
2018-10-21 11:27:55.859592:	Training iteration: 141200, Loss: 0.12037408351898193
2018-10-21 11:28:22.815435:	Training iteration: 141400, Loss: 0.15669827163219452
2018-10-21 11:28:50.119201:	Training iteration: 141600, Loss: 0.10957076400518417
2018-10-21 11:29:16.522030:	Training iteration: 141800, Loss: 0.1018962413072586
2018-10-21 11:29:43.400139:	Training iteration: 142000, Loss: 0.13799074292182922
2018-10-21 11:30:10.822628:	Training iteration: 142200, Loss: 0.12112728506326675
2018-10-21 11:30:37.658605:	Training iteration: 142400, Loss: 0.12812313437461853
2018-10-21 11:31:04.542409:	Training iteration: 142600, Loss: 0.11685106158256531
2018-10-21 11:31:30.835477:	Training iteration: 142800, Loss: 0.1088242307305336
2018-10-21 11:31:57.569226:	Training iteration: 143000, Loss: 0.09896962344646454
2018-10-21 11:32:24.770144:	Training iteration: 143200, Loss: 0.1926957368850708
2018-10-21 11:32:51.894855:	Training iteration: 143400, Loss: 0.14437025785446167
2018-10-21 11:33:18.252954:	Training iteration: 143600, Loss: 0.08642767369747162
2018-10-21 11:33:45.059736:	Training iteration: 143800, Loss: 0.14779603481292725
2018-10-21 11:34:12.122131:	Training iteration: 144000, Loss: 0.11433173716068268
2018-10-21 11:34:39.075515:	Training iteration: 144200, Loss: 0.1269204020500183
2018-10-21 11:35:06.013521:	Training iteration: 144400, Loss: 0.09806074947118759
2018-10-21 11:35:32.997680:	Training iteration: 144600, Loss: 0.09339779615402222
2018-10-21 11:35:58.945598:	Training iteration: 144800, Loss: 0.14188259840011597
2018-10-21 11:36:11.094654: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 775 of 1000
2018-10-21 11:36:13.733992: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 11:36:40.386629:	Training iteration: 145000, Loss: 0.05396851897239685
2018-10-21 11:37:07.432739:	Training iteration: 145200, Loss: 0.11400526016950607
2018-10-21 11:37:34.025371:	Training iteration: 145400, Loss: 0.07650993019342422
2018-10-21 11:38:01.312444:	Training iteration: 145600, Loss: 0.09011754393577576
2018-10-21 11:38:28.331791:	Training iteration: 145800, Loss: 0.13184911012649536
2018-10-21 11:38:55.332409:	Training iteration: 146000, Loss: 0.13023942708969116
2018-10-21 11:39:21.890010:	Training iteration: 146200, Loss: 0.11661964654922485
2018-10-21 11:39:49.142345:	Training iteration: 146400, Loss: 0.07821134477853775
2018-10-21 11:40:16.086930:	Training iteration: 146600, Loss: 0.09166780114173889
2018-10-21 11:40:42.703088:	Training iteration: 146800, Loss: 0.061940111219882965
2018-10-21 11:41:08.325685:	Training iteration: 147000, Loss: 0.0869516059756279
2018-10-21 11:41:35.275708:	Training iteration: 147200, Loss: 0.0768057107925415
2018-10-21 11:42:02.414845:	Training iteration: 147400, Loss: 0.07847969233989716
2018-10-21 11:42:29.155383:	Training iteration: 147600, Loss: 0.12201816588640213
2018-10-21 11:42:54.554790:	Training iteration: 147800, Loss: 0.07766605913639069
2018-10-21 11:43:21.302187:	Training iteration: 148000, Loss: 0.09631691873073578
2018-10-21 11:43:48.871872:	Training iteration: 148200, Loss: 0.10077788680791855
2018-10-21 11:44:15.264682:	Training iteration: 148400, Loss: 0.08383572101593018
2018-10-21 11:44:42.858607:	Training iteration: 148600, Loss: 0.1311987340450287
2018-10-21 11:45:09.976518:	Training iteration: 148800, Loss: 0.11288511753082275
2018-10-21 11:45:36.325705:	Training iteration: 149000, Loss: 0.08832387626171112
2018-10-21 11:46:02.331791:	Training iteration: 149200, Loss: 0.15997841954231262
2018-10-21 11:46:28.902513:	Training iteration: 149400, Loss: 0.07880516350269318
2018-10-21 11:46:57.213300:	Training iteration: 149600, Loss: 0.07491737604141235
2018-10-21 11:47:24.292022:	Training iteration: 149800, Loss: 0.07700861245393753
2018-10-21 11:47:50.163731:	Training iteration: 150000, Loss: 0.08876670897006989
2018-10-21 11:48:17.444988:	Training iteration: 150200, Loss: 0.1057109460234642
2018-10-21 11:48:45.126153:	Training iteration: 150400, Loss: 0.12629783153533936
2018-10-21 11:49:12.365603:	Training iteration: 150600, Loss: 0.13841032981872559
2018-10-21 11:49:39.700116:	Training iteration: 150800, Loss: 0.10069675743579865
2018-10-21 11:50:06.688551:	Training iteration: 151000, Loss: 0.07481439411640167
2018-10-21 11:50:34.007795:	Training iteration: 151200, Loss: 0.17544065415859222
2018-10-21 11:51:01.858087:	Training iteration: 151400, Loss: 0.06829363852739334
2018-10-21 11:51:28.357166:	Training iteration: 151600, Loss: 0.1236100047826767
2018-10-21 11:51:55.082423:	Training iteration: 151800, Loss: 0.07605400681495667
2018-10-21 11:52:22.110531:	Training iteration: 152000, Loss: 0.08075577020645142
2018-10-21 11:52:48.871555:	Training iteration: 152200, Loss: 0.09581680595874786
2018-10-21 11:53:15.864228:	Training iteration: 152400, Loss: 0.08385933935642242
2018-10-21 11:53:42.276871:	Training iteration: 152600, Loss: 0.08537764102220535
2018-10-21 11:54:09.708584:	Training iteration: 152800, Loss: 0.09339992702007294
2018-10-21 11:54:36.345185:	Training iteration: 153000, Loss: 0.07113538682460785
2018-10-21 11:55:02.944190:	Training iteration: 153200, Loss: 0.08944989740848541
2018-10-21 11:55:29.392995:	Training iteration: 153400, Loss: 0.09044397622346878
2018-10-21 11:55:56.032787:	Training iteration: 153600, Loss: 0.07904289662837982
2018-10-21 11:56:22.859794:	Training iteration: 153800, Loss: 0.12336282432079315
2018-10-21 11:56:49.313265:	Training iteration: 154000, Loss: 0.08850502967834473
2018-10-21 11:57:16.245555:	Training iteration: 154200, Loss: 0.0719929188489914
2018-10-21 11:57:42.775767:	Training iteration: 154400, Loss: 0.07833496481180191
2018-10-21 11:58:09.733543:	Training iteration: 154600, Loss: 0.09705410152673721
2018-10-21 11:58:37.408694:	Training iteration: 154800, Loss: 0.09164756536483765
2018-10-21 11:59:04.068677:	Training iteration: 155000, Loss: 0.1121254414319992
2018-10-21 11:59:30.744068:	Training iteration: 155200, Loss: 0.11648061871528625
2018-10-21 11:59:57.884988:	Training iteration: 155400, Loss: 0.0978773906826973
2018-10-21 12:00:24.216057:	Training iteration: 155600, Loss: 0.10856691002845764
2018-10-21 12:00:51.018443:	Training iteration: 155800, Loss: 0.0845586508512497
2018-10-21 12:01:17.851370:	Training iteration: 156000, Loss: 0.07357385009527206
2018-10-21 12:01:45.616996:	Training iteration: 156200, Loss: 0.09925802052021027
2018-10-21 12:02:12.621304:	Training iteration: 156400, Loss: 0.13878455758094788
2018-10-21 12:02:39.208514:	Training iteration: 156600, Loss: 0.11021631956100464
2018-10-21 12:03:06.024289:	Training iteration: 156800, Loss: 0.09109683334827423
2018-10-21 12:03:32.889739:	Training iteration: 157000, Loss: 0.11950419098138809
2018-10-21 12:03:59.947742:	Training iteration: 157200, Loss: 0.10076627135276794
2018-10-21 12:04:20.399633: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 716 of 1000
2018-10-21 12:04:23.727557: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 12:04:38.982329:	Training iteration: 157400, Loss: 0.1223321333527565
2018-10-21 12:05:04.912487:	Training iteration: 157600, Loss: 0.09434394538402557
2018-10-21 12:05:31.424966:	Training iteration: 157800, Loss: 0.1246563196182251
2018-10-21 12:05:57.243724:	Training iteration: 158000, Loss: 0.13693583011627197
2018-10-21 12:06:25.652411:	Training iteration: 158200, Loss: 0.1252693235874176
2018-10-21 12:06:52.511178:	Training iteration: 158400, Loss: 0.16622057557106018
2018-10-21 12:07:19.567678:	Training iteration: 158600, Loss: 0.15529009699821472
2018-10-21 12:07:45.287171:	Training iteration: 158800, Loss: 0.19197988510131836
2018-10-21 12:08:11.537499:	Training iteration: 159000, Loss: 0.10111770033836365
2018-10-21 12:08:38.000684:	Training iteration: 159200, Loss: 0.1582677811384201
2018-10-21 12:09:04.848333:	Training iteration: 159400, Loss: 0.14156785607337952
2018-10-21 12:09:31.582187:	Training iteration: 159600, Loss: 0.08935385942459106
2018-10-21 12:09:57.605077:	Training iteration: 159800, Loss: 0.22538024187088013
2018-10-21 12:10:25.106180:	Training iteration: 160000, Loss: 0.07348804175853729
2018-10-21 12:10:52.138014:	Training iteration: 160200, Loss: 0.16617026925086975
2018-10-21 12:11:18.995849:	Training iteration: 160400, Loss: 0.12788906693458557
2018-10-21 12:11:45.355353:	Training iteration: 160600, Loss: 0.17654597759246826
2018-10-21 12:12:11.854323:	Training iteration: 160800, Loss: 0.0838417261838913
2018-10-21 12:12:38.129976:	Training iteration: 161000, Loss: 0.10850310325622559
2018-10-21 12:13:05.374598:	Training iteration: 161200, Loss: 0.1579870879650116
2018-10-21 12:13:33.970889:	Training iteration: 161400, Loss: 0.17289458215236664
2018-10-21 12:13:59.744458:	Training iteration: 161600, Loss: 0.14693807065486908
2018-10-21 12:14:26.346658:	Training iteration: 161800, Loss: 0.12102830410003662
2018-10-21 12:14:53.758760:	Training iteration: 162000, Loss: 0.14991632103919983
2018-10-21 12:15:29.913395:	Training iteration: 162200, Loss: 0.14675891399383545
2018-10-21 12:15:56.172752:	Training iteration: 162400, Loss: 0.15385273098945618
2018-10-21 12:16:23.371255:	Training iteration: 162600, Loss: 0.14478212594985962
2018-10-21 12:16:49.400614:	Training iteration: 162800, Loss: 0.09942472726106644
2018-10-21 12:17:15.567034:	Training iteration: 163000, Loss: 0.11926232278347015
2018-10-21 12:17:41.813728:	Training iteration: 163200, Loss: 0.1544201672077179
2018-10-21 12:18:08.197130:	Training iteration: 163400, Loss: 0.11890783905982971
2018-10-21 12:18:34.393748:	Training iteration: 163600, Loss: 0.16758327186107635
2018-10-21 12:19:01.556284:	Training iteration: 163800, Loss: 0.15129992365837097
2018-10-21 12:19:27.615251:	Training iteration: 164000, Loss: 0.13581518828868866
2018-10-21 12:19:53.852795:	Training iteration: 164200, Loss: 0.13623881340026855
2018-10-21 12:20:20.851800:	Training iteration: 164400, Loss: 0.14859065413475037
2018-10-21 12:20:47.669121:	Training iteration: 164600, Loss: 0.12856405973434448
2018-10-21 12:21:14.704982:	Training iteration: 164800, Loss: 0.09389220923185349
2018-10-21 12:21:41.575676:	Training iteration: 165000, Loss: 0.18551769852638245
2018-10-21 12:22:08.547737:	Training iteration: 165200, Loss: 0.08894233405590057
2018-10-21 12:22:35.813664:	Training iteration: 165400, Loss: 0.15300071239471436
2018-10-21 12:23:02.846504:	Training iteration: 165600, Loss: 0.14966660737991333
2018-10-21 12:23:30.086193:	Training iteration: 165800, Loss: 0.08755480498075485
2018-10-21 12:23:57.261289:	Training iteration: 166000, Loss: 0.11012238264083862
2018-10-21 12:24:24.541834:	Training iteration: 166200, Loss: 0.0959695428609848
2018-10-21 12:24:52.173931:	Training iteration: 166400, Loss: 0.12209269404411316
2018-10-21 12:25:19.326508:	Training iteration: 166600, Loss: 0.08274436742067337
2018-10-21 12:25:46.848752:	Training iteration: 166800, Loss: 0.11866271495819092
2018-10-21 12:26:15.714684:	Training iteration: 167000, Loss: 0.21313728392124176
2018-10-21 12:26:46.431152:	Training iteration: 167200, Loss: 0.08804653584957123
2018-10-21 12:27:13.084978:	Training iteration: 167400, Loss: 0.10621078312397003
2018-10-21 12:27:40.947905:	Training iteration: 167600, Loss: 0.1893007755279541
2018-10-21 12:28:08.616121:	Training iteration: 167800, Loss: 0.16246069967746735
2018-10-21 12:28:36.049838:	Training iteration: 168000, Loss: 0.11541086435317993
2018-10-21 12:29:03.800218:	Training iteration: 168200, Loss: 0.1744074821472168
2018-10-21 12:29:31.594833:	Training iteration: 168400, Loss: 0.12029263377189636
2018-10-21 12:29:59.353641:	Training iteration: 168600, Loss: 0.0937717854976654
2018-10-21 12:30:26.672727:	Training iteration: 168800, Loss: 0.1616695523262024
2018-10-21 12:30:54.423331:	Training iteration: 169000, Loss: 0.10310092568397522
2018-10-21 12:31:22.155961:	Training iteration: 169200, Loss: 0.11292055994272232
2018-10-21 12:31:49.559628:	Training iteration: 169400, Loss: 0.11469167470932007
2018-10-21 12:32:17.617307:	Training iteration: 169600, Loss: 0.11873745918273926
2018-10-21 12:32:45.247178:	Training iteration: 169800, Loss: 0.13809429109096527
2018-10-21 12:33:13.023346:	Training iteration: 170000, Loss: 0.12129680067300797
2018-10-21 12:33:40.903490:	Training iteration: 170200, Loss: 0.10084214806556702
2018-10-21 12:34:03.999123: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 859 of 1000
2018-10-21 12:34:05.677456: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 12:34:20.296609:	Training iteration: 170400, Loss: 0.11664894223213196
2018-10-21 12:34:47.066589:	Training iteration: 170600, Loss: 0.1813613921403885
2018-10-21 12:35:14.821149:	Training iteration: 170800, Loss: 0.1617012321949005
2018-10-21 12:35:42.356549:	Training iteration: 171000, Loss: 0.2003461718559265
2018-10-21 12:36:10.178134:	Training iteration: 171200, Loss: 0.15458589792251587
2018-10-21 12:36:38.221446:	Training iteration: 171400, Loss: 0.25686153769493103
2018-10-21 12:37:06.190853:	Training iteration: 171600, Loss: 0.196217879652977
2018-10-21 12:37:34.198739:	Training iteration: 171800, Loss: 0.0923525020480156
2018-10-21 12:38:01.957266:	Training iteration: 172000, Loss: 0.14576122164726257
2018-10-21 12:38:29.749767:	Training iteration: 172200, Loss: 0.1408662348985672
2018-10-21 12:38:58.321097:	Training iteration: 172400, Loss: 0.14180055260658264
2018-10-21 12:39:26.983400:	Training iteration: 172600, Loss: 0.17433510720729828
2018-10-21 12:39:55.043389:	Training iteration: 172800, Loss: 0.10909393429756165
2018-10-21 12:40:22.938390:	Training iteration: 173000, Loss: 0.15281450748443604
2018-10-21 12:40:50.961414:	Training iteration: 173200, Loss: 0.15423452854156494
2018-10-21 12:41:19.015411:	Training iteration: 173400, Loss: 0.1816597431898117
2018-10-21 12:41:46.813005:	Training iteration: 173600, Loss: 0.2808237671852112
2018-10-21 12:42:14.553658:	Training iteration: 173800, Loss: 0.11641756445169449
2018-10-21 12:42:42.545182:	Training iteration: 174000, Loss: 0.17074480652809143
2018-10-21 12:43:10.515341:	Training iteration: 174200, Loss: 0.14232133328914642
2018-10-21 12:43:38.429677:	Training iteration: 174400, Loss: 0.09724017232656479
2018-10-21 12:44:06.612883:	Training iteration: 174600, Loss: 0.14564482867717743
2018-10-21 12:44:34.230362:	Training iteration: 174800, Loss: 0.13686491549015045
2018-10-21 12:45:01.814283:	Training iteration: 175000, Loss: 0.18022039532661438
2018-10-21 12:45:29.897914:	Training iteration: 175200, Loss: 0.18682175874710083
2018-10-21 12:45:58.119597:	Training iteration: 175400, Loss: 0.13482119143009186
2018-10-21 12:46:26.056432:	Training iteration: 175600, Loss: 0.1433960348367691
2018-10-21 12:46:53.948808:	Training iteration: 175800, Loss: 0.1267625391483307
2018-10-21 12:47:22.082508:	Training iteration: 176000, Loss: 0.20811353623867035
2018-10-21 12:47:49.894521:	Training iteration: 176200, Loss: 0.18307775259017944
2018-10-21 12:48:17.837872:	Training iteration: 176400, Loss: 0.18342256546020508
2018-10-21 12:48:45.634467:	Training iteration: 176600, Loss: 0.16090133786201477
2018-10-21 12:49:13.584567:	Training iteration: 176800, Loss: 0.2116880714893341
2018-10-21 12:49:41.563016:	Training iteration: 177000, Loss: 0.1607649326324463
2018-10-21 12:50:09.254975:	Training iteration: 177200, Loss: 0.1679336279630661
2018-10-21 12:50:37.363605:	Training iteration: 177400, Loss: 0.21980299055576324
2018-10-21 12:51:05.469817:	Training iteration: 177600, Loss: 0.16563701629638672
2018-10-21 12:51:33.555253:	Training iteration: 177800, Loss: 0.22332870960235596
2018-10-21 12:52:01.362589:	Training iteration: 178000, Loss: 0.15531301498413086
2018-10-21 12:52:29.458637:	Training iteration: 178200, Loss: 0.2190587818622589
2018-10-21 12:52:57.132756:	Training iteration: 178400, Loss: 0.175410196185112
2018-10-21 12:53:26.591830:	Training iteration: 178600, Loss: 0.19709978997707367
2018-10-21 12:53:54.375108:	Training iteration: 178800, Loss: 0.1652155965566635
2018-10-21 12:54:22.151920:	Training iteration: 179000, Loss: 0.17379139363765717
2018-10-21 12:54:49.938566:	Training iteration: 179200, Loss: 0.16499048471450806
2018-10-21 12:55:17.804929:	Training iteration: 179400, Loss: 0.13388791680335999
2018-10-21 12:55:45.458547:	Training iteration: 179600, Loss: 0.16964101791381836
2018-10-21 12:56:13.625896:	Training iteration: 179800, Loss: 0.17024639248847961
2018-10-21 12:56:41.337337:	Training iteration: 180000, Loss: 0.17031067609786987
2018-10-21 12:57:09.119363:	Training iteration: 180200, Loss: 0.1545252799987793
2018-10-21 12:57:36.727876:	Training iteration: 180400, Loss: 0.20439961552619934
2018-10-21 12:58:04.257903:	Training iteration: 180600, Loss: 0.168299600481987
2018-10-21 12:58:32.130223:	Training iteration: 180800, Loss: 0.15021343529224396
2018-10-21 12:58:59.887281:	Training iteration: 181000, Loss: 0.18312737345695496
2018-10-21 12:59:27.623616:	Training iteration: 181200, Loss: 0.154079869389534
2018-10-21 12:59:55.523872:	Training iteration: 181400, Loss: 0.18030023574829102
2018-10-21 13:00:23.177674:	Training iteration: 181600, Loss: 0.15481406450271606
2018-10-21 13:00:50.691812:	Training iteration: 181800, Loss: 0.13417404890060425
2018-10-21 13:01:18.186363:	Training iteration: 182000, Loss: 0.1760164052248001
2018-10-21 13:01:46.388451:	Training iteration: 182200, Loss: 0.13696469366550446
2018-10-21 13:02:13.532761:	Training iteration: 182400, Loss: 0.17593935132026672
2018-10-21 13:02:41.294556:	Training iteration: 182600, Loss: 0.20826417207717896
2018-10-21 13:03:08.987490:	Training iteration: 182800, Loss: 0.1792474091053009
2018-10-21 13:03:36.763591:	Training iteration: 183000, Loss: 0.13446912169456482
2018-10-21 13:04:04.405415:	Training iteration: 183200, Loss: 0.12217333912849426
2018-10-21 13:04:32.065759:	Training iteration: 183400, Loss: 0.1403181403875351
2018-10-21 13:04:59.424754:	Training iteration: 183600, Loss: 0.13637429475784302
2018-10-21 13:05:26.690678:	Training iteration: 183800, Loss: 0.18616941571235657
2018-10-21 13:05:54.342990:	Training iteration: 184000, Loss: 0.212885320186615
2018-10-21 13:06:23.200303:	Training iteration: 184200, Loss: 0.1574249267578125
2018-10-21 13:06:50.745158:	Training iteration: 184400, Loss: 0.1759105622768402
2018-10-21 13:07:17.686384:	Training iteration: 184600, Loss: 0.1407926082611084
2018-10-21 13:07:45.140742:	Training iteration: 184800, Loss: 0.1771162748336792
2018-10-21 13:08:12.637339:	Training iteration: 185000, Loss: 0.12076759338378906
2018-10-21 13:08:39.882400:	Training iteration: 185200, Loss: 0.17970067262649536
2018-10-21 13:09:07.505413:	Training iteration: 185400, Loss: 0.1455516219139099
2018-10-21 13:09:34.666337:	Training iteration: 185600, Loss: 0.13860881328582764
2018-10-21 13:10:05.414073:	Training iteration: 185800, Loss: 0.13470244407653809
2018-10-21 13:10:34.157656:	Training iteration: 186000, Loss: 0.1544848382472992
2018-10-21 13:11:02.245985:	Training iteration: 186200, Loss: 0.15467000007629395
2018-10-21 13:11:29.763840:	Training iteration: 186400, Loss: 0.1395178735256195
2018-10-21 13:11:57.155773:	Training iteration: 186600, Loss: 0.17442911863327026
2018-10-21 13:12:24.668435:	Training iteration: 186800, Loss: 0.2691305875778198
2018-10-21 13:12:51.848275:	Training iteration: 187000, Loss: 0.2841454744338989
2018-10-21 13:13:19.648214:	Training iteration: 187200, Loss: 0.1666678786277771
2018-10-21 13:13:47.219567:	Training iteration: 187400, Loss: 0.1817876398563385
2018-10-21 13:14:14.828999:	Training iteration: 187600, Loss: 0.16914445161819458
2018-10-21 13:14:42.658878:	Training iteration: 187800, Loss: 0.1829344928264618
2018-10-21 13:15:10.103924:	Training iteration: 188000, Loss: 0.11348581314086914
2018-10-21 13:15:38.061150:	Training iteration: 188200, Loss: 0.09101688861846924
2018-10-21 13:16:06.036640:	Training iteration: 188400, Loss: 0.1928808093070984
2018-10-21 13:16:33.776080:	Training iteration: 188600, Loss: 0.1408243477344513
2018-10-21 13:17:01.540115:	Training iteration: 188800, Loss: 0.16086721420288086
2018-10-21 13:17:29.341325:	Training iteration: 189000, Loss: 0.18720680475234985
2018-10-21 13:17:57.018995:	Training iteration: 189200, Loss: 0.15875403583049774
2018-10-21 13:18:24.944774:	Training iteration: 189400, Loss: 0.08819027245044708
2018-10-21 13:18:52.667540:	Training iteration: 189600, Loss: 0.13911251723766327
2018-10-21 13:19:20.873663:	Training iteration: 189800, Loss: 0.18030646443367004
2018-10-21 13:19:48.513176:	Training iteration: 190000, Loss: 0.16624517738819122
2018-10-21 13:20:16.623868:	Training iteration: 190200, Loss: 0.16640344262123108
2018-10-21 13:20:44.700084:	Training iteration: 190400, Loss: 0.16835317015647888
2018-10-21 13:21:12.617257:	Training iteration: 190600, Loss: 0.15893957018852234
2018-10-21 13:21:40.459766:	Training iteration: 190800, Loss: 0.19821269810199738
2018-10-21 13:22:08.461443:	Training iteration: 191000, Loss: 0.19066989421844482
2018-10-21 13:22:36.215692:	Training iteration: 191200, Loss: 0.17865484952926636
2018-10-21 13:23:03.889305:	Training iteration: 191400, Loss: 0.1444307267665863
2018-10-21 13:23:31.600703:	Training iteration: 191600, Loss: 0.16204211115837097
2018-10-21 13:23:59.449627:	Training iteration: 191800, Loss: 0.13147446513175964
2018-10-21 13:24:27.186778:	Training iteration: 192000, Loss: 0.16425436735153198
2018-10-21 13:24:55.352639:	Training iteration: 192200, Loss: 0.15525051951408386
2018-10-21 13:25:22.962282:	Training iteration: 192400, Loss: 0.17522558569908142
2018-10-21 13:25:50.906755:	Training iteration: 192600, Loss: 0.13026246428489685
2018-10-21 13:26:18.589521:	Training iteration: 192800, Loss: 0.1905798316001892
2018-10-21 13:26:46.394400:	Training iteration: 193000, Loss: 0.14530137181282043
2018-10-21 13:27:14.536422:	Training iteration: 193200, Loss: 0.20903804898262024
2018-10-21 13:27:42.373910:	Training iteration: 193400, Loss: 0.15515871345996857
2018-10-21 13:28:10.125295:	Training iteration: 193600, Loss: 0.17339105904102325
2018-10-21 13:28:38.170893:	Training iteration: 193800, Loss: 0.18734246492385864
2018-10-21 13:29:05.968593:	Training iteration: 194000, Loss: 0.17074015736579895
2018-10-21 13:29:34.034855:	Training iteration: 194200, Loss: 0.1303626000881195
2018-10-21 13:30:02.121286:	Training iteration: 194400, Loss: 0.16848289966583252
2018-10-21 13:30:30.164776:	Training iteration: 194600, Loss: 0.1395495980978012
2018-10-21 13:30:58.086109:	Training iteration: 194800, Loss: 0.21737727522850037
2018-10-21 13:31:26.041321:	Training iteration: 195000, Loss: 0.13128575682640076
2018-10-21 13:31:53.965337:	Training iteration: 195200, Loss: 0.15959477424621582
2018-10-21 13:32:22.127048:	Training iteration: 195400, Loss: 0.13288161158561707
2018-10-21 13:32:50.071236:	Training iteration: 195600, Loss: 0.10753266513347626
2018-10-21 13:33:17.980144:	Training iteration: 195800, Loss: 0.12308025360107422
2018-10-21 13:33:45.981332:	Training iteration: 196000, Loss: 0.15352004766464233
2018-10-21 13:34:13.797304:	Training iteration: 196200, Loss: 0.11300066113471985
2018-10-21 13:34:41.687975:	Training iteration: 196400, Loss: 0.15638357400894165
2018-10-21 13:35:09.932135:	Training iteration: 196600, Loss: 0.10840131342411041
2018-10-21 13:35:37.914761:	Training iteration: 196800, Loss: 0.1556924283504486
2018-10-21 13:36:06.179125:	Training iteration: 197000, Loss: 0.15922462940216064
2018-10-21 13:36:34.063836:	Training iteration: 197200, Loss: 0.1680319607257843
2018-10-21 13:37:02.001956:	Training iteration: 197400, Loss: 0.11431567370891571
2018-10-21 13:37:30.099380:	Training iteration: 197600, Loss: 0.11968132853507996
2018-10-21 13:37:57.813864:	Training iteration: 197800, Loss: 0.15252311527729034
2018-10-21 13:38:25.714701:	Training iteration: 198000, Loss: 0.13072469830513
2018-10-21 13:38:53.619790:	Training iteration: 198200, Loss: 0.20851972699165344
2018-10-21 13:39:21.744855:	Training iteration: 198400, Loss: 0.13844650983810425
2018-10-21 13:39:49.681281:	Training iteration: 198600, Loss: 0.12488341331481934
2018-10-21 13:40:17.817490:	Training iteration: 198800, Loss: 0.15086787939071655
2018-10-21 13:40:46.115825:	Training iteration: 199000, Loss: 0.12849214673042297
2018-10-21 13:41:13.786710:	Training iteration: 199200, Loss: 0.1785542368888855
2018-10-21 13:41:41.594631:	Training iteration: 199400, Loss: 0.16345685720443726
2018-10-21 13:42:09.749848:	Training iteration: 199600, Loss: 0.17379549145698547
2018-10-21 13:42:37.907034:	Training iteration: 199800, Loss: 0.20517653226852417
2018-10-21 13:43:05.985764:	Training iteration: 200000, Loss: 0.1695231795310974
2018-10-21 13:43:34.950788:	Training iteration: 200200, Loss: 0.12633799016475677
2018-10-21 13:44:02.944770:	Training iteration: 200400, Loss: 0.14050017297267914
2018-10-21 13:44:30.679802:	Training iteration: 200600, Loss: 0.168746218085289
2018-10-21 13:44:58.582803:	Training iteration: 200800, Loss: 0.11655405908823013
2018-10-21 13:45:27.220671:	Training iteration: 201000, Loss: 0.14748406410217285
2018-10-21 13:45:56.521877:	Training iteration: 201200, Loss: 0.17903736233711243
2018-10-21 13:46:25.516361:	Training iteration: 201400, Loss: 0.1542539894580841
2018-10-21 13:46:54.722797:	Training iteration: 201600, Loss: 0.16305914521217346
2018-10-21 13:47:23.973788:	Training iteration: 201800, Loss: 0.14912112057209015
2018-10-21 13:47:53.266017:	Training iteration: 202000, Loss: 0.14633014798164368
2018-10-21 13:48:22.269120:	Training iteration: 202200, Loss: 0.16569426655769348
2018-10-21 13:48:51.550622:	Training iteration: 202400, Loss: 0.1596025675535202
2018-10-21 13:49:20.826668:	Training iteration: 202600, Loss: 0.16808775067329407
2018-10-21 13:49:50.112258:	Training iteration: 202800, Loss: 0.12212331593036652
2018-10-21 13:50:20.959256:	Training iteration: 203000, Loss: 0.15625546872615814
2018-10-21 13:50:49.659719:	Training iteration: 203200, Loss: 0.14649629592895508
2018-10-21 13:51:19.382342:	Training iteration: 203400, Loss: 0.13576963543891907
2018-10-21 13:51:48.680210:	Training iteration: 203600, Loss: 0.19042369723320007
2018-10-21 13:52:17.459035:	Training iteration: 203800, Loss: 0.16364344954490662
2018-10-21 13:52:46.652902:	Training iteration: 204000, Loss: 0.21487177908420563
2018-10-21 13:53:16.228965:	Training iteration: 204200, Loss: 0.19992300868034363
2018-10-21 13:53:47.115127:	Training iteration: 204400, Loss: 0.17674343287944794
2018-10-21 13:54:16.776225:	Training iteration: 204600, Loss: 0.20051315426826477
2018-10-21 13:54:45.221284:	Training iteration: 204800, Loss: 0.17969274520874023
2018-10-21 13:55:14.990256:	Training iteration: 205000, Loss: 0.1654547154903412
2018-10-21 13:55:44.685398:	Training iteration: 205200, Loss: 0.17335176467895508
2018-10-21 13:56:14.578126:	Training iteration: 205400, Loss: 0.13688379526138306
2018-10-21 13:56:44.743855:	Training iteration: 205600, Loss: 0.13767430186271667
2018-10-21 13:57:13.983910:	Training iteration: 205800, Loss: 0.14802244305610657
2018-10-21 13:57:44.013290:	Training iteration: 206000, Loss: 0.14584176242351532
2018-10-21 13:58:13.286682:	Training iteration: 206200, Loss: 0.16875284910202026
2018-10-21 13:58:42.785248:	Training iteration: 206400, Loss: 0.16805176436901093
2018-10-21 13:59:11.998451:	Training iteration: 206600, Loss: 0.1719261109828949
2018-10-21 13:59:40.942939:	Training iteration: 206800, Loss: 0.14081141352653503
2018-10-21 14:00:10.557960:	Training iteration: 207000, Loss: 0.14922696352005005
2018-10-21 14:00:40.265128:	Training iteration: 207200, Loss: 0.10875507444143295
2018-10-21 14:01:09.405113:	Training iteration: 207400, Loss: 0.18237164616584778
2018-10-21 14:01:39.419816:	Training iteration: 207600, Loss: 0.16363868117332458
2018-10-21 14:02:08.418938:	Training iteration: 207800, Loss: 0.16700178384780884
2018-10-21 14:02:39.998894:	Training iteration: 208000, Loss: 0.16730773448944092
2018-10-21 14:03:09.111453:	Training iteration: 208200, Loss: 0.15431927144527435
2018-10-21 14:03:38.176725:	Training iteration: 208400, Loss: 0.17486858367919922
2018-10-21 14:04:06.929629:	Training iteration: 208600, Loss: 0.14995989203453064
2018-10-21 14:04:37.198019:	Training iteration: 208800, Loss: 0.17119760811328888
2018-10-21 14:05:08.626250:	Training iteration: 209000, Loss: 0.2019154131412506
2018-10-21 14:05:37.653120:	Training iteration: 209200, Loss: 0.13296648859977722
2018-10-21 14:06:07.432975:	Training iteration: 209400, Loss: 0.13797004520893097
2018-10-21 14:06:38.615890:	Training iteration: 209600, Loss: 0.20601072907447815
2018-10-21 14:07:08.556246:	Training iteration: 209800, Loss: 0.176958829164505
2018-10-21 14:07:41.326217:	Training iteration: 210000, Loss: 0.14814651012420654
2018-10-21 14:08:10.403180:	Training iteration: 210200, Loss: 0.17673641443252563
2018-10-21 14:08:39.329327:	Training iteration: 210400, Loss: 0.16802847385406494
2018-10-21 14:09:09.033087:	Training iteration: 210600, Loss: 0.1815212368965149
2018-10-21 14:09:38.618077:	Training iteration: 210800, Loss: 0.1511799395084381
2018-10-21 14:10:08.234970:	Training iteration: 211000, Loss: 0.13156473636627197
2018-10-21 14:10:38.251485:	Training iteration: 211200, Loss: 0.12047227472066879
2018-10-21 14:11:07.397231:	Training iteration: 211400, Loss: 0.11142085492610931
2018-10-21 14:11:37.543510:	Training iteration: 211600, Loss: 0.1420593559741974
2018-10-21 14:12:07.148203:	Training iteration: 211800, Loss: 0.22932890057563782
2018-10-21 14:12:37.692159:	Training iteration: 212000, Loss: 0.13011910021305084
2018-10-21 14:13:07.411466:	Training iteration: 212200, Loss: 0.1584840714931488
2018-10-21 14:13:36.199739:	Training iteration: 212400, Loss: 0.13883784413337708
2018-10-21 14:14:05.738728:	Training iteration: 212600, Loss: 0.1672120839357376
2018-10-21 14:14:35.441236:	Training iteration: 212800, Loss: 0.10524944961071014
2018-10-21 14:15:04.742459:	Training iteration: 213000, Loss: 0.12402792274951935
2018-10-21 14:15:34.737166:	Training iteration: 213200, Loss: 0.18852943181991577
2018-10-21 14:16:03.213341:	Training iteration: 213400, Loss: 0.1745791733264923
2018-10-21 14:16:32.885913:	Training iteration: 213600, Loss: 0.12207795679569244
2018-10-21 14:17:01.867900:	Training iteration: 213800, Loss: 0.12307848036289215
2018-10-21 14:17:31.744858:	Training iteration: 214000, Loss: 0.3706350028514862
2018-10-21 14:18:01.512026:	Training iteration: 214200, Loss: 0.15922723710536957
2018-10-21 14:18:30.850728:	Training iteration: 214400, Loss: 0.13266032934188843
2018-10-21 14:19:00.043205:	Training iteration: 214600, Loss: 0.13719896972179413
2018-10-21 14:19:29.875878:	Training iteration: 214800, Loss: 0.11396703124046326
2018-10-21 14:19:59.607042:	Training iteration: 215000, Loss: 0.14348876476287842
2018-10-21 14:20:29.233696:	Training iteration: 215200, Loss: 0.13615955412387848
2018-10-21 14:20:59.487658:	Training iteration: 215400, Loss: 0.20768636465072632
2018-10-21 14:21:29.107726:	Training iteration: 215600, Loss: 0.178666889667511
2018-10-21 14:21:58.840408:	Training iteration: 215800, Loss: 0.18025065958499908
2018-10-21 14:22:29.081027:	Training iteration: 216000, Loss: 0.1593990921974182
2018-10-21 14:22:58.613215:	Training iteration: 216200, Loss: 0.17790749669075012
2018-10-21 14:23:28.355257:	Training iteration: 216400, Loss: 0.15824568271636963
2018-10-21 14:23:58.087722:	Training iteration: 216600, Loss: 0.12141382694244385
2018-10-21 14:24:27.853750:	Training iteration: 216800, Loss: 0.17045371234416962
2018-10-21 14:24:56.602389:	Training iteration: 217000, Loss: 0.1857139617204666
2018-10-21 14:25:26.707791:	Training iteration: 217200, Loss: 0.1570679247379303
2018-10-21 14:25:56.391666:	Training iteration: 217400, Loss: 0.1120891124010086
2018-10-21 14:26:26.276306:	Training iteration: 217600, Loss: 0.10454386472702026
2018-10-21 14:26:56.141545:	Training iteration: 217800, Loss: 0.13766184449195862
2018-10-21 14:27:25.001298:	Training iteration: 218000, Loss: 0.15631026029586792
2018-10-21 14:27:53.780893:	Training iteration: 218200, Loss: 0.130793496966362
2018-10-21 14:28:23.172195:	Training iteration: 218400, Loss: 0.15226227045059204
2018-10-21 14:28:52.525853:	Training iteration: 218600, Loss: 0.16370263695716858
2018-10-21 14:29:22.020218:	Training iteration: 218800, Loss: 0.14144466817378998
2018-10-21 14:29:51.521718:	Training iteration: 219000, Loss: 0.1716991513967514
2018-10-21 14:30:20.461009:	Training iteration: 219200, Loss: 0.16321063041687012
2018-10-21 14:30:50.463728:	Training iteration: 219400, Loss: 0.13019061088562012
2018-10-21 14:31:19.919895:	Training iteration: 219600, Loss: 0.2235800325870514
2018-10-21 14:31:49.221071:	Training iteration: 219800, Loss: 0.16525709629058838
2018-10-21 14:32:19.691047:	Training iteration: 220000, Loss: 0.1389034390449524
2018-10-21 14:32:48.761747:	Training iteration: 220200, Loss: 0.11318473517894745
2018-10-21 14:33:18.717202:	Training iteration: 220400, Loss: 0.1601790189743042
2018-10-21 14:33:48.804897:	Training iteration: 220600, Loss: 0.16003337502479553
2018-10-21 14:34:18.392306:	Training iteration: 220800, Loss: 0.1458425521850586
2018-10-21 14:34:47.155944:	Training iteration: 221000, Loss: 0.12115127593278885
2018-10-21 14:35:16.903271:	Training iteration: 221200, Loss: 0.13441422581672668
2018-10-21 14:35:47.546061:	Training iteration: 221400, Loss: 0.09854791313409805
2018-10-21 14:36:16.985827:	Training iteration: 221600, Loss: 0.23776018619537354
2018-10-21 14:36:46.590284:	Training iteration: 221800, Loss: 0.2048834264278412
2018-10-21 14:37:15.480227:	Training iteration: 222000, Loss: 0.12969452142715454
2018-10-21 14:37:45.397364:	Training iteration: 222200, Loss: 0.1388634890317917
2018-10-21 14:38:15.327109:	Training iteration: 222400, Loss: 0.13601285219192505
2018-10-21 14:38:44.217669:	Training iteration: 222600, Loss: 0.1706543266773224
2018-10-21 14:39:13.278556:	Training iteration: 222800, Loss: 0.123520627617836
2018-10-21 14:39:42.451079:	Training iteration: 223000, Loss: 0.15636685490608215
2018-10-21 14:40:11.929662:	Training iteration: 223200, Loss: 0.1572074592113495
2018-10-21 14:40:41.245596:	Training iteration: 223400, Loss: 0.2493402361869812
2018-10-21 14:41:10.751356:	Training iteration: 223600, Loss: 0.17550601065158844
2018-10-21 14:41:40.546952:	Training iteration: 223800, Loss: 0.16173316538333893
2018-10-21 14:42:10.686451:	Training iteration: 224000, Loss: 0.15147283673286438
2018-10-21 14:42:40.030784:	Training iteration: 224200, Loss: 0.16321222484111786
2018-10-21 14:43:09.169989:	Training iteration: 224400, Loss: 0.13276469707489014
2018-10-21 14:43:38.455348:	Training iteration: 224600, Loss: 0.12417365610599518
2018-10-21 14:44:07.471422:	Training iteration: 224800, Loss: 0.16577191650867462
2018-10-21 14:44:36.946099:	Training iteration: 225000, Loss: 0.1345306783914566
2018-10-21 14:45:06.590055:	Training iteration: 225200, Loss: 0.14830952882766724
2018-10-21 14:45:35.567651:	Training iteration: 225400, Loss: 0.19207486510276794
2018-10-21 14:46:04.745946:	Training iteration: 225600, Loss: 0.1421034038066864
2018-10-21 14:46:35.147652:	Training iteration: 225800, Loss: 0.20460005104541779
2018-10-21 14:47:05.070858:	Training iteration: 226000, Loss: 0.14668124914169312
2018-10-21 14:47:34.456426:	Training iteration: 226200, Loss: 0.16678789258003235
2018-10-21 14:48:04.517138:	Training iteration: 226400, Loss: 0.16570167243480682
2018-10-21 14:48:33.864377:	Training iteration: 226600, Loss: 0.13039124011993408
2018-10-21 14:49:03.412771:	Training iteration: 226800, Loss: 0.09255269169807434
2018-10-21 14:49:33.153022:	Training iteration: 227000, Loss: 0.14393001794815063
2018-10-21 14:50:03.121735:	Training iteration: 227200, Loss: 0.1506805121898651
2018-10-21 14:50:32.366273:	Training iteration: 227400, Loss: 0.13659614324569702
2018-10-21 14:51:02.359968:	Training iteration: 227600, Loss: 0.2063887119293213
2018-10-21 14:51:32.014380:	Training iteration: 227800, Loss: 0.15521594882011414
2018-10-21 14:52:02.188865:	Training iteration: 228000, Loss: 0.14030484855175018
2018-10-21 14:52:32.208027:	Training iteration: 228200, Loss: 0.1603875756263733
2018-10-21 14:53:01.667248:	Training iteration: 228400, Loss: 0.1269318163394928
2018-10-21 14:53:30.216980:	Training iteration: 228600, Loss: 0.17229753732681274
2018-10-21 14:53:59.524039:	Training iteration: 228800, Loss: 0.2496422678232193
2018-10-21 14:54:29.480329:	Training iteration: 229000, Loss: 0.18938300013542175
2018-10-21 14:54:59.320463:	Training iteration: 229200, Loss: 0.2246398627758026
2018-10-21 14:55:29.027900:	Training iteration: 229400, Loss: 0.13590022921562195
2018-10-21 14:55:59.004816:	Training iteration: 229600, Loss: 0.11520291864871979
2018-10-21 14:56:27.949714:	Training iteration: 229800, Loss: 0.1406843215227127
2018-10-21 14:56:58.558751:	Training iteration: 230000, Loss: 0.1397029459476471
2018-10-21 14:57:27.866039:	Training iteration: 230200, Loss: 0.12244299054145813
2018-10-21 14:57:58.040296:	Training iteration: 230400, Loss: 0.14793077111244202
2018-10-21 14:58:28.231725:	Training iteration: 230600, Loss: 0.13665664196014404
2018-10-21 14:58:57.788003:	Training iteration: 230800, Loss: 0.12291553616523743
2018-10-21 14:59:27.385052:	Training iteration: 231000, Loss: 0.1419970989227295
2018-10-21 14:59:57.097506:	Training iteration: 231200, Loss: 0.15346527099609375
2018-10-21 15:00:26.727421:	Training iteration: 231400, Loss: 0.17913424968719482
2018-10-21 15:01:04.557260:	Training iteration: 231600, Loss: 0.1069999486207962
2018-10-21 15:01:34.499155:	Training iteration: 231800, Loss: 0.15181702375411987
2018-10-21 15:02:03.845016:	Training iteration: 232000, Loss: 0.1400003284215927
2018-10-21 15:02:33.208651:	Training iteration: 232200, Loss: 0.17443117499351501
2018-10-21 15:03:02.526097:	Training iteration: 232400, Loss: 0.1582852452993393
2018-10-21 15:03:32.821680:	Training iteration: 232600, Loss: 0.13412082195281982
2018-10-21 15:04:02.589176:	Training iteration: 232800, Loss: 0.12253127247095108
2018-10-21 15:04:32.218360:	Training iteration: 233000, Loss: 0.14291760325431824
2018-10-21 15:05:01.113417:	Training iteration: 233200, Loss: 0.2130817174911499
2018-10-21 15:05:30.399829:	Training iteration: 233400, Loss: 0.11510224640369415
2018-10-21 15:05:59.610039:	Training iteration: 233600, Loss: 0.13143548369407654
2018-10-21 15:06:28.734929:	Training iteration: 233800, Loss: 0.16950830817222595
2018-10-21 15:07:00.565869:	Training iteration: 234000, Loss: 0.16504864394664764
2018-10-21 15:07:29.253994:	Training iteration: 234200, Loss: 0.1687973439693451
2018-10-21 15:07:59.451592:	Training iteration: 234400, Loss: 0.14787256717681885
2018-10-21 15:08:29.100688:	Training iteration: 234600, Loss: 0.17860475182533264
2018-10-21 15:08:58.336071:	Training iteration: 234800, Loss: 0.14990921318531036
2018-10-21 15:09:28.919768:	Training iteration: 235000, Loss: 0.14297889173030853
2018-10-21 15:09:57.727552:	Training iteration: 235200, Loss: 0.1192479133605957
2018-10-21 15:10:26.784519:	Training iteration: 235400, Loss: 0.12892717123031616
2018-10-21 15:10:56.697086:	Training iteration: 235600, Loss: 0.15904009342193604
2018-10-21 15:11:26.182664:	Training iteration: 235800, Loss: 0.17623409628868103
2018-10-21 15:11:55.768589:	Training iteration: 236000, Loss: 0.22921305894851685
2018-10-21 15:12:43.557045:	Training iteration: 236200, Loss: 0.17657122015953064
2018-10-21 15:13:11.040393:	Training iteration: 236400, Loss: 0.14723163843154907
2018-10-21 15:13:39.738983:	Training iteration: 236600, Loss: 0.22354507446289062
2018-10-21 15:14:09.133174:	Training iteration: 236800, Loss: 0.15378695726394653
2018-10-21 15:14:38.597771:	Training iteration: 237000, Loss: 0.11143742501735687
2018-10-21 15:15:08.029157:	Training iteration: 237200, Loss: 0.14640802145004272
2018-10-21 15:15:37.594681:	Training iteration: 237400, Loss: 0.1424335539340973
2018-10-21 15:16:06.916451:	Training iteration: 237600, Loss: 0.14290563762187958
2018-10-21 15:16:36.723088:	Training iteration: 237800, Loss: 0.14591382443904877
2018-10-21 15:17:06.339137:	Training iteration: 238000, Loss: 0.1629968285560608
2018-10-21 15:17:36.015673:	Training iteration: 238200, Loss: 0.21965280175209045
2018-10-21 15:18:05.703454:	Training iteration: 238400, Loss: 0.15780386328697205
2018-10-21 15:18:35.577110:	Training iteration: 238600, Loss: 0.11365708708763123
2018-10-21 15:19:05.248436:	Training iteration: 238800, Loss: 0.16585353016853333
2018-10-21 15:19:35.118420:	Training iteration: 239000, Loss: 0.16897520422935486
2018-10-21 15:20:07.381520:	Training iteration: 239200, Loss: 0.3612058758735657
2018-10-21 15:20:35.980414:	Training iteration: 239400, Loss: 0.1547030210494995
2018-10-21 15:21:05.279868:	Training iteration: 239600, Loss: 0.22773292660713196
2018-10-21 15:21:35.292025:	Training iteration: 239800, Loss: 0.1919378936290741
2018-10-21 15:21:50.540039:	Epoch 1 finished after 239903 iterations.
No images to record
Validating
2018-10-21 15:21:51.032686:	Entering validation loop
2018-10-21 15:22:01.098849: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 650 of 1000
2018-10-21 15:22:06.011211: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 15:22:20.528697:	Validation iteration: 200, Loss: 0.18260668218135834
2018-10-21 15:22:33.772332:	Validation iteration: 400, Loss: 0.15373946726322174
2018-10-21 15:22:48.101489:	Validation iteration: 600, Loss: 0.1631244719028473
2018-10-21 15:23:02.985767:	Validation iteration: 800, Loss: 0.20640835165977478
2018-10-21 15:23:16.948084:	Validation iteration: 1000, Loss: 0.17262637615203857
2018-10-21 15:23:31.079754:	Validation iteration: 1200, Loss: 0.1545991599559784
2018-10-21 15:23:45.261409:	Validation iteration: 1400, Loss: 0.22369951009750366
2018-10-21 15:23:59.795567:	Validation iteration: 1600, Loss: 0.1841273307800293
2018-10-21 15:24:13.908628:	Validation iteration: 1800, Loss: 0.1491420865058899
2018-10-21 15:24:27.636769:	Validation iteration: 2000, Loss: 0.22936654090881348
2018-10-21 15:24:43.710079: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 685 of 1000
2018-10-21 15:24:48.024117: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 15:24:55.938745:	Validation iteration: 2200, Loss: 0.23628413677215576
2018-10-21 15:25:09.350756:	Validation iteration: 2400, Loss: 0.1818419098854065
2018-10-21 15:25:23.797236:	Validation iteration: 2600, Loss: 0.27465254068374634
2018-10-21 15:25:38.774393:	Validation iteration: 2800, Loss: 0.2078484743833542
2018-10-21 15:25:52.727538:	Validation iteration: 3000, Loss: 0.22951707243919373
2018-10-21 15:26:07.046829:	Validation iteration: 3200, Loss: 0.27949562668800354
2018-10-21 15:26:21.493196:	Validation iteration: 3400, Loss: 0.3017256259918213
2018-10-21 15:26:35.538821:	Validation iteration: 3600, Loss: 0.21077081561088562
2018-10-21 15:26:49.393415:	Validation iteration: 3800, Loss: 0.20367151498794556
2018-10-21 15:27:03.653756:	Validation iteration: 4000, Loss: 0.2505128085613251
2018-10-21 15:27:26.085864: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 650 of 1000
2018-10-21 15:27:30.880962: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 15:27:32.471425:	Validation iteration: 4200, Loss: 0.21082989871501923
2018-10-21 15:27:45.663940:	Validation iteration: 4400, Loss: 0.26266831159591675
2018-10-21 15:27:59.350672:	Validation iteration: 4600, Loss: 0.2275378555059433
2018-10-21 15:28:15.115953:	Validation iteration: 4800, Loss: 0.22800251841545105
2018-10-21 15:28:29.006184:	Validation iteration: 5000, Loss: 0.14440232515335083
2018-10-21 15:28:43.442843:	Validation iteration: 5200, Loss: 0.2363712340593338
2018-10-21 15:28:57.410674:	Validation iteration: 5400, Loss: 0.17323420941829681
2018-10-21 15:29:11.884245:	Validation iteration: 5600, Loss: 0.20364446938037872
2018-10-21 15:29:26.027900:	Validation iteration: 5800, Loss: 0.16606569290161133
2018-10-21 15:29:40.311607:	Validation iteration: 6000, Loss: 0.1619434952735901
2018-10-21 15:29:54.721645:	Validation iteration: 6200, Loss: 0.1554032564163208
2018-10-21 15:30:09.232086: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 658 of 1000
2018-10-21 15:30:14.035367: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 15:30:22.944629:	Validation iteration: 6400, Loss: 0.2058049589395523
2018-10-21 15:30:37.783314:	Validation iteration: 6600, Loss: 0.1784260869026184
2018-10-21 15:30:51.724790:	Validation iteration: 6800, Loss: 0.21701383590698242
2018-10-21 15:31:05.727499:	Validation iteration: 7000, Loss: 0.18561768531799316
2018-10-21 15:31:19.858488:	Validation iteration: 7200, Loss: 0.21285587549209595
2018-10-21 15:31:34.071463:	Validation iteration: 7400, Loss: 0.19597119092941284
2018-10-21 15:31:48.244687:	Validation iteration: 7600, Loss: 0.18190598487854004
2018-10-21 15:32:02.796852:	Validation iteration: 7800, Loss: 0.1272105872631073
2018-10-21 15:32:16.891035:	Validation iteration: 8000, Loss: 0.25149470567703247
2018-10-21 15:32:31.022267:	Validation iteration: 8200, Loss: 0.26012054085731506
2018-10-21 15:32:52.015117: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 628 of 1000
2018-10-21 15:32:57.496450: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 15:33:00.688077:	Validation iteration: 8400, Loss: 0.247666135430336
2018-10-21 15:33:13.863066:	Validation iteration: 8600, Loss: 0.1467309445142746
2018-10-21 15:33:27.243047:	Validation iteration: 8800, Loss: 0.1125883013010025
2018-10-21 15:33:42.826058:	Validation iteration: 9000, Loss: 0.16672594845294952
2018-10-21 15:33:56.995405:	Validation iteration: 9200, Loss: 0.1339397430419922
2018-10-21 15:34:10.902826:	Validation iteration: 9400, Loss: 0.1008637547492981
2018-10-21 15:34:25.133200:	Validation iteration: 9600, Loss: 0.2181629240512848
2018-10-21 15:34:39.568350:	Validation iteration: 9800, Loss: 0.14662638306617737
2018-10-21 15:34:53.849427:	Validation iteration: 10000, Loss: 0.17459653317928314
2018-10-21 15:35:08.343312:	Validation iteration: 10200, Loss: 0.1634131371974945
2018-10-21 15:35:23.155481:	Validation iteration: 10400, Loss: 0.21668016910552979
2018-10-21 15:35:37.543882:	Validation iteration: 10600, Loss: 0.10185161232948303
2018-10-21 15:35:52.026054:	Validation iteration: 10800, Loss: 0.2494332194328308
2018-10-21 15:36:06.050590:	Validation iteration: 11000, Loss: 0.14842589199543
2018-10-21 15:36:20.399650:	Validation iteration: 11200, Loss: 0.1972893476486206
Validation check mean loss: 0.19583308528535692
Validation loss has worsened. worse_val_checks = 1
Checkpoint
2018-10-21 15:43:48.513699: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 729 of 1000
2018-10-21 15:43:52.119988: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 15:44:04.247877:	Training iteration: 240000, Loss: 0.13713622093200684
2018-10-21 15:44:28.601645:	Training iteration: 240200, Loss: 0.12938930094242096
2018-10-21 15:44:52.903995:	Training iteration: 240400, Loss: 0.09327234327793121
2018-10-21 15:45:17.364099:	Training iteration: 240600, Loss: 0.1262323260307312
2018-10-21 15:45:41.832511:	Training iteration: 240800, Loss: 0.10795524716377258
2018-10-21 15:46:08.164221:	Training iteration: 241000, Loss: 0.11143572628498077
2018-10-21 15:46:34.712509:	Training iteration: 241200, Loss: 0.12491215765476227
2018-10-21 15:47:02.186899:	Training iteration: 241400, Loss: 0.152250736951828
2018-10-21 15:47:30.888372:	Training iteration: 241600, Loss: 0.12697497010231018
2018-10-21 15:48:06.043960:	Training iteration: 241800, Loss: 0.08250149339437485
2018-10-21 15:48:32.313992:	Training iteration: 242000, Loss: 0.1144171804189682
2018-10-21 15:48:59.216720:	Training iteration: 242200, Loss: 0.13352887332439423
2018-10-21 15:49:27.215365:	Training iteration: 242400, Loss: 0.09209762513637543
2018-10-21 15:49:57.325299:	Training iteration: 242600, Loss: 0.10820464789867401
2018-10-21 15:50:28.092219:	Training iteration: 242800, Loss: 0.07871326059103012
2018-10-21 15:50:56.111125:	Training iteration: 243000, Loss: 0.08844073116779327
2018-10-21 15:51:28.802958:	Training iteration: 243200, Loss: 0.1386599838733673
2018-10-21 15:51:57.659513:	Training iteration: 243400, Loss: 0.13969393074512482
2018-10-21 15:52:44.122279:	Training iteration: 243600, Loss: 0.1325874626636505
2018-10-21 15:53:10.913524:	Training iteration: 243800, Loss: 0.16799446940422058
2018-10-21 15:53:40.341892:	Training iteration: 244000, Loss: 0.12138855457305908
2018-10-21 15:54:09.698725:	Training iteration: 244200, Loss: 0.13825908303260803
2018-10-21 15:54:39.151366:	Training iteration: 244400, Loss: 0.11706794798374176
2018-10-21 15:55:08.291944:	Training iteration: 244600, Loss: 0.09818631410598755
2018-10-21 15:55:38.893275:	Training iteration: 244800, Loss: 0.11981593072414398
2018-10-21 15:56:08.898593:	Training iteration: 245000, Loss: 0.10060843825340271
2018-10-21 15:56:38.242597:	Training iteration: 245200, Loss: 0.10085730254650116
2018-10-21 15:57:08.388698:	Training iteration: 245400, Loss: 0.1031615138053894
2018-10-21 15:57:37.993093:	Training iteration: 245600, Loss: 0.09519636631011963
2018-10-21 15:58:07.570043:	Training iteration: 245800, Loss: 0.09411399066448212
2018-10-21 15:58:37.324351:	Training iteration: 246000, Loss: 0.08889694511890411
2018-10-21 15:59:06.480659:	Training iteration: 246200, Loss: 0.1248474046587944
2018-10-21 15:59:35.918240:	Training iteration: 246400, Loss: 0.08975708484649658
2018-10-21 16:00:05.015990:	Training iteration: 246600, Loss: 0.11118902266025543
2018-10-21 16:00:35.158393:	Training iteration: 246800, Loss: 0.09994783997535706
2018-10-21 16:01:05.329052:	Training iteration: 247000, Loss: 0.10069458186626434
2018-10-21 16:01:39.486680:	Training iteration: 247200, Loss: 0.11280378699302673
2018-10-21 16:02:08.653414:	Training iteration: 247400, Loss: 0.1495501548051834
2018-10-21 16:02:38.272080:	Training iteration: 247600, Loss: 0.09534497559070587
2018-10-21 16:03:07.911584:	Training iteration: 247800, Loss: 0.12833455204963684
2018-10-21 16:03:37.563047:	Training iteration: 248000, Loss: 0.14183998107910156
2018-10-21 16:04:07.557756:	Training iteration: 248200, Loss: 0.073056161403656
2018-10-21 16:04:41.732049:	Training iteration: 248400, Loss: 0.09293045103549957
2018-10-21 16:05:11.461899:	Training iteration: 248600, Loss: 0.1415329873561859
2018-10-21 16:05:40.621677:	Training iteration: 248800, Loss: 0.11456155776977539
2018-10-21 16:06:12.685332:	Training iteration: 249000, Loss: 0.10811332613229752
2018-10-21 16:06:46.498404:	Training iteration: 249200, Loss: 0.0841476172208786
2018-10-21 16:07:16.478203:	Training iteration: 249400, Loss: 0.12654048204421997
2018-10-21 16:07:46.808866:	Training iteration: 249600, Loss: 0.09151482582092285
2018-10-21 16:08:17.045011:	Training iteration: 249800, Loss: 0.11186237633228302
2018-10-21 16:08:47.078246:	Training iteration: 250000, Loss: 0.17260903120040894
2018-10-21 16:09:17.144022:	Training iteration: 250200, Loss: 0.12861764430999756
2018-10-21 16:09:46.876316:	Training iteration: 250400, Loss: 0.10967760533094406
2018-10-21 16:10:17.468482:	Training iteration: 250600, Loss: 0.10965088754892349
2018-10-21 16:10:47.195410:	Training iteration: 250800, Loss: 0.11392848193645477
2018-10-21 16:11:18.304917:	Training iteration: 251000, Loss: 0.1132722869515419
2018-10-21 16:11:47.837759:	Training iteration: 251200, Loss: 0.15304194390773773
2018-10-21 16:12:18.559559:	Training iteration: 251400, Loss: 0.10776546597480774
2018-10-21 16:12:48.852318:	Training iteration: 251600, Loss: 0.13991400599479675
2018-10-21 16:13:19.008512:	Training iteration: 251800, Loss: 0.16565850377082825
2018-10-21 16:13:49.098769:	Training iteration: 252000, Loss: 0.13421231508255005
2018-10-21 16:14:19.431176:	Training iteration: 252200, Loss: 0.1403409242630005
2018-10-21 16:14:44.113305: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 707 of 1000
2018-10-21 16:14:47.666167: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 16:15:02.073547:	Training iteration: 252400, Loss: 0.12924262881278992
2018-10-21 16:15:30.638127:	Training iteration: 252600, Loss: 0.10452810674905777
2018-10-21 16:16:01.189922:	Training iteration: 252800, Loss: 0.11372411251068115
2018-10-21 16:16:31.079350:	Training iteration: 253000, Loss: 0.20556890964508057
2018-10-21 16:17:01.124107:	Training iteration: 253200, Loss: 0.08845849335193634
2018-10-21 16:17:31.496425:	Training iteration: 253400, Loss: 0.13095276057720184
2018-10-21 16:18:01.717169:	Training iteration: 253600, Loss: 0.09686644375324249
2018-10-21 16:18:30.953921:	Training iteration: 253800, Loss: 0.1397845596075058
2018-10-21 16:19:00.870713:	Training iteration: 254000, Loss: 0.12984870374202728
2018-10-21 16:19:30.687752:	Training iteration: 254200, Loss: 0.13825738430023193
2018-10-21 16:20:00.281871:	Training iteration: 254400, Loss: 0.13939017057418823
2018-10-21 16:20:29.790558:	Training iteration: 254600, Loss: 0.12496799230575562
2018-10-21 16:21:00.003458:	Training iteration: 254800, Loss: 0.10098208487033844
2018-10-21 16:21:29.359712:	Training iteration: 255000, Loss: 0.10482816398143768
2018-10-21 16:21:58.862765:	Training iteration: 255200, Loss: 0.10336209833621979
2018-10-21 16:22:28.333685:	Training iteration: 255400, Loss: 0.13869529962539673
2018-10-21 16:22:57.744641:	Training iteration: 255600, Loss: 0.09281828999519348
2018-10-21 16:23:27.329612:	Training iteration: 255800, Loss: 0.16416405141353607
2018-10-21 16:23:57.794139:	Training iteration: 256000, Loss: 0.10283613950014114
2018-10-21 16:24:27.684809:	Training iteration: 256200, Loss: 0.13717269897460938
2018-10-21 16:24:57.149250:	Training iteration: 256400, Loss: 0.16940546035766602
2018-10-21 16:25:27.832241:	Training iteration: 256600, Loss: 0.12035425007343292
2018-10-21 16:25:59.501050:	Training iteration: 256800, Loss: 0.13727015256881714
2018-10-21 16:26:29.295448:	Training iteration: 257000, Loss: 0.18938641250133514
2018-10-21 16:26:59.230491:	Training iteration: 257200, Loss: 0.08354464173316956
2018-10-21 16:27:29.604320:	Training iteration: 257400, Loss: 0.06497875601053238
2018-10-21 16:27:58.545281:	Training iteration: 257600, Loss: 0.12497451156377792
2018-10-21 16:28:29.222944:	Training iteration: 257800, Loss: 0.06815638393163681
2018-10-21 16:28:59.170211:	Training iteration: 258000, Loss: 0.10162414610385895
2018-10-21 16:29:28.895939:	Training iteration: 258200, Loss: 0.0989564061164856
2018-10-21 16:29:59.002812:	Training iteration: 258400, Loss: 0.10176815092563629
2018-10-21 16:30:28.781515:	Training iteration: 258600, Loss: 0.07775913178920746
2018-10-21 16:30:57.983562:	Training iteration: 258800, Loss: 0.12718605995178223
2018-10-21 16:31:26.443823:	Training iteration: 259000, Loss: 0.12946048378944397
2018-10-21 16:31:55.596190:	Training iteration: 259200, Loss: 0.10377782583236694
2018-10-21 16:32:24.977287:	Training iteration: 259400, Loss: 0.0913039892911911
2018-10-21 16:32:54.628833:	Training iteration: 259600, Loss: 0.1314891129732132
2018-10-21 16:33:24.997245:	Training iteration: 259800, Loss: 0.1315183937549591
2018-10-21 16:33:54.853971:	Training iteration: 260000, Loss: 0.1180804967880249
2018-10-21 16:34:24.532460:	Training iteration: 260200, Loss: 0.09696526825428009
2018-10-21 16:34:54.885317:	Training iteration: 260400, Loss: 0.119244784116745
2018-10-21 16:35:24.571340:	Training iteration: 260600, Loss: 0.11318031698465347
2018-10-21 16:35:54.727782:	Training iteration: 260800, Loss: 0.08755065500736237
2018-10-21 16:36:24.630112:	Training iteration: 261000, Loss: 0.12634170055389404
2018-10-21 16:36:54.428211:	Training iteration: 261200, Loss: 0.1493903398513794
2018-10-21 16:37:23.502867:	Training iteration: 261400, Loss: 0.10841955244541168
2018-10-21 16:37:53.006274:	Training iteration: 261600, Loss: 0.11073195189237595
2018-10-21 16:38:22.626801:	Training iteration: 261800, Loss: 0.09673081338405609
2018-10-21 16:38:53.411144:	Training iteration: 262000, Loss: 0.1121356338262558
2018-10-21 16:39:23.461844:	Training iteration: 262200, Loss: 0.08111134171485901
2018-10-21 16:39:54.989937:	Training iteration: 262400, Loss: 0.13143110275268555
2018-10-21 16:40:25.215177:	Training iteration: 262600, Loss: 0.15729311108589172
2018-10-21 16:40:56.448131:	Training iteration: 262800, Loss: 0.10137289017438889
2018-10-21 16:41:25.294285:	Training iteration: 263000, Loss: 0.16881637275218964
2018-10-21 16:42:00.654838:	Training iteration: 263200, Loss: 0.09094209969043732
2018-10-21 16:42:37.438555:	Training iteration: 263400, Loss: 0.12016058713197708
2018-10-21 16:43:04.653041:	Training iteration: 263600, Loss: 0.0898328572511673
2018-10-21 16:43:32.608566:	Training iteration: 263800, Loss: 0.10094400495290756
2018-10-21 16:44:01.544663:	Training iteration: 264000, Loss: 0.0856173187494278
2018-10-21 16:44:30.440342:	Training iteration: 264200, Loss: 0.11583904922008514
2018-10-21 16:45:00.503753:	Training iteration: 264400, Loss: 0.11066502332687378
2018-10-21 16:45:29.871021:	Training iteration: 264600, Loss: 0.0800066739320755
2018-10-21 16:46:04.185289: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 743 of 1000
2018-10-21 16:46:09.252982: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 16:46:14.105326:	Training iteration: 264800, Loss: 0.08750317990779877
2018-10-21 16:46:42.539036:	Training iteration: 265000, Loss: 0.08762083947658539
2018-10-21 16:47:12.774358:	Training iteration: 265200, Loss: 0.09282544255256653
2018-10-21 16:47:41.916205:	Training iteration: 265400, Loss: 0.06605221331119537
2018-10-21 16:48:10.906361:	Training iteration: 265600, Loss: 0.10483483970165253
2018-10-21 16:48:40.755544:	Training iteration: 265800, Loss: 0.08725976198911667
2018-10-21 16:49:10.471539:	Training iteration: 266000, Loss: 0.09986724704504013
2018-10-21 16:49:40.195495:	Training iteration: 266200, Loss: 0.1070837527513504
2018-10-21 16:50:09.531671:	Training iteration: 266400, Loss: 0.15199196338653564
2018-10-21 16:50:38.678425:	Training iteration: 266600, Loss: 0.12316513061523438
2018-10-21 16:51:07.301069:	Training iteration: 266800, Loss: 0.10439325869083405
2018-10-21 16:51:37.245894:	Training iteration: 267000, Loss: 0.09683046489953995
2018-10-21 16:52:07.073742:	Training iteration: 267200, Loss: 0.0765351951122284
2018-10-21 16:52:36.893157:	Training iteration: 267400, Loss: 0.09224678575992584
2018-10-21 16:53:06.656276:	Training iteration: 267600, Loss: 0.09863537549972534
2018-10-21 16:53:37.347622:	Training iteration: 267800, Loss: 0.07991273701190948
2018-10-21 16:54:06.764800:	Training iteration: 268000, Loss: 0.04913109540939331
2018-10-21 16:54:36.283965:	Training iteration: 268200, Loss: 0.08890864998102188
2018-10-21 16:55:06.181512:	Training iteration: 268400, Loss: 0.06882736086845398
2018-10-21 16:55:35.693526:	Training iteration: 268600, Loss: 0.07829548418521881
2018-10-21 16:56:05.731846:	Training iteration: 268800, Loss: 0.1251584142446518
2018-10-21 16:56:35.336201:	Training iteration: 269000, Loss: 0.08067736774682999
2018-10-21 16:57:05.105823:	Training iteration: 269200, Loss: 0.06839063763618469
2018-10-21 16:57:35.216247:	Training iteration: 269400, Loss: 0.06274168193340302
2018-10-21 16:58:05.339060:	Training iteration: 269600, Loss: 0.12364993989467621
2018-10-21 16:58:34.813421:	Training iteration: 269800, Loss: 0.07698370516300201
2018-10-21 16:59:04.678219:	Training iteration: 270000, Loss: 0.0802149549126625
2018-10-21 16:59:34.409974:	Training iteration: 270200, Loss: 0.08706662058830261
2018-10-21 17:00:04.409078:	Training iteration: 270400, Loss: 0.0784185379743576
2018-10-21 17:00:34.568500:	Training iteration: 270600, Loss: 0.09600812941789627
2018-10-21 17:01:04.225684:	Training iteration: 270800, Loss: 0.09173336625099182
2018-10-21 17:01:33.899598:	Training iteration: 271000, Loss: 0.07182396948337555
2018-10-21 17:02:03.792781:	Training iteration: 271200, Loss: 0.12938819825649261
2018-10-21 17:02:35.317203:	Training iteration: 271400, Loss: 0.10163791477680206
2018-10-21 17:03:05.216821:	Training iteration: 271600, Loss: 0.07121004164218903
2018-10-21 17:03:34.462560:	Training iteration: 271800, Loss: 0.08242222666740417
2018-10-21 17:04:04.897283:	Training iteration: 272000, Loss: 0.0738859623670578
2018-10-21 17:04:34.837532:	Training iteration: 272200, Loss: 0.06734418869018555
2018-10-21 17:05:04.275233:	Training iteration: 272400, Loss: 0.08419068902730942
2018-10-21 17:05:34.359529:	Training iteration: 272600, Loss: 0.15243227779865265
2018-10-21 17:06:04.449508:	Training iteration: 272800, Loss: 0.07093051075935364
2018-10-21 17:06:33.966921:	Training iteration: 273000, Loss: 0.09843049943447113
2018-10-21 17:07:03.466312:	Training iteration: 273200, Loss: 0.15348392724990845
2018-10-21 17:07:32.635457:	Training iteration: 273400, Loss: 0.1151750236749649
2018-10-21 17:08:02.435033:	Training iteration: 273600, Loss: 0.06863971054553986
2018-10-21 17:08:31.564667:	Training iteration: 273800, Loss: 0.08754708617925644
2018-10-21 17:09:01.064609:	Training iteration: 274000, Loss: 0.08725713938474655
2018-10-21 17:09:30.663152:	Training iteration: 274200, Loss: 0.06738273799419403
2018-10-21 17:10:00.288273:	Training iteration: 274400, Loss: 0.08644799888134003
2018-10-21 17:10:30.583050:	Training iteration: 274600, Loss: 0.10026752948760986
2018-10-21 17:10:59.494058:	Training iteration: 274800, Loss: 0.07759033143520355
2018-10-21 17:11:29.204044:	Training iteration: 275000, Loss: 0.07768125832080841
2018-10-21 17:11:58.235115:	Training iteration: 275200, Loss: 0.10814091563224792
2018-10-21 17:12:27.500115:	Training iteration: 275400, Loss: 0.14454591274261475
2018-10-21 17:12:57.051026:	Training iteration: 275600, Loss: 0.08162853121757507
2018-10-21 17:13:26.801653:	Training iteration: 275800, Loss: 0.10917846858501434
2018-10-21 17:13:56.248336:	Training iteration: 276000, Loss: 0.08489970117807388
2018-10-21 17:14:25.941886:	Training iteration: 276200, Loss: 0.08979728072881699
2018-10-21 17:14:55.800526:	Training iteration: 276400, Loss: 0.11051084101200104
2018-10-21 17:15:25.071941:	Training iteration: 276600, Loss: 0.08003805577754974
2018-10-21 17:15:54.666009:	Training iteration: 276800, Loss: 0.07336682081222534
2018-10-21 17:16:24.654514:	Training iteration: 277000, Loss: 0.08362577855587006
2018-10-21 17:16:55.088147:	Training iteration: 277200, Loss: 0.07265287637710571
2018-10-21 17:17:09.029234: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 719 of 1000
2018-10-21 17:17:12.370111: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 17:17:38.446289:	Training iteration: 277400, Loss: 0.17991690337657928
2018-10-21 17:18:08.085736:	Training iteration: 277600, Loss: 0.11976145952939987
2018-10-21 17:18:38.668695:	Training iteration: 277800, Loss: 0.07620024681091309
2018-10-21 17:19:08.522198:	Training iteration: 278000, Loss: 0.19672149419784546
2018-10-21 17:19:38.352729:	Training iteration: 278200, Loss: 0.1653599739074707
2018-10-21 17:20:08.030544:	Training iteration: 278400, Loss: 0.09681296348571777
2018-10-21 17:20:37.069089:	Training iteration: 278600, Loss: 0.09250489622354507
2018-10-21 17:21:07.189451:	Training iteration: 278800, Loss: 0.14534682035446167
2018-10-21 17:21:37.125599:	Training iteration: 279000, Loss: 0.0859130471944809
2018-10-21 17:22:06.954906:	Training iteration: 279200, Loss: 0.1263241469860077
2018-10-21 17:22:36.584671:	Training iteration: 279400, Loss: 0.16178378462791443
2018-10-21 17:23:04.836310:	Training iteration: 279600, Loss: 0.13340383768081665
2018-10-21 17:23:33.927913:	Training iteration: 279800, Loss: 0.11609017848968506
2018-10-21 17:24:03.317531:	Training iteration: 280000, Loss: 0.14916250109672546
2018-10-21 17:24:33.147931:	Training iteration: 280200, Loss: 0.15325304865837097
2018-10-21 17:25:02.213302:	Training iteration: 280400, Loss: 0.11700177192687988
2018-10-21 17:25:32.723291:	Training iteration: 280600, Loss: 0.15936851501464844
2018-10-21 17:26:03.710671:	Training iteration: 280800, Loss: 0.07250082492828369
2018-10-21 17:26:34.330807:	Training iteration: 281000, Loss: 0.16147954761981964
2018-10-21 17:27:03.999187:	Training iteration: 281200, Loss: 0.10723699629306793
2018-10-21 17:27:33.257747:	Training iteration: 281400, Loss: 0.13859432935714722
2018-10-21 17:28:01.833720:	Training iteration: 281600, Loss: 0.07581309974193573
2018-10-21 17:28:30.930891:	Training iteration: 281800, Loss: 0.1790419965982437
2018-10-21 17:29:00.770440:	Training iteration: 282000, Loss: 0.2284678816795349
2018-10-21 17:29:29.465049:	Training iteration: 282200, Loss: 0.05868373066186905
2018-10-21 17:29:59.140243:	Training iteration: 282400, Loss: 0.09824074059724808
2018-10-21 17:30:28.841246:	Training iteration: 282600, Loss: 0.17181536555290222
2018-10-21 17:30:59.138965:	Training iteration: 282800, Loss: 0.13745775818824768
2018-10-21 17:31:29.745173:	Training iteration: 283000, Loss: 0.17582277953624725
2018-10-21 17:31:59.967942:	Training iteration: 283200, Loss: 0.159709632396698
2018-10-21 17:32:29.637481:	Training iteration: 283400, Loss: 0.11430713534355164
2018-10-21 17:32:59.091183:	Training iteration: 283600, Loss: 0.1223876103758812
2018-10-21 17:33:28.112139:	Training iteration: 283800, Loss: 0.11706335097551346
2018-10-21 17:33:58.479678:	Training iteration: 284000, Loss: 0.12052898854017258
2018-10-21 17:34:28.579343:	Training iteration: 284200, Loss: 0.07183454930782318
2018-10-21 17:34:58.530141:	Training iteration: 284400, Loss: 0.09062375128269196
2018-10-21 17:35:28.599965:	Training iteration: 284600, Loss: 0.11784659326076508
2018-10-21 17:35:57.878666:	Training iteration: 284800, Loss: 0.10367606580257416
2018-10-21 17:36:27.806837:	Training iteration: 285000, Loss: 0.08713988959789276
2018-10-21 17:36:56.797325:	Training iteration: 285200, Loss: 0.1472875475883484
2018-10-21 17:37:29.554250:	Training iteration: 285400, Loss: 0.19364814460277557
2018-10-21 17:37:58.911364:	Training iteration: 285600, Loss: 0.08851523697376251
2018-10-21 17:38:29.174921:	Training iteration: 285800, Loss: 0.08666889369487762
2018-10-21 17:38:58.733115:	Training iteration: 286000, Loss: 0.15870097279548645
2018-10-21 17:39:29.140806:	Training iteration: 286200, Loss: 0.16576868295669556
2018-10-21 17:39:58.647957:	Training iteration: 286400, Loss: 0.10876777023077011
2018-10-21 17:40:28.576866:	Training iteration: 286600, Loss: 0.11482705175876617
2018-10-21 17:40:58.641090:	Training iteration: 286800, Loss: 0.11912745237350464
2018-10-21 17:41:27.455405:	Training iteration: 287000, Loss: 0.11419960856437683
2018-10-21 17:41:56.133089:	Training iteration: 287200, Loss: 0.13534879684448242
2018-10-21 17:42:24.769694:	Training iteration: 287400, Loss: 0.1519758701324463
2018-10-21 17:42:54.599073:	Training iteration: 287600, Loss: 0.09791237860918045
2018-10-21 17:43:23.587809:	Training iteration: 287800, Loss: 0.1798805296421051
2018-10-21 17:43:53.009796:	Training iteration: 288000, Loss: 0.10046906769275665
2018-10-21 17:44:22.387500:	Training iteration: 288200, Loss: 0.14077642560005188
2018-10-21 17:44:54.230052:	Training iteration: 288400, Loss: 0.1453283429145813
2018-10-21 17:45:24.231991:	Training iteration: 288600, Loss: 0.23731637001037598
2018-10-21 17:45:53.471695:	Training iteration: 288800, Loss: 0.11992256343364716
2018-10-21 17:46:22.748589:	Training iteration: 289000, Loss: 0.06597086787223816
2018-10-21 17:46:53.111632:	Training iteration: 289200, Loss: 0.1536751389503479
2018-10-21 17:47:21.566768:	Training iteration: 289400, Loss: 0.0619162917137146
2018-10-21 17:47:50.634715:	Training iteration: 289600, Loss: 0.1629331111907959
2018-10-21 17:48:21.000230:	Training iteration: 289800, Loss: 0.1534510850906372
2018-10-21 17:48:50.490812:	Training iteration: 290000, Loss: 0.09888231009244919
2018-10-21 17:49:19.850062:	Training iteration: 290200, Loss: 0.14148935675621033
2018-10-21 17:49:36.519582: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 841 of 1000
2018-10-21 17:49:38.313936: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 17:50:01.184204:	Training iteration: 290400, Loss: 0.14959564805030823
2018-10-21 17:50:29.941896:	Training iteration: 290600, Loss: 0.16088862717151642
2018-10-21 17:50:58.485335:	Training iteration: 290800, Loss: 0.14746569097042084
2018-10-21 17:51:28.328896:	Training iteration: 291000, Loss: 0.15073752403259277
2018-10-21 17:51:57.248209:	Training iteration: 291200, Loss: 0.2555769681930542
2018-10-21 17:52:26.373016:	Training iteration: 291400, Loss: 0.14524821937084198
2018-10-21 17:52:55.994627:	Training iteration: 291600, Loss: 0.1415158212184906
2018-10-21 17:53:25.713906:	Training iteration: 291800, Loss: 0.19257287681102753
2018-10-21 17:53:55.235369:	Training iteration: 292000, Loss: 0.11665012687444687
2018-10-21 17:54:24.934884:	Training iteration: 292200, Loss: 0.159292533993721
2018-10-21 17:54:55.223244:	Training iteration: 292400, Loss: 0.13051919639110565
2018-10-21 17:55:24.895865:	Training iteration: 292600, Loss: 0.12387160211801529
2018-10-21 17:55:54.583969:	Training iteration: 292800, Loss: 0.11970562487840652
2018-10-21 17:56:23.760596:	Training iteration: 293000, Loss: 0.1380138397216797
2018-10-21 17:56:53.837717:	Training iteration: 293200, Loss: 0.16348797082901
2018-10-21 17:57:24.075381:	Training iteration: 293400, Loss: 0.13523681461811066
2018-10-21 17:57:53.531192:	Training iteration: 293600, Loss: 0.15290221571922302
2018-10-21 17:58:23.172924:	Training iteration: 293800, Loss: 0.1453811228275299
2018-10-21 17:58:52.340796:	Training iteration: 294000, Loss: 0.15672442317008972
2018-10-21 17:59:22.320149:	Training iteration: 294200, Loss: 0.19093813002109528
2018-10-21 17:59:52.557740:	Training iteration: 294400, Loss: 0.1415129452943802
2018-10-21 18:00:21.615650:	Training iteration: 294600, Loss: 0.09295400232076645
2018-10-21 18:00:50.656347:	Training iteration: 294800, Loss: 0.11355043947696686
2018-10-21 18:01:20.465194:	Training iteration: 295000, Loss: 0.15394674241542816
2018-10-21 18:01:50.191264:	Training iteration: 295200, Loss: 0.12198016047477722
2018-10-21 18:02:21.916339:	Training iteration: 295400, Loss: 0.17799070477485657
2018-10-21 18:02:52.171232:	Training iteration: 295600, Loss: 0.13633573055267334
2018-10-21 18:03:22.222056:	Training iteration: 295800, Loss: 0.16958725452423096
2018-10-21 18:03:51.705097:	Training iteration: 296000, Loss: 0.16283702850341797
2018-10-21 18:04:21.843932:	Training iteration: 296200, Loss: 0.2284640669822693
2018-10-21 18:04:52.217907:	Training iteration: 296400, Loss: 0.11456570029258728
2018-10-21 18:05:21.880659:	Training iteration: 296600, Loss: 0.12743569910526276
2018-10-21 18:05:51.701892:	Training iteration: 296800, Loss: 0.17052379250526428
2018-10-21 18:06:21.308127:	Training iteration: 297000, Loss: 0.16851946711540222
2018-10-21 18:06:51.283970:	Training iteration: 297200, Loss: 0.20610311627388
2018-10-21 18:07:21.190907:	Training iteration: 297400, Loss: 0.1704331636428833
2018-10-21 18:07:50.904016:	Training iteration: 297600, Loss: 0.15516512095928192
2018-10-21 18:08:20.673527:	Training iteration: 297800, Loss: 0.17011895775794983
2018-10-21 18:08:50.695959:	Training iteration: 298000, Loss: 0.16846048831939697
2018-10-21 18:09:20.272165:	Training iteration: 298200, Loss: 0.1627841591835022
2018-10-21 18:09:49.618269:	Training iteration: 298400, Loss: 0.17747914791107178
2018-10-21 18:10:19.441309:	Training iteration: 298600, Loss: 0.16685879230499268
2018-10-21 18:10:49.271115:	Training iteration: 298800, Loss: 0.14278832077980042
2018-10-21 18:11:18.313061:	Training iteration: 299000, Loss: 0.11680321395397186
2018-10-21 18:11:47.885651:	Training iteration: 299200, Loss: 0.16958531737327576
2018-10-21 18:12:18.643841:	Training iteration: 299400, Loss: 0.1664571911096573
2018-10-21 18:12:48.104327:	Training iteration: 299600, Loss: 0.13674205541610718
2018-10-21 18:13:18.346245:	Training iteration: 299800, Loss: 0.19303204119205475
2018-10-21 18:13:48.752983:	Training iteration: 300000, Loss: 0.13366065919399261
2018-10-21 18:14:18.892437:	Training iteration: 300200, Loss: 0.17424511909484863
2018-10-21 18:14:48.323304:	Training iteration: 300400, Loss: 0.19443035125732422
2018-10-21 18:15:17.963395:	Training iteration: 300600, Loss: 0.16816386580467224
2018-10-21 18:15:48.393467:	Training iteration: 300800, Loss: 0.1414879560470581
2018-10-21 18:16:17.548733:	Training iteration: 301000, Loss: 0.15070568025112152
2018-10-21 18:16:47.434907:	Training iteration: 301200, Loss: 0.13988642394542694
2018-10-21 18:17:16.466155:	Training iteration: 301400, Loss: 0.1354643553495407
2018-10-21 18:17:45.033255:	Training iteration: 301600, Loss: 0.17787545919418335
2018-10-21 18:18:14.039038:	Training iteration: 301800, Loss: 0.21303510665893555
2018-10-21 18:18:43.545212:	Training iteration: 302000, Loss: 0.13560092449188232
2018-10-21 18:19:13.682659:	Training iteration: 302200, Loss: 0.25025084614753723
2018-10-21 18:19:42.672390:	Training iteration: 302400, Loss: 0.1708780974149704
2018-10-21 18:20:11.604799:	Training iteration: 302600, Loss: 0.2125144898891449
2018-10-21 18:20:41.301735:	Training iteration: 302800, Loss: 0.14866916835308075
2018-10-21 18:21:10.616715:	Training iteration: 303000, Loss: 0.16906635463237762
2018-10-21 18:21:40.259515:	Training iteration: 303200, Loss: 0.13531529903411865
2018-10-21 18:22:10.023959:	Training iteration: 303400, Loss: 0.19612416625022888
2018-10-21 18:22:40.220863:	Training iteration: 303600, Loss: 0.18164971470832825
2018-10-21 18:23:09.418808:	Training iteration: 303800, Loss: 0.13142234086990356
2018-10-21 18:23:38.315362:	Training iteration: 304000, Loss: 0.14831507205963135
2018-10-21 18:24:08.622659:	Training iteration: 304200, Loss: 0.13816139101982117
2018-10-21 18:24:38.865495:	Training iteration: 304400, Loss: 0.2168642282485962
2018-10-21 18:25:08.993337:	Training iteration: 304600, Loss: 0.17606008052825928
2018-10-21 18:25:38.091940:	Training iteration: 304800, Loss: 0.15975463390350342
2018-10-21 18:26:07.286663:	Training iteration: 305000, Loss: 0.18464088439941406
2018-10-21 18:26:36.284269:	Training iteration: 305200, Loss: 0.16852226853370667
2018-10-21 18:27:06.593502:	Training iteration: 305400, Loss: 0.1270008087158203
2018-10-21 18:27:36.717692:	Training iteration: 305600, Loss: 0.15458564460277557
2018-10-21 18:28:06.914015:	Training iteration: 305800, Loss: 0.12296721339225769
2018-10-21 18:28:36.269953:	Training iteration: 306000, Loss: 0.12846434116363525
2018-10-21 18:29:06.530288:	Training iteration: 306200, Loss: 0.1516091525554657
2018-10-21 18:29:36.886194:	Training iteration: 306400, Loss: 0.13376420736312866
2018-10-21 18:30:06.371580:	Training iteration: 306600, Loss: 0.18527933955192566
2018-10-21 18:30:36.446339:	Training iteration: 306800, Loss: 0.18449294567108154
2018-10-21 18:31:05.909744:	Training iteration: 307000, Loss: 0.2868672013282776
2018-10-21 18:31:35.307445:	Training iteration: 307200, Loss: 0.15361559391021729
2018-10-21 18:32:04.836232:	Training iteration: 307400, Loss: 0.1872563660144806
2018-10-21 18:32:34.339236:	Training iteration: 307600, Loss: 0.1869489699602127
2018-10-21 18:33:03.518913:	Training iteration: 307800, Loss: 0.18683481216430664
2018-10-21 18:33:33.353449:	Training iteration: 308000, Loss: 0.1198546290397644
2018-10-21 18:34:02.776430:	Training iteration: 308200, Loss: 0.19126784801483154
2018-10-21 18:34:32.169371:	Training iteration: 308400, Loss: 0.17279092967510223
2018-10-21 18:35:01.490063:	Training iteration: 308600, Loss: 0.1728055328130722
2018-10-21 18:35:31.055042:	Training iteration: 308800, Loss: 0.1487421989440918
2018-10-21 18:36:00.289683:	Training iteration: 309000, Loss: 0.17710146307945251
2018-10-21 18:36:30.390410:	Training iteration: 309200, Loss: 0.17544788122177124
2018-10-21 18:37:00.878351:	Training iteration: 309400, Loss: 0.1649046242237091
2018-10-21 18:37:30.309387:	Training iteration: 309600, Loss: 0.10331916809082031
2018-10-21 18:37:59.946781:	Training iteration: 309800, Loss: 0.13666224479675293
2018-10-21 18:38:30.156801:	Training iteration: 310000, Loss: 0.17125163972377777
2018-10-21 18:38:59.884152:	Training iteration: 310200, Loss: 0.20950302481651306
2018-10-21 18:39:29.754251:	Training iteration: 310400, Loss: 0.15208575129508972
2018-10-21 18:39:59.516295:	Training iteration: 310600, Loss: 0.134657621383667
2018-10-21 18:40:29.141590:	Training iteration: 310800, Loss: 0.16300733387470245
2018-10-21 18:40:58.485272:	Training iteration: 311000, Loss: 0.17477500438690186
2018-10-21 18:41:27.809718:	Training iteration: 311200, Loss: 0.203187957406044
2018-10-21 18:41:56.824243:	Training iteration: 311400, Loss: 0.16638705134391785
2018-10-21 18:42:25.829134:	Training iteration: 311600, Loss: 0.1983184814453125
2018-10-21 18:42:54.727357:	Training iteration: 311800, Loss: 0.20410498976707458
2018-10-21 18:43:25.475576:	Training iteration: 312000, Loss: 0.13684353232383728
2018-10-21 18:43:54.498681:	Training iteration: 312200, Loss: 0.1748596429824829
2018-10-21 18:44:24.776411:	Training iteration: 312400, Loss: 0.13844823837280273
2018-10-21 18:44:54.514173:	Training iteration: 312600, Loss: 0.16632774472236633
2018-10-21 18:45:24.268553:	Training iteration: 312800, Loss: 0.15698137879371643
2018-10-21 18:45:54.538446:	Training iteration: 313000, Loss: 0.16703246533870697
2018-10-21 18:46:24.076048:	Training iteration: 313200, Loss: 0.15738311409950256
2018-10-21 18:46:53.688193:	Training iteration: 313400, Loss: 0.20171934366226196
2018-10-21 18:47:23.071592:	Training iteration: 313600, Loss: 0.13281410932540894
2018-10-21 18:47:52.508702:	Training iteration: 313800, Loss: 0.16846993565559387
2018-10-21 18:48:22.179443:	Training iteration: 314000, Loss: 0.1359713077545166
2018-10-21 18:48:51.115000:	Training iteration: 314200, Loss: 0.17753130197525024
2018-10-21 18:49:20.535960:	Training iteration: 314400, Loss: 0.12926490604877472
2018-10-21 18:49:50.233608:	Training iteration: 314600, Loss: 0.16559261083602905
2018-10-21 18:50:19.546855:	Training iteration: 314800, Loss: 0.15615442395210266
2018-10-21 18:50:49.583784:	Training iteration: 315000, Loss: 0.1412845104932785
2018-10-21 18:51:19.361577:	Training iteration: 315200, Loss: 0.1446288824081421
2018-10-21 18:51:48.844612:	Training iteration: 315400, Loss: 0.1743357926607132
2018-10-21 18:52:18.721751:	Training iteration: 315600, Loss: 0.1402674913406372
2018-10-21 18:52:48.663091:	Training iteration: 315800, Loss: 0.12688171863555908
2018-10-21 18:53:18.736217:	Training iteration: 316000, Loss: 0.17702513933181763
2018-10-21 18:53:47.922362:	Training iteration: 316200, Loss: 0.09957945346832275
2018-10-21 18:54:18.282742:	Training iteration: 316400, Loss: 0.19168968498706818
2018-10-21 18:54:47.772635:	Training iteration: 316600, Loss: 0.12041693180799484
2018-10-21 18:55:17.604298:	Training iteration: 316800, Loss: 0.13306652009487152
2018-10-21 18:55:47.955561:	Training iteration: 317000, Loss: 0.15153813362121582
2018-10-21 18:56:17.063310:	Training iteration: 317200, Loss: 0.17663508653640747
2018-10-21 18:56:46.499133:	Training iteration: 317400, Loss: 0.11158424615859985
2018-10-21 18:57:16.278605:	Training iteration: 317600, Loss: 0.1560009866952896
2018-10-21 18:57:45.724680:	Training iteration: 317800, Loss: 0.13130712509155273
2018-10-21 18:58:16.119613:	Training iteration: 318000, Loss: 0.14849445223808289
2018-10-21 18:58:46.816304:	Training iteration: 318200, Loss: 0.16732797026634216
2018-10-21 18:59:16.350594:	Training iteration: 318400, Loss: 0.13481789827346802
2018-10-21 18:59:46.191583:	Training iteration: 318600, Loss: 0.11078445613384247
2018-10-21 19:00:15.273294:	Training iteration: 318800, Loss: 0.15868955850601196
2018-10-21 19:00:44.501025:	Training iteration: 319000, Loss: 0.12611941993236542
2018-10-21 19:01:15.210975:	Training iteration: 319200, Loss: 0.1566189080476761
2018-10-21 19:01:44.355562:	Training iteration: 319400, Loss: 0.16286975145339966
2018-10-21 19:02:13.786630:	Training iteration: 319600, Loss: 0.1476474404335022
2018-10-21 19:02:43.493402:	Training iteration: 319800, Loss: 0.1678120195865631
2018-10-21 19:03:13.672673:	Training iteration: 320000, Loss: 0.17431160807609558
2018-10-21 19:03:42.969240:	Training iteration: 320200, Loss: 0.1381254494190216
2018-10-21 19:04:13.657309:	Training iteration: 320400, Loss: 0.17287424206733704
2018-10-21 19:04:42.528392:	Training iteration: 320600, Loss: 0.15785527229309082
2018-10-21 19:05:12.020168:	Training iteration: 320800, Loss: 0.21333812177181244
2018-10-21 19:05:41.639690:	Training iteration: 321000, Loss: 0.13435393571853638
2018-10-21 19:06:11.682740:	Training iteration: 321200, Loss: 0.11716640740633011
2018-10-21 19:06:42.435707:	Training iteration: 321400, Loss: 0.14432713389396667
2018-10-21 19:07:11.499386:	Training iteration: 321600, Loss: 0.16242502629756927
2018-10-21 19:07:41.162136:	Training iteration: 321800, Loss: 0.15356986224651337
2018-10-21 19:08:11.860543:	Training iteration: 322000, Loss: 0.16319377720355988
2018-10-21 19:08:41.502228:	Training iteration: 322200, Loss: 0.16681940853595734
2018-10-21 19:09:10.577907:	Training iteration: 322400, Loss: 0.18773718178272247
2018-10-21 19:09:39.311792:	Training iteration: 322600, Loss: 0.13581842184066772
2018-10-21 19:10:08.722450:	Training iteration: 322800, Loss: 0.1622437685728073
2018-10-21 19:10:39.115427:	Training iteration: 323000, Loss: 0.16780126094818115
2018-10-21 19:11:09.551077:	Training iteration: 323200, Loss: 0.1899745762348175
2018-10-21 19:11:39.235471:	Training iteration: 323400, Loss: 0.1751534342765808
2018-10-21 19:12:08.817520:	Training iteration: 323600, Loss: 0.15304115414619446
2018-10-21 19:12:38.611417:	Training iteration: 323800, Loss: 0.19258394837379456
2018-10-21 19:13:07.783988:	Training iteration: 324000, Loss: 0.20118199288845062
2018-10-21 19:13:37.510766:	Training iteration: 324200, Loss: 0.152258038520813
2018-10-21 19:14:08.147746:	Training iteration: 324400, Loss: 0.24425607919692993
2018-10-21 19:14:37.104492:	Training iteration: 324600, Loss: 0.1942259669303894
2018-10-21 19:15:06.747493:	Training iteration: 324800, Loss: 0.14823663234710693
2018-10-21 19:15:36.689596:	Training iteration: 325000, Loss: 0.1470283567905426
2018-10-21 19:16:05.946517:	Training iteration: 325200, Loss: 0.15892235934734344
2018-10-21 19:16:35.413831:	Training iteration: 325400, Loss: 0.12671753764152527
2018-10-21 19:17:04.381902:	Training iteration: 325600, Loss: 0.2775363624095917
2018-10-21 19:17:34.073440:	Training iteration: 325800, Loss: 0.16573935747146606
2018-10-21 19:18:03.632271:	Training iteration: 326000, Loss: 0.18998205661773682
2018-10-21 19:18:33.141804:	Training iteration: 326200, Loss: 0.13464275002479553
2018-10-21 19:19:02.303290:	Training iteration: 326400, Loss: 0.16507266461849213
2018-10-21 19:19:32.473861:	Training iteration: 326600, Loss: 0.16556286811828613
2018-10-21 19:20:01.815684:	Training iteration: 326800, Loss: 0.181477889418602
2018-10-21 19:20:31.083944:	Training iteration: 327000, Loss: 0.14554709196090698
2018-10-21 19:21:00.991481:	Training iteration: 327200, Loss: 0.16054241359233856
2018-10-21 19:21:30.606086:	Training iteration: 327400, Loss: 0.2290026694536209
2018-10-21 19:22:00.164050:	Training iteration: 327600, Loss: 0.1568593829870224
2018-10-21 19:22:29.187136:	Training iteration: 327800, Loss: 0.1352124810218811
2018-10-21 19:22:59.491803:	Training iteration: 328000, Loss: 0.12212172150611877
2018-10-21 19:23:30.203028:	Training iteration: 328200, Loss: 0.148019477725029
2018-10-21 19:23:59.230627:	Training iteration: 328400, Loss: 0.16304248571395874
2018-10-21 19:24:28.112276:	Training iteration: 328600, Loss: 0.14385053515434265
2018-10-21 19:24:57.595471:	Training iteration: 328800, Loss: 0.168716162443161
2018-10-21 19:25:27.186244:	Training iteration: 329000, Loss: 0.16408061981201172
2018-10-21 19:25:55.934981:	Training iteration: 329200, Loss: 0.15440227091312408
2018-10-21 19:26:24.788379:	Training iteration: 329400, Loss: 0.13573580980300903
2018-10-21 19:26:53.860948:	Training iteration: 329600, Loss: 0.16954305768013
2018-10-21 19:27:23.171059:	Training iteration: 329800, Loss: 0.1676730215549469
2018-10-21 19:27:53.853778:	Training iteration: 330000, Loss: 0.16296255588531494
2018-10-21 19:28:23.530091:	Training iteration: 330200, Loss: 0.1715216040611267
2018-10-21 19:28:53.902001:	Training iteration: 330400, Loss: 0.24145041406154633
2018-10-21 19:29:23.280608:	Training iteration: 330600, Loss: 0.21373756229877472
2018-10-21 19:29:53.014084:	Training iteration: 330800, Loss: 0.10816244781017303
2018-10-21 19:30:22.528541:	Training iteration: 331000, Loss: 0.11912395060062408
2018-10-21 19:30:52.313906:	Training iteration: 331200, Loss: 0.1600314825773239
2018-10-21 19:31:21.853674:	Training iteration: 331400, Loss: 0.17007070779800415
2018-10-21 19:31:52.255063:	Training iteration: 331600, Loss: 0.1913345754146576
2018-10-21 19:32:21.850993:	Training iteration: 331800, Loss: 0.14687228202819824
2018-10-21 19:32:51.659262:	Training iteration: 332000, Loss: 0.1324358582496643
2018-10-21 19:33:20.853550:	Training iteration: 332200, Loss: 0.1573491394519806
2018-10-21 19:33:49.996360:	Training iteration: 332400, Loss: 0.12281157076358795
2018-10-21 19:34:20.238625:	Training iteration: 332600, Loss: 0.14709284901618958
2018-10-21 19:34:50.474358:	Training iteration: 332800, Loss: 0.12858569622039795
2018-10-21 19:35:20.315283:	Training iteration: 333000, Loss: 0.1706293225288391
2018-10-21 19:35:49.837087:	Training iteration: 333200, Loss: 0.1437024474143982
2018-10-21 19:36:19.892868:	Training iteration: 333400, Loss: 0.1818685084581375
2018-10-21 19:36:48.911452:	Training iteration: 333600, Loss: 0.11302074790000916
2018-10-21 19:37:19.089928:	Training iteration: 333800, Loss: 0.34647881984710693
2018-10-21 19:37:48.625701:	Training iteration: 334000, Loss: 0.11071433126926422
2018-10-21 19:38:18.226735:	Training iteration: 334200, Loss: 0.11345928907394409
2018-10-21 19:38:48.458550:	Training iteration: 334400, Loss: 0.20335963368415833
2018-10-21 19:39:18.115411:	Training iteration: 334600, Loss: 0.16741949319839478
2018-10-21 19:39:47.881903:	Training iteration: 334800, Loss: 0.1434377133846283
2018-10-21 19:40:18.089439:	Training iteration: 335000, Loss: 0.16485494375228882
2018-10-21 19:40:47.500534:	Training iteration: 335200, Loss: 0.14850713312625885
2018-10-21 19:41:17.069478:	Training iteration: 335400, Loss: 0.17933842539787292
2018-10-21 19:41:45.996560:	Training iteration: 335600, Loss: 0.16094747185707092
2018-10-21 19:42:14.738323:	Training iteration: 335800, Loss: 0.1581903100013733
2018-10-21 19:42:48.569186:	Training iteration: 336000, Loss: 0.18745863437652588
2018-10-21 19:43:18.154531:	Training iteration: 336200, Loss: 0.14569397270679474
2018-10-21 19:43:47.515972:	Training iteration: 336400, Loss: 0.16279934346675873
2018-10-21 19:44:17.145348:	Training iteration: 336600, Loss: 0.15244263410568237
2018-10-21 19:44:46.628743:	Training iteration: 336800, Loss: 0.16062234342098236
2018-10-21 19:45:16.001252:	Training iteration: 337000, Loss: 0.13401636481285095
2018-10-21 19:45:45.707282:	Training iteration: 337200, Loss: 0.13764536380767822
2018-10-21 19:46:15.736813:	Training iteration: 337400, Loss: 0.127704918384552
2018-10-21 19:46:45.815364:	Training iteration: 337600, Loss: 0.15933063626289368
2018-10-21 19:47:15.143991:	Training iteration: 337800, Loss: 0.10971281677484512
2018-10-21 19:47:44.428541:	Training iteration: 338000, Loss: 0.11104731261730194
2018-10-21 19:48:14.049655:	Training iteration: 338200, Loss: 0.13405662775039673
2018-10-21 19:48:43.847876:	Training iteration: 338400, Loss: 0.1675911545753479
2018-10-21 19:49:13.774500:	Training iteration: 338600, Loss: 0.1655181646347046
2018-10-21 19:49:43.464062:	Training iteration: 338800, Loss: 0.13084015250205994
2018-10-21 19:50:11.796615:	Training iteration: 339000, Loss: 0.11158344149589539
2018-10-21 19:50:41.160087:	Training iteration: 339200, Loss: 0.138096421957016
2018-10-21 19:51:10.389453:	Training iteration: 339400, Loss: 0.1459517478942871
2018-10-21 19:51:39.859904:	Training iteration: 339600, Loss: 0.1506187915802002
2018-10-21 19:52:10.303577:	Training iteration: 339800, Loss: 0.16072070598602295
2018-10-21 19:52:39.686826:	Training iteration: 340000, Loss: 0.1362682580947876
2018-10-21 19:53:09.075533:	Training iteration: 340200, Loss: 0.21652942895889282
2018-10-21 19:53:38.833775:	Training iteration: 340400, Loss: 0.15750834345817566
2018-10-21 19:54:08.594809:	Training iteration: 340600, Loss: 0.13387887179851532
2018-10-21 19:54:38.520660:	Training iteration: 340800, Loss: 0.1188805103302002
2018-10-21 19:55:08.202674:	Training iteration: 341000, Loss: 0.11799365282058716
2018-10-21 19:55:38.217444:	Training iteration: 341200, Loss: 0.142814502120018
2018-10-21 19:56:06.851315:	Training iteration: 341400, Loss: 0.1845160573720932
2018-10-21 19:56:36.349330:	Training iteration: 341600, Loss: 0.12379707396030426
2018-10-21 19:57:06.354900:	Training iteration: 341800, Loss: 0.12419380247592926
2018-10-21 19:57:36.083447:	Training iteration: 342000, Loss: 0.21982406079769135
2018-10-21 19:58:05.345346:	Training iteration: 342200, Loss: 0.155981183052063
2018-10-21 19:58:34.686465:	Training iteration: 342400, Loss: 0.27037179470062256
2018-10-21 19:59:04.422950:	Training iteration: 342600, Loss: 0.13771727681159973
2018-10-21 19:59:34.202652:	Training iteration: 342800, Loss: 0.1552700698375702
2018-10-21 20:00:04.131219:	Training iteration: 343000, Loss: 0.13476622104644775
2018-10-21 20:00:34.103807:	Training iteration: 343200, Loss: 0.15785959362983704
2018-10-21 20:01:03.891313:	Training iteration: 343400, Loss: 0.2158447802066803
2018-10-21 20:01:32.965914:	Training iteration: 343600, Loss: 0.15116675198078156
2018-10-21 20:02:03.847515:	Training iteration: 343800, Loss: 0.13160490989685059
2018-10-21 20:02:33.551292:	Training iteration: 344000, Loss: 0.16260987520217896
2018-10-21 20:03:03.176243:	Training iteration: 344200, Loss: 0.16611842811107635
2018-10-21 20:03:32.307164:	Training iteration: 344400, Loss: 0.16591140627861023
2018-10-21 20:04:02.170543:	Training iteration: 344600, Loss: 0.12057515978813171
2018-10-21 20:04:32.224124:	Training iteration: 344800, Loss: 0.11441489309072495
2018-10-21 20:05:01.880219:	Training iteration: 345000, Loss: 0.13582703471183777
2018-10-21 20:05:31.999293:	Training iteration: 345200, Loss: 0.14161156117916107
2018-10-21 20:06:01.669705:	Training iteration: 345400, Loss: 0.10940688848495483
2018-10-21 20:06:31.417207:	Training iteration: 345600, Loss: 0.16819538176059723
2018-10-21 20:07:01.397100:	Training iteration: 345800, Loss: 0.18095722794532776
2018-10-21 20:07:30.291890:	Training iteration: 346000, Loss: 0.16758935153484344
2018-10-21 20:07:59.995536:	Training iteration: 346200, Loss: 0.2267349660396576
2018-10-21 20:08:29.341826:	Training iteration: 346400, Loss: 0.15321454405784607
2018-10-21 20:08:59.250258:	Training iteration: 346600, Loss: 0.14906622469425201
2018-10-21 20:09:28.180602:	Training iteration: 346800, Loss: 0.17096959054470062
2018-10-21 20:09:57.475486:	Training iteration: 347000, Loss: 0.18431976437568665
2018-10-21 20:10:28.252653:	Training iteration: 347200, Loss: 0.15754207968711853
2018-10-21 20:10:57.947340:	Training iteration: 347400, Loss: 0.19478808343410492
2018-10-21 20:11:27.931264:	Training iteration: 347600, Loss: 0.15659335255622864
2018-10-21 20:11:57.679716:	Training iteration: 347800, Loss: 0.13186830282211304
2018-10-21 20:12:27.100150:	Training iteration: 348000, Loss: 0.08094489574432373
2018-10-21 20:12:57.544452:	Training iteration: 348200, Loss: 0.16835446655750275
2018-10-21 20:13:26.982200:	Training iteration: 348400, Loss: 0.16310946643352509
2018-10-21 20:13:56.527564:	Training iteration: 348600, Loss: 0.19914576411247253
2018-10-21 20:14:26.131794:	Training iteration: 348800, Loss: 0.27402713894844055
2018-10-21 20:14:55.933587:	Training iteration: 349000, Loss: 0.14234715700149536
2018-10-21 20:15:24.756100:	Training iteration: 349200, Loss: 0.17293474078178406
2018-10-21 20:15:54.711991:	Training iteration: 349400, Loss: 0.16273394227027893
2018-10-21 20:16:25.415810:	Training iteration: 349600, Loss: 0.1582777053117752
2018-10-21 20:16:54.973046:	Training iteration: 349800, Loss: 0.16028717160224915
2018-10-21 20:17:24.705573:	Training iteration: 350000, Loss: 0.18533141911029816
2018-10-21 20:17:54.651515:	Training iteration: 350200, Loss: 0.16685715317726135
2018-10-21 20:18:23.564329:	Training iteration: 350400, Loss: 0.14329761266708374
2018-10-21 20:18:53.618581:	Training iteration: 350600, Loss: 0.12455427646636963
2018-10-21 20:19:23.660281:	Training iteration: 350800, Loss: 0.10872235894203186
2018-10-21 20:19:53.634885:	Training iteration: 351000, Loss: 0.14569830894470215
2018-10-21 20:20:23.317176:	Training iteration: 351200, Loss: 0.18041254580020905
2018-10-21 20:20:52.416555:	Training iteration: 351400, Loss: 0.16306355595588684
2018-10-21 20:21:22.313475:	Training iteration: 351600, Loss: 0.12088675796985626
2018-10-21 20:21:51.856185:	Training iteration: 351800, Loss: 0.12646286189556122
2018-10-21 20:22:21.438964:	Training iteration: 352000, Loss: 0.17305347323417664
2018-10-21 20:22:51.837256:	Training iteration: 352200, Loss: 0.11021973192691803
2018-10-21 20:23:21.455556:	Training iteration: 352400, Loss: 0.13158965110778809
2018-10-21 20:23:50.964046:	Training iteration: 352600, Loss: 0.16170133650302887
2018-10-21 20:24:20.815678:	Training iteration: 352800, Loss: 0.1329071819782257
2018-10-21 20:24:50.328339:	Training iteration: 353000, Loss: 0.18899337947368622
2018-10-21 20:25:19.226584:	Training iteration: 353200, Loss: 0.165012925863266
2018-10-21 20:25:48.809803:	Training iteration: 353400, Loss: 0.16103166341781616
2018-10-21 20:26:18.047525:	Training iteration: 353600, Loss: 0.15234246850013733
2018-10-21 20:26:47.839638:	Training iteration: 353800, Loss: 0.18137897551059723
2018-10-21 20:27:17.981278:	Training iteration: 354000, Loss: 0.18041104078292847
2018-10-21 20:27:47.685174:	Training iteration: 354200, Loss: 0.13931849598884583
2018-10-21 20:28:18.491599:	Training iteration: 354400, Loss: 0.13365581631660461
2018-10-21 20:28:48.026136:	Training iteration: 354600, Loss: 0.13271474838256836
2018-10-21 20:29:17.492779:	Training iteration: 354800, Loss: 0.14041131734848022
2018-10-21 20:29:46.621221:	Training iteration: 355000, Loss: 0.19486816227436066
2018-10-21 20:30:16.072931:	Training iteration: 355200, Loss: 0.10717009007930756
2018-10-21 20:30:45.049134:	Training iteration: 355400, Loss: 0.20723460614681244
2018-10-21 20:31:14.813329:	Training iteration: 355600, Loss: 0.23546087741851807
2018-10-21 20:31:43.806730:	Training iteration: 355800, Loss: 0.09394294023513794
2018-10-21 20:32:12.868432:	Training iteration: 356000, Loss: 0.2189098447561264
2018-10-21 20:32:42.938398:	Training iteration: 356200, Loss: 0.20609840750694275
2018-10-21 20:33:12.816014:	Training iteration: 356400, Loss: 0.2730514407157898
2018-10-21 20:33:43.517382:	Training iteration: 356600, Loss: 0.16269530355930328
2018-10-21 20:34:12.615047:	Training iteration: 356800, Loss: 0.1270783245563507
2018-10-21 20:34:42.397433:	Training iteration: 357000, Loss: 0.1498802900314331
2018-10-21 20:35:12.684341:	Training iteration: 357200, Loss: 0.1361044943332672
2018-10-21 20:35:42.466446:	Training iteration: 357400, Loss: 0.11845986545085907
2018-10-21 20:36:12.953164:	Training iteration: 357600, Loss: 0.13330715894699097
2018-10-21 20:36:41.765880:	Training iteration: 357800, Loss: 0.09508053213357925
2018-10-21 20:37:10.584642:	Training iteration: 358000, Loss: 0.1338878720998764
2018-10-21 20:37:40.786781:	Training iteration: 358200, Loss: 0.148273766040802
2018-10-21 20:38:10.574317:	Training iteration: 358400, Loss: 0.19868533313274384
2018-10-21 20:38:40.569353:	Training iteration: 358600, Loss: 0.1431805044412613
2018-10-21 20:39:09.616984:	Training iteration: 358800, Loss: 0.1784476935863495
2018-10-21 20:39:39.825099:	Training iteration: 359000, Loss: 0.16644364595413208
2018-10-21 20:40:10.300718:	Training iteration: 359200, Loss: 0.24364721775054932
2018-10-21 20:40:39.725572:	Training iteration: 359400, Loss: 0.14707596600055695
2018-10-21 20:41:09.195198:	Training iteration: 359600, Loss: 0.08274909853935242
2018-10-21 20:41:39.491652:	Training iteration: 359800, Loss: 0.1730150580406189
2018-10-21 20:41:47.451191:	Epoch 2 finished after 359854 iterations.
No images to record
Validating
2018-10-21 20:41:48.139306:	Entering validation loop
2018-10-21 20:41:58.228255: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 652 of 1000
2018-10-21 20:42:03.868401: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 20:42:17.356547:	Validation iteration: 200, Loss: 0.1971469521522522
2018-10-21 20:42:30.874405:	Validation iteration: 400, Loss: 0.17398357391357422
2018-10-21 20:42:45.182078:	Validation iteration: 600, Loss: 0.3317491412162781
2018-10-21 20:42:59.710463:	Validation iteration: 800, Loss: 0.1539526879787445
2018-10-21 20:43:14.139212:	Validation iteration: 1000, Loss: 0.24393399059772491
2018-10-21 20:43:28.322171:	Validation iteration: 1200, Loss: 0.2406737208366394
2018-10-21 20:43:42.444662:	Validation iteration: 1400, Loss: 0.17797589302062988
2018-10-21 20:43:56.027042:	Validation iteration: 1600, Loss: 0.18267343938350677
2018-10-21 20:44:10.608207:	Validation iteration: 1800, Loss: 0.18847481906414032
2018-10-21 20:44:24.539764:	Validation iteration: 2000, Loss: 0.20015336573123932
2018-10-21 20:44:40.982492: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 692 of 1000
2018-10-21 20:44:45.341579: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 20:44:52.955889:	Validation iteration: 2200, Loss: 0.3084893524646759
2018-10-21 20:45:06.234990:	Validation iteration: 2400, Loss: 0.21441984176635742
2018-10-21 20:45:20.276311:	Validation iteration: 2600, Loss: 0.2372874617576599
2018-10-21 20:45:33.998044:	Validation iteration: 2800, Loss: 0.24357691407203674
2018-10-21 20:45:48.337892:	Validation iteration: 3000, Loss: 0.2508201003074646
2018-10-21 20:46:02.435905:	Validation iteration: 3200, Loss: 0.19425714015960693
2018-10-21 20:46:16.835882:	Validation iteration: 3400, Loss: 0.208640456199646
2018-10-21 20:46:32.436152:	Validation iteration: 3600, Loss: 0.2533791959285736
2018-10-21 20:46:46.464871:	Validation iteration: 3800, Loss: 0.17097139358520508
2018-10-21 20:47:00.750115:	Validation iteration: 4000, Loss: 0.2230452299118042
2018-10-21 20:47:23.692926: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 653 of 1000
2018-10-21 20:47:28.352668: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 20:47:29.903695:	Validation iteration: 4200, Loss: 0.19977635145187378
2018-10-21 20:47:43.079392:	Validation iteration: 4400, Loss: 0.20055130124092102
2018-10-21 20:47:56.885544:	Validation iteration: 4600, Loss: 0.17661334574222565
2018-10-21 20:48:11.054601:	Validation iteration: 4800, Loss: 0.13132795691490173
2018-10-21 20:48:25.138907:	Validation iteration: 5000, Loss: 0.1791571080684662
2018-10-21 20:48:39.275698:	Validation iteration: 5200, Loss: 0.17859295010566711
2018-10-21 20:48:53.509446:	Validation iteration: 5400, Loss: 0.205905020236969
2018-10-21 20:49:07.813718:	Validation iteration: 5600, Loss: 0.24226753413677216
2018-10-21 20:49:21.725127:	Validation iteration: 5800, Loss: 0.3143782615661621
2018-10-21 20:49:35.868998:	Validation iteration: 6000, Loss: 0.20636527240276337
2018-10-21 20:49:50.061325:	Validation iteration: 6200, Loss: 0.17830580472946167
2018-10-21 20:50:04.389796: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 672 of 1000
2018-10-21 20:50:08.912343: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 20:50:18.170544:	Validation iteration: 6400, Loss: 0.18841400742530823
2018-10-21 20:50:31.784211:	Validation iteration: 6600, Loss: 0.18037483096122742
2018-10-21 20:50:45.837762:	Validation iteration: 6800, Loss: 0.29811620712280273
2018-10-21 20:51:00.108186:	Validation iteration: 7000, Loss: 0.25473934412002563
2018-10-21 20:51:14.331530:	Validation iteration: 7200, Loss: 0.20451804995536804
2018-10-21 20:51:28.741277:	Validation iteration: 7400, Loss: 0.19711828231811523
2018-10-21 20:51:43.497797:	Validation iteration: 7600, Loss: 0.23268406093120575
2018-10-21 20:51:58.061581:	Validation iteration: 7800, Loss: 0.27598103880882263
2018-10-21 20:52:12.370312:	Validation iteration: 8000, Loss: 0.18866220116615295
2018-10-21 20:52:26.285843:	Validation iteration: 8200, Loss: 0.2627311050891876
2018-10-21 20:52:47.540354: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 651 of 1000
2018-10-21 20:52:52.437469: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 20:52:55.678812:	Validation iteration: 8400, Loss: 0.17005306482315063
2018-10-21 20:53:08.961702:	Validation iteration: 8600, Loss: 0.16465061902999878
2018-10-21 20:53:22.367334:	Validation iteration: 8800, Loss: 0.1606246829032898
2018-10-21 20:53:35.948461:	Validation iteration: 9000, Loss: 0.19536921381950378
2018-10-21 20:53:49.836360:	Validation iteration: 9200, Loss: 0.18141230940818787
2018-10-21 20:54:04.540344:	Validation iteration: 9400, Loss: 0.13099455833435059
2018-10-21 20:54:18.304884:	Validation iteration: 9600, Loss: 0.1822957843542099
2018-10-21 20:54:32.656780:	Validation iteration: 9800, Loss: 0.15459933876991272
2018-10-21 20:54:46.705162:	Validation iteration: 10000, Loss: 0.17731058597564697
2018-10-21 20:55:01.161731:	Validation iteration: 10200, Loss: 0.14219914376735687
2018-10-21 20:55:15.449154:	Validation iteration: 10400, Loss: 0.1751956045627594
2018-10-21 20:55:30.222955:	Validation iteration: 10600, Loss: 0.3694068491458893
2018-10-21 20:55:44.189601:	Validation iteration: 10800, Loss: 0.1911955326795578
2018-10-21 20:55:58.789638:	Validation iteration: 11000, Loss: 0.21884498000144958
2018-10-21 20:56:13.315794:	Validation iteration: 11200, Loss: 0.15548530220985413
Validation check mean loss: 0.19824018597930124
Validation loss has worsened. worse_val_checks = 2
Checkpoint
2018-10-21 20:57:40.436936: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 773 of 1000
2018-10-21 20:57:43.281933: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 20:58:02.026853:	Training iteration: 360000, Loss: 0.09775002300739288
2018-10-21 20:58:27.656708:	Training iteration: 360200, Loss: 0.1396877020597458
2018-10-21 20:58:54.712051:	Training iteration: 360400, Loss: 0.15346278250217438
2018-10-21 20:59:22.823764:	Training iteration: 360600, Loss: 0.16414225101470947
2018-10-21 20:59:52.118507:	Training iteration: 360800, Loss: 0.13404500484466553
2018-10-21 21:00:21.374742:	Training iteration: 361000, Loss: 0.12084752321243286
2018-10-21 21:00:51.009208:	Training iteration: 361200, Loss: 0.1078275740146637
2018-10-21 21:01:20.630072:	Training iteration: 361400, Loss: 0.12577062845230103
2018-10-21 21:01:49.426223:	Training iteration: 361600, Loss: 0.13512815535068512
2018-10-21 21:02:18.870822:	Training iteration: 361800, Loss: 0.1934073567390442
2018-10-21 21:02:47.663336:	Training iteration: 362000, Loss: 0.08657456189393997
2018-10-21 21:03:17.986383:	Training iteration: 362200, Loss: 0.10437731444835663
2018-10-21 21:03:48.021669:	Training iteration: 362400, Loss: 0.12503094971179962
2018-10-21 21:04:17.550873:	Training iteration: 362600, Loss: 0.0928778201341629
2018-10-21 21:04:47.088887:	Training iteration: 362800, Loss: 0.08552959561347961
2018-10-21 21:05:16.739651:	Training iteration: 363000, Loss: 0.08361741900444031
2018-10-21 21:05:45.752659:	Training iteration: 363200, Loss: 0.12684395909309387
2018-10-21 21:06:15.364527:	Training iteration: 363400, Loss: 0.12468604743480682
2018-10-21 21:06:43.806705:	Training iteration: 363600, Loss: 0.11637823283672333
2018-10-21 21:07:13.402668:	Training iteration: 363800, Loss: 0.11941905319690704
2018-10-21 21:07:43.171456:	Training iteration: 364000, Loss: 0.11762067675590515
2018-10-21 21:08:13.431311:	Training iteration: 364200, Loss: 0.09917744994163513
2018-10-21 21:08:43.095926:	Training iteration: 364400, Loss: 0.1407202035188675
2018-10-21 21:09:12.932315:	Training iteration: 364600, Loss: 0.14543668925762177
2018-10-21 21:09:42.657642:	Training iteration: 364800, Loss: 0.12458016723394394
2018-10-21 21:10:12.335014:	Training iteration: 365000, Loss: 0.17012013494968414
2018-10-21 21:10:42.473321:	Training iteration: 365200, Loss: 0.15432944893836975
2018-10-21 21:11:11.764095:	Training iteration: 365400, Loss: 0.10832107067108154
2018-10-21 21:11:41.721106:	Training iteration: 365600, Loss: 0.12090851366519928
2018-10-21 21:12:11.150711:	Training iteration: 365800, Loss: 0.06983008980751038
2018-10-21 21:12:40.579309:	Training iteration: 366000, Loss: 0.14706888794898987
2018-10-21 21:13:10.211549:	Training iteration: 366200, Loss: 0.15171724557876587
2018-10-21 21:13:40.161233:	Training iteration: 366400, Loss: 0.11522607505321503
2018-10-21 21:14:10.566958:	Training iteration: 366600, Loss: 0.10341794043779373
2018-10-21 21:14:41.352056:	Training iteration: 366800, Loss: 0.12570038437843323
2018-10-21 21:15:10.433929:	Training iteration: 367000, Loss: 0.14197778701782227
2018-10-21 21:15:40.468403:	Training iteration: 367200, Loss: 0.1362999528646469
2018-10-21 21:16:09.562684:	Training iteration: 367400, Loss: 0.12265892326831818
2018-10-21 21:16:39.787979:	Training iteration: 367600, Loss: 0.12876483798027039
2018-10-21 21:17:09.806320:	Training iteration: 367800, Loss: 0.10463826358318329
2018-10-21 21:17:40.007915:	Training iteration: 368000, Loss: 0.1800243854522705
2018-10-21 21:18:09.809806:	Training iteration: 368200, Loss: 0.16665174067020416
2018-10-21 21:18:40.983654:	Training iteration: 368400, Loss: 0.22525855898857117
2018-10-21 21:19:11.247568:	Training iteration: 368600, Loss: 0.1678878217935562
2018-10-21 21:19:40.358704:	Training iteration: 368800, Loss: 0.10276065766811371
2018-10-21 21:20:09.985032:	Training iteration: 369000, Loss: 0.1405007243156433
2018-10-21 21:20:40.585138:	Training iteration: 369200, Loss: 0.13627827167510986
2018-10-21 21:21:09.591799:	Training iteration: 369400, Loss: 0.12988048791885376
2018-10-21 21:21:39.209079:	Training iteration: 369600, Loss: 0.1287776678800583
2018-10-21 21:22:09.488430:	Training iteration: 369800, Loss: 0.12579570710659027
2018-10-21 21:22:40.021850:	Training iteration: 370000, Loss: 0.11363033205270767
2018-10-21 21:23:10.187493:	Training iteration: 370200, Loss: 0.07381220161914825
2018-10-21 21:23:39.797866:	Training iteration: 370400, Loss: 0.11597251892089844
2018-10-21 21:24:10.370620:	Training iteration: 370600, Loss: 0.11024287343025208
2018-10-21 21:24:39.605476:	Training iteration: 370800, Loss: 0.09735782444477081
2018-10-21 21:25:09.135546:	Training iteration: 371000, Loss: 0.08123171329498291
2018-10-21 21:25:39.194606:	Training iteration: 371200, Loss: 0.09407880157232285
2018-10-21 21:26:08.104181:	Training iteration: 371400, Loss: 0.15833112597465515
2018-10-21 21:26:38.120200:	Training iteration: 371600, Loss: 0.11054567992687225
2018-10-21 21:27:08.379746:	Training iteration: 371800, Loss: 0.12183969467878342
2018-10-21 21:27:37.864071:	Training iteration: 372000, Loss: 0.15435412526130676
2018-10-21 21:28:07.851955:	Training iteration: 372200, Loss: 0.15163642168045044
2018-10-21 21:28:24.988337: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 707 of 1000
2018-10-21 21:28:28.589916: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 21:28:50.603912:	Training iteration: 372400, Loss: 0.12385319173336029
2018-10-21 21:29:20.638841:	Training iteration: 372600, Loss: 0.07184770703315735
2018-10-21 21:29:50.276586:	Training iteration: 372800, Loss: 0.11492836475372314
2018-10-21 21:30:19.117796:	Training iteration: 373000, Loss: 0.15161435306072235
2018-10-21 21:30:48.778798:	Training iteration: 373200, Loss: 0.08342737704515457
2018-10-21 21:31:18.361119:	Training iteration: 373400, Loss: 0.12988781929016113
2018-10-21 21:31:47.778299:	Training iteration: 373600, Loss: 0.14599594473838806
2018-10-21 21:32:16.439489:	Training iteration: 373800, Loss: 0.1168641522526741
2018-10-21 21:32:46.412956:	Training iteration: 374000, Loss: 0.09693389385938644
2018-10-21 21:33:16.792455:	Training iteration: 374200, Loss: 0.11692412197589874
2018-10-21 21:33:46.187321:	Training iteration: 374400, Loss: 0.1411377191543579
2018-10-21 21:34:16.314125:	Training iteration: 374600, Loss: 0.09889313578605652
2018-10-21 21:34:46.556290:	Training iteration: 374800, Loss: 0.11992893368005753
2018-10-21 21:35:16.475530:	Training iteration: 375000, Loss: 0.10468596965074539
2018-10-21 21:35:46.425563:	Training iteration: 375200, Loss: 0.10914835333824158
2018-10-21 21:36:16.454285:	Training iteration: 375400, Loss: 0.14782759547233582
2018-10-21 21:36:45.508224:	Training iteration: 375600, Loss: 0.1571161448955536
2018-10-21 21:37:14.808221:	Training iteration: 375800, Loss: 0.07602760195732117
2018-10-21 21:37:45.404863:	Training iteration: 376000, Loss: 0.11243656277656555
2018-10-21 21:38:14.716479:	Training iteration: 376200, Loss: 0.11968572437763214
2018-10-21 21:38:44.322811:	Training iteration: 376400, Loss: 0.10617107897996902
2018-10-21 21:39:15.677095:	Training iteration: 376600, Loss: 0.08027878403663635
2018-10-21 21:39:45.440403:	Training iteration: 376800, Loss: 0.08888848125934601
2018-10-21 21:40:14.896453:	Training iteration: 377000, Loss: 0.0981525108218193
2018-10-21 21:40:44.481574:	Training iteration: 377200, Loss: 0.1638430505990982
2018-10-21 21:41:14.643579:	Training iteration: 377400, Loss: 0.1104927659034729
2018-10-21 21:41:44.465735:	Training iteration: 377600, Loss: 0.128746896982193
2018-10-21 21:42:14.243312:	Training iteration: 377800, Loss: 0.10147903859615326
2018-10-21 21:42:43.383336:	Training iteration: 378000, Loss: 0.14249318838119507
2018-10-21 21:43:12.943046:	Training iteration: 378200, Loss: 0.12696680426597595
2018-10-21 21:43:42.225801:	Training iteration: 378400, Loss: 0.12978069484233856
2018-10-21 21:44:11.555583:	Training iteration: 378600, Loss: 0.08248305320739746
2018-10-21 21:44:41.635798:	Training iteration: 378800, Loss: 0.08558608591556549
2018-10-21 21:45:11.564367:	Training iteration: 379000, Loss: 0.15456068515777588
2018-10-21 21:45:41.833449:	Training iteration: 379200, Loss: 0.14710727334022522
2018-10-21 21:46:13.103432:	Training iteration: 379400, Loss: 0.13579556345939636
2018-10-21 21:46:42.866755:	Training iteration: 379600, Loss: 0.1297517865896225
2018-10-21 21:47:12.973631:	Training iteration: 379800, Loss: 0.10954944789409637
2018-10-21 21:47:43.355086:	Training iteration: 380000, Loss: 0.10764328390359879
2018-10-21 21:48:13.105833:	Training iteration: 380200, Loss: 0.14023879170417786
2018-10-21 21:48:42.768785:	Training iteration: 380400, Loss: 0.0947268158197403
2018-10-21 21:49:12.124125:	Training iteration: 380600, Loss: 0.15459641814231873
2018-10-21 21:49:42.339725:	Training iteration: 380800, Loss: 0.11012406647205353
2018-10-21 21:50:13.374646:	Training iteration: 381000, Loss: 0.1785801649093628
2018-10-21 21:50:43.344377:	Training iteration: 381200, Loss: 0.13363751769065857
2018-10-21 21:51:13.497170:	Training iteration: 381400, Loss: 0.15461039543151855
2018-10-21 21:51:43.155975:	Training iteration: 381600, Loss: 0.10201643407344818
2018-10-21 21:52:12.096063:	Training iteration: 381800, Loss: 0.11072863638401031
2018-10-21 21:52:41.661050:	Training iteration: 382000, Loss: 0.11569811403751373
2018-10-21 21:53:10.686984:	Training iteration: 382200, Loss: 0.09366841614246368
2018-10-21 21:53:40.284175:	Training iteration: 382400, Loss: 0.08015227317810059
2018-10-21 21:54:09.468701:	Training iteration: 382600, Loss: 0.08970090001821518
2018-10-21 21:54:39.671621:	Training iteration: 382800, Loss: 0.10935395956039429
2018-10-21 21:55:09.372229:	Training iteration: 383000, Loss: 0.1330144703388214
2018-10-21 21:55:39.362602:	Training iteration: 383200, Loss: 0.09252624958753586
2018-10-21 21:56:09.285607:	Training iteration: 383400, Loss: 0.07372391223907471
2018-10-21 21:56:38.594202:	Training iteration: 383600, Loss: 0.14005908370018005
2018-10-21 21:57:07.602136:	Training iteration: 383800, Loss: 0.08091193437576294
2018-10-21 21:57:37.730431:	Training iteration: 384000, Loss: 0.11504542082548141
2018-10-21 21:58:07.049865:	Training iteration: 384200, Loss: 0.1205098032951355
2018-10-21 21:58:37.241991:	Training iteration: 384400, Loss: 0.10152905434370041
2018-10-21 21:59:07.709863:	Training iteration: 384600, Loss: 0.18203049898147583
2018-10-21 21:59:35.215184: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 740 of 1000
2018-10-21 21:59:38.323370: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 21:59:50.531602:	Training iteration: 384800, Loss: 0.10459375381469727
2018-10-21 22:00:19.323580:	Training iteration: 385000, Loss: 0.08737501502037048
2018-10-21 22:00:49.132605:	Training iteration: 385200, Loss: 0.0823146253824234
2018-10-21 22:01:23.266005:	Training iteration: 385400, Loss: 0.06794627010822296
2018-10-21 22:01:52.862961:	Training iteration: 385600, Loss: 0.09099331498146057
2018-10-21 22:02:22.274638:	Training iteration: 385800, Loss: 0.10135062038898468
2018-10-21 22:02:52.814570:	Training iteration: 386000, Loss: 0.14881424605846405
2018-10-21 22:03:22.342902:	Training iteration: 386200, Loss: 0.09291204810142517
2018-10-21 22:03:52.076525:	Training iteration: 386400, Loss: 0.09377898275852203
2018-10-21 22:04:21.694838:	Training iteration: 386600, Loss: 0.0662982165813446
2018-10-21 22:04:51.164448:	Training iteration: 386800, Loss: 0.10062818229198456
2018-10-21 22:05:19.657900:	Training iteration: 387000, Loss: 0.1042201817035675
2018-10-21 22:05:49.437619:	Training iteration: 387200, Loss: 0.09027042984962463
2018-10-21 22:06:19.307007:	Training iteration: 387400, Loss: 0.13493822515010834
2018-10-21 22:06:48.820739:	Training iteration: 387600, Loss: 0.07940194010734558
2018-10-21 22:07:19.375681:	Training iteration: 387800, Loss: 0.10138086974620819
2018-10-21 22:07:49.509033:	Training iteration: 388000, Loss: 0.08128899335861206
2018-10-21 22:08:19.318281:	Training iteration: 388200, Loss: 0.08794049918651581
2018-10-21 22:08:49.134285:	Training iteration: 388400, Loss: 0.14380785822868347
2018-10-21 22:09:18.372827:	Training iteration: 388600, Loss: 0.10038252919912338
2018-10-21 22:09:48.399862:	Training iteration: 388800, Loss: 0.1155182272195816
2018-10-21 22:10:17.516713:	Training iteration: 389000, Loss: 0.11099039018154144
2018-10-21 22:10:47.161825:	Training iteration: 389200, Loss: 0.11676158010959625
2018-10-21 22:11:17.996113:	Training iteration: 389400, Loss: 0.1292376071214676
2018-10-21 22:11:48.209278:	Training iteration: 389600, Loss: 0.07745520770549774
2018-10-21 22:12:17.838112:	Training iteration: 389800, Loss: 0.10647368431091309
2018-10-21 22:12:48.282738:	Training iteration: 390000, Loss: 0.08738788962364197
2018-10-21 22:13:18.549481:	Training iteration: 390200, Loss: 0.1530427634716034
2018-10-21 22:13:49.171454:	Training iteration: 390400, Loss: 0.13650941848754883
2018-10-21 22:14:18.451331:	Training iteration: 390600, Loss: 0.06314486265182495
2018-10-21 22:14:48.618322:	Training iteration: 390800, Loss: 0.07218581438064575
2018-10-21 22:15:17.943708:	Training iteration: 391000, Loss: 0.13560336828231812
2018-10-21 22:15:47.743375:	Training iteration: 391200, Loss: 0.08459660410881042
2018-10-21 22:16:17.453132:	Training iteration: 391400, Loss: 0.16065236926078796
2018-10-21 22:16:46.536219:	Training iteration: 391600, Loss: 0.057584017515182495
2018-10-21 22:17:15.751015:	Training iteration: 391800, Loss: 0.09367556869983673
2018-10-21 22:17:46.447107:	Training iteration: 392000, Loss: 0.10593584924936295
2018-10-21 22:18:16.568087:	Training iteration: 392200, Loss: 0.07912218570709229
2018-10-21 22:18:46.689447:	Training iteration: 392400, Loss: 0.07692022621631622
2018-10-21 22:19:16.027580:	Training iteration: 392600, Loss: 0.19256386160850525
2018-10-21 22:19:46.024475:	Training iteration: 392800, Loss: 0.09776827692985535
2018-10-21 22:20:15.876126:	Training iteration: 393000, Loss: 0.08847680687904358
2018-10-21 22:20:46.174392:	Training iteration: 393200, Loss: 0.08127942681312561
2018-10-21 22:21:17.678081:	Training iteration: 393400, Loss: 0.07176342606544495
2018-10-21 22:21:47.389227:	Training iteration: 393600, Loss: 0.08657538145780563
2018-10-21 22:22:16.905963:	Training iteration: 393800, Loss: 0.07508502900600433
2018-10-21 22:22:47.324833:	Training iteration: 394000, Loss: 0.058929089456796646
2018-10-21 22:23:17.774839:	Training iteration: 394200, Loss: 0.09963063895702362
2018-10-21 22:23:48.041523:	Training iteration: 394400, Loss: 0.1338963657617569
2018-10-21 22:24:17.743541:	Training iteration: 394600, Loss: 0.1256553828716278
2018-10-21 22:24:48.133433:	Training iteration: 394800, Loss: 0.11164671182632446
2018-10-21 22:25:17.916627:	Training iteration: 395000, Loss: 0.10967772454023361
2018-10-21 22:25:48.290313:	Training iteration: 395200, Loss: 0.0937155932188034
2018-10-21 22:26:16.938488:	Training iteration: 395400, Loss: 0.08498094230890274
2018-10-21 22:26:47.754967:	Training iteration: 395600, Loss: 0.10201482474803925
2018-10-21 22:27:17.926454:	Training iteration: 395800, Loss: 0.10920576006174088
2018-10-21 22:27:47.941216:	Training iteration: 396000, Loss: 0.08507595211267471
2018-10-21 22:28:17.356002:	Training iteration: 396200, Loss: 0.10208844393491745
2018-10-21 22:28:47.676190:	Training iteration: 396400, Loss: 0.13299241662025452
2018-10-21 22:29:17.776589:	Training iteration: 396600, Loss: 0.10703548789024353
2018-10-21 22:29:47.509753:	Training iteration: 396800, Loss: 0.06555137038230896
2018-10-21 22:30:17.906800:	Training iteration: 397000, Loss: 0.12263374030590057
2018-10-21 22:30:54.118033: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 717 of 1000
2018-10-21 22:30:57.459201: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 22:31:01.230861:	Training iteration: 397200, Loss: 0.13670000433921814
2018-10-21 22:31:30.030577:	Training iteration: 397400, Loss: 0.1387256681919098
2018-10-21 22:31:59.958053:	Training iteration: 397600, Loss: 0.0909648984670639
2018-10-21 22:32:29.051836:	Training iteration: 397800, Loss: 0.2195013463497162
2018-10-21 22:32:58.661766:	Training iteration: 398000, Loss: 0.08541395515203476
2018-10-21 22:33:28.437746:	Training iteration: 398200, Loss: 0.145035058259964
2018-10-21 22:33:58.536319:	Training iteration: 398400, Loss: 0.1773747354745865
2018-10-21 22:34:28.150583:	Training iteration: 398600, Loss: 0.10006693005561829
2018-10-21 22:34:57.975771:	Training iteration: 398800, Loss: 0.15408998727798462
2018-10-21 22:35:27.947416:	Training iteration: 399000, Loss: 0.1325344443321228
2018-10-21 22:35:57.977404:	Training iteration: 399200, Loss: 0.20981474220752716
2018-10-21 22:36:27.931332:	Training iteration: 399400, Loss: 0.22257578372955322
2018-10-21 22:36:58.357889:	Training iteration: 399600, Loss: 0.11329096555709839
2018-10-21 22:37:28.038810:	Training iteration: 399800, Loss: 0.1250459849834442
2018-10-21 22:37:57.703282:	Training iteration: 400000, Loss: 0.1816481351852417
2018-10-21 22:38:27.588664:	Training iteration: 400200, Loss: 0.13682103157043457
2018-10-21 22:38:57.296521:	Training iteration: 400400, Loss: 0.09562332183122635
2018-10-21 22:39:27.112435:	Training iteration: 400600, Loss: 0.13842429220676422
2018-10-21 22:39:56.586137:	Training iteration: 400800, Loss: 0.145573228597641
2018-10-21 22:40:26.937127:	Training iteration: 401000, Loss: 0.18729953467845917
2018-10-21 22:40:56.966336:	Training iteration: 401200, Loss: 0.20908239483833313
2018-10-21 22:41:26.677513:	Training iteration: 401400, Loss: 0.1774800419807434
2018-10-21 22:41:56.505371:	Training iteration: 401600, Loss: 0.18412646651268005
2018-10-21 22:42:26.316450:	Training iteration: 401800, Loss: 0.12221363186836243
2018-10-21 22:42:55.753762:	Training iteration: 402000, Loss: 0.1456824541091919
2018-10-21 22:43:25.079508:	Training iteration: 402200, Loss: 0.14381054043769836
2018-10-21 22:43:54.999710:	Training iteration: 402400, Loss: 0.18164432048797607
2018-10-21 22:44:23.975707:	Training iteration: 402600, Loss: 0.1499999761581421
2018-10-21 22:44:53.216316:	Training iteration: 402800, Loss: 0.08489854633808136
2018-10-21 22:45:23.997933:	Training iteration: 403000, Loss: 0.17718885838985443
2018-10-21 22:45:54.128921:	Training iteration: 403200, Loss: 0.1869792640209198
2018-10-21 22:46:24.026859:	Training iteration: 403400, Loss: 0.15083014965057373
2018-10-21 22:46:53.075600:	Training iteration: 403600, Loss: 0.13374000787734985
2018-10-21 22:47:22.477603:	Training iteration: 403800, Loss: 0.09149961918592453
2018-10-21 22:47:54.360707:	Training iteration: 404000, Loss: 0.07185487449169159
2018-10-21 22:48:24.183204:	Training iteration: 404200, Loss: 0.15627378225326538
2018-10-21 22:48:54.137218:	Training iteration: 404400, Loss: 0.2092171162366867
2018-10-21 22:49:23.458732:	Training iteration: 404600, Loss: 0.14390258491039276
2018-10-21 22:49:52.453418:	Training iteration: 404800, Loss: 0.12194820493459702
2018-10-21 22:50:22.919856:	Training iteration: 405000, Loss: 0.1429624706506729
2018-10-21 22:50:52.329182:	Training iteration: 405200, Loss: 0.09071604907512665
2018-10-21 22:51:21.698404:	Training iteration: 405400, Loss: 0.14737939834594727
2018-10-21 22:51:52.183301:	Training iteration: 405600, Loss: 0.10272173583507538
2018-10-21 22:52:21.641198:	Training iteration: 405800, Loss: 0.15677568316459656
2018-10-21 22:52:51.415091:	Training iteration: 406000, Loss: 0.10052430629730225
2018-10-21 22:53:21.215389:	Training iteration: 406200, Loss: 0.09455392509698868
2018-10-21 22:53:51.102628:	Training iteration: 406400, Loss: 0.10373314470052719
2018-10-21 22:54:21.646708:	Training iteration: 406600, Loss: 0.10493345558643341
2018-10-21 22:54:53.732514:	Training iteration: 406800, Loss: 0.1701985001564026
2018-10-21 22:55:23.489419:	Training iteration: 407000, Loss: 0.12295135855674744
2018-10-21 22:55:52.689488:	Training iteration: 407200, Loss: 0.12234683334827423
2018-10-21 22:56:23.019785:	Training iteration: 407400, Loss: 0.08968514204025269
2018-10-21 22:56:52.700318:	Training iteration: 407600, Loss: 0.12194527685642242
2018-10-21 22:57:21.968762:	Training iteration: 407800, Loss: 0.09844015538692474
2018-10-21 22:57:51.992171:	Training iteration: 408000, Loss: 0.08220180869102478
2018-10-21 22:58:20.714779:	Training iteration: 408200, Loss: 0.07949725538492203
2018-10-21 22:58:50.371677:	Training iteration: 408400, Loss: 0.16350585222244263
2018-10-21 22:59:20.749178:	Training iteration: 408600, Loss: 0.15482456982135773
2018-10-21 22:59:50.321455:	Training iteration: 408800, Loss: 0.13358062505722046
2018-10-21 23:00:19.064778:	Training iteration: 409000, Loss: 0.12054809927940369
2018-10-21 23:00:49.776401:	Training iteration: 409200, Loss: 0.12673303484916687
2018-10-21 23:01:19.397786:	Training iteration: 409400, Loss: 0.13949580490589142
2018-10-21 23:01:49.016218:	Training iteration: 409600, Loss: 0.17835906147956848
2018-10-21 23:02:18.729766:	Training iteration: 409800, Loss: 0.0907878428697586
2018-10-21 23:02:48.812259:	Training iteration: 410000, Loss: 0.1871105432510376
2018-10-21 23:03:27.534792: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 866 of 1000
2018-10-21 23:03:29.001290: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-21 23:03:29.675191:	Training iteration: 410200, Loss: 0.09850337356328964
2018-10-21 23:03:58.285851:	Training iteration: 410400, Loss: 0.24140506982803345
2018-10-21 23:04:27.849116:	Training iteration: 410600, Loss: 0.14392933249473572
2018-10-21 23:04:57.718937:	Training iteration: 410800, Loss: 0.2967647910118103
2018-10-21 23:05:26.965747:	Training iteration: 411000, Loss: 0.12290025502443314
2018-10-21 23:05:56.782302:	Training iteration: 411200, Loss: 0.2656281292438507
2018-10-21 23:06:26.097420:	Training iteration: 411400, Loss: 0.15527722239494324
2018-10-21 23:06:55.684364:	Training iteration: 411600, Loss: 0.08926913142204285
2018-10-21 23:07:24.837392:	Training iteration: 411800, Loss: 0.17817199230194092
2018-10-21 23:07:54.722163:	Training iteration: 412000, Loss: 0.12720271944999695
2018-10-21 23:08:24.775674:	Training iteration: 412200, Loss: 0.22118359804153442
2018-10-21 23:08:54.280883:	Training iteration: 412400, Loss: 0.1441533863544464
2018-10-21 23:09:24.483566:	Training iteration: 412600, Loss: 0.17438259720802307
2018-10-21 23:09:53.643351:	Training iteration: 412800, Loss: 0.12386197596788406
2018-10-21 23:10:23.329137:	Training iteration: 413000, Loss: 0.14302188158035278
2018-10-21 23:10:53.216959:	Training iteration: 413200, Loss: 0.181998610496521
2018-10-21 23:11:23.240553:	Training iteration: 413400, Loss: 0.17014141380786896
2018-10-21 23:11:53.040650:	Training iteration: 413600, Loss: 0.19492337107658386
2018-10-21 23:12:22.315680:	Training iteration: 413800, Loss: 0.1524926722049713
2018-10-21 23:12:52.390735:	Training iteration: 414000, Loss: 0.1284974217414856
2018-10-21 23:13:21.973555:	Training iteration: 414200, Loss: 0.11477889120578766
2018-10-21 23:13:52.186742:	Training iteration: 414400, Loss: 0.1702403426170349
2018-10-21 23:14:21.890752:	Training iteration: 414600, Loss: 0.16752761602401733
2018-10-21 23:14:52.090429:	Training iteration: 414800, Loss: 0.11796444654464722
2018-10-21 23:15:21.147534:	Training iteration: 415000, Loss: 0.16072876751422882
2018-10-21 23:15:51.210229:	Training iteration: 415200, Loss: 0.19223976135253906
2018-10-21 23:16:20.824218:	Training iteration: 415400, Loss: 0.13103020191192627
2018-10-21 23:16:50.581464:	Training iteration: 415600, Loss: 0.1296248435974121
2018-10-21 23:17:20.103336:	Training iteration: 415800, Loss: 0.13759516179561615
2018-10-21 23:17:50.182454:	Training iteration: 416000, Loss: 0.13528603315353394
2018-10-21 23:18:19.681329:	Training iteration: 416200, Loss: 0.21412646770477295
2018-10-21 23:18:49.356606:	Training iteration: 416400, Loss: 0.14634864032268524
2018-10-21 23:19:18.722160:	Training iteration: 416600, Loss: 0.20672863721847534
2018-10-21 23:19:48.433831:	Training iteration: 416800, Loss: 0.19309958815574646
2018-10-21 23:20:18.165589:	Training iteration: 417000, Loss: 0.14338670670986176
2018-10-21 23:20:48.861952:	Training iteration: 417200, Loss: 0.16702678799629211
2018-10-21 23:21:19.156832:	Training iteration: 417400, Loss: 0.17183509469032288
2018-10-21 23:21:48.507973:	Training iteration: 417600, Loss: 0.2543814778327942
2018-10-21 23:22:17.786332:	Training iteration: 417800, Loss: 0.14677155017852783
2018-10-21 23:22:46.944407:	Training iteration: 418000, Loss: 0.1949935257434845
2018-10-21 23:23:16.153999:	Training iteration: 418200, Loss: 0.1363700032234192
2018-10-21 23:23:47.242135:	Training iteration: 418400, Loss: 0.16891273856163025
2018-10-21 23:24:16.874714:	Training iteration: 418600, Loss: 0.14499616622924805
2018-10-21 23:24:46.471269:	Training iteration: 418800, Loss: 0.13884776830673218
2018-10-21 23:25:16.764935:	Training iteration: 419000, Loss: 0.13482040166854858
2018-10-21 23:25:46.669047:	Training iteration: 419200, Loss: 0.1631375253200531
2018-10-21 23:26:16.344888:	Training iteration: 419400, Loss: 0.14636100828647614
2018-10-21 23:26:45.877877:	Training iteration: 419600, Loss: 0.16724909842014313
2018-10-21 23:27:15.691006:	Training iteration: 419800, Loss: 0.22127313911914825
2018-10-21 23:27:45.636381:	Training iteration: 420000, Loss: 0.1302960067987442
2018-10-21 23:28:15.886282:	Training iteration: 420200, Loss: 0.1297277957201004
2018-10-21 23:28:44.891661:	Training iteration: 420400, Loss: 0.16337692737579346
2018-10-21 23:29:14.946066:	Training iteration: 420600, Loss: 0.16065490245819092
2018-10-21 23:29:44.736091:	Training iteration: 420800, Loss: 0.15699970722198486
2018-10-21 23:30:14.387303:	Training iteration: 421000, Loss: 0.21583068370819092
2018-10-21 23:30:43.880596:	Training iteration: 421200, Loss: 0.10889434814453125
2018-10-21 23:31:14.273983:	Training iteration: 421400, Loss: 0.12468267977237701
2018-10-21 23:31:43.792806:	Training iteration: 421600, Loss: 0.13790854811668396
2018-10-21 23:32:13.450546:	Training iteration: 421800, Loss: 0.1317354440689087
2018-10-21 23:32:43.561435:	Training iteration: 422000, Loss: 0.19129885733127594
2018-10-21 23:33:13.077372:	Training iteration: 422200, Loss: 0.1834845244884491
2018-10-21 23:33:41.862830:	Training iteration: 422400, Loss: 0.20564746856689453
2018-10-21 23:34:11.598740:	Training iteration: 422600, Loss: 0.2490035593509674
2018-10-21 23:34:41.294741:	Training iteration: 422800, Loss: 0.1564793884754181
2018-10-21 23:35:11.459104:	Training iteration: 423000, Loss: 0.11782156676054001
2018-10-21 23:35:40.798966:	Training iteration: 423200, Loss: 0.18781761825084686
2018-10-21 23:36:11.093249:	Training iteration: 423400, Loss: 0.11375408619642258
2018-10-21 23:36:41.184612:	Training iteration: 423600, Loss: 0.1290004700422287
2018-10-21 23:37:11.292469:	Training iteration: 423800, Loss: 0.20984326303005219
2018-10-21 23:37:41.377462:	Training iteration: 424000, Loss: 0.15292617678642273
2018-10-21 23:38:11.823700:	Training iteration: 424200, Loss: 0.1835564523935318
2018-10-21 23:38:40.565382:	Training iteration: 424400, Loss: 0.1780998706817627
2018-10-21 23:39:10.653468:	Training iteration: 424600, Loss: 0.10601504892110825
2018-10-21 23:39:40.363091:	Training iteration: 424800, Loss: 0.2109748274087906
2018-10-21 23:40:09.788488:	Training iteration: 425000, Loss: 0.18504779040813446
2018-10-21 23:40:39.892791:	Training iteration: 425200, Loss: 0.1610330045223236
2018-10-21 23:41:09.238458:	Training iteration: 425400, Loss: 0.15419350564479828
2018-10-21 23:41:39.027591:	Training iteration: 425600, Loss: 0.1520528495311737
2018-10-21 23:42:08.760843:	Training iteration: 425800, Loss: 0.13529005646705627
2018-10-21 23:42:38.658906:	Training iteration: 426000, Loss: 0.1596505343914032
2018-10-21 23:43:07.493652:	Training iteration: 426200, Loss: 0.19973602890968323
2018-10-21 23:43:37.154269:	Training iteration: 426400, Loss: 0.15073883533477783
2018-10-21 23:44:06.709604:	Training iteration: 426600, Loss: 0.237740620970726
2018-10-21 23:44:37.078239:	Training iteration: 426800, Loss: 0.15579059720039368
2018-10-21 23:45:07.118240:	Training iteration: 427000, Loss: 0.2837119698524475
2018-10-21 23:45:37.011676:	Training iteration: 427200, Loss: 0.18400248885154724
2018-10-21 23:46:06.247531:	Training iteration: 427400, Loss: 0.16903236508369446
2018-10-21 23:46:35.963330:	Training iteration: 427600, Loss: 0.23083342611789703
2018-10-21 23:47:05.955572:	Training iteration: 427800, Loss: 0.22082215547561646
2018-10-21 23:47:36.121011:	Training iteration: 428000, Loss: 0.24338389933109283
2018-10-21 23:48:05.204244:	Training iteration: 428200, Loss: 0.14219607412815094
2018-10-21 23:48:34.186275:	Training iteration: 428400, Loss: 0.17876066267490387
2018-10-21 23:49:04.092472:	Training iteration: 428600, Loss: 0.1348540186882019
2018-10-21 23:49:33.930688:	Training iteration: 428800, Loss: 0.15585875511169434
2018-10-21 23:50:03.904710:	Training iteration: 429000, Loss: 0.1910611093044281
2018-10-21 23:50:34.097403:	Training iteration: 429200, Loss: 0.13805675506591797
2018-10-21 23:51:03.298991:	Training iteration: 429400, Loss: 0.12921980023384094
2018-10-21 23:51:32.750101:	Training iteration: 429600, Loss: 0.18546269834041595
2018-10-21 23:52:03.226161:	Training iteration: 429800, Loss: 0.14412082731723785
2018-10-21 23:52:33.160976:	Training iteration: 430000, Loss: 0.17342662811279297
2018-10-21 23:53:02.545006:	Training iteration: 430200, Loss: 0.15269756317138672
2018-10-21 23:53:32.326506:	Training iteration: 430400, Loss: 0.18119391798973083
2018-10-21 23:54:01.683943:	Training iteration: 430600, Loss: 0.18645593523979187
2018-10-21 23:54:32.018608:	Training iteration: 430800, Loss: 0.18497151136398315
2018-10-21 23:55:01.705838:	Training iteration: 431000, Loss: 0.15369366109371185
2018-10-21 23:55:31.780010:	Training iteration: 431200, Loss: 0.159078910946846
2018-10-21 23:56:01.108752:	Training iteration: 431400, Loss: 0.14621314406394958
2018-10-21 23:56:31.851199:	Training iteration: 431600, Loss: 0.14441753923892975
2018-10-21 23:57:02.093993:	Training iteration: 431800, Loss: 0.1501321643590927
2018-10-21 23:57:31.899477:	Training iteration: 432000, Loss: 0.1456070840358734
2018-10-21 23:58:01.363100:	Training iteration: 432200, Loss: 0.14030630886554718
2018-10-21 23:58:31.382687:	Training iteration: 432400, Loss: 0.17311975359916687
2018-10-21 23:59:00.872508:	Training iteration: 432600, Loss: 0.20370277762413025
2018-10-21 23:59:31.013836:	Training iteration: 432800, Loss: 0.21701782941818237
2018-10-22 00:00:00.253758:	Training iteration: 433000, Loss: 0.14429324865341187
2018-10-22 00:00:29.690817:	Training iteration: 433200, Loss: 0.19893226027488708
2018-10-22 00:00:59.004166:	Training iteration: 433400, Loss: 0.18760135769844055
2018-10-22 00:01:28.109867:	Training iteration: 433600, Loss: 0.16774499416351318
2018-10-22 00:01:58.498104:	Training iteration: 433800, Loss: 0.16278716921806335
2018-10-22 00:02:27.834331:	Training iteration: 434000, Loss: 0.1811012327671051
2018-10-22 00:02:57.723069:	Training iteration: 434200, Loss: 0.1258547306060791
2018-10-22 00:03:28.030179:	Training iteration: 434400, Loss: 0.1335700899362564
2018-10-22 00:03:57.584997:	Training iteration: 434600, Loss: 0.15923446416854858
2018-10-22 00:04:27.480700:	Training iteration: 434800, Loss: 0.16123169660568237
2018-10-22 00:04:56.304270:	Training iteration: 435000, Loss: 0.13316833972930908
2018-10-22 00:05:25.596475:	Training iteration: 435200, Loss: 0.1865583062171936
2018-10-22 00:05:57.125594:	Training iteration: 435400, Loss: 0.14036792516708374
2018-10-22 00:06:27.087549:	Training iteration: 435600, Loss: 0.18543007969856262
2018-10-22 00:06:56.670624:	Training iteration: 435800, Loss: 0.1312413215637207
2018-10-22 00:07:26.147596:	Training iteration: 436000, Loss: 0.12720920145511627
2018-10-22 00:07:56.439474:	Training iteration: 436200, Loss: 0.13329312205314636
2018-10-22 00:08:26.018137:	Training iteration: 436400, Loss: 0.14258134365081787
2018-10-22 00:08:56.146675:	Training iteration: 436600, Loss: 0.15736985206604004
2018-10-22 00:09:25.996557:	Training iteration: 436800, Loss: 0.11872825771570206
2018-10-22 00:09:55.503799:	Training iteration: 437000, Loss: 0.13231201469898224
2018-10-22 00:10:25.455631:	Training iteration: 437200, Loss: 0.17641371488571167
2018-10-22 00:10:55.691869:	Training iteration: 437400, Loss: 0.15650200843811035
2018-10-22 00:11:26.231527:	Training iteration: 437600, Loss: 0.14429150521755219
2018-10-22 00:11:56.162516:	Training iteration: 437800, Loss: 0.12654325366020203
2018-10-22 00:12:26.841339:	Training iteration: 438000, Loss: 0.17420804500579834
2018-10-22 00:12:57.248360:	Training iteration: 438200, Loss: 0.1355239897966385
2018-10-22 00:13:26.872964:	Training iteration: 438400, Loss: 0.18868054449558258
2018-10-22 00:13:56.721186:	Training iteration: 438600, Loss: 0.12591469287872314
2018-10-22 00:14:26.509923:	Training iteration: 438800, Loss: 0.12406714260578156
2018-10-22 00:14:56.053807:	Training iteration: 439000, Loss: 0.16045844554901123
2018-10-22 00:15:25.830866:	Training iteration: 439200, Loss: 0.18728286027908325
2018-10-22 00:15:55.479180:	Training iteration: 439400, Loss: 0.12203852087259293
2018-10-22 00:16:25.576211:	Training iteration: 439600, Loss: 0.1378721296787262
2018-10-22 00:16:54.918189:	Training iteration: 439800, Loss: 0.1467941701412201
2018-10-22 00:17:23.345323:	Training iteration: 440000, Loss: 0.177079975605011
2018-10-22 00:17:53.419812:	Training iteration: 440200, Loss: 0.15972092747688293
2018-10-22 00:18:22.624919:	Training iteration: 440400, Loss: 0.1539558321237564
2018-10-22 00:18:52.206557:	Training iteration: 440600, Loss: 0.21111589670181274
2018-10-22 00:19:22.074665:	Training iteration: 440800, Loss: 0.15535303950309753
2018-10-22 00:19:52.015323:	Training iteration: 441000, Loss: 0.1340089589357376
2018-10-22 00:20:22.014321:	Training iteration: 441200, Loss: 0.1330297589302063
2018-10-22 00:20:51.872751:	Training iteration: 441400, Loss: 0.21859563887119293
2018-10-22 00:21:21.291195:	Training iteration: 441600, Loss: 0.14145085215568542
2018-10-22 00:21:50.459553:	Training iteration: 441800, Loss: 0.17149049043655396
2018-10-22 00:22:19.898219:	Training iteration: 442000, Loss: 0.17185479402542114
2018-10-22 00:22:50.337626:	Training iteration: 442200, Loss: 0.16058270633220673
2018-10-22 00:23:19.923397:	Training iteration: 442400, Loss: 0.15267187356948853
2018-10-22 00:23:50.325615:	Training iteration: 442600, Loss: 0.14791999757289886
2018-10-22 00:24:20.254625:	Training iteration: 442800, Loss: 0.18683944642543793
2018-10-22 00:24:50.049222:	Training iteration: 443000, Loss: 0.18740226328372955
2018-10-22 00:25:20.482751:	Training iteration: 443200, Loss: 0.1711954027414322
2018-10-22 00:25:50.048009:	Training iteration: 443400, Loss: 0.18929344415664673
2018-10-22 00:26:19.699479:	Training iteration: 443600, Loss: 0.18439029157161713
2018-10-22 00:26:49.046113:	Training iteration: 443800, Loss: 0.17775201797485352
2018-10-22 00:27:19.073876:	Training iteration: 444000, Loss: 0.17786341905593872
2018-10-22 00:27:48.764654:	Training iteration: 444200, Loss: 0.14730040729045868
2018-10-22 00:28:18.044463:	Training iteration: 444400, Loss: 0.14942121505737305
2018-10-22 00:28:48.897679:	Training iteration: 444600, Loss: 0.18693962693214417
2018-10-22 00:29:18.891893:	Training iteration: 444800, Loss: 0.1759594976902008
2018-10-22 00:29:48.973886:	Training iteration: 445000, Loss: 0.10248522460460663
2018-10-22 00:30:18.836402:	Training iteration: 445200, Loss: 0.1752898395061493
2018-10-22 00:30:48.697301:	Training iteration: 445400, Loss: 0.15222404897212982
2018-10-22 00:31:18.974045:	Training iteration: 445600, Loss: 0.145666241645813
2018-10-22 00:31:48.444510:	Training iteration: 445800, Loss: 0.18337500095367432
2018-10-22 00:32:18.414960:	Training iteration: 446000, Loss: 0.20345234870910645
2018-10-22 00:32:48.077158:	Training iteration: 446200, Loss: 0.18630993366241455
2018-10-22 00:33:18.166461:	Training iteration: 446400, Loss: 0.18382896482944489
2018-10-22 00:33:47.969249:	Training iteration: 446600, Loss: 0.15985265374183655
2018-10-22 00:34:18.103887:	Training iteration: 446800, Loss: 0.14945510029792786
2018-10-22 00:34:47.787969:	Training iteration: 447000, Loss: 0.18171636760234833
2018-10-22 00:35:16.730757:	Training iteration: 447200, Loss: 0.20498046278953552
2018-10-22 00:35:46.847899:	Training iteration: 447400, Loss: 0.14733943343162537
2018-10-22 00:36:17.310769:	Training iteration: 447600, Loss: 0.1832873821258545
2018-10-22 00:36:46.282018:	Training iteration: 447800, Loss: 0.1468232274055481
2018-10-22 00:37:15.655218:	Training iteration: 448000, Loss: 0.18139222264289856
2018-10-22 00:37:45.673976:	Training iteration: 448200, Loss: 0.14266559481620789
2018-10-22 00:38:15.288926:	Training iteration: 448400, Loss: 0.1589750051498413
2018-10-22 00:38:44.780647:	Training iteration: 448600, Loss: 0.1747850924730301
2018-10-22 00:39:14.884525:	Training iteration: 448800, Loss: 0.14978089928627014
2018-10-22 00:39:44.463112:	Training iteration: 449000, Loss: 0.16641820967197418
2018-10-22 00:40:14.225298:	Training iteration: 449200, Loss: 0.2003539651632309
2018-10-22 00:40:44.111715:	Training iteration: 449400, Loss: 0.15146276354789734
2018-10-22 00:41:14.789082:	Training iteration: 449600, Loss: 0.2593015432357788
2018-10-22 00:41:43.981129:	Training iteration: 449800, Loss: 0.11453191936016083
2018-10-22 00:42:13.909138:	Training iteration: 450000, Loss: 0.17023813724517822
2018-10-22 00:42:43.631313:	Training iteration: 450200, Loss: 0.13417527079582214
2018-10-22 00:43:12.948256:	Training iteration: 450400, Loss: 0.21406278014183044
2018-10-22 00:43:42.478284:	Training iteration: 450600, Loss: 0.22284385561943054
2018-10-22 00:44:12.337302:	Training iteration: 450800, Loss: 0.12722322344779968
2018-10-22 00:44:42.198178:	Training iteration: 451000, Loss: 0.19234317541122437
2018-10-22 00:45:12.364705:	Training iteration: 451200, Loss: 0.15164050459861755
2018-10-22 00:45:43.017985:	Training iteration: 451400, Loss: 0.17148301005363464
2018-10-22 00:46:12.104446:	Training iteration: 451600, Loss: 0.15460291504859924
2018-10-22 00:46:42.273748:	Training iteration: 451800, Loss: 0.19578132033348083
2018-10-22 00:47:11.633035:	Training iteration: 452000, Loss: 0.25563037395477295
2018-10-22 00:47:41.440139:	Training iteration: 452200, Loss: 0.1575077474117279
2018-10-22 00:48:10.832780:	Training iteration: 452400, Loss: 0.14422377943992615
2018-10-22 00:48:40.144229:	Training iteration: 452600, Loss: 0.14484335482120514
2018-10-22 00:49:09.157629:	Training iteration: 452800, Loss: 0.13348260521888733
2018-10-22 00:49:38.068591:	Training iteration: 453000, Loss: 0.1570930927991867
2018-10-22 00:50:08.255695:	Training iteration: 453200, Loss: 0.12779009342193604
2018-10-22 00:50:37.942285:	Training iteration: 453400, Loss: 0.10118608176708221
2018-10-22 00:51:07.781578:	Training iteration: 453600, Loss: 0.18442218005657196
2018-10-22 00:51:38.185605:	Training iteration: 453800, Loss: 0.15687385201454163
2018-10-22 00:52:07.620463:	Training iteration: 454000, Loss: 0.3323691189289093
2018-10-22 00:52:37.666402:	Training iteration: 454200, Loss: 0.141556054353714
2018-10-22 00:53:07.177432:	Training iteration: 454400, Loss: 0.15583795309066772
2018-10-22 00:53:37.736071:	Training iteration: 454600, Loss: 0.21303756535053253
2018-10-22 00:54:07.357582:	Training iteration: 454800, Loss: 0.16065485775470734
2018-10-22 00:54:37.416662:	Training iteration: 455000, Loss: 0.13497108221054077
2018-10-22 00:55:07.013071:	Training iteration: 455200, Loss: 0.20330148935317993
2018-10-22 00:55:36.904619:	Training iteration: 455400, Loss: 0.16731806099414825
2018-10-22 00:56:06.522372:	Training iteration: 455600, Loss: 0.18588674068450928
2018-10-22 00:56:34.471057:	Training iteration: 455800, Loss: 0.13133645057678223
2018-10-22 00:57:03.749781:	Training iteration: 456000, Loss: 0.15089528262615204
2018-10-22 00:57:32.911487:	Training iteration: 456200, Loss: 0.129214346408844
2018-10-22 00:58:01.851566:	Training iteration: 456400, Loss: 0.16525134444236755
2018-10-22 00:58:31.157733:	Training iteration: 456600, Loss: 0.14618352055549622
2018-10-22 00:59:00.642259:	Training iteration: 456800, Loss: 0.17565885186195374
2018-10-22 00:59:30.855724:	Training iteration: 457000, Loss: 0.10327330231666565
2018-10-22 01:00:00.233355:	Training iteration: 457200, Loss: 0.15463930368423462
2018-10-22 01:00:30.517691:	Training iteration: 457400, Loss: 0.11489969491958618
2018-10-22 01:01:00.792148:	Training iteration: 457600, Loss: 0.14292621612548828
2018-10-22 01:01:30.812115:	Training iteration: 457800, Loss: 0.16027918457984924
2018-10-22 01:02:00.293813:	Training iteration: 458000, Loss: 0.14777426421642303
2018-10-22 01:02:29.567287:	Training iteration: 458200, Loss: 0.17046737670898438
2018-10-22 01:02:59.514832:	Training iteration: 458400, Loss: 0.16730567812919617
2018-10-22 01:03:28.771622:	Training iteration: 458600, Loss: 0.14908422529697418
2018-10-22 01:03:58.534611:	Training iteration: 458800, Loss: 0.1349639892578125
2018-10-22 01:04:28.535194:	Training iteration: 459000, Loss: 0.11924854665994644
2018-10-22 01:04:58.960844:	Training iteration: 459200, Loss: 0.11974573135375977
2018-10-22 01:05:28.035132:	Training iteration: 459400, Loss: 0.20713278651237488
2018-10-22 01:05:58.153566:	Training iteration: 459600, Loss: 0.15088942646980286
2018-10-22 01:06:27.123541:	Training iteration: 459800, Loss: 0.12928660213947296
2018-10-22 01:06:57.392197:	Training iteration: 460000, Loss: 0.17274703085422516
2018-10-22 01:07:27.176072:	Training iteration: 460200, Loss: 0.17413803935050964
2018-10-22 01:07:57.322355:	Training iteration: 460400, Loss: 0.18422308564186096
2018-10-22 01:08:26.203233:	Training iteration: 460600, Loss: 0.1614169180393219
2018-10-22 01:08:56.153547:	Training iteration: 460800, Loss: 0.13258373737335205
2018-10-22 01:09:25.780212:	Training iteration: 461000, Loss: 0.1371668428182602
2018-10-22 01:09:55.534473:	Training iteration: 461200, Loss: 0.187095046043396
2018-10-22 01:10:25.783864:	Training iteration: 461400, Loss: 0.13828319311141968
2018-10-22 01:10:56.252301:	Training iteration: 461600, Loss: 0.0990263894200325
2018-10-22 01:11:26.804901:	Training iteration: 461800, Loss: 0.16669537127017975
2018-10-22 01:11:56.560032:	Training iteration: 462000, Loss: 0.09961065649986267
2018-10-22 01:12:25.830464:	Training iteration: 462200, Loss: 0.1253751516342163
2018-10-22 01:12:55.672967:	Training iteration: 462400, Loss: 0.17416250705718994
2018-10-22 01:13:25.105884:	Training iteration: 462600, Loss: 0.15302005410194397
2018-10-22 01:13:54.456500:	Training iteration: 462800, Loss: 0.1330808401107788
2018-10-22 01:14:24.645426:	Training iteration: 463000, Loss: 0.14025694131851196
2018-10-22 01:14:54.759628:	Training iteration: 463200, Loss: 0.14925500750541687
2018-10-22 01:15:23.891701:	Training iteration: 463400, Loss: 0.159145325422287
2018-10-22 01:15:52.745327:	Training iteration: 463600, Loss: 0.14774587750434875
2018-10-22 01:16:22.974501:	Training iteration: 463800, Loss: 0.1765151470899582
2018-10-22 01:16:52.795406:	Training iteration: 464000, Loss: 0.1659410148859024
2018-10-22 01:17:21.962317:	Training iteration: 464200, Loss: 0.1605411171913147
2018-10-22 01:17:51.681154:	Training iteration: 464400, Loss: 0.28443413972854614
2018-10-22 01:18:21.529720:	Training iteration: 464600, Loss: 0.1328391134738922
2018-10-22 01:18:50.626511:	Training iteration: 464800, Loss: 0.12028387933969498
2018-10-22 01:19:20.729679:	Training iteration: 465000, Loss: 0.16663378477096558
2018-10-22 01:19:49.522898:	Training iteration: 465200, Loss: 0.1676045060157776
2018-10-22 01:20:20.454323:	Training iteration: 465400, Loss: 0.12701603770256042
2018-10-22 01:20:50.799330:	Training iteration: 465600, Loss: 0.15095391869544983
2018-10-22 01:21:20.292196:	Training iteration: 465800, Loss: 0.1721910834312439
2018-10-22 01:21:50.512342:	Training iteration: 466000, Loss: 0.16804328560829163
2018-10-22 01:22:20.047545:	Training iteration: 466200, Loss: 0.13613486289978027
2018-10-22 01:22:48.639293:	Training iteration: 466400, Loss: 0.1571403443813324
2018-10-22 01:23:18.739370:	Training iteration: 466600, Loss: 0.1222844123840332
2018-10-22 01:23:49.192138:	Training iteration: 466800, Loss: 0.16389718651771545
2018-10-22 01:24:19.131281:	Training iteration: 467000, Loss: 0.20336027443408966
2018-10-22 01:24:49.048514:	Training iteration: 467200, Loss: 0.13766798377037048
2018-10-22 01:25:19.200770:	Training iteration: 467400, Loss: 0.16084441542625427
2018-10-22 01:25:48.040568:	Training iteration: 467600, Loss: 0.18990680575370789
2018-10-22 01:26:17.521090:	Training iteration: 467800, Loss: 0.14333249628543854
2018-10-22 01:26:46.719697:	Training iteration: 468000, Loss: 0.13810861110687256
2018-10-22 01:27:16.142093:	Training iteration: 468200, Loss: 0.24168232083320618
2018-10-22 01:27:45.555073:	Training iteration: 468400, Loss: 0.15757128596305847
2018-10-22 01:28:14.810219:	Training iteration: 468600, Loss: 0.15415902435779572
2018-10-22 01:28:45.568320:	Training iteration: 468800, Loss: 0.12506935000419617
2018-10-22 01:29:15.689864:	Training iteration: 469000, Loss: 0.16430199146270752
2018-10-22 01:29:45.115894:	Training iteration: 469200, Loss: 0.1515057384967804
2018-10-22 01:30:15.192163:	Training iteration: 469400, Loss: 0.1232680231332779
2018-10-22 01:30:44.507361:	Training iteration: 469600, Loss: 0.10588096082210541
2018-10-22 01:31:14.394620:	Training iteration: 469800, Loss: 0.1627330332994461
2018-10-22 01:31:44.532216:	Training iteration: 470000, Loss: 0.1606714427471161
2018-10-22 01:32:14.920147:	Training iteration: 470200, Loss: 0.17732375860214233
2018-10-22 01:32:45.015923:	Training iteration: 470400, Loss: 0.12854811549186707
2018-10-22 01:33:14.879866:	Training iteration: 470600, Loss: 0.1446753591299057
2018-10-22 01:33:44.620378:	Training iteration: 470800, Loss: 0.1618879735469818
2018-10-22 01:34:14.232971:	Training iteration: 471000, Loss: 0.16123074293136597
2018-10-22 01:34:44.969620:	Training iteration: 471200, Loss: 0.14894318580627441
2018-10-22 01:35:14.958685:	Training iteration: 471400, Loss: 0.1755131483078003
2018-10-22 01:35:44.155177:	Training iteration: 471600, Loss: 0.1198870912194252
2018-10-22 01:36:14.415707:	Training iteration: 471800, Loss: 0.1806280016899109
2018-10-22 01:36:44.579376:	Training iteration: 472000, Loss: 0.2074964940547943
2018-10-22 01:37:15.284460:	Training iteration: 472200, Loss: 0.1448904275894165
2018-10-22 01:37:45.162117:	Training iteration: 472400, Loss: 0.13692016899585724
2018-10-22 01:38:14.595064:	Training iteration: 472600, Loss: 0.16496515274047852
2018-10-22 01:38:44.758678:	Training iteration: 472800, Loss: 0.17839454114437103
2018-10-22 01:39:14.261015:	Training iteration: 473000, Loss: 0.13210849463939667
2018-10-22 01:39:43.763857:	Training iteration: 473200, Loss: 0.16809244453907013
2018-10-22 01:40:13.334709:	Training iteration: 473400, Loss: 0.17283764481544495
2018-10-22 01:40:43.266206:	Training iteration: 473600, Loss: 0.14001493155956268
2018-10-22 01:41:12.524432:	Training iteration: 473800, Loss: 0.17054741084575653
2018-10-22 01:41:41.386525:	Training iteration: 474000, Loss: 0.14790770411491394
2018-10-22 01:42:10.847073:	Training iteration: 474200, Loss: 0.17019382119178772
2018-10-22 01:42:40.409336:	Training iteration: 474400, Loss: 0.22290027141571045
2018-10-22 01:43:10.122835:	Training iteration: 474600, Loss: 0.2356715202331543
2018-10-22 01:43:39.982942:	Training iteration: 474800, Loss: 0.17208436131477356
2018-10-22 01:44:09.826952:	Training iteration: 475000, Loss: 0.1256890743970871
2018-10-22 01:44:39.360123:	Training iteration: 475200, Loss: 0.11275827884674072
2018-10-22 01:45:08.674277:	Training iteration: 475400, Loss: 0.17185786366462708
2018-10-22 01:45:38.417133:	Training iteration: 475600, Loss: 0.16482552886009216
2018-10-22 01:46:08.016221:	Training iteration: 475800, Loss: 0.21483606100082397
2018-10-22 01:46:37.194950:	Training iteration: 476000, Loss: 0.2159883677959442
2018-10-22 01:47:06.991474:	Training iteration: 476200, Loss: 0.14077165722846985
2018-10-22 01:47:37.592199:	Training iteration: 476400, Loss: 0.1995583176612854
2018-10-22 01:48:06.534693:	Training iteration: 476600, Loss: 0.18232472240924835
2018-10-22 01:48:34.954364:	Training iteration: 476800, Loss: 0.1626403033733368
2018-10-22 01:49:03.884971:	Training iteration: 477000, Loss: 0.14273735880851746
2018-10-22 01:49:33.689720:	Training iteration: 477200, Loss: 0.15873634815216064
2018-10-22 01:50:03.055337:	Training iteration: 477400, Loss: 0.12974917888641357
2018-10-22 01:50:32.821377:	Training iteration: 477600, Loss: 0.14735367894172668
2018-10-22 01:51:03.058413:	Training iteration: 477800, Loss: 0.14142300188541412
2018-10-22 01:51:32.339095:	Training iteration: 478000, Loss: 0.16904263198375702
2018-10-22 01:52:01.347088:	Training iteration: 478200, Loss: 0.23755207657814026
2018-10-22 01:52:30.787245:	Training iteration: 478400, Loss: 0.2029641717672348
2018-10-22 01:53:01.821552:	Training iteration: 478600, Loss: 0.16297996044158936
2018-10-22 01:53:30.754005:	Training iteration: 478800, Loss: 0.3381432592868805
2018-10-22 01:54:00.069042:	Training iteration: 479000, Loss: 0.16736257076263428
2018-10-22 01:54:29.433221:	Training iteration: 479200, Loss: 0.14243018627166748
2018-10-22 01:54:58.741392:	Training iteration: 479400, Loss: 0.15997816622257233
2018-10-22 01:55:27.885082:	Training iteration: 479600, Loss: 0.17752735316753387
2018-10-22 01:55:57.960436:	Training iteration: 479800, Loss: 0.1555625945329666
2018-10-22 01:55:58.543364:	Epoch 3 finished after 479805 iterations.
No images to record
Validating
2018-10-22 01:55:58.585333:	Entering validation loop
2018-10-22 01:56:08.695203: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 618 of 1000
2018-10-22 01:56:14.152837: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 01:56:27.590503:	Validation iteration: 200, Loss: 0.23857749998569489
2018-10-22 01:56:41.363638:	Validation iteration: 400, Loss: 0.1943802535533905
2018-10-22 01:56:55.414889:	Validation iteration: 600, Loss: 0.18818703293800354
2018-10-22 01:57:09.326438:	Validation iteration: 800, Loss: 0.248179093003273
2018-10-22 01:57:23.155586:	Validation iteration: 1000, Loss: 0.2380746304988861
2018-10-22 01:57:36.961431:	Validation iteration: 1200, Loss: 0.15871009230613708
2018-10-22 01:57:51.226701:	Validation iteration: 1400, Loss: 0.18113252520561218
2018-10-22 01:58:05.739156:	Validation iteration: 1600, Loss: 0.16258448362350464
2018-10-22 01:58:19.289051:	Validation iteration: 1800, Loss: 0.215837299823761
2018-10-22 01:58:32.819333:	Validation iteration: 2000, Loss: 0.2829405963420868
2018-10-22 01:58:48.537149: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 629 of 1000
2018-10-22 01:58:54.062052: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 01:59:01.600322:	Validation iteration: 2200, Loss: 0.18999752402305603
2018-10-22 01:59:14.893148:	Validation iteration: 2400, Loss: 0.22387227416038513
2018-10-22 01:59:28.490519:	Validation iteration: 2600, Loss: 0.24813903868198395
2018-10-22 01:59:42.638067:	Validation iteration: 2800, Loss: 0.22135081887245178
2018-10-22 01:59:57.440822:	Validation iteration: 3000, Loss: 0.22629381716251373
2018-10-22 02:00:11.695577:	Validation iteration: 3200, Loss: 0.18839314579963684
2018-10-22 02:00:25.916565:	Validation iteration: 3400, Loss: 0.2801571488380432
2018-10-22 02:00:40.237800:	Validation iteration: 3600, Loss: 0.178440660238266
2018-10-22 02:00:54.158176:	Validation iteration: 3800, Loss: 0.22606372833251953
2018-10-22 02:01:07.970385:	Validation iteration: 4000, Loss: 0.2240738570690155
2018-10-22 02:01:30.724926: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 620 of 1000
2018-10-22 02:01:36.321526: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:01:37.934001:	Validation iteration: 4200, Loss: 0.18691572546958923
2018-10-22 02:01:51.113749:	Validation iteration: 4400, Loss: 0.2683447003364563
2018-10-22 02:02:04.676401:	Validation iteration: 4600, Loss: 0.1553640067577362
2018-10-22 02:02:18.381460:	Validation iteration: 4800, Loss: 0.2839510440826416
2018-10-22 02:02:32.744010:	Validation iteration: 5000, Loss: 0.1919892430305481
2018-10-22 02:02:47.660261:	Validation iteration: 5200, Loss: 0.19862425327301025
2018-10-22 02:03:03.314991:	Validation iteration: 5400, Loss: 0.14477261900901794
2018-10-22 02:03:17.335814:	Validation iteration: 5600, Loss: 0.17763003706932068
2018-10-22 02:03:31.217893:	Validation iteration: 5800, Loss: 0.20668673515319824
2018-10-22 02:03:45.176199:	Validation iteration: 6000, Loss: 0.21991077065467834
2018-10-22 02:03:58.633284:	Validation iteration: 6200, Loss: 0.2198212891817093
2018-10-22 02:04:12.821691: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 628 of 1000
2018-10-22 02:04:19.973945: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:04:29.019875:	Validation iteration: 6400, Loss: 0.19517502188682556
2018-10-22 02:04:42.451415:	Validation iteration: 6600, Loss: 0.271059513092041
2018-10-22 02:05:00.594303:	Validation iteration: 6800, Loss: 0.21508237719535828
2018-10-22 02:05:14.260542:	Validation iteration: 7000, Loss: 0.17668208479881287
2018-10-22 02:05:28.369308:	Validation iteration: 7200, Loss: 0.22930197417736053
2018-10-22 02:05:42.464897:	Validation iteration: 7400, Loss: 0.2238825559616089
2018-10-22 02:05:56.412174:	Validation iteration: 7600, Loss: 0.2042284607887268
2018-10-22 02:06:10.411262:	Validation iteration: 7800, Loss: 0.2798610031604767
2018-10-22 02:06:24.513060:	Validation iteration: 8000, Loss: 0.18601366877555847
2018-10-22 02:06:38.413831:	Validation iteration: 8200, Loss: 0.20638565719127655
2018-10-22 02:06:58.866519: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 579 of 1000
2018-10-22 02:07:05.106384: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:07:08.137532:	Validation iteration: 8400, Loss: 0.1542101800441742
2018-10-22 02:07:21.516012:	Validation iteration: 8600, Loss: 0.1845361292362213
2018-10-22 02:07:35.017220:	Validation iteration: 8800, Loss: 0.17078013718128204
2018-10-22 02:07:48.799286:	Validation iteration: 9000, Loss: 0.14678291976451874
2018-10-22 02:08:02.536544:	Validation iteration: 9200, Loss: 0.15005069971084595
2018-10-22 02:08:16.976117:	Validation iteration: 9400, Loss: 0.14559867978096008
2018-10-22 02:08:31.587307:	Validation iteration: 9600, Loss: 0.13378731906414032
2018-10-22 02:08:45.927490:	Validation iteration: 9800, Loss: 0.12248751521110535
2018-10-22 02:09:01.711644:	Validation iteration: 10000, Loss: 0.12978078424930573
2018-10-22 02:09:16.812010:	Validation iteration: 10200, Loss: 0.1284635365009308
2018-10-22 02:09:30.585503:	Validation iteration: 10400, Loss: 0.19379779696464539
2018-10-22 02:09:44.460207:	Validation iteration: 10600, Loss: 0.319795161485672
2018-10-22 02:09:58.358467:	Validation iteration: 10800, Loss: 0.23273156583309174
2018-10-22 02:10:12.273111:	Validation iteration: 11000, Loss: 0.16821636259555817
2018-10-22 02:10:26.331107:	Validation iteration: 11200, Loss: 0.19328352808952332
Validation check mean loss: 0.1967279236116444
Validation loss has improved!
Checkpoint
2018-10-22 02:11:04.560566: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 722 of 1000
2018-10-22 02:11:08.076752: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:11:34.892107:	Training iteration: 480000, Loss: 0.11749650537967682
2018-10-22 02:12:03.148608:	Training iteration: 480200, Loss: 0.07442733645439148
2018-10-22 02:12:32.183745:	Training iteration: 480400, Loss: 0.19380855560302734
2018-10-22 02:13:00.850080:	Training iteration: 480600, Loss: 0.08310842514038086
2018-10-22 02:13:31.503037:	Training iteration: 480800, Loss: 0.09975120425224304
2018-10-22 02:14:00.668052:	Training iteration: 481000, Loss: 0.10804073512554169
2018-10-22 02:14:30.173639:	Training iteration: 481200, Loss: 0.12026199698448181
2018-10-22 02:14:59.559947:	Training iteration: 481400, Loss: 0.1026262566447258
2018-10-22 02:15:28.437452:	Training iteration: 481600, Loss: 0.08245930075645447
2018-10-22 02:15:57.419092:	Training iteration: 481800, Loss: 0.09594370424747467
2018-10-22 02:16:28.367035:	Training iteration: 482000, Loss: 0.11033939570188522
2018-10-22 02:16:56.813127:	Training iteration: 482200, Loss: 0.08392201364040375
2018-10-22 02:17:25.677557:	Training iteration: 482400, Loss: 0.1256938874721527
2018-10-22 02:17:55.269192:	Training iteration: 482600, Loss: 0.15807361900806427
2018-10-22 02:18:24.821266:	Training iteration: 482800, Loss: 0.1342218965291977
2018-10-22 02:18:55.103207:	Training iteration: 483000, Loss: 0.13697099685668945
2018-10-22 02:19:25.219935:	Training iteration: 483200, Loss: 0.1256558746099472
2018-10-22 02:19:54.517968:	Training iteration: 483400, Loss: 0.10502299666404724
2018-10-22 02:20:24.601600:	Training iteration: 483600, Loss: 0.0889931321144104
2018-10-22 02:20:54.203897:	Training iteration: 483800, Loss: 0.13016879558563232
2018-10-22 02:21:23.902784:	Training iteration: 484000, Loss: 0.13914436101913452
2018-10-22 02:21:53.273397:	Training iteration: 484200, Loss: 0.11639927327632904
2018-10-22 02:22:23.119939:	Training iteration: 484400, Loss: 0.11713872104883194
2018-10-22 02:22:52.580897:	Training iteration: 484600, Loss: 0.09805558621883392
2018-10-22 02:23:23.268490:	Training iteration: 484800, Loss: 0.10732146352529526
2018-10-22 02:23:52.490831:	Training iteration: 485000, Loss: 0.10756926238536835
2018-10-22 02:24:22.595469:	Training iteration: 485200, Loss: 0.12819607555866241
2018-10-22 02:24:52.461947:	Training iteration: 485400, Loss: 0.12716403603553772
2018-10-22 02:25:21.465808:	Training iteration: 485600, Loss: 0.15786577761173248
2018-10-22 02:25:50.233161:	Training iteration: 485800, Loss: 0.12682342529296875
2018-10-22 02:26:19.556577:	Training iteration: 486000, Loss: 0.11924804747104645
2018-10-22 02:26:49.024876:	Training iteration: 486200, Loss: 0.09607207775115967
2018-10-22 02:27:18.089960:	Training iteration: 486400, Loss: 0.13052250444889069
2018-10-22 02:27:47.686469:	Training iteration: 486600, Loss: 0.158988356590271
2018-10-22 02:28:17.967155:	Training iteration: 486800, Loss: 0.13453561067581177
2018-10-22 02:28:47.944833:	Training iteration: 487000, Loss: 0.10803232342004776
2018-10-22 02:29:17.074552:	Training iteration: 487200, Loss: 0.13198906183242798
2018-10-22 02:29:46.796708:	Training iteration: 487400, Loss: 0.1154613047838211
2018-10-22 02:30:15.736030:	Training iteration: 487600, Loss: 0.15259625017642975
2018-10-22 02:30:45.207762:	Training iteration: 487800, Loss: 0.13373228907585144
2018-10-22 02:31:14.817048:	Training iteration: 488000, Loss: 0.12548530101776123
2018-10-22 02:31:44.611481:	Training iteration: 488200, Loss: 0.14989414811134338
2018-10-22 02:32:15.140964:	Training iteration: 488400, Loss: 0.09682446718215942
2018-10-22 02:32:45.148753:	Training iteration: 488600, Loss: 0.12314781546592712
2018-10-22 02:33:14.391832:	Training iteration: 488800, Loss: 0.11226446181535721
2018-10-22 02:33:44.102570:	Training iteration: 489000, Loss: 0.13729920983314514
2018-10-22 02:34:13.676040:	Training iteration: 489200, Loss: 0.1329021155834198
2018-10-22 02:34:42.967300:	Training iteration: 489400, Loss: 0.14345036447048187
2018-10-22 02:35:14.581686:	Training iteration: 489600, Loss: 0.0939265638589859
2018-10-22 02:35:44.051192:	Training iteration: 489800, Loss: 0.15000581741333008
2018-10-22 02:36:14.248227:	Training iteration: 490000, Loss: 0.09728270769119263
2018-10-22 02:36:43.576058:	Training iteration: 490200, Loss: 0.0864594578742981
2018-10-22 02:37:13.442916:	Training iteration: 490400, Loss: 0.12365377694368362
2018-10-22 02:37:43.600986:	Training iteration: 490600, Loss: 0.13917043805122375
2018-10-22 02:38:13.316968:	Training iteration: 490800, Loss: 0.12843966484069824
2018-10-22 02:38:42.875871:	Training iteration: 491000, Loss: 0.14249077439308167
2018-10-22 02:39:12.901267:	Training iteration: 491200, Loss: 0.1311621218919754
2018-10-22 02:39:43.917786:	Training iteration: 491400, Loss: 0.10807555913925171
2018-10-22 02:40:13.944059:	Training iteration: 491600, Loss: 0.10940037667751312
2018-10-22 02:40:43.474103:	Training iteration: 491800, Loss: 0.1277363896369934
2018-10-22 02:41:13.577805:	Training iteration: 492000, Loss: 0.12561717629432678
2018-10-22 02:41:43.227197:	Training iteration: 492200, Loss: 0.1023731678724289
2018-10-22 02:41:53.288627: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 696 of 1000
2018-10-22 02:41:56.957472: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 02:42:25.869289:	Training iteration: 492400, Loss: 0.10196352750062943
2018-10-22 02:42:55.501288:	Training iteration: 492600, Loss: 0.088119737803936
2018-10-22 02:43:25.081714:	Training iteration: 492800, Loss: 0.13879474997520447
2018-10-22 02:43:55.415838:	Training iteration: 493000, Loss: 0.15717826783657074
2018-10-22 02:44:24.239530:	Training iteration: 493200, Loss: 0.10494065284729004
2018-10-22 02:44:53.783069:	Training iteration: 493400, Loss: 0.1016802191734314
2018-10-22 02:45:24.714342:	Training iteration: 493600, Loss: 0.21878710389137268
2018-10-22 02:45:54.375369:	Training iteration: 493800, Loss: 0.12046149373054504
2018-10-22 02:46:23.308264:	Training iteration: 494000, Loss: 0.10365565866231918
2018-10-22 02:46:53.486346:	Training iteration: 494200, Loss: 0.14803794026374817
2018-10-22 02:47:23.272576:	Training iteration: 494400, Loss: 0.10268665850162506
2018-10-22 02:47:53.034215:	Training iteration: 494600, Loss: 0.10696777701377869
2018-10-22 02:48:22.932001:	Training iteration: 494800, Loss: 0.1294991374015808
2018-10-22 02:48:51.979943:	Training iteration: 495000, Loss: 0.15348413586616516
2018-10-22 02:49:21.475489:	Training iteration: 495200, Loss: 0.1069103330373764
2018-10-22 02:49:50.638944:	Training iteration: 495400, Loss: 0.0788392722606659
2018-10-22 02:50:20.553322:	Training iteration: 495600, Loss: 0.13246093690395355
2018-10-22 02:50:49.374031:	Training iteration: 495800, Loss: 0.12245741486549377
2018-10-22 02:51:19.617889:	Training iteration: 496000, Loss: 0.13072532415390015
2018-10-22 02:51:51.013537:	Training iteration: 496200, Loss: 0.14230665564537048
2018-10-22 02:52:19.895122:	Training iteration: 496400, Loss: 0.13818980753421783
2018-10-22 02:52:49.441195:	Training iteration: 496600, Loss: 0.11831165850162506
2018-10-22 02:53:18.476480:	Training iteration: 496800, Loss: 0.12671932578086853
2018-10-22 02:53:48.147325:	Training iteration: 497000, Loss: 0.08528359234333038
2018-10-22 02:54:17.885466:	Training iteration: 497200, Loss: 0.1096382886171341
2018-10-22 02:54:47.110153:	Training iteration: 497400, Loss: 0.10511600226163864
2018-10-22 02:55:19.102234:	Training iteration: 497600, Loss: 0.12580007314682007
2018-10-22 02:55:48.694791:	Training iteration: 497800, Loss: 0.17380496859550476
2018-10-22 02:56:17.618911:	Training iteration: 498000, Loss: 0.11511284857988358
2018-10-22 02:56:47.420691:	Training iteration: 498200, Loss: 0.1838696002960205
2018-10-22 02:57:17.482595:	Training iteration: 498400, Loss: 0.11882805824279785
2018-10-22 02:57:47.985705:	Training iteration: 498600, Loss: 0.06257086247205734
2018-10-22 02:58:18.272924:	Training iteration: 498800, Loss: 0.10375579446554184
2018-10-22 02:58:47.580723:	Training iteration: 499000, Loss: 0.10595384240150452
2018-10-22 02:59:16.914644:	Training iteration: 499200, Loss: 0.08752721548080444
2018-10-22 02:59:47.106425:	Training iteration: 499400, Loss: 0.09963370859622955
2018-10-22 03:00:16.625685:	Training iteration: 499600, Loss: 0.11303680390119553
2018-10-22 03:00:45.949320:	Training iteration: 499800, Loss: 0.08588697761297226
2018-10-22 03:01:15.969141:	Training iteration: 500000, Loss: 0.13390237092971802
2018-10-22 03:01:46.663113:	Training iteration: 500200, Loss: 0.15512311458587646
2018-10-22 03:02:15.993051:	Training iteration: 500400, Loss: 0.13139936327934265
2018-10-22 03:02:46.254112:	Training iteration: 500600, Loss: 0.1341947317123413
2018-10-22 03:03:15.914790:	Training iteration: 500800, Loss: 0.13174529373645782
2018-10-22 03:03:45.481422:	Training iteration: 501000, Loss: 0.164109468460083
2018-10-22 03:04:14.737450:	Training iteration: 501200, Loss: 0.08951622992753983
2018-10-22 03:04:44.883015:	Training iteration: 501400, Loss: 0.10549971461296082
2018-10-22 03:05:14.401590:	Training iteration: 501600, Loss: 0.12942780554294586
2018-10-22 03:05:44.323062:	Training iteration: 501800, Loss: 0.18455252051353455
2018-10-22 03:06:13.897753:	Training iteration: 502000, Loss: 0.13042236864566803
2018-10-22 03:06:44.758207:	Training iteration: 502200, Loss: 0.13162365555763245
2018-10-22 03:07:14.187677:	Training iteration: 502400, Loss: 0.09327138960361481
2018-10-22 03:07:42.965834:	Training iteration: 502600, Loss: 0.14234474301338196
2018-10-22 03:08:12.446664:	Training iteration: 502800, Loss: 0.1254989206790924
2018-10-22 03:08:42.517892:	Training iteration: 503000, Loss: 0.15735116600990295
2018-10-22 03:09:13.044885:	Training iteration: 503200, Loss: 0.1397971361875534
2018-10-22 03:09:43.680533:	Training iteration: 503400, Loss: 0.15033496916294098
2018-10-22 03:10:12.910730:	Training iteration: 503600, Loss: 0.11789153516292572
2018-10-22 03:10:42.153172:	Training iteration: 503800, Loss: 0.09434901922941208
2018-10-22 03:11:11.863718:	Training iteration: 504000, Loss: 0.14479467272758484
2018-10-22 03:11:43.103719:	Training iteration: 504200, Loss: 0.08634063601493835
2018-10-22 03:12:12.603578:	Training iteration: 504400, Loss: 0.15194666385650635
2018-10-22 03:12:42.257966:	Training iteration: 504600, Loss: 0.07949014008045197
2018-10-22 03:13:02.228413: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 731 of 1000
2018-10-22 03:13:05.284900: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 03:13:24.738804:	Training iteration: 504800, Loss: 0.08310753107070923
2018-10-22 03:13:54.456831:	Training iteration: 505000, Loss: 0.1079162135720253
2018-10-22 03:14:23.702543:	Training iteration: 505200, Loss: 0.11296266317367554
2018-10-22 03:14:53.708829:	Training iteration: 505400, Loss: 0.09526494145393372
2018-10-22 03:15:23.641860:	Training iteration: 505600, Loss: 0.06639287620782852
2018-10-22 03:15:53.777214:	Training iteration: 505800, Loss: 0.1298915147781372
2018-10-22 03:16:23.253050:	Training iteration: 506000, Loss: 0.07846637070178986
2018-10-22 03:16:52.406279:	Training iteration: 506200, Loss: 0.10401767492294312
2018-10-22 03:17:22.683548:	Training iteration: 506400, Loss: 0.12313485145568848
2018-10-22 03:17:52.715393:	Training iteration: 506600, Loss: 0.10200788080692291
2018-10-22 03:18:22.106222:	Training iteration: 506800, Loss: 0.07548905909061432
2018-10-22 03:18:51.576654:	Training iteration: 507000, Loss: 0.11573053896427155
2018-10-22 03:19:21.817022:	Training iteration: 507200, Loss: 0.11350777745246887
2018-10-22 03:19:52.657423:	Training iteration: 507400, Loss: 0.10342669486999512
2018-10-22 03:20:21.843347:	Training iteration: 507600, Loss: 0.12486792355775833
2018-10-22 03:20:51.942971:	Training iteration: 507800, Loss: 0.0775584951043129
2018-10-22 03:21:21.352451:	Training iteration: 508000, Loss: 0.059452686458826065
2018-10-22 03:21:50.338440:	Training iteration: 508200, Loss: 0.07293227314949036
2018-10-22 03:22:21.117034:	Training iteration: 508400, Loss: 0.14090496301651
2018-10-22 03:22:50.024797:	Training iteration: 508600, Loss: 0.09118921309709549
2018-10-22 03:23:19.976010:	Training iteration: 508800, Loss: 0.11860650777816772
2018-10-22 03:23:49.942470:	Training iteration: 509000, Loss: 0.09378230571746826
2018-10-22 03:24:19.462349:	Training iteration: 509200, Loss: 0.07251624763011932
2018-10-22 03:24:49.408840:	Training iteration: 509400, Loss: 0.08301395922899246
2018-10-22 03:25:19.241698:	Training iteration: 509600, Loss: 0.12152084708213806
2018-10-22 03:25:50.707295:	Training iteration: 509800, Loss: 0.09529456496238708
2018-10-22 03:26:20.092365:	Training iteration: 510000, Loss: 0.12010879814624786
2018-10-22 03:26:49.938346:	Training iteration: 510200, Loss: 0.10019145160913467
2018-10-22 03:27:19.704380:	Training iteration: 510400, Loss: 0.11508636176586151
2018-10-22 03:27:49.906169:	Training iteration: 510600, Loss: 0.09214843809604645
2018-10-22 03:28:19.282327:	Training iteration: 510800, Loss: 0.11342733353376389
2018-10-22 03:28:49.268164:	Training iteration: 511000, Loss: 0.13476680219173431
2018-10-22 03:29:19.155338:	Training iteration: 511200, Loss: 0.12761938571929932
2018-10-22 03:29:48.689974:	Training iteration: 511400, Loss: 0.07064665108919144
2018-10-22 03:30:18.129491:	Training iteration: 511600, Loss: 0.07536657154560089
2018-10-22 03:30:47.942818:	Training iteration: 511800, Loss: 0.09452313184738159
2018-10-22 03:31:17.627192:	Training iteration: 512000, Loss: 0.09723825752735138
2018-10-22 03:31:49.314210:	Training iteration: 512200, Loss: 0.05905391275882721
2018-10-22 03:32:18.288237:	Training iteration: 512400, Loss: 0.1159028634428978
2018-10-22 03:32:47.664613:	Training iteration: 512600, Loss: 0.06648494303226471
2018-10-22 03:33:17.300943:	Training iteration: 512800, Loss: 0.14029929041862488
2018-10-22 03:33:47.776213:	Training iteration: 513000, Loss: 0.07952955365180969
2018-10-22 03:34:16.591928:	Training iteration: 513200, Loss: 0.08730047941207886
2018-10-22 03:34:45.554917:	Training iteration: 513400, Loss: 0.06598284840583801
2018-10-22 03:35:15.480649:	Training iteration: 513600, Loss: 0.15031489729881287
2018-10-22 03:35:45.404401:	Training iteration: 513800, Loss: 0.10783883184194565
2018-10-22 03:36:14.957406:	Training iteration: 514000, Loss: 0.11801072955131531
2018-10-22 03:36:43.486194:	Training iteration: 514200, Loss: 0.08898340910673141
2018-10-22 03:37:12.532545:	Training iteration: 514400, Loss: 0.0748211145401001
2018-10-22 03:37:42.654383:	Training iteration: 514600, Loss: 0.10318370163440704
2018-10-22 03:38:12.373531:	Training iteration: 514800, Loss: 0.07917995750904083
2018-10-22 03:38:40.863334:	Training iteration: 515000, Loss: 0.12998977303504944
2018-10-22 03:39:10.765727:	Training iteration: 515200, Loss: 0.14100517332553864
2018-10-22 03:39:40.215674:	Training iteration: 515400, Loss: 0.08194702863693237
2018-10-22 03:40:09.890527:	Training iteration: 515600, Loss: 0.11283618956804276
2018-10-22 03:40:39.212045:	Training iteration: 515800, Loss: 0.06305571645498276
2018-10-22 03:41:08.110750:	Training iteration: 516000, Loss: 0.09788496792316437
2018-10-22 03:41:37.735795:	Training iteration: 516200, Loss: 0.11778108775615692
2018-10-22 03:42:07.299026:	Training iteration: 516400, Loss: 0.09995197504758835
2018-10-22 03:42:37.429667:	Training iteration: 516600, Loss: 0.10126698762178421
2018-10-22 03:43:07.758859:	Training iteration: 516800, Loss: 0.10279997438192368
2018-10-22 03:43:37.541990:	Training iteration: 517000, Loss: 0.11250938475131989
2018-10-22 03:44:06.494625: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 738 of 1000
2018-10-22 03:44:09.672589: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 03:44:20.559512:	Training iteration: 517200, Loss: 0.10937446355819702
2018-10-22 03:44:49.566987:	Training iteration: 517400, Loss: 0.05423178896307945
2018-10-22 03:45:18.770206:	Training iteration: 517600, Loss: 0.12715208530426025
2018-10-22 03:45:48.479345:	Training iteration: 517800, Loss: 0.17782428860664368
2018-10-22 03:46:17.379348:	Training iteration: 518000, Loss: 0.20711223781108856
2018-10-22 03:46:46.845154:	Training iteration: 518200, Loss: 0.09527075290679932
2018-10-22 03:47:16.113146:	Training iteration: 518400, Loss: 0.1418793797492981
2018-10-22 03:47:45.364025:	Training iteration: 518600, Loss: 0.14119982719421387
2018-10-22 03:48:15.802313:	Training iteration: 518800, Loss: 0.24255937337875366
2018-10-22 03:48:45.004735:	Training iteration: 519000, Loss: 0.1519068032503128
2018-10-22 03:49:15.278248:	Training iteration: 519200, Loss: 0.14486312866210938
2018-10-22 03:49:44.495456:	Training iteration: 519400, Loss: 0.1101125031709671
2018-10-22 03:50:14.424082:	Training iteration: 519600, Loss: 0.10691361129283905
2018-10-22 03:50:43.720228:	Training iteration: 519800, Loss: 0.15305423736572266
2018-10-22 03:51:14.531514:	Training iteration: 520000, Loss: 0.192708358168602
2018-10-22 03:51:43.505276:	Training iteration: 520200, Loss: 0.186201810836792
2018-10-22 03:52:13.859198:	Training iteration: 520400, Loss: 0.07390178740024567
2018-10-22 03:52:43.390346:	Training iteration: 520600, Loss: 0.11996887624263763
2018-10-22 03:53:13.475427:	Training iteration: 520800, Loss: 0.19843265414237976
2018-10-22 03:53:43.039670:	Training iteration: 521000, Loss: 0.1167810931801796
2018-10-22 03:54:12.346838:	Training iteration: 521200, Loss: 0.11256281286478043
2018-10-22 03:54:41.895997:	Training iteration: 521400, Loss: 0.13617366552352905
2018-10-22 03:55:11.304957:	Training iteration: 521600, Loss: 0.1281019151210785
2018-10-22 03:55:40.709323:	Training iteration: 521800, Loss: 0.17865532636642456
2018-10-22 03:56:11.024506:	Training iteration: 522000, Loss: 0.11710677295923233
2018-10-22 03:56:39.712148:	Training iteration: 522200, Loss: 0.1658172905445099
2018-10-22 03:57:09.245773:	Training iteration: 522400, Loss: 0.0897664874792099
2018-10-22 03:57:39.733944:	Training iteration: 522600, Loss: 0.15615971386432648
2018-10-22 03:58:10.492994:	Training iteration: 522800, Loss: 0.1331278383731842
2018-10-22 03:58:39.728028:	Training iteration: 523000, Loss: 0.10945014655590057
2018-10-22 03:59:08.900052:	Training iteration: 523200, Loss: 0.11824589967727661
2018-10-22 03:59:38.664136:	Training iteration: 523400, Loss: 0.21612128615379333
2018-10-22 04:00:08.291834:	Training iteration: 523600, Loss: 0.16613253951072693
2018-10-22 04:00:38.525982:	Training iteration: 523800, Loss: 0.12830211222171783
2018-10-22 04:01:09.834154:	Training iteration: 524000, Loss: 0.14501306414604187
2018-10-22 04:01:39.982350:	Training iteration: 524200, Loss: 0.09981749206781387
2018-10-22 04:02:12.942602:	Training iteration: 524400, Loss: 0.07095223665237427
2018-10-22 04:02:43.132368:	Training iteration: 524600, Loss: 0.13819442689418793
2018-10-22 04:03:12.352436:	Training iteration: 524800, Loss: 0.12056376785039902
2018-10-22 04:03:42.002875:	Training iteration: 525000, Loss: 0.16045038402080536
2018-10-22 04:04:12.092019:	Training iteration: 525200, Loss: 0.11640064418315887
2018-10-22 04:04:42.036628:	Training iteration: 525400, Loss: 0.11608731746673584
2018-10-22 04:05:11.634816:	Training iteration: 525600, Loss: 0.13926106691360474
2018-10-22 04:05:41.677172:	Training iteration: 525800, Loss: 0.12426884472370148
2018-10-22 04:06:13.536902:	Training iteration: 526000, Loss: 0.08246174454689026
2018-10-22 04:06:43.390542:	Training iteration: 526200, Loss: 0.07859699428081512
2018-10-22 04:07:13.147144:	Training iteration: 526400, Loss: 0.1069113165140152
2018-10-22 04:07:42.894218:	Training iteration: 526600, Loss: 0.14982551336288452
2018-10-22 04:08:12.690905:	Training iteration: 526800, Loss: 0.1142413318157196
2018-10-22 04:08:42.947648:	Training iteration: 527000, Loss: 0.155035138130188
2018-10-22 04:09:12.767392:	Training iteration: 527200, Loss: 0.1939019113779068
2018-10-22 04:09:41.475475:	Training iteration: 527400, Loss: 0.20195508003234863
2018-10-22 04:10:11.755538:	Training iteration: 527600, Loss: 0.10958124697208405
2018-10-22 04:10:42.083576:	Training iteration: 527800, Loss: 0.10488758236169815
2018-10-22 04:11:12.276315:	Training iteration: 528000, Loss: 0.18943636119365692
2018-10-22 04:11:42.732311:	Training iteration: 528200, Loss: 0.14010240137577057
2018-10-22 04:12:12.661142:	Training iteration: 528400, Loss: 0.13245221972465515
2018-10-22 04:12:42.498893:	Training iteration: 528600, Loss: 0.15879657864570618
2018-10-22 04:13:12.186107:	Training iteration: 528800, Loss: 0.1583002507686615
2018-10-22 04:13:41.330537:	Training iteration: 529000, Loss: 0.09652251750230789
2018-10-22 04:14:10.964565:	Training iteration: 529200, Loss: 0.11419254541397095
2018-10-22 04:14:39.992692:	Training iteration: 529400, Loss: 0.12039870023727417
2018-10-22 04:15:08.907441:	Training iteration: 529600, Loss: 0.08716829121112823
2018-10-22 04:15:38.646956:	Training iteration: 529800, Loss: 0.062062181532382965
2018-10-22 04:16:07.989473:	Training iteration: 530000, Loss: 0.11382755637168884
2018-10-22 04:16:39.652952: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 872 of 1000
2018-10-22 04:16:41.099006: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 04:16:48.737729:	Training iteration: 530200, Loss: 0.17820695042610168
2018-10-22 04:17:17.605540:	Training iteration: 530400, Loss: 0.17311832308769226
2018-10-22 04:17:46.429787:	Training iteration: 530600, Loss: 0.16078194975852966
2018-10-22 04:18:16.803082:	Training iteration: 530800, Loss: 0.13254514336585999
2018-10-22 04:18:45.903859:	Training iteration: 531000, Loss: 0.318049818277359
2018-10-22 04:19:15.373501:	Training iteration: 531200, Loss: 0.09187349677085876
2018-10-22 04:19:44.603997:	Training iteration: 531400, Loss: 0.1615748107433319
2018-10-22 04:20:14.929415:	Training iteration: 531600, Loss: 0.09915942698717117
2018-10-22 04:20:45.170057:	Training iteration: 531800, Loss: 0.14924675226211548
2018-10-22 04:21:15.281782:	Training iteration: 532000, Loss: 0.13452574610710144
2018-10-22 04:21:45.404098:	Training iteration: 532200, Loss: 0.10869131237268448
2018-10-22 04:22:14.737768:	Training iteration: 532400, Loss: 0.15226039290428162
2018-10-22 04:22:44.028620:	Training iteration: 532600, Loss: 0.16837695240974426
2018-10-22 04:23:14.363996:	Training iteration: 532800, Loss: 0.1168174296617508
2018-10-22 04:23:43.510521:	Training iteration: 533000, Loss: 0.15390700101852417
2018-10-22 04:24:13.566966:	Training iteration: 533200, Loss: 0.11928783357143402
2018-10-22 04:24:43.810596:	Training iteration: 533400, Loss: 0.15412229299545288
2018-10-22 04:25:13.462520:	Training iteration: 533600, Loss: 0.18903592228889465
2018-10-22 04:25:42.723885:	Training iteration: 533800, Loss: 0.15169590711593628
2018-10-22 04:26:11.287603:	Training iteration: 534000, Loss: 0.10229352116584778
2018-10-22 04:26:41.638786:	Training iteration: 534200, Loss: 0.15616680681705475
2018-10-22 04:27:11.204696:	Training iteration: 534400, Loss: 0.15741762518882751
2018-10-22 04:27:40.600942:	Training iteration: 534600, Loss: 0.10682396590709686
2018-10-22 04:28:10.130972:	Training iteration: 534800, Loss: 0.13445106148719788
2018-10-22 04:28:39.277240:	Training iteration: 535000, Loss: 0.1658334732055664
2018-10-22 04:29:09.030627:	Training iteration: 535200, Loss: 0.15803061425685883
2018-10-22 04:29:37.683839:	Training iteration: 535400, Loss: 0.14525562524795532
2018-10-22 04:30:07.877895:	Training iteration: 535600, Loss: 0.11988344043493271
2018-10-22 04:30:37.740404:	Training iteration: 535800, Loss: 0.2118712067604065
2018-10-22 04:31:08.090486:	Training iteration: 536000, Loss: 0.1578361690044403
2018-10-22 04:31:37.605065:	Training iteration: 536200, Loss: 0.16862675547599792
2018-10-22 04:32:07.667438:	Training iteration: 536400, Loss: 0.12372526526451111
2018-10-22 04:32:37.619434:	Training iteration: 536600, Loss: 0.21948477625846863
2018-10-22 04:33:07.430028:	Training iteration: 536800, Loss: 0.16289225220680237
2018-10-22 04:33:37.022829:	Training iteration: 537000, Loss: 0.15130680799484253
2018-10-22 04:34:05.818157:	Training iteration: 537200, Loss: 0.18082433938980103
2018-10-22 04:34:35.327541:	Training iteration: 537400, Loss: 0.2203703224658966
2018-10-22 04:35:05.645241:	Training iteration: 537600, Loss: 0.14998814463615417
2018-10-22 04:35:35.809880:	Training iteration: 537800, Loss: 0.2174128144979477
2018-10-22 04:36:05.919667:	Training iteration: 538000, Loss: 0.19704370200634003
2018-10-22 04:36:35.423411:	Training iteration: 538200, Loss: 0.19235965609550476
2018-10-22 04:37:05.645590:	Training iteration: 538400, Loss: 0.12155399471521378
2018-10-22 04:37:35.754888:	Training iteration: 538600, Loss: 0.16312837600708008
2018-10-22 04:38:06.318554:	Training iteration: 538800, Loss: 0.14669767022132874
2018-10-22 04:38:35.702095:	Training iteration: 539000, Loss: 0.1464671492576599
2018-10-22 04:39:05.834117:	Training iteration: 539200, Loss: 0.26463234424591064
2018-10-22 04:39:35.913033:	Training iteration: 539400, Loss: 0.15504884719848633
2018-10-22 04:40:04.654529:	Training iteration: 539600, Loss: 0.1828116923570633
2018-10-22 04:40:33.826657:	Training iteration: 539800, Loss: 0.17450593411922455
2018-10-22 04:41:03.845227:	Training iteration: 540000, Loss: 0.13419219851493835
2018-10-22 04:41:33.803249:	Training iteration: 540200, Loss: 0.18784403800964355
2018-10-22 04:42:02.949092:	Training iteration: 540400, Loss: 0.11788469552993774
2018-10-22 04:42:32.463774:	Training iteration: 540600, Loss: 0.17486479878425598
2018-10-22 04:43:02.553962:	Training iteration: 540800, Loss: 0.15021812915802002
2018-10-22 04:43:32.379215:	Training iteration: 541000, Loss: 0.12063311040401459
2018-10-22 04:44:02.029377:	Training iteration: 541200, Loss: 0.14646872878074646
2018-10-22 04:44:32.180211:	Training iteration: 541400, Loss: 0.13722772896289825
2018-10-22 04:45:02.107345:	Training iteration: 541600, Loss: 0.20977282524108887
2018-10-22 04:45:32.032699:	Training iteration: 541800, Loss: 0.13419252634048462
2018-10-22 04:46:02.163266:	Training iteration: 542000, Loss: 0.16440968215465546
2018-10-22 04:46:31.694056:	Training iteration: 542200, Loss: 0.13747966289520264
2018-10-22 04:47:02.610699:	Training iteration: 542400, Loss: 0.1479913294315338
2018-10-22 04:47:32.246469:	Training iteration: 542600, Loss: 0.10008087754249573
2018-10-22 04:48:01.729912:	Training iteration: 542800, Loss: 0.21693167090415955
2018-10-22 04:48:31.362119:	Training iteration: 543000, Loss: 0.13824453949928284
2018-10-22 04:49:00.946026:	Training iteration: 543200, Loss: 0.127037912607193
2018-10-22 04:49:30.057031:	Training iteration: 543400, Loss: 0.1259918510913849
2018-10-22 04:49:59.760078:	Training iteration: 543600, Loss: 0.15420059859752655
2018-10-22 04:50:29.337472:	Training iteration: 543800, Loss: 0.12040109932422638
2018-10-22 04:50:58.548159:	Training iteration: 544000, Loss: 0.2610158920288086
2018-10-22 04:51:28.138349:	Training iteration: 544200, Loss: 0.17846567928791046
2018-10-22 04:51:57.739420:	Training iteration: 544400, Loss: 0.17225374281406403
2018-10-22 04:52:27.357570:	Training iteration: 544600, Loss: 0.1758929044008255
2018-10-22 04:52:57.685744:	Training iteration: 544800, Loss: 0.16221383213996887
2018-10-22 04:53:27.836732:	Training iteration: 545000, Loss: 0.14791962504386902
2018-10-22 04:53:57.871109:	Training iteration: 545200, Loss: 0.13055329024791718
2018-10-22 04:54:28.480274:	Training iteration: 545400, Loss: 0.113935686647892
2018-10-22 04:54:58.727528:	Training iteration: 545600, Loss: 0.12852080166339874
2018-10-22 04:55:28.813898:	Training iteration: 545800, Loss: 0.1602156013250351
2018-10-22 04:55:57.799152:	Training iteration: 546000, Loss: 0.20472808182239532
2018-10-22 04:56:28.047413:	Training iteration: 546200, Loss: 0.2318423092365265
2018-10-22 04:56:57.012504:	Training iteration: 546400, Loss: 0.19081951677799225
2018-10-22 04:57:26.692028:	Training iteration: 546600, Loss: 0.25874263048171997
2018-10-22 04:57:56.433580:	Training iteration: 546800, Loss: 0.18275442719459534
2018-10-22 04:58:26.394770:	Training iteration: 547000, Loss: 0.18965931236743927
2018-10-22 04:58:55.930532:	Training iteration: 547200, Loss: 0.16204582154750824
2018-10-22 04:59:25.899725:	Training iteration: 547400, Loss: 0.18849830329418182
2018-10-22 04:59:55.518700:	Training iteration: 547600, Loss: 0.17930757999420166
2018-10-22 05:00:24.057031:	Training iteration: 547800, Loss: 0.13947725296020508
2018-10-22 05:00:54.360063:	Training iteration: 548000, Loss: 0.15470317006111145
2018-10-22 05:01:23.978203:	Training iteration: 548200, Loss: 0.17405523359775543
2018-10-22 05:01:54.338491:	Training iteration: 548400, Loss: 0.1312309205532074
2018-10-22 05:02:24.072330:	Training iteration: 548600, Loss: 0.13112911581993103
2018-10-22 05:02:54.441480:	Training iteration: 548800, Loss: 0.18576431274414062
2018-10-22 05:03:24.260881:	Training iteration: 549000, Loss: 0.15007054805755615
2018-10-22 05:03:54.108117:	Training iteration: 549200, Loss: 0.09214341640472412
2018-10-22 05:04:23.624581:	Training iteration: 549400, Loss: 0.13452014327049255
2018-10-22 05:04:52.840183:	Training iteration: 549600, Loss: 0.15016192197799683
2018-10-22 05:05:22.248323:	Training iteration: 549800, Loss: 0.15372291207313538
2018-10-22 05:05:51.742723:	Training iteration: 550000, Loss: 0.09372429549694061
2018-10-22 05:06:21.108136:	Training iteration: 550200, Loss: 0.1714099496603012
2018-10-22 05:06:50.524376:	Training iteration: 550400, Loss: 0.13331690430641174
2018-10-22 05:07:20.299097:	Training iteration: 550600, Loss: 0.1809823215007782
2018-10-22 05:07:50.286474:	Training iteration: 550800, Loss: 0.20291082561016083
2018-10-22 05:08:19.825796:	Training iteration: 551000, Loss: 0.16854628920555115
2018-10-22 05:08:49.253676:	Training iteration: 551200, Loss: 0.1886560469865799
2018-10-22 05:09:19.617236:	Training iteration: 551400, Loss: 0.18860724568367004
2018-10-22 05:09:49.613186:	Training iteration: 551600, Loss: 0.12449813634157181
2018-10-22 05:10:19.514144:	Training iteration: 551800, Loss: 0.10357505083084106
2018-10-22 05:10:49.963339:	Training iteration: 552000, Loss: 0.14834988117218018
2018-10-22 05:11:19.744749:	Training iteration: 552200, Loss: 0.17671038210391998
2018-10-22 05:11:49.032614:	Training iteration: 552400, Loss: 0.16038140654563904
2018-10-22 05:12:17.973598:	Training iteration: 552600, Loss: 0.17822016775608063
2018-10-22 05:12:48.181722:	Training iteration: 552800, Loss: 0.23956158757209778
2018-10-22 05:13:18.569062:	Training iteration: 553000, Loss: 0.13056546449661255
2018-10-22 05:13:48.228233:	Training iteration: 553200, Loss: 0.2302265167236328
2018-10-22 05:14:18.211066:	Training iteration: 553400, Loss: 0.16620519757270813
2018-10-22 05:14:47.914426:	Training iteration: 553600, Loss: 0.14302048087120056
2018-10-22 05:15:17.426606:	Training iteration: 553800, Loss: 0.13541734218597412
2018-10-22 05:15:47.671859:	Training iteration: 554000, Loss: 0.21435140073299408
2018-10-22 05:16:17.441866:	Training iteration: 554200, Loss: 0.1618053913116455
2018-10-22 05:16:47.040084:	Training iteration: 554400, Loss: 0.1929832547903061
2018-10-22 05:17:16.580179:	Training iteration: 554600, Loss: 0.1478160172700882
2018-10-22 05:17:46.552091:	Training iteration: 554800, Loss: 0.1398770809173584
2018-10-22 05:18:16.000339:	Training iteration: 555000, Loss: 0.13755421340465546
2018-10-22 05:18:45.281392:	Training iteration: 555200, Loss: 0.15435653924942017
2018-10-22 05:19:15.247133:	Training iteration: 555400, Loss: 0.20671001076698303
2018-10-22 05:19:45.007455:	Training iteration: 555600, Loss: 0.13086988031864166
2018-10-22 05:20:15.107401:	Training iteration: 555800, Loss: 0.13003642857074738
2018-10-22 05:20:44.981318:	Training iteration: 556000, Loss: 0.1380111426115036
2018-10-22 05:21:14.296026:	Training iteration: 556200, Loss: 0.13066819310188293
2018-10-22 05:21:43.103508:	Training iteration: 556400, Loss: 0.15682265162467957
2018-10-22 05:22:12.348742:	Training iteration: 556600, Loss: 0.16181620955467224
2018-10-22 05:22:41.450652:	Training iteration: 556800, Loss: 0.14839643239974976
2018-10-22 05:23:11.272661:	Training iteration: 557000, Loss: 0.13925516605377197
2018-10-22 05:23:40.650958:	Training iteration: 557200, Loss: 0.12326827645301819
2018-10-22 05:24:10.296034:	Training iteration: 557400, Loss: 0.1222238689661026
2018-10-22 05:24:40.102017:	Training iteration: 557600, Loss: 0.13076704740524292
2018-10-22 05:25:10.341171:	Training iteration: 557800, Loss: 0.11929446458816528
2018-10-22 05:25:39.972744:	Training iteration: 558000, Loss: 0.10827388614416122
2018-10-22 05:26:08.989208:	Training iteration: 558200, Loss: 0.12322431802749634
2018-10-22 05:26:39.146721:	Training iteration: 558400, Loss: 0.17577219009399414
2018-10-22 05:27:08.830376:	Training iteration: 558600, Loss: 0.1738503873348236
2018-10-22 05:27:37.656396:	Training iteration: 558800, Loss: 0.18722113966941833
2018-10-22 05:28:08.532603:	Training iteration: 559000, Loss: 0.17917536199092865
2018-10-22 05:28:38.076526:	Training iteration: 559200, Loss: 0.14592275023460388
2018-10-22 05:29:07.825437:	Training iteration: 559400, Loss: 0.20105214416980743
2018-10-22 05:29:37.160079:	Training iteration: 559600, Loss: 0.2439941167831421
2018-10-22 05:30:06.328001:	Training iteration: 559800, Loss: 0.15430283546447754
2018-10-22 05:30:35.872018:	Training iteration: 560000, Loss: 0.14609092473983765
2018-10-22 05:31:05.905273:	Training iteration: 560200, Loss: 0.17027322947978973
2018-10-22 05:31:35.422879:	Training iteration: 560400, Loss: 0.12870971858501434
2018-10-22 05:32:05.567023:	Training iteration: 560600, Loss: 0.09947448968887329
2018-10-22 05:32:35.607276:	Training iteration: 560800, Loss: 0.1768859177827835
2018-10-22 05:33:05.581485:	Training iteration: 561000, Loss: 0.18190979957580566
2018-10-22 05:33:35.354340:	Training iteration: 561200, Loss: 0.1455724537372589
2018-10-22 05:34:04.757884:	Training iteration: 561400, Loss: 0.14325685799121857
2018-10-22 05:34:34.668868:	Training iteration: 561600, Loss: 0.17456035315990448
2018-10-22 05:35:03.878638:	Training iteration: 561800, Loss: 0.15101435780525208
2018-10-22 05:35:33.507951:	Training iteration: 562000, Loss: 0.15877291560173035
2018-10-22 05:36:02.788619:	Training iteration: 562200, Loss: 0.1506398618221283
2018-10-22 05:36:31.572848:	Training iteration: 562400, Loss: 0.14155486226081848
2018-10-22 05:37:00.996645:	Training iteration: 562600, Loss: 0.20156054198741913
2018-10-22 05:37:31.590199:	Training iteration: 562800, Loss: 0.1555982083082199
2018-10-22 05:38:00.922482:	Training iteration: 563000, Loss: 0.16262227296829224
2018-10-22 05:38:30.286764:	Training iteration: 563200, Loss: 0.20889723300933838
2018-10-22 05:39:00.249269:	Training iteration: 563400, Loss: 0.16105619072914124
2018-10-22 05:39:29.763856:	Training iteration: 563600, Loss: 0.11550819128751755
2018-10-22 05:39:59.979383:	Training iteration: 563800, Loss: 0.21128404140472412
2018-10-22 05:40:29.983158:	Training iteration: 564000, Loss: 0.13259369134902954
2018-10-22 05:40:58.641659:	Training iteration: 564200, Loss: 0.18912939727306366
2018-10-22 05:41:26.914262:	Training iteration: 564400, Loss: 0.1266585737466812
2018-10-22 05:41:57.539936:	Training iteration: 564600, Loss: 0.15528450906276703
2018-10-22 05:42:27.426415:	Training iteration: 564800, Loss: 0.14546512067317963
2018-10-22 05:42:57.117971:	Training iteration: 565000, Loss: 0.18951323628425598
2018-10-22 05:43:26.844220:	Training iteration: 565200, Loss: 0.1580199897289276
2018-10-22 05:43:57.346994:	Training iteration: 565400, Loss: 0.1946435421705246
2018-10-22 05:44:27.144844:	Training iteration: 565600, Loss: 0.1889311671257019
2018-10-22 05:44:56.993719:	Training iteration: 565800, Loss: 0.14963796734809875
2018-10-22 05:45:26.202421:	Training iteration: 566000, Loss: 0.19553978741168976
2018-10-22 05:45:55.575629:	Training iteration: 566200, Loss: 0.17487120628356934
2018-10-22 05:46:25.736646:	Training iteration: 566400, Loss: 0.18571269512176514
2018-10-22 05:46:55.444776:	Training iteration: 566600, Loss: 0.1404171884059906
2018-10-22 05:47:25.368352:	Training iteration: 566800, Loss: 0.16291062533855438
2018-10-22 05:47:55.360283:	Training iteration: 567000, Loss: 0.15844902396202087
2018-10-22 05:48:25.805508:	Training iteration: 567200, Loss: 0.14551833271980286
2018-10-22 05:48:55.998360:	Training iteration: 567400, Loss: 0.17551589012145996
2018-10-22 05:49:25.316817:	Training iteration: 567600, Loss: 0.16483116149902344
2018-10-22 05:49:55.184290:	Training iteration: 567800, Loss: 0.16161686182022095
2018-10-22 05:50:24.961964:	Training iteration: 568000, Loss: 0.18274438381195068
2018-10-22 05:50:54.525475:	Training iteration: 568200, Loss: 0.1724909543991089
2018-10-22 05:51:23.583903:	Training iteration: 568400, Loss: 0.1931288242340088
2018-10-22 05:51:53.472129:	Training iteration: 568600, Loss: 0.16485944390296936
2018-10-22 05:52:22.838164:	Training iteration: 568800, Loss: 0.1988706886768341
2018-10-22 05:52:52.572196:	Training iteration: 569000, Loss: 0.14363861083984375
2018-10-22 05:53:22.458524:	Training iteration: 569200, Loss: 0.19947931170463562
2018-10-22 05:53:52.169426:	Training iteration: 569400, Loss: 0.1889476478099823
2018-10-22 05:54:21.206104:	Training iteration: 569600, Loss: 0.16698652505874634
2018-10-22 05:54:50.294337:	Training iteration: 569800, Loss: 0.1821201741695404
2018-10-22 05:55:19.809643:	Training iteration: 570000, Loss: 0.13195502758026123
2018-10-22 05:55:49.773962:	Training iteration: 570200, Loss: 0.17714864015579224
2018-10-22 05:56:19.513824:	Training iteration: 570400, Loss: 0.16300420463085175
2018-10-22 05:56:48.922013:	Training iteration: 570600, Loss: 0.15093551576137543
2018-10-22 05:57:17.806722:	Training iteration: 570800, Loss: 0.12146230041980743
2018-10-22 05:57:46.481894:	Training iteration: 571000, Loss: 0.1708926558494568
2018-10-22 05:58:15.955198:	Training iteration: 571200, Loss: 0.10099142789840698
2018-10-22 05:58:45.495431:	Training iteration: 571400, Loss: 0.24018776416778564
2018-10-22 05:59:15.358092:	Training iteration: 571600, Loss: 0.14692158997058868
2018-10-22 05:59:44.869687:	Training iteration: 571800, Loss: 0.15154577791690826
2018-10-22 06:00:15.408614:	Training iteration: 572000, Loss: 0.14326830208301544
2018-10-22 06:00:45.159444:	Training iteration: 572200, Loss: 0.1266745924949646
2018-10-22 06:01:15.645770:	Training iteration: 572400, Loss: 0.12221818417310715
2018-10-22 06:01:45.348188:	Training iteration: 572600, Loss: 0.16165128350257874
2018-10-22 06:02:14.339411:	Training iteration: 572800, Loss: 0.12523238360881805
2018-10-22 06:02:43.776616:	Training iteration: 573000, Loss: 0.15341639518737793
2018-10-22 06:03:13.857453:	Training iteration: 573200, Loss: 0.15131843090057373
2018-10-22 06:03:43.607587:	Training iteration: 573400, Loss: 0.13303926587104797
2018-10-22 06:04:13.010334:	Training iteration: 573600, Loss: 0.14970968663692474
2018-10-22 06:04:43.117691:	Training iteration: 573800, Loss: 0.410519540309906
2018-10-22 06:05:12.917258:	Training iteration: 574000, Loss: 0.15208297967910767
2018-10-22 06:05:41.823873:	Training iteration: 574200, Loss: 0.18095234036445618
2018-10-22 06:06:11.353576:	Training iteration: 574400, Loss: 0.205088809132576
2018-10-22 06:06:40.577878:	Training iteration: 574600, Loss: 0.12118010222911835
2018-10-22 06:07:09.429216:	Training iteration: 574800, Loss: 0.14964376389980316
2018-10-22 06:07:38.780583:	Training iteration: 575000, Loss: 0.12769633531570435
2018-10-22 06:08:08.643168:	Training iteration: 575200, Loss: 0.12045777589082718
2018-10-22 06:08:37.875236:	Training iteration: 575400, Loss: 0.186165452003479
2018-10-22 06:09:07.508618:	Training iteration: 575600, Loss: 0.21799790859222412
2018-10-22 06:09:37.241764:	Training iteration: 575800, Loss: 0.17929449677467346
2018-10-22 06:10:06.502233:	Training iteration: 576000, Loss: 0.14110758900642395
2018-10-22 06:10:35.881578:	Training iteration: 576200, Loss: 0.13635313510894775
2018-10-22 06:11:04.999306:	Training iteration: 576400, Loss: 0.13074737787246704
2018-10-22 06:11:34.433900:	Training iteration: 576600, Loss: 0.15632647275924683
2018-10-22 06:12:05.183430:	Training iteration: 576800, Loss: 0.11756311357021332
2018-10-22 06:12:35.137769:	Training iteration: 577000, Loss: 0.13463079929351807
2018-10-22 06:13:04.655338:	Training iteration: 577200, Loss: 0.14653673768043518
2018-10-22 06:13:34.052541:	Training iteration: 577400, Loss: 0.14016114175319672
2018-10-22 06:14:03.951696:	Training iteration: 577600, Loss: 0.10143771767616272
2018-10-22 06:14:33.740166:	Training iteration: 577800, Loss: 0.1629350483417511
2018-10-22 06:15:03.551006:	Training iteration: 578000, Loss: 0.12870541214942932
2018-10-22 06:15:32.377521:	Training iteration: 578200, Loss: 0.19053633511066437
2018-10-22 06:16:01.828533:	Training iteration: 578400, Loss: 0.13232174515724182
2018-10-22 06:16:31.663310:	Training iteration: 578600, Loss: 0.15836556255817413
2018-10-22 06:17:02.247032:	Training iteration: 578800, Loss: 0.15343192219734192
2018-10-22 06:17:31.969610:	Training iteration: 579000, Loss: 0.13169093430042267
2018-10-22 06:18:01.382893:	Training iteration: 579200, Loss: 0.13216213881969452
2018-10-22 06:18:31.151583:	Training iteration: 579400, Loss: 0.147828608751297
2018-10-22 06:19:00.621823:	Training iteration: 579600, Loss: 0.1850074827671051
2018-10-22 06:19:29.868587:	Training iteration: 579800, Loss: 0.16407740116119385
2018-10-22 06:19:59.686839:	Training iteration: 580000, Loss: 0.17195455729961395
2018-10-22 06:20:29.432621:	Training iteration: 580200, Loss: 0.19670239090919495
2018-10-22 06:20:59.777276:	Training iteration: 580400, Loss: 0.18829087913036346
2018-10-22 06:21:29.120431:	Training iteration: 580600, Loss: 0.13596123456954956
2018-10-22 06:21:58.524526:	Training iteration: 580800, Loss: 0.15755711495876312
2018-10-22 06:22:28.149163:	Training iteration: 581000, Loss: 0.17302224040031433
2018-10-22 06:22:57.982769:	Training iteration: 581200, Loss: 0.13171744346618652
2018-10-22 06:23:28.702107:	Training iteration: 581400, Loss: 0.20773117244243622
2018-10-22 06:23:58.571795:	Training iteration: 581600, Loss: 0.11790475994348526
2018-10-22 06:24:28.367893:	Training iteration: 581800, Loss: 0.2232019603252411
2018-10-22 06:24:57.806709:	Training iteration: 582000, Loss: 0.22320422530174255
2018-10-22 06:25:39.468062:	Training iteration: 582200, Loss: 0.22243182361125946
2018-10-22 06:26:08.471284:	Training iteration: 582400, Loss: 0.15181143581867218
2018-10-22 06:26:37.870191:	Training iteration: 582600, Loss: 0.15388700366020203
2018-10-22 06:27:07.967098:	Training iteration: 582800, Loss: 0.23432444036006927
2018-10-22 06:27:37.779386:	Training iteration: 583000, Loss: 0.23367533087730408
2018-10-22 06:28:07.439784:	Training iteration: 583200, Loss: 0.2210584282875061
2018-10-22 06:28:37.135415:	Training iteration: 583400, Loss: 0.1704353392124176
2018-10-22 06:29:06.877490:	Training iteration: 583600, Loss: 0.18988335132598877
2018-10-22 06:29:36.461118:	Training iteration: 583800, Loss: 0.19777776300907135
2018-10-22 06:30:05.448691:	Training iteration: 584000, Loss: 0.1675952672958374
2018-10-22 06:30:35.390943:	Training iteration: 584200, Loss: 0.15594536066055298
2018-10-22 06:31:04.916372:	Training iteration: 584400, Loss: 0.13488784432411194
2018-10-22 06:31:34.412216:	Training iteration: 584600, Loss: 0.1870633065700531
2018-10-22 06:32:04.003835:	Training iteration: 584800, Loss: 0.20155233144760132
2018-10-22 06:32:34.114263:	Training iteration: 585000, Loss: 0.20780938863754272
2018-10-22 06:33:04.013665:	Training iteration: 585200, Loss: 0.14015035331249237
2018-10-22 06:33:33.318766:	Training iteration: 585400, Loss: 0.14372602105140686
2018-10-22 06:34:04.725552:	Training iteration: 585600, Loss: 0.15923143923282623
2018-10-22 06:34:34.353651:	Training iteration: 585800, Loss: 0.1592959314584732
2018-10-22 06:35:03.496601:	Training iteration: 586000, Loss: 0.18623203039169312
2018-10-22 06:35:33.791268:	Training iteration: 586200, Loss: 0.1857525110244751
2018-10-22 06:36:03.346378:	Training iteration: 586400, Loss: 0.16702330112457275
2018-10-22 06:36:33.368611:	Training iteration: 586600, Loss: 0.24356867372989655
2018-10-22 06:37:02.975651:	Training iteration: 586800, Loss: 0.19599047303199768
2018-10-22 06:37:32.537653:	Training iteration: 587000, Loss: 0.1702699065208435
2018-10-22 06:38:02.358149:	Training iteration: 587200, Loss: 0.15083914995193481
2018-10-22 06:38:32.046683:	Training iteration: 587400, Loss: 0.16212135553359985
2018-10-22 06:39:01.797053:	Training iteration: 587600, Loss: 0.1584242582321167
2018-10-22 06:39:31.015153:	Training iteration: 587800, Loss: 0.1358998715877533
2018-10-22 06:40:00.995213:	Training iteration: 588000, Loss: 0.09672689437866211
2018-10-22 06:40:30.973832:	Training iteration: 588200, Loss: 0.1499231457710266
2018-10-22 06:41:00.509507:	Training iteration: 588400, Loss: 0.18480923771858215
2018-10-22 06:41:29.775454:	Training iteration: 588600, Loss: 0.16604840755462646
2018-10-22 06:42:00.266475:	Training iteration: 588800, Loss: 0.16735732555389404
2018-10-22 06:42:29.856798:	Training iteration: 589000, Loss: 0.22703298926353455
2018-10-22 06:42:59.917098:	Training iteration: 589200, Loss: 0.13599029183387756
2018-10-22 06:43:29.276399:	Training iteration: 589400, Loss: 0.12698513269424438
2018-10-22 06:43:58.936510:	Training iteration: 589600, Loss: 0.16368213295936584
2018-10-22 06:44:28.193614:	Training iteration: 589800, Loss: 0.1517641842365265
2018-10-22 06:44:58.423530:	Training iteration: 590000, Loss: 0.12563520669937134
2018-10-22 06:45:27.973313:	Training iteration: 590200, Loss: 0.21772035956382751
2018-10-22 06:45:56.984179:	Training iteration: 590400, Loss: 0.1483001708984375
2018-10-22 06:46:26.131666:	Training iteration: 590600, Loss: 0.14673592150211334
2018-10-22 06:46:55.530121:	Training iteration: 590800, Loss: 0.09878325462341309
2018-10-22 06:47:25.272717:	Training iteration: 591000, Loss: 0.14287669956684113
2018-10-22 06:47:55.129450:	Training iteration: 591200, Loss: 0.18986567854881287
2018-10-22 06:48:24.405176:	Training iteration: 591400, Loss: 0.10300815850496292
2018-10-22 06:48:54.732600:	Training iteration: 591600, Loss: 0.13419707119464874
2018-10-22 06:49:25.137990:	Training iteration: 591800, Loss: 0.13151919841766357
2018-10-22 06:49:54.412892:	Training iteration: 592000, Loss: 0.19184553623199463
2018-10-22 06:50:23.544232:	Training iteration: 592200, Loss: 0.1885773241519928
2018-10-22 06:50:52.311279:	Training iteration: 592400, Loss: 0.14487195014953613
2018-10-22 06:51:22.327196:	Training iteration: 592600, Loss: 0.1358758509159088
2018-10-22 06:51:52.198437:	Training iteration: 592800, Loss: 0.1607305258512497
2018-10-22 06:52:21.830491:	Training iteration: 593000, Loss: 0.16177596151828766
2018-10-22 06:52:51.012447:	Training iteration: 593200, Loss: 0.1775083839893341
2018-10-22 06:53:19.927783:	Training iteration: 593400, Loss: 0.16018646955490112
2018-10-22 06:53:49.385284:	Training iteration: 593600, Loss: 0.16789984703063965
2018-10-22 06:54:19.238838:	Training iteration: 593800, Loss: 0.17707639932632446
2018-10-22 06:54:49.024609:	Training iteration: 594000, Loss: 0.12354014813899994
2018-10-22 06:55:18.546730:	Training iteration: 594200, Loss: 0.171126127243042
2018-10-22 06:55:48.013364:	Training iteration: 594400, Loss: 0.1464371681213379
2018-10-22 06:56:17.583304:	Training iteration: 594600, Loss: 0.2127818614244461
2018-10-22 06:56:47.281250:	Training iteration: 594800, Loss: 0.17762532830238342
2018-10-22 06:57:16.310124:	Training iteration: 595000, Loss: 0.1717711240053177
2018-10-22 06:57:45.136982:	Training iteration: 595200, Loss: 0.15043622255325317
2018-10-22 06:58:15.331601:	Training iteration: 595400, Loss: 0.15955393016338348
2018-10-22 06:58:44.781412:	Training iteration: 595600, Loss: 0.20280668139457703
2018-10-22 06:59:13.485345:	Training iteration: 595800, Loss: 0.276743620634079
2018-10-22 06:59:42.753980:	Training iteration: 596000, Loss: 0.1866636872291565
2018-10-22 07:00:12.297925:	Training iteration: 596200, Loss: 0.14866894483566284
2018-10-22 07:00:41.832265:	Training iteration: 596400, Loss: 0.21189060807228088
2018-10-22 07:01:11.291018:	Training iteration: 596600, Loss: 0.17615048587322235
2018-10-22 07:01:41.213025:	Training iteration: 596800, Loss: 0.1813032627105713
2018-10-22 07:02:10.587922:	Training iteration: 597000, Loss: 0.14037951827049255
2018-10-22 07:02:41.651016:	Training iteration: 597200, Loss: 0.15119026601314545
2018-10-22 07:03:11.031727:	Training iteration: 597400, Loss: 0.1277124285697937
2018-10-22 07:03:40.547571:	Training iteration: 597600, Loss: 0.12700258195400238
2018-10-22 07:04:10.583350:	Training iteration: 597800, Loss: 0.1470305323600769
2018-10-22 07:04:40.598660:	Training iteration: 598000, Loss: 0.17827489972114563
2018-10-22 07:05:09.435519:	Training iteration: 598200, Loss: 0.20539969205856323
2018-10-22 07:05:38.750152:	Training iteration: 598400, Loss: 0.12702041864395142
2018-10-22 07:06:08.373854:	Training iteration: 598600, Loss: 0.17774763703346252
2018-10-22 07:06:37.930831:	Training iteration: 598800, Loss: 0.21554416418075562
2018-10-22 07:07:07.975644:	Training iteration: 599000, Loss: 0.15112188458442688
2018-10-22 07:07:37.623554:	Training iteration: 599200, Loss: 0.24886983633041382
2018-10-22 07:08:06.595481:	Training iteration: 599400, Loss: 0.18711267411708832
2018-10-22 07:08:35.901904:	Training iteration: 599600, Loss: 0.2197646200656891
2018-10-22 07:08:58.980900:	Epoch 4 finished after 599756 iterations.
No images to record
Validating
2018-10-22 07:08:59.163681:	Entering validation loop
2018-10-22 07:09:09.198100: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 647 of 1000
2018-10-22 07:09:14.276285: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 07:09:27.531920:	Validation iteration: 200, Loss: 0.1368372142314911
2018-10-22 07:09:40.907011:	Validation iteration: 400, Loss: 0.22368860244750977
2018-10-22 07:09:54.772681:	Validation iteration: 600, Loss: 0.12205897271633148
2018-10-22 07:10:09.003623:	Validation iteration: 800, Loss: 0.23935765027999878
2018-10-22 07:10:23.097853:	Validation iteration: 1000, Loss: 0.1994936466217041
2018-10-22 07:10:39.235846:	Validation iteration: 1200, Loss: 0.1904532015323639
2018-10-22 07:10:52.945220:	Validation iteration: 1400, Loss: 0.17203953862190247
2018-10-22 07:11:06.810368:	Validation iteration: 1600, Loss: 0.1438123881816864
2018-10-22 07:11:20.825216:	Validation iteration: 1800, Loss: 0.19364216923713684
2018-10-22 07:11:35.101769:	Validation iteration: 2000, Loss: 0.1537233293056488
2018-10-22 07:11:51.581016: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 673 of 1000
2018-10-22 07:11:56.091852: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 07:12:03.587473:	Validation iteration: 2200, Loss: 0.2379012405872345
2018-10-22 07:12:17.131630:	Validation iteration: 2400, Loss: 0.22912144660949707
2018-10-22 07:12:30.479655:	Validation iteration: 2600, Loss: 0.16252917051315308
2018-10-22 07:12:44.386385:	Validation iteration: 2800, Loss: 0.16363057494163513
2018-10-22 07:12:58.615375:	Validation iteration: 3000, Loss: 0.1596178561449051
2018-10-22 07:13:12.888882:	Validation iteration: 3200, Loss: 0.24603745341300964
2018-10-22 07:13:26.986357:	Validation iteration: 3400, Loss: 0.16570040583610535
2018-10-22 07:13:40.738366:	Validation iteration: 3600, Loss: 0.17073935270309448
2018-10-22 07:13:54.909719:	Validation iteration: 3800, Loss: 0.18970340490341187
2018-10-22 07:14:09.194839:	Validation iteration: 4000, Loss: 0.17318326234817505
2018-10-22 07:14:31.904792: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 664 of 1000
2018-10-22 07:14:36.476080: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 07:14:38.076258:	Validation iteration: 4200, Loss: 0.1625857949256897
2018-10-22 07:14:51.371831:	Validation iteration: 4400, Loss: 0.2112722396850586
2018-10-22 07:15:04.805723:	Validation iteration: 4600, Loss: 0.16172285377979279
2018-10-22 07:15:20.591900:	Validation iteration: 4800, Loss: 0.14521987736225128
2018-10-22 07:15:34.174963:	Validation iteration: 5000, Loss: 0.10387265682220459
2018-10-22 07:15:48.321046:	Validation iteration: 5200, Loss: 0.18088825047016144
2018-10-22 07:16:02.072430:	Validation iteration: 5400, Loss: 0.11617474257946014
2018-10-22 07:16:15.948399:	Validation iteration: 5600, Loss: 0.23947033286094666
2018-10-22 07:16:29.926263:	Validation iteration: 5800, Loss: 0.19081521034240723
2018-10-22 07:16:43.826606:	Validation iteration: 6000, Loss: 0.1571977436542511
2018-10-22 07:16:58.171619:	Validation iteration: 6200, Loss: 0.22490158677101135
2018-10-22 07:17:12.690490: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 648 of 1000
2018-10-22 07:17:17.392958: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 07:17:26.184414:	Validation iteration: 6400, Loss: 0.16190987825393677
2018-10-22 07:17:39.799615:	Validation iteration: 6600, Loss: 0.17798104882240295
2018-10-22 07:17:55.045214:	Validation iteration: 6800, Loss: 0.2583458423614502
2018-10-22 07:18:09.009155:	Validation iteration: 7000, Loss: 0.24019522964954376
2018-10-22 07:18:23.550706:	Validation iteration: 7200, Loss: 0.18090704083442688
2018-10-22 07:18:37.678165:	Validation iteration: 7400, Loss: 0.19910556077957153
2018-10-22 07:18:51.627708:	Validation iteration: 7600, Loss: 0.18976545333862305
2018-10-22 07:19:05.847868:	Validation iteration: 7800, Loss: 0.19042056798934937
2018-10-22 07:19:19.698250:	Validation iteration: 8000, Loss: 0.21348348259925842
2018-10-22 07:19:33.477712:	Validation iteration: 8200, Loss: 0.18421342968940735
2018-10-22 07:19:54.330917: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 611 of 1000
2018-10-22 07:19:59.986478: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 07:20:03.346910:	Validation iteration: 8400, Loss: 0.1411445438861847
2018-10-22 07:20:16.521107:	Validation iteration: 8600, Loss: 0.16582301259040833
2018-10-22 07:20:29.726837:	Validation iteration: 8800, Loss: 0.12965694069862366
2018-10-22 07:20:43.509577:	Validation iteration: 9000, Loss: 0.17608682811260223
2018-10-22 07:20:57.534364:	Validation iteration: 9200, Loss: 0.11533607542514801
2018-10-22 07:21:11.877102:	Validation iteration: 9400, Loss: 0.16100439429283142
2018-10-22 07:21:26.085048:	Validation iteration: 9600, Loss: 0.19572970271110535
2018-10-22 07:21:40.087968:	Validation iteration: 9800, Loss: 0.15063637495040894
2018-10-22 07:21:54.179422:	Validation iteration: 10000, Loss: 0.1746683269739151
2018-10-22 07:22:08.603931:	Validation iteration: 10200, Loss: 0.20281195640563965
2018-10-22 07:22:22.945369:	Validation iteration: 10400, Loss: 0.1682884693145752
2018-10-22 07:22:36.971272:	Validation iteration: 10600, Loss: 0.16545018553733826
2018-10-22 07:22:50.667853:	Validation iteration: 10800, Loss: 0.22513261437416077
2018-10-22 07:23:04.914645:	Validation iteration: 11000, Loss: 0.2072245478630066
2018-10-22 07:23:19.198537:	Validation iteration: 11200, Loss: 0.23693636059761047
Validation check mean loss: 0.18700539684954984
Validation loss has improved!
New best validation cost!
Checkpoint
2018-10-22 07:23:57.500594: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 750 of 1000
2018-10-22 07:24:00.521152: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 07:24:06.447353:	Training iteration: 599800, Loss: 0.1209038496017456
2018-10-22 07:24:33.213912:	Training iteration: 600000, Loss: 0.10876400023698807
2018-10-22 07:25:01.485834:	Training iteration: 600200, Loss: 0.16198047995567322
2018-10-22 07:25:30.794794:	Training iteration: 600400, Loss: 0.13327230513095856
2018-10-22 07:26:00.069546:	Training iteration: 600600, Loss: 0.13200673460960388
2018-10-22 07:26:29.475023:	Training iteration: 600800, Loss: 0.12589168548583984
2018-10-22 07:26:59.430983:	Training iteration: 601000, Loss: 0.11799866706132889
2018-10-22 07:27:29.481630:	Training iteration: 601200, Loss: 0.14540931582450867
2018-10-22 07:27:58.823258:	Training iteration: 601400, Loss: 0.09980308264493942
2018-10-22 07:28:28.076581:	Training iteration: 601600, Loss: 0.10118768364191055
2018-10-22 07:28:57.737443:	Training iteration: 601800, Loss: 0.0889410674571991
2018-10-22 07:29:27.376530:	Training iteration: 602000, Loss: 0.10108543187379837
2018-10-22 07:29:56.706159:	Training iteration: 602200, Loss: 0.11373695731163025
2018-10-22 07:30:26.032670:	Training iteration: 602400, Loss: 0.15789128839969635
2018-10-22 07:30:55.469540:	Training iteration: 602600, Loss: 0.10768336802721024
2018-10-22 07:31:25.637495:	Training iteration: 602800, Loss: 0.1408228874206543
2018-10-22 07:31:54.581026:	Training iteration: 603000, Loss: 0.07871109247207642
2018-10-22 07:32:23.329114:	Training iteration: 603200, Loss: 0.13398930430412292
2018-10-22 07:32:54.225482:	Training iteration: 603400, Loss: 0.06549805402755737
2018-10-22 07:33:23.546588:	Training iteration: 603600, Loss: 0.09704004228115082
2018-10-22 07:33:52.289053:	Training iteration: 603800, Loss: 0.12167195975780487
2018-10-22 07:34:21.650013:	Training iteration: 604000, Loss: 0.11216218769550323
2018-10-22 07:34:50.854695:	Training iteration: 604200, Loss: 0.0657193586230278
2018-10-22 07:35:20.615716:	Training iteration: 604400, Loss: 0.09992033988237381
2018-10-22 07:35:49.471081:	Training iteration: 604600, Loss: 0.1482638120651245
2018-10-22 07:36:19.435231:	Training iteration: 604800, Loss: 0.1514885574579239
2018-10-22 07:36:49.608466:	Training iteration: 605000, Loss: 0.16422972083091736
2018-10-22 07:37:18.618324:	Training iteration: 605200, Loss: 0.14561223983764648
2018-10-22 07:37:50.572426:	Training iteration: 605400, Loss: 0.14944568276405334
2018-10-22 07:38:19.939670:	Training iteration: 605600, Loss: 0.08419395238161087
2018-10-22 07:38:49.344173:	Training iteration: 605800, Loss: 0.11648771166801453
2018-10-22 07:39:18.969251:	Training iteration: 606000, Loss: 0.08344821631908417
2018-10-22 07:39:48.859603:	Training iteration: 606200, Loss: 0.1580691933631897
2018-10-22 07:40:18.641883:	Training iteration: 606400, Loss: 0.11022919416427612
2018-10-22 07:40:48.069774:	Training iteration: 606600, Loss: 0.13293024897575378
2018-10-22 07:41:17.927497:	Training iteration: 606800, Loss: 0.10113180428743362
2018-10-22 07:41:46.804319:	Training iteration: 607000, Loss: 0.16747689247131348
2018-10-22 07:42:16.879738:	Training iteration: 607200, Loss: 0.10210935026407242
2018-10-22 07:42:46.997706:	Training iteration: 607400, Loss: 0.132950097322464
2018-10-22 07:43:16.376383:	Training iteration: 607600, Loss: 0.1137012466788292
2018-10-22 07:43:45.197659:	Training iteration: 607800, Loss: 0.09398156404495239
2018-10-22 07:44:14.439377:	Training iteration: 608000, Loss: 0.1551813930273056
2018-10-22 07:44:43.834743:	Training iteration: 608200, Loss: 0.11257599294185638
2018-10-22 07:45:13.474325:	Training iteration: 608400, Loss: 0.1291663497686386
2018-10-22 07:45:43.288618:	Training iteration: 608600, Loss: 0.11692321300506592
2018-10-22 07:46:13.385753:	Training iteration: 608800, Loss: 0.1736448109149933
2018-10-22 07:46:43.101229:	Training iteration: 609000, Loss: 0.14469695091247559
2018-10-22 07:47:12.057441:	Training iteration: 609200, Loss: 0.09900903701782227
2018-10-22 07:47:41.092830:	Training iteration: 609400, Loss: 0.1271725296974182
2018-10-22 07:48:11.508863:	Training iteration: 609600, Loss: 0.0848270058631897
2018-10-22 07:48:42.396254:	Training iteration: 609800, Loss: 0.08147135376930237
2018-10-22 07:49:12.263996:	Training iteration: 610000, Loss: 0.14368782937526703
2018-10-22 07:49:41.686969:	Training iteration: 610200, Loss: 0.07485722005367279
2018-10-22 07:50:11.476875:	Training iteration: 610400, Loss: 0.14078253507614136
2018-10-22 07:50:40.693501:	Training iteration: 610600, Loss: 0.1168268620967865
2018-10-22 07:51:09.954172:	Training iteration: 610800, Loss: 0.13853919506072998
2018-10-22 07:51:39.942040:	Training iteration: 611000, Loss: 0.1204259842634201
2018-10-22 07:52:09.188025:	Training iteration: 611200, Loss: 0.16418543457984924
2018-10-22 07:52:38.745311:	Training iteration: 611400, Loss: 0.1522645354270935
2018-10-22 07:53:08.596888:	Training iteration: 611600, Loss: 0.09583421051502228
2018-10-22 07:53:38.203981:	Training iteration: 611800, Loss: 0.11021643877029419
2018-10-22 07:54:07.421593:	Training iteration: 612000, Loss: 0.1285315304994583
2018-10-22 07:54:39.620448: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 702 of 1000
2018-10-22 07:54:43.324613: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 07:54:50.377645:	Training iteration: 612200, Loss: 0.10837220400571823
2018-10-22 07:55:19.403034:	Training iteration: 612400, Loss: 0.09451356530189514
2018-10-22 07:55:48.890354:	Training iteration: 612600, Loss: 0.13840559124946594
2018-10-22 07:56:18.294138:	Training iteration: 612800, Loss: 0.1494975984096527
2018-10-22 07:56:47.408618:	Training iteration: 613000, Loss: 0.12045086920261383
2018-10-22 07:57:16.778428:	Training iteration: 613200, Loss: 0.11690974980592728
2018-10-22 07:57:45.444438:	Training iteration: 613400, Loss: 0.09259757399559021
2018-10-22 07:58:15.119365:	Training iteration: 613600, Loss: 0.10538250207901001
2018-10-22 07:58:44.301454:	Training iteration: 613800, Loss: 0.16403058171272278
2018-10-22 07:59:14.180498:	Training iteration: 614000, Loss: 0.14570996165275574
2018-10-22 07:59:44.412549:	Training iteration: 614200, Loss: 0.1592261642217636
2018-10-22 08:00:14.618733:	Training iteration: 614400, Loss: 0.2154110223054886
2018-10-22 08:00:44.114836:	Training iteration: 614600, Loss: 0.12420710176229477
2018-10-22 08:01:13.607005:	Training iteration: 614800, Loss: 0.08938552439212799
2018-10-22 08:01:43.759840:	Training iteration: 615000, Loss: 0.1060856506228447
2018-10-22 08:02:14.928546:	Training iteration: 615200, Loss: 0.10108710825443268
2018-10-22 08:02:44.152299:	Training iteration: 615400, Loss: 0.1069745346903801
2018-10-22 08:03:13.576384:	Training iteration: 615600, Loss: 0.10366670042276382
2018-10-22 08:03:42.908991:	Training iteration: 615800, Loss: 0.1297197937965393
2018-10-22 08:04:13.419849:	Training iteration: 616000, Loss: 0.1399708390235901
2018-10-22 08:04:42.935844:	Training iteration: 616200, Loss: 0.14885872602462769
2018-10-22 08:05:12.654704:	Training iteration: 616400, Loss: 0.09486226737499237
2018-10-22 08:05:43.892183:	Training iteration: 616600, Loss: 0.14107680320739746
2018-10-22 08:06:14.232033:	Training iteration: 616800, Loss: 0.1270459145307541
2018-10-22 08:06:44.034550:	Training iteration: 617000, Loss: 0.08374066650867462
2018-10-22 08:07:13.560614:	Training iteration: 617200, Loss: 0.1620905101299286
2018-10-22 08:07:42.435311:	Training iteration: 617400, Loss: 0.12820778787136078
2018-10-22 08:08:11.735189:	Training iteration: 617600, Loss: 0.12557090818881989
2018-10-22 08:08:41.752091:	Training iteration: 617800, Loss: 0.09500187635421753
2018-10-22 08:09:11.647189:	Training iteration: 618000, Loss: 0.10506490617990494
2018-10-22 08:09:39.786177:	Training iteration: 618200, Loss: 0.1118873581290245
2018-10-22 08:10:09.289937:	Training iteration: 618400, Loss: 0.19325444102287292
2018-10-22 08:10:39.305505:	Training iteration: 618600, Loss: 0.12168709933757782
2018-10-22 08:11:09.024368:	Training iteration: 618800, Loss: 0.141097292304039
2018-10-22 08:11:38.871937:	Training iteration: 619000, Loss: 0.08841073513031006
2018-10-22 08:12:08.834206:	Training iteration: 619200, Loss: 0.09882336109876633
2018-10-22 08:12:38.719936:	Training iteration: 619400, Loss: 0.11262447386980057
2018-10-22 08:13:07.801940:	Training iteration: 619600, Loss: 0.10007086396217346
2018-10-22 08:13:37.352263:	Training iteration: 619800, Loss: 0.1045430451631546
2018-10-22 08:14:06.089727:	Training iteration: 620000, Loss: 0.12301628291606903
2018-10-22 08:14:35.802545:	Training iteration: 620200, Loss: 0.10509160906076431
2018-10-22 08:15:05.507454:	Training iteration: 620400, Loss: 0.09839601814746857
2018-10-22 08:15:34.930268:	Training iteration: 620600, Loss: 0.09798519313335419
2018-10-22 08:16:04.772878:	Training iteration: 620800, Loss: 0.1583230197429657
2018-10-22 08:16:34.128705:	Training iteration: 621000, Loss: 0.14841735363006592
2018-10-22 08:17:03.576761:	Training iteration: 621200, Loss: 0.13552901148796082
2018-10-22 08:17:32.849548:	Training iteration: 621400, Loss: 0.09620265662670135
2018-10-22 08:18:02.064952:	Training iteration: 621600, Loss: 0.09351129829883575
2018-10-22 08:18:31.963331:	Training iteration: 621800, Loss: 0.15133431553840637
2018-10-22 08:19:01.088606:	Training iteration: 622000, Loss: 0.15217575430870056
2018-10-22 08:19:31.210700:	Training iteration: 622200, Loss: 0.16168934106826782
2018-10-22 08:20:01.157224:	Training iteration: 622400, Loss: 0.11408883333206177
2018-10-22 08:20:30.786213:	Training iteration: 622600, Loss: 0.11592456698417664
2018-10-22 08:20:59.915683:	Training iteration: 622800, Loss: 0.059781745076179504
2018-10-22 08:21:29.105858:	Training iteration: 623000, Loss: 0.11616231501102448
2018-10-22 08:21:58.992923:	Training iteration: 623200, Loss: 0.12177252769470215
2018-10-22 08:22:28.424083:	Training iteration: 623400, Loss: 0.08255377411842346
2018-10-22 08:22:57.408846:	Training iteration: 623600, Loss: 0.0978887677192688
2018-10-22 08:23:27.196747:	Training iteration: 623800, Loss: 0.11469773203134537
2018-10-22 08:23:56.386717:	Training iteration: 624000, Loss: 0.12155470252037048
2018-10-22 08:24:26.046534:	Training iteration: 624200, Loss: 0.11766669154167175
2018-10-22 08:24:55.454307:	Training iteration: 624400, Loss: 0.10930122435092926
2018-10-22 08:25:25.162296:	Training iteration: 624600, Loss: 0.11668318510055542
2018-10-22 08:25:38.099805: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 712 of 1000
2018-10-22 08:25:41.496467: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 08:26:06.960936:	Training iteration: 624800, Loss: 0.08216476440429688
2018-10-22 08:26:35.977813:	Training iteration: 625000, Loss: 0.05914347618818283
2018-10-22 08:27:05.216673:	Training iteration: 625200, Loss: 0.0886123850941658
2018-10-22 08:27:34.170122:	Training iteration: 625400, Loss: 0.10408470034599304
2018-10-22 08:28:03.874136:	Training iteration: 625600, Loss: 0.07359367609024048
2018-10-22 08:28:33.658546:	Training iteration: 625800, Loss: 0.0866982638835907
2018-10-22 08:29:03.902278:	Training iteration: 626000, Loss: 0.11005612462759018
2018-10-22 08:29:32.510591:	Training iteration: 626200, Loss: 0.11401739716529846
2018-10-22 08:30:02.739986:	Training iteration: 626400, Loss: 0.06541834771633148
2018-10-22 08:30:31.825992:	Training iteration: 626600, Loss: 0.10969705879688263
2018-10-22 08:31:01.651385:	Training iteration: 626800, Loss: 0.10566350817680359
2018-10-22 08:31:31.046657:	Training iteration: 627000, Loss: 0.0843927338719368
2018-10-22 08:32:00.478051:	Training iteration: 627200, Loss: 0.08355730772018433
2018-10-22 08:32:29.471006:	Training iteration: 627400, Loss: 0.09272436797618866
2018-10-22 08:32:59.376125:	Training iteration: 627600, Loss: 0.20162522792816162
2018-10-22 08:33:29.135535:	Training iteration: 627800, Loss: 0.08681119978427887
2018-10-22 08:33:58.821855:	Training iteration: 628000, Loss: 0.1211119294166565
2018-10-22 08:34:27.559730:	Training iteration: 628200, Loss: 0.12469282746315002
2018-10-22 08:34:56.933730:	Training iteration: 628400, Loss: 0.07353514432907104
2018-10-22 08:35:26.002498:	Training iteration: 628600, Loss: 0.11122419685125351
2018-10-22 08:35:55.435066:	Training iteration: 628800, Loss: 0.08359687030315399
2018-10-22 08:36:24.745706:	Training iteration: 629000, Loss: 0.07386375963687897
2018-10-22 08:36:53.849144:	Training iteration: 629200, Loss: 0.10167911648750305
2018-10-22 08:37:23.290104:	Training iteration: 629400, Loss: 0.0767696350812912
2018-10-22 08:37:52.323326:	Training iteration: 629600, Loss: 0.1603051871061325
2018-10-22 08:38:21.945115:	Training iteration: 629800, Loss: 0.07751905918121338
2018-10-22 08:38:51.983212:	Training iteration: 630000, Loss: 0.10124535858631134
2018-10-22 08:39:21.083371:	Training iteration: 630200, Loss: 0.10385461896657944
2018-10-22 08:39:49.546164:	Training iteration: 630400, Loss: 0.08916056156158447
2018-10-22 08:40:19.418657:	Training iteration: 630600, Loss: 0.06305018067359924
2018-10-22 08:40:48.092086:	Training iteration: 630800, Loss: 0.10473049432039261
2018-10-22 08:41:17.827820:	Training iteration: 631000, Loss: 0.08972930163145065
2018-10-22 08:41:47.130105:	Training iteration: 631200, Loss: 0.09639459103345871
2018-10-22 08:42:17.271815:	Training iteration: 631400, Loss: 0.08818681538105011
2018-10-22 08:42:46.690783:	Training iteration: 631600, Loss: 0.08410126715898514
2018-10-22 08:43:16.456610:	Training iteration: 631800, Loss: 0.10890494287014008
2018-10-22 08:43:46.330137:	Training iteration: 632000, Loss: 0.0939493477344513
2018-10-22 08:44:16.320146:	Training iteration: 632200, Loss: 0.10666786134243011
2018-10-22 08:44:45.280164:	Training iteration: 632400, Loss: 0.06177292764186859
2018-10-22 08:45:15.062395:	Training iteration: 632600, Loss: 0.08459914475679398
2018-10-22 08:45:44.585402:	Training iteration: 632800, Loss: 0.0679449588060379
2018-10-22 08:46:13.598014:	Training iteration: 633000, Loss: 0.16821874678134918
2018-10-22 08:46:44.290265:	Training iteration: 633200, Loss: 0.0937219113111496
2018-10-22 08:47:14.504592:	Training iteration: 633400, Loss: 0.06669266521930695
2018-10-22 08:47:44.295634:	Training iteration: 633600, Loss: 0.07074642926454544
2018-10-22 08:48:14.104613:	Training iteration: 633800, Loss: 0.09592187404632568
2018-10-22 08:48:43.602384:	Training iteration: 634000, Loss: 0.1012176126241684
2018-10-22 08:49:13.176702:	Training iteration: 634200, Loss: 0.06975945085287094
2018-10-22 08:49:42.419913:	Training iteration: 634400, Loss: 0.09113689512014389
2018-10-22 08:50:11.883761:	Training iteration: 634600, Loss: 0.08342206478118896
2018-10-22 08:50:41.829979:	Training iteration: 634800, Loss: 0.09311845898628235
2018-10-22 08:51:11.009228:	Training iteration: 635000, Loss: 0.10162723064422607
2018-10-22 08:51:40.667620:	Training iteration: 635200, Loss: 0.10753815621137619
2018-10-22 08:52:09.840638:	Training iteration: 635400, Loss: 0.12287236005067825
2018-10-22 08:52:38.560513:	Training iteration: 635600, Loss: 0.11934620887041092
2018-10-22 08:53:07.862046:	Training iteration: 635800, Loss: 0.0880492776632309
2018-10-22 08:53:37.977493:	Training iteration: 636000, Loss: 0.09519310295581818
2018-10-22 08:54:08.033792:	Training iteration: 636200, Loss: 0.12688088417053223
2018-10-22 08:54:37.599838:	Training iteration: 636400, Loss: 0.10170052945613861
2018-10-22 08:55:07.045864:	Training iteration: 636600, Loss: 0.07267743349075317
2018-10-22 08:55:36.349672:	Training iteration: 636800, Loss: 0.10574992001056671
2018-10-22 08:56:05.915317:	Training iteration: 637000, Loss: 0.07789837568998337
2018-10-22 08:56:27.695831: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 724 of 1000
2018-10-22 08:56:30.982649: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 08:56:47.875813:	Training iteration: 637200, Loss: 0.09951657056808472
2018-10-22 08:57:16.537461:	Training iteration: 637400, Loss: 0.1661255806684494
2018-10-22 08:57:44.688609:	Training iteration: 637600, Loss: 0.14146044850349426
2018-10-22 08:58:14.005652:	Training iteration: 637800, Loss: 0.2333122193813324
2018-10-22 08:58:42.935652:	Training iteration: 638000, Loss: 0.08568956702947617
2018-10-22 08:59:11.898325:	Training iteration: 638200, Loss: 0.14050143957138062
2018-10-22 08:59:41.102018:	Training iteration: 638400, Loss: 0.0978749468922615
2018-10-22 09:00:11.805721:	Training iteration: 638600, Loss: 0.17707328498363495
2018-10-22 09:00:41.821038:	Training iteration: 638800, Loss: 0.22012758255004883
2018-10-22 09:01:10.732428:	Training iteration: 639000, Loss: 0.10159269720315933
2018-10-22 09:01:40.877642:	Training iteration: 639200, Loss: 0.18846166133880615
2018-10-22 09:02:09.848700:	Training iteration: 639400, Loss: 0.08705297857522964
2018-10-22 09:02:43.659093:	Training iteration: 639600, Loss: 0.12431003153324127
2018-10-22 09:03:13.522577:	Training iteration: 639800, Loss: 0.0916484147310257
2018-10-22 09:03:42.868970:	Training iteration: 640000, Loss: 0.10140934586524963
2018-10-22 09:04:12.469220:	Training iteration: 640200, Loss: 0.09663304686546326
2018-10-22 09:04:42.552080:	Training iteration: 640400, Loss: 0.15520134568214417
2018-10-22 09:05:12.698600:	Training iteration: 640600, Loss: 0.12072774767875671
2018-10-22 09:05:42.180992:	Training iteration: 640800, Loss: 0.15190482139587402
2018-10-22 09:06:11.427767:	Training iteration: 641000, Loss: 0.1500893533229828
2018-10-22 09:06:41.131103:	Training iteration: 641200, Loss: 0.14163386821746826
2018-10-22 09:07:11.030204:	Training iteration: 641400, Loss: 0.15950661897659302
2018-10-22 09:07:40.674678:	Training iteration: 641600, Loss: 0.2189847081899643
2018-10-22 09:08:09.401245:	Training iteration: 641800, Loss: 0.1985771358013153
2018-10-22 09:08:38.405975:	Training iteration: 642000, Loss: 0.11367203295230865
2018-10-22 09:09:10.051798:	Training iteration: 642200, Loss: 0.16953760385513306
2018-10-22 09:09:40.170454:	Training iteration: 642400, Loss: 0.16750192642211914
2018-10-22 09:10:08.969402:	Training iteration: 642600, Loss: 0.11806713044643402
2018-10-22 09:10:38.131192:	Training iteration: 642800, Loss: 0.13043242692947388
2018-10-22 09:11:07.395327:	Training iteration: 643000, Loss: 0.163070410490036
2018-10-22 09:11:37.680802:	Training iteration: 643200, Loss: 0.125259131193161
2018-10-22 09:12:07.702425:	Training iteration: 643400, Loss: 0.15376602113246918
2018-10-22 09:12:36.647306:	Training iteration: 643600, Loss: 0.10348571836948395
2018-10-22 09:13:05.869900:	Training iteration: 643800, Loss: 0.07821918278932571
2018-10-22 09:13:35.881073:	Training iteration: 644000, Loss: 0.19424189627170563
2018-10-22 09:14:05.385444:	Training iteration: 644200, Loss: 0.08756961673498154
2018-10-22 09:14:34.572647:	Training iteration: 644400, Loss: 0.08619894087314606
2018-10-22 09:15:04.016773:	Training iteration: 644600, Loss: 0.09197234362363815
2018-10-22 09:15:33.339450:	Training iteration: 644800, Loss: 0.19876296818256378
2018-10-22 09:16:02.289702:	Training iteration: 645000, Loss: 0.12525036931037903
2018-10-22 09:16:31.193991:	Training iteration: 645200, Loss: 0.1906362622976303
2018-10-22 09:17:02.110390:	Training iteration: 645400, Loss: 0.09443147480487823
2018-10-22 09:17:32.067847:	Training iteration: 645600, Loss: 0.0867466926574707
2018-10-22 09:18:01.302708:	Training iteration: 645800, Loss: 0.10239473730325699
2018-10-22 09:18:30.700784:	Training iteration: 646000, Loss: 0.12733067572116852
2018-10-22 09:18:59.980573:	Training iteration: 646200, Loss: 0.08874596655368805
2018-10-22 09:19:29.733879:	Training iteration: 646400, Loss: 0.11081284284591675
2018-10-22 09:19:59.171838:	Training iteration: 646600, Loss: 0.11519136279821396
2018-10-22 09:20:28.565546:	Training iteration: 646800, Loss: 0.0921383947134018
2018-10-22 09:20:58.276754:	Training iteration: 647000, Loss: 0.15113389492034912
2018-10-22 09:21:27.751540:	Training iteration: 647200, Loss: 0.11118465662002563
2018-10-22 09:21:57.277083:	Training iteration: 647400, Loss: 0.1456877589225769
2018-10-22 09:22:27.111453:	Training iteration: 647600, Loss: 0.11965084820985794
2018-10-22 09:22:57.099034:	Training iteration: 647800, Loss: 0.1545642465353012
2018-10-22 09:23:26.949668:	Training iteration: 648000, Loss: 0.09597359597682953
2018-10-22 09:23:56.180435:	Training iteration: 648200, Loss: 0.11676543951034546
2018-10-22 09:24:25.852213:	Training iteration: 648400, Loss: 0.16792018711566925
2018-10-22 09:24:56.603976:	Training iteration: 648600, Loss: 0.15271705389022827
2018-10-22 09:25:26.297263:	Training iteration: 648800, Loss: 0.1065337061882019
2018-10-22 09:25:56.044293:	Training iteration: 649000, Loss: 0.12329286336898804
2018-10-22 09:26:24.956772:	Training iteration: 649200, Loss: 0.11574988067150116
2018-10-22 09:26:55.086731:	Training iteration: 649400, Loss: 0.1588689386844635
2018-10-22 09:27:24.163739:	Training iteration: 649600, Loss: 0.12195002287626266
2018-10-22 09:27:53.371119:	Training iteration: 649800, Loss: 0.10670430213212967
2018-10-22 09:28:22.730489:	Training iteration: 650000, Loss: 0.14356988668441772
2018-10-22 09:28:47.655969: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 859 of 1000
2018-10-22 09:28:49.150137: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 09:29:03.507899:	Training iteration: 650200, Loss: 0.1024530753493309
2018-10-22 09:29:32.395849:	Training iteration: 650400, Loss: 0.18091686069965363
2018-10-22 09:30:02.088017:	Training iteration: 650600, Loss: 0.16061395406723022
2018-10-22 09:30:31.304792:	Training iteration: 650800, Loss: 0.2537442445755005
2018-10-22 09:31:00.267772:	Training iteration: 651000, Loss: 0.17012935876846313
2018-10-22 09:31:29.589632:	Training iteration: 651200, Loss: 0.13222935795783997
2018-10-22 09:31:59.050179:	Training iteration: 651400, Loss: 0.23057401180267334
2018-10-22 09:32:28.508006:	Training iteration: 651600, Loss: 0.11485718190670013
2018-10-22 09:32:57.767562:	Training iteration: 651800, Loss: 0.23113006353378296
2018-10-22 09:33:27.487784:	Training iteration: 652000, Loss: 0.19192388653755188
2018-10-22 09:33:56.345178:	Training iteration: 652200, Loss: 0.11789397895336151
2018-10-22 09:34:25.800103:	Training iteration: 652400, Loss: 0.17017708718776703
2018-10-22 09:34:54.717463:	Training iteration: 652600, Loss: 0.13921178877353668
2018-10-22 09:35:24.712427:	Training iteration: 652800, Loss: 0.16053014993667603
2018-10-22 09:35:53.946495:	Training iteration: 653000, Loss: 0.17330580949783325
2018-10-22 09:36:23.162470:	Training iteration: 653200, Loss: 0.11280936002731323
2018-10-22 09:36:52.732011:	Training iteration: 653400, Loss: 0.18591007590293884
2018-10-22 09:37:22.571565:	Training iteration: 653600, Loss: 0.17879021167755127
2018-10-22 09:37:51.425347:	Training iteration: 653800, Loss: 0.19699832797050476
2018-10-22 09:38:21.405850:	Training iteration: 654000, Loss: 0.20831254124641418
2018-10-22 09:38:50.175817:	Training iteration: 654200, Loss: 0.15005624294281006
2018-10-22 09:39:19.565473:	Training iteration: 654400, Loss: 0.13010144233703613
2018-10-22 09:39:49.328749:	Training iteration: 654600, Loss: 0.15598243474960327
2018-10-22 09:40:18.460604:	Training iteration: 654800, Loss: 0.08853203058242798
2018-10-22 09:40:48.145224:	Training iteration: 655000, Loss: 0.1252640187740326
2018-10-22 09:41:17.275407:	Training iteration: 655200, Loss: 0.11336766183376312
2018-10-22 09:41:47.042254:	Training iteration: 655400, Loss: 0.11948627233505249
2018-10-22 09:42:16.812747:	Training iteration: 655600, Loss: 0.12294242531061172
2018-10-22 09:42:46.398929:	Training iteration: 655800, Loss: 0.17227616906166077
2018-10-22 09:43:15.260631:	Training iteration: 656000, Loss: 0.1450691670179367
2018-10-22 09:43:44.248154:	Training iteration: 656200, Loss: 0.18043094873428345
2018-10-22 09:44:13.993965:	Training iteration: 656400, Loss: 0.12723231315612793
2018-10-22 09:44:43.413701:	Training iteration: 656600, Loss: 0.1606525331735611
2018-10-22 09:45:12.818866:	Training iteration: 656800, Loss: 0.17579802870750427
2018-10-22 09:45:42.593697:	Training iteration: 657000, Loss: 0.1685655117034912
2018-10-22 09:46:11.629045:	Training iteration: 657200, Loss: 0.21020057797431946
2018-10-22 09:46:41.400440:	Training iteration: 657400, Loss: 0.16203045845031738
2018-10-22 09:47:10.849330:	Training iteration: 657600, Loss: 0.2545364499092102
2018-10-22 09:47:40.131585:	Training iteration: 657800, Loss: 0.1526353508234024
2018-10-22 09:48:10.282597:	Training iteration: 658000, Loss: 0.17077744007110596
2018-10-22 09:48:40.059207:	Training iteration: 658200, Loss: 0.18976294994354248
2018-10-22 09:49:09.096776:	Training iteration: 658400, Loss: 0.11922082304954529
2018-10-22 09:49:38.364227:	Training iteration: 658600, Loss: 0.14044073224067688
2018-10-22 09:50:06.908987:	Training iteration: 658800, Loss: 0.15200388431549072
2018-10-22 09:50:36.661390:	Training iteration: 659000, Loss: 0.15800783038139343
2018-10-22 09:51:06.623769:	Training iteration: 659200, Loss: 0.17244647443294525
2018-10-22 09:51:36.184550:	Training iteration: 659400, Loss: 0.11966029554605484
2018-10-22 09:52:04.960488:	Training iteration: 659600, Loss: 0.2392420768737793
2018-10-22 09:52:34.871865:	Training iteration: 659800, Loss: 0.12716779112815857
2018-10-22 09:53:03.289583:	Training iteration: 660000, Loss: 0.1307322382926941
2018-10-22 09:53:32.773914:	Training iteration: 660200, Loss: 0.17147672176361084
2018-10-22 09:54:01.955570:	Training iteration: 660400, Loss: 0.17227035760879517
2018-10-22 09:54:31.760636:	Training iteration: 660600, Loss: 0.21689876914024353
2018-10-22 09:55:01.349527:	Training iteration: 660800, Loss: 0.17112593352794647
2018-10-22 09:55:31.146720:	Training iteration: 661000, Loss: 0.12783508002758026
2018-10-22 09:56:00.205610:	Training iteration: 661200, Loss: 0.16417255997657776
2018-10-22 09:56:29.885794:	Training iteration: 661400, Loss: 0.16282758116722107
2018-10-22 09:56:59.191586:	Training iteration: 661600, Loss: 0.1694483458995819
2018-10-22 09:57:28.863378:	Training iteration: 661800, Loss: 0.1386258751153946
2018-10-22 09:57:59.083600:	Training iteration: 662000, Loss: 0.15783578157424927
2018-10-22 09:58:28.238044:	Training iteration: 662200, Loss: 0.2094113826751709
2018-10-22 09:58:57.882185:	Training iteration: 662400, Loss: 0.18299418687820435
2018-10-22 09:59:27.355289:	Training iteration: 662600, Loss: 0.12411057949066162
2018-10-22 09:59:56.398853:	Training iteration: 662800, Loss: 0.11816507577896118
2018-10-22 10:00:25.793171:	Training iteration: 663000, Loss: 0.1408066302537918
2018-10-22 10:01:03.214726:	Training iteration: 663200, Loss: 0.17866432666778564
2018-10-22 10:01:32.555721:	Training iteration: 663400, Loss: 0.13507267832756042
2018-10-22 10:02:01.742667:	Training iteration: 663600, Loss: 0.1707608699798584
2018-10-22 10:02:30.674307:	Training iteration: 663800, Loss: 0.2519896924495697
2018-10-22 10:02:59.740185:	Training iteration: 664000, Loss: 0.1978483498096466
2018-10-22 10:03:29.125125:	Training iteration: 664200, Loss: 0.147203266620636
2018-10-22 10:03:58.219578:	Training iteration: 664400, Loss: 0.17903634905815125
2018-10-22 10:04:28.444317:	Training iteration: 664600, Loss: 0.16407880187034607
2018-10-22 10:04:57.158632:	Training iteration: 664800, Loss: 0.12193398177623749
2018-10-22 10:05:27.159166:	Training iteration: 665000, Loss: 0.1698130965232849
2018-10-22 10:05:56.496579:	Training iteration: 665200, Loss: 0.10807542502880096
2018-10-22 10:06:26.332584:	Training iteration: 665400, Loss: 0.14913691580295563
2018-10-22 10:06:55.503160:	Training iteration: 665600, Loss: 0.18454790115356445
2018-10-22 10:07:24.923652:	Training iteration: 665800, Loss: 0.17063647508621216
2018-10-22 10:07:54.860269:	Training iteration: 666000, Loss: 0.17842310667037964
2018-10-22 10:08:23.668925:	Training iteration: 666200, Loss: 0.15466442704200745
2018-10-22 10:08:53.322287:	Training iteration: 666400, Loss: 0.1933726966381073
2018-10-22 10:09:22.836599:	Training iteration: 666600, Loss: 0.4228594899177551
2018-10-22 10:09:52.246747:	Training iteration: 666800, Loss: 0.16278043389320374
2018-10-22 10:10:21.559975:	Training iteration: 667000, Loss: 0.14321312308311462
2018-10-22 10:10:50.741559:	Training iteration: 667200, Loss: 0.13770049810409546
2018-10-22 10:11:20.209628:	Training iteration: 667400, Loss: 0.2721328139305115
2018-10-22 10:11:50.014909:	Training iteration: 667600, Loss: 0.1443593055009842
2018-10-22 10:12:20.206070:	Training iteration: 667800, Loss: 0.09335741400718689
2018-10-22 10:12:49.847814:	Training iteration: 668000, Loss: 0.18853197991847992
2018-10-22 10:13:19.267515:	Training iteration: 668200, Loss: 0.249583899974823
2018-10-22 10:13:48.597796:	Training iteration: 668400, Loss: 0.1444544494152069
2018-10-22 10:14:18.493919:	Training iteration: 668600, Loss: 0.19979619979858398
2018-10-22 10:14:48.184862:	Training iteration: 668800, Loss: 0.1313016712665558
2018-10-22 10:15:18.033754:	Training iteration: 669000, Loss: 0.15198799967765808
2018-10-22 10:15:48.027752:	Training iteration: 669200, Loss: 0.14311948418617249
2018-10-22 10:16:17.499126:	Training iteration: 669400, Loss: 0.14167240262031555
2018-10-22 10:16:46.959602:	Training iteration: 669600, Loss: 0.1803835928440094
2018-10-22 10:17:16.753549:	Training iteration: 669800, Loss: 0.18223153054714203
2018-10-22 10:17:45.776202:	Training iteration: 670000, Loss: 0.17654886841773987
2018-10-22 10:18:15.615424:	Training iteration: 670200, Loss: 0.16201931238174438
2018-10-22 10:18:45.678147:	Training iteration: 670400, Loss: 0.14095556735992432
2018-10-22 10:19:14.711158:	Training iteration: 670600, Loss: 0.1809280812740326
2018-10-22 10:19:44.600576:	Training iteration: 670800, Loss: 0.185723215341568
2018-10-22 10:20:13.954752:	Training iteration: 671000, Loss: 0.17865896224975586
2018-10-22 10:20:43.501930:	Training iteration: 671200, Loss: 0.15503725409507751
2018-10-22 10:21:13.544607:	Training iteration: 671400, Loss: 0.16152264177799225
2018-10-22 10:21:42.957894:	Training iteration: 671600, Loss: 0.18942296504974365
2018-10-22 10:22:12.606371:	Training iteration: 671800, Loss: 0.12577545642852783
2018-10-22 10:22:42.524127:	Training iteration: 672000, Loss: 0.14807859063148499
2018-10-22 10:23:12.617902:	Training iteration: 672200, Loss: 0.1514418125152588
2018-10-22 10:23:42.032634:	Training iteration: 672400, Loss: 0.14267978072166443
2018-10-22 10:24:12.078992:	Training iteration: 672600, Loss: 0.20611846446990967
2018-10-22 10:24:41.864268:	Training iteration: 672800, Loss: 0.21805256605148315
2018-10-22 10:25:12.916501:	Training iteration: 673000, Loss: 0.17949478328227997
2018-10-22 10:25:42.232689:	Training iteration: 673200, Loss: 0.16413474082946777
2018-10-22 10:26:11.926174:	Training iteration: 673400, Loss: 0.17153964936733246
2018-10-22 10:26:41.329679:	Training iteration: 673600, Loss: 0.1833556890487671
2018-10-22 10:27:10.621785:	Training iteration: 673800, Loss: 0.14432580769062042
2018-10-22 10:27:40.175000:	Training iteration: 674000, Loss: 0.12370751798152924
2018-10-22 10:28:09.528839:	Training iteration: 674200, Loss: 0.11638480424880981
2018-10-22 10:28:39.379730:	Training iteration: 674400, Loss: 0.15810513496398926
2018-10-22 10:29:08.931745:	Training iteration: 674600, Loss: 0.17720869183540344
2018-10-22 10:29:38.582405:	Training iteration: 674800, Loss: 0.1382361352443695
2018-10-22 10:30:08.085790:	Training iteration: 675000, Loss: 0.18610233068466187
2018-10-22 10:30:37.325415:	Training iteration: 675200, Loss: 0.1621895134449005
2018-10-22 10:31:07.615727:	Training iteration: 675400, Loss: 0.17118953168392181
2018-10-22 10:31:37.514361:	Training iteration: 675600, Loss: 0.16735094785690308
2018-10-22 10:32:07.037340:	Training iteration: 675800, Loss: 0.15274420380592346
2018-10-22 10:32:36.984835:	Training iteration: 676000, Loss: 0.14509209990501404
2018-10-22 10:33:05.874270:	Training iteration: 676200, Loss: 0.1503651738166809
2018-10-22 10:33:35.878272:	Training iteration: 676400, Loss: 0.19633494317531586
2018-10-22 10:34:04.985247:	Training iteration: 676600, Loss: 0.18314415216445923
2018-10-22 10:34:34.398623:	Training iteration: 676800, Loss: 0.14912471175193787
2018-10-22 10:35:04.299267:	Training iteration: 677000, Loss: 0.14757725596427917
2018-10-22 10:35:34.195052:	Training iteration: 677200, Loss: 0.1495184600353241
2018-10-22 10:36:03.690229:	Training iteration: 677400, Loss: 0.12638089060783386
2018-10-22 10:36:33.464729:	Training iteration: 677600, Loss: 0.22306638956069946
2018-10-22 10:37:02.938457:	Training iteration: 677800, Loss: 0.12929590046405792
2018-10-22 10:37:32.804329:	Training iteration: 678000, Loss: 0.0931970626115799
2018-10-22 10:38:01.939728:	Training iteration: 678200, Loss: 0.2154553234577179
2018-10-22 10:38:31.697018:	Training iteration: 678400, Loss: 0.1501752734184265
2018-10-22 10:39:01.392417:	Training iteration: 678600, Loss: 0.16370952129364014
2018-10-22 10:39:31.011964:	Training iteration: 678800, Loss: 0.18856926262378693
2018-10-22 10:39:59.628705:	Training iteration: 679000, Loss: 0.18424472212791443
2018-10-22 10:40:29.391442:	Training iteration: 679200, Loss: 0.12797871232032776
2018-10-22 10:40:59.050378:	Training iteration: 679400, Loss: 0.11777590960264206
2018-10-22 10:41:28.653531:	Training iteration: 679600, Loss: 0.13013063371181488
2018-10-22 10:41:58.005497:	Training iteration: 679800, Loss: 0.1364540159702301
2018-10-22 10:42:27.857416:	Training iteration: 680000, Loss: 0.14915131032466888
2018-10-22 10:42:57.225444:	Training iteration: 680200, Loss: 0.1242847666144371
2018-10-22 10:43:27.315236:	Training iteration: 680400, Loss: 0.1443302482366562
2018-10-22 10:43:56.943241:	Training iteration: 680600, Loss: 0.1650322824716568
2018-10-22 10:44:25.091719:	Training iteration: 680800, Loss: 0.13397589325904846
2018-10-22 10:44:55.048712:	Training iteration: 681000, Loss: 0.15997149050235748
2018-10-22 10:45:24.546135:	Training iteration: 681200, Loss: 0.18019317090511322
2018-10-22 10:45:52.981989:	Training iteration: 681400, Loss: 0.15896600484848022
2018-10-22 10:46:22.927114:	Training iteration: 681600, Loss: 0.1989152431488037
2018-10-22 10:46:52.221120:	Training iteration: 681800, Loss: 0.14018020033836365
2018-10-22 10:47:22.137094:	Training iteration: 682000, Loss: 0.1678745597600937
2018-10-22 10:47:51.236180:	Training iteration: 682200, Loss: 0.18675383925437927
2018-10-22 10:48:21.307264:	Training iteration: 682400, Loss: 0.1746373325586319
2018-10-22 10:48:50.495172:	Training iteration: 682600, Loss: 0.1727888137102127
2018-10-22 10:49:20.288663:	Training iteration: 682800, Loss: 0.17263495922088623
2018-10-22 10:49:49.879112:	Training iteration: 683000, Loss: 0.18980662524700165
2018-10-22 10:50:19.417955:	Training iteration: 683200, Loss: 0.22079981863498688
2018-10-22 10:50:49.555998:	Training iteration: 683400, Loss: 0.20034873485565186
2018-10-22 10:51:19.309399:	Training iteration: 683600, Loss: 0.17757898569107056
2018-10-22 10:51:48.763145:	Training iteration: 683800, Loss: 0.2081584334373474
2018-10-22 10:52:18.158640:	Training iteration: 684000, Loss: 0.21450552344322205
2018-10-22 10:52:47.723670:	Training iteration: 684200, Loss: 0.1884206384420395
2018-10-22 10:53:17.330858:	Training iteration: 684400, Loss: 0.12730512022972107
2018-10-22 10:53:46.888691:	Training iteration: 684600, Loss: 0.2217634916305542
2018-10-22 10:54:16.439124:	Training iteration: 684800, Loss: 0.15889711678028107
2018-10-22 10:54:45.380423:	Training iteration: 685000, Loss: 0.12173376977443695
2018-10-22 10:55:15.067196:	Training iteration: 685200, Loss: 0.15192770957946777
2018-10-22 10:55:44.617378:	Training iteration: 685400, Loss: 0.20293012261390686
2018-10-22 10:56:14.726175:	Training iteration: 685600, Loss: 0.16199544072151184
2018-10-22 10:56:44.277805:	Training iteration: 685800, Loss: 0.13788291811943054
2018-10-22 10:57:14.039933:	Training iteration: 686000, Loss: 0.18862178921699524
2018-10-22 10:57:43.584991:	Training iteration: 686200, Loss: 0.12106208503246307
2018-10-22 10:58:13.325410:	Training iteration: 686400, Loss: 0.1422780454158783
2018-10-22 10:58:42.844613:	Training iteration: 686600, Loss: 0.1044931411743164
2018-10-22 10:59:11.825745:	Training iteration: 686800, Loss: 0.18948042392730713
2018-10-22 10:59:41.367277:	Training iteration: 687000, Loss: 0.15443487465381622
2018-10-22 11:00:10.624243:	Training iteration: 687200, Loss: 0.1815904974937439
2018-10-22 11:00:40.088631:	Training iteration: 687400, Loss: 0.1666715443134308
2018-10-22 11:01:11.833096:	Training iteration: 687600, Loss: 0.15823543071746826
2018-10-22 11:01:41.346827:	Training iteration: 687800, Loss: 0.17032982409000397
2018-10-22 11:02:11.034655:	Training iteration: 688000, Loss: 0.18016274273395538
2018-10-22 11:02:40.644099:	Training iteration: 688200, Loss: 0.18539315462112427
2018-10-22 11:03:10.275507:	Training iteration: 688400, Loss: 0.17269733548164368
2018-10-22 11:03:39.123959:	Training iteration: 688600, Loss: 0.22696813941001892
2018-10-22 11:04:08.598616:	Training iteration: 688800, Loss: 0.19298267364501953
2018-10-22 11:04:38.106449:	Training iteration: 689000, Loss: 0.17859956622123718
2018-10-22 11:05:07.343325:	Training iteration: 689200, Loss: 0.12654772400856018
2018-10-22 11:05:37.389291:	Training iteration: 689400, Loss: 0.20828577876091003
2018-10-22 11:06:06.712561:	Training iteration: 689600, Loss: 0.13975566625595093
2018-10-22 11:06:36.322996:	Training iteration: 689800, Loss: 0.17387856543064117
2018-10-22 11:07:05.476378:	Training iteration: 690000, Loss: 0.13328546285629272
2018-10-22 11:07:34.741471:	Training iteration: 690200, Loss: 0.17665012180805206
2018-10-22 11:08:03.697608:	Training iteration: 690400, Loss: 0.16485881805419922
2018-10-22 11:08:33.655972:	Training iteration: 690600, Loss: 0.1622188687324524
2018-10-22 11:09:03.370448:	Training iteration: 690800, Loss: 0.15176734328269958
2018-10-22 11:09:32.385910:	Training iteration: 691000, Loss: 0.2218453288078308
2018-10-22 11:10:01.801609:	Training iteration: 691200, Loss: 0.2021184116601944
2018-10-22 11:10:31.600898:	Training iteration: 691400, Loss: 0.24191822111606598
2018-10-22 11:11:01.131094:	Training iteration: 691600, Loss: 0.14453819394111633
2018-10-22 11:11:30.673662:	Training iteration: 691800, Loss: 0.15249809622764587
2018-10-22 11:12:00.656329:	Training iteration: 692000, Loss: 0.14722619950771332
2018-10-22 11:12:30.322125:	Training iteration: 692200, Loss: 0.1365741491317749
2018-10-22 11:12:59.820175:	Training iteration: 692400, Loss: 0.1459728479385376
2018-10-22 11:13:29.401449:	Training iteration: 692600, Loss: 0.16786274313926697
2018-10-22 11:13:59.121694:	Training iteration: 692800, Loss: 0.13982459902763367
2018-10-22 11:14:27.821334:	Training iteration: 693000, Loss: 0.17878998816013336
2018-10-22 11:14:58.057005:	Training iteration: 693200, Loss: 0.21472994983196259
2018-10-22 11:15:26.981754:	Training iteration: 693400, Loss: 0.141180157661438
2018-10-22 11:15:55.813023:	Training iteration: 693600, Loss: 0.43050843477249146
2018-10-22 11:16:24.726060:	Training iteration: 693800, Loss: 0.12825532257556915
2018-10-22 11:16:54.599822:	Training iteration: 694000, Loss: 0.1854846030473709
2018-10-22 11:17:23.629594:	Training iteration: 694200, Loss: 0.21146616339683533
2018-10-22 11:17:52.688589:	Training iteration: 694400, Loss: 0.1355830430984497
2018-10-22 11:18:22.119383:	Training iteration: 694600, Loss: 0.15363755822181702
2018-10-22 11:18:51.940947:	Training iteration: 694800, Loss: 0.3485872149467468
2018-10-22 11:19:21.316989:	Training iteration: 695000, Loss: 0.16851744055747986
2018-10-22 11:19:51.383161:	Training iteration: 695200, Loss: 0.17392735183238983
2018-10-22 11:20:21.253766:	Training iteration: 695400, Loss: 0.1625577211380005
2018-10-22 11:20:51.153232:	Training iteration: 695600, Loss: 0.19731390476226807
2018-10-22 11:21:20.703151:	Training iteration: 695800, Loss: 0.15788224339485168
2018-10-22 11:21:49.870809:	Training iteration: 696000, Loss: 0.12558963894844055
2018-10-22 11:22:19.589230:	Training iteration: 696200, Loss: 0.13760894536972046
2018-10-22 11:22:49.336629:	Training iteration: 696400, Loss: 0.14520373940467834
2018-10-22 11:23:19.128758:	Training iteration: 696600, Loss: 0.13893553614616394
2018-10-22 11:23:48.564247:	Training iteration: 696800, Loss: 0.08657015860080719
2018-10-22 11:24:17.882786:	Training iteration: 697000, Loss: 0.14652374386787415
2018-10-22 11:24:47.486991:	Training iteration: 697200, Loss: 0.13143664598464966
2018-10-22 11:25:17.426169:	Training iteration: 697400, Loss: 0.14054979383945465
2018-10-22 11:25:46.589247:	Training iteration: 697600, Loss: 0.11615271866321564
2018-10-22 11:26:16.046239:	Training iteration: 697800, Loss: 0.14343208074569702
2018-10-22 11:26:46.043846:	Training iteration: 698000, Loss: 0.10781222581863403
2018-10-22 11:27:15.326013:	Training iteration: 698200, Loss: 0.13349144160747528
2018-10-22 11:27:45.051926:	Training iteration: 698400, Loss: 0.1382196545600891
2018-10-22 11:28:14.599659:	Training iteration: 698600, Loss: 0.20092357695102692
2018-10-22 11:28:43.854609:	Training iteration: 698800, Loss: 0.12309378385543823
2018-10-22 11:29:13.274461:	Training iteration: 699000, Loss: 0.1246766448020935
2018-10-22 11:29:41.854352:	Training iteration: 699200, Loss: 0.197739839553833
2018-10-22 11:30:11.725997:	Training iteration: 699400, Loss: 0.19916747510433197
2018-10-22 11:30:41.635080:	Training iteration: 699600, Loss: 0.13141770660877228
2018-10-22 11:31:11.017218:	Training iteration: 699800, Loss: 0.22705824673175812
2018-10-22 11:31:40.195109:	Training iteration: 700000, Loss: 0.14851388335227966
2018-10-22 11:32:09.176066:	Training iteration: 700200, Loss: 0.12231359630823135
2018-10-22 11:32:39.102383:	Training iteration: 700400, Loss: 0.19403891265392303
2018-10-22 11:33:08.735015:	Training iteration: 700600, Loss: 0.12798568606376648
2018-10-22 11:33:38.026932:	Training iteration: 700800, Loss: 0.19601812958717346
2018-10-22 11:34:07.860054:	Training iteration: 701000, Loss: 0.11764617264270782
2018-10-22 11:34:37.967203:	Training iteration: 701200, Loss: 0.16038352251052856
2018-10-22 11:35:07.451171:	Training iteration: 701400, Loss: 0.37545347213745117
2018-10-22 11:35:36.591495:	Training iteration: 701600, Loss: 0.32404112815856934
2018-10-22 11:36:06.579236:	Training iteration: 701800, Loss: 0.17710250616073608
2018-10-22 11:36:35.908446:	Training iteration: 702000, Loss: 0.2148819863796234
2018-10-22 11:37:05.779454:	Training iteration: 702200, Loss: 0.23450139164924622
2018-10-22 11:37:35.680589:	Training iteration: 702400, Loss: 0.17315813899040222
2018-10-22 11:38:05.254077:	Training iteration: 702600, Loss: 0.19610294699668884
2018-10-22 11:38:34.498312:	Training iteration: 702800, Loss: 0.21603429317474365
2018-10-22 11:39:03.321373:	Training iteration: 703000, Loss: 0.11970412731170654
2018-10-22 11:39:33.063018:	Training iteration: 703200, Loss: 0.1656162142753601
2018-10-22 11:40:03.046634:	Training iteration: 703400, Loss: 0.25088566541671753
2018-10-22 11:40:32.628865:	Training iteration: 703600, Loss: 0.15546660125255585
2018-10-22 11:41:01.998577:	Training iteration: 703800, Loss: 0.17864476144313812
2018-10-22 11:41:31.615953:	Training iteration: 704000, Loss: 0.17633932828903198
2018-10-22 11:42:01.285664:	Training iteration: 704200, Loss: 0.11109607666730881
2018-10-22 11:42:30.233834:	Training iteration: 704400, Loss: 0.1596343219280243
2018-10-22 11:42:59.400687:	Training iteration: 704600, Loss: 0.11058887094259262
2018-10-22 11:43:28.700020:	Training iteration: 704800, Loss: 0.1738804131746292
2018-10-22 11:43:58.177768:	Training iteration: 705000, Loss: 0.17852655053138733
2018-10-22 11:44:28.636045:	Training iteration: 705200, Loss: 0.1829269379377365
2018-10-22 11:44:58.481384:	Training iteration: 705400, Loss: 0.20430296659469604
2018-10-22 11:45:28.857537:	Training iteration: 705600, Loss: 0.1854161024093628
2018-10-22 11:45:58.264455:	Training iteration: 705800, Loss: 0.15129795670509338
2018-10-22 11:46:27.230935:	Training iteration: 706000, Loss: 0.21599076688289642
2018-10-22 11:46:57.205222:	Training iteration: 706200, Loss: 0.17263701558113098
2018-10-22 11:47:27.069380:	Training iteration: 706400, Loss: 0.21589063107967377
2018-10-22 11:47:56.822390:	Training iteration: 706600, Loss: 0.14907187223434448
2018-10-22 11:48:26.196256:	Training iteration: 706800, Loss: 0.13704480230808258
2018-10-22 11:48:55.953022:	Training iteration: 707000, Loss: 0.13206374645233154
2018-10-22 11:49:25.670322:	Training iteration: 707200, Loss: 0.18861818313598633
2018-10-22 11:49:55.268180:	Training iteration: 707400, Loss: 0.11305008828639984
2018-10-22 11:50:24.912296:	Training iteration: 707600, Loss: 0.14115235209465027
2018-10-22 11:50:54.393696:	Training iteration: 707800, Loss: 0.15762513875961304
2018-10-22 11:51:23.665677:	Training iteration: 708000, Loss: 0.13627228140830994
2018-10-22 11:51:53.133187:	Training iteration: 708200, Loss: 0.16807541251182556
2018-10-22 11:52:22.990590:	Training iteration: 708400, Loss: 0.27633923292160034
2018-10-22 11:52:52.510939:	Training iteration: 708600, Loss: 0.14981406927108765
2018-10-22 11:53:21.937238:	Training iteration: 708800, Loss: 0.13403229415416718
2018-10-22 11:53:51.718825:	Training iteration: 709000, Loss: 0.17307016253471375
2018-10-22 11:54:20.600866:	Training iteration: 709200, Loss: 0.1372392624616623
2018-10-22 11:54:50.193571:	Training iteration: 709400, Loss: 0.1552143692970276
2018-10-22 11:55:20.077004:	Training iteration: 709600, Loss: 0.17228466272354126
2018-10-22 11:55:49.828450:	Training iteration: 709800, Loss: 0.15543857216835022
2018-10-22 11:56:19.352549:	Training iteration: 710000, Loss: 0.12014329433441162
2018-10-22 11:56:49.089336:	Training iteration: 710200, Loss: 0.18533048033714294
2018-10-22 11:57:18.975141:	Training iteration: 710400, Loss: 0.21491366624832153
2018-10-22 11:57:48.342690:	Training iteration: 710600, Loss: 0.13289740681648254
2018-10-22 11:58:17.709998:	Training iteration: 710800, Loss: 0.14982903003692627
2018-10-22 11:58:47.305353:	Training iteration: 711000, Loss: 0.1392562985420227
2018-10-22 11:59:17.210918:	Training iteration: 711200, Loss: 0.10394131392240524
2018-10-22 11:59:46.740639:	Training iteration: 711400, Loss: 0.18447977304458618
2018-10-22 12:00:16.426896:	Training iteration: 711600, Loss: 0.1331906020641327
2018-10-22 12:00:45.242424:	Training iteration: 711800, Loss: 0.27446067333221436
2018-10-22 12:01:25.433169:	Training iteration: 712000, Loss: 0.181321382522583
2018-10-22 12:01:54.155381:	Training iteration: 712200, Loss: 0.1668878197669983
2018-10-22 12:02:23.195617:	Training iteration: 712400, Loss: 0.13835075497627258
2018-10-22 12:02:52.520337:	Training iteration: 712600, Loss: 0.1445631980895996
2018-10-22 12:03:26.562245:	Training iteration: 712800, Loss: 0.13369733095169067
2018-10-22 12:03:55.667517:	Training iteration: 713000, Loss: 0.1274922788143158
2018-10-22 12:04:25.234700:	Training iteration: 713200, Loss: 0.18170784413814545
2018-10-22 12:04:54.895747:	Training iteration: 713400, Loss: 0.16320055723190308
2018-10-22 12:05:24.323796:	Training iteration: 713600, Loss: 0.12023890018463135
2018-10-22 12:05:54.432695:	Training iteration: 713800, Loss: 0.15903335809707642
2018-10-22 12:06:23.555084:	Training iteration: 714000, Loss: 0.17327123880386353
2018-10-22 12:06:52.883836:	Training iteration: 714200, Loss: 0.1512569785118103
2018-10-22 12:07:22.033097:	Training iteration: 714400, Loss: 0.13987261056900024
2018-10-22 12:07:51.216540:	Training iteration: 714600, Loss: 0.14202827215194702
2018-10-22 12:08:21.038355:	Training iteration: 714800, Loss: 0.18045663833618164
2018-10-22 12:08:50.243072:	Training iteration: 715000, Loss: 0.1380065679550171
2018-10-22 12:09:19.799781:	Training iteration: 715200, Loss: 0.09527528285980225
2018-10-22 12:09:49.718959:	Training iteration: 715400, Loss: 0.20673105120658875
2018-10-22 12:10:19.435400:	Training iteration: 715600, Loss: 0.19504457712173462
2018-10-22 12:10:48.819739:	Training iteration: 715800, Loss: 0.109047532081604
2018-10-22 12:11:17.777727:	Training iteration: 716000, Loss: 0.23696660995483398
2018-10-22 12:11:47.517338:	Training iteration: 716200, Loss: 0.19899195432662964
2018-10-22 12:12:17.180417:	Training iteration: 716400, Loss: 0.16142013669013977
2018-10-22 12:12:46.486105:	Training iteration: 716600, Loss: 0.18383747339248657
2018-10-22 12:13:16.303324:	Training iteration: 716800, Loss: 0.1720186471939087
2018-10-22 12:13:45.810875:	Training iteration: 717000, Loss: 0.24797426164150238
2018-10-22 12:14:15.332487:	Training iteration: 717200, Loss: 0.16981977224349976
2018-10-22 12:14:44.704609:	Training iteration: 717400, Loss: 0.11632809042930603
2018-10-22 12:15:14.849168:	Training iteration: 717600, Loss: 0.34668809175491333
2018-10-22 12:15:44.031258:	Training iteration: 717800, Loss: 0.14413662254810333
2018-10-22 12:16:13.143586:	Training iteration: 718000, Loss: 0.1709158718585968
2018-10-22 12:16:42.955475:	Training iteration: 718200, Loss: 0.21066808700561523
2018-10-22 12:17:12.413915:	Training iteration: 718400, Loss: 0.12561142444610596
2018-10-22 12:17:42.646070:	Training iteration: 718600, Loss: 0.12008703500032425
2018-10-22 12:18:11.822587:	Training iteration: 718800, Loss: 0.1761949062347412
2018-10-22 12:18:41.115230:	Training iteration: 719000, Loss: 0.21849371492862701
2018-10-22 12:19:09.915694:	Training iteration: 719200, Loss: 0.22238631546497345
2018-10-22 12:19:39.746039:	Training iteration: 719400, Loss: 0.19661781191825867
2018-10-22 12:20:09.356344:	Training iteration: 719600, Loss: 0.15859808027744293
2018-10-22 12:20:24.686936:	Epoch 5 finished after 719707 iterations.
No images to record
Validating
2018-10-22 12:20:24.848957:	Entering validation loop
2018-10-22 12:20:34.869882: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 637 of 1000
2018-10-22 12:20:39.882583: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 12:20:53.416388:	Validation iteration: 200, Loss: 0.11840420961380005
2018-10-22 12:21:06.960147:	Validation iteration: 400, Loss: 0.2030915915966034
2018-10-22 12:21:20.973069:	Validation iteration: 600, Loss: 0.19951966404914856
2018-10-22 12:21:35.258341:	Validation iteration: 800, Loss: 0.16893503069877625
2018-10-22 12:21:50.349174:	Validation iteration: 1000, Loss: 0.245982363820076
2018-10-22 12:22:04.211388:	Validation iteration: 1200, Loss: 0.2108340561389923
2018-10-22 12:22:18.014561:	Validation iteration: 1400, Loss: 0.15732702612876892
2018-10-22 12:22:32.413883:	Validation iteration: 1600, Loss: 0.15904930233955383
2018-10-22 12:22:46.715322:	Validation iteration: 1800, Loss: 0.16049374639987946
2018-10-22 12:23:00.629658:	Validation iteration: 2000, Loss: 0.16624651849269867
2018-10-22 12:23:16.656675: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 673 of 1000
2018-10-22 12:23:21.309395: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 12:23:28.718269:	Validation iteration: 2200, Loss: 0.25794655084609985
2018-10-22 12:23:42.001570:	Validation iteration: 2400, Loss: 0.28884169459342957
2018-10-22 12:23:55.809817:	Validation iteration: 2600, Loss: 0.1780608892440796
2018-10-22 12:24:09.780953:	Validation iteration: 2800, Loss: 0.16838525235652924
2018-10-22 12:24:23.955144:	Validation iteration: 3000, Loss: 0.2351367175579071
2018-10-22 12:24:37.754196:	Validation iteration: 3200, Loss: 0.23106154799461365
2018-10-22 12:24:51.974859:	Validation iteration: 3400, Loss: 0.2383442223072052
2018-10-22 12:25:06.167699:	Validation iteration: 3600, Loss: 0.24042466282844543
2018-10-22 12:25:20.354265:	Validation iteration: 3800, Loss: 0.21564170718193054
2018-10-22 12:25:34.259878:	Validation iteration: 4000, Loss: 0.30702775716781616
2018-10-22 12:25:56.849470: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 666 of 1000
2018-10-22 12:26:01.412131: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 12:26:03.061689:	Validation iteration: 4200, Loss: 0.17111864686012268
2018-10-22 12:26:16.089543:	Validation iteration: 4400, Loss: 0.23240028321743011
2018-10-22 12:26:29.634823:	Validation iteration: 4600, Loss: 0.20483073592185974
2018-10-22 12:26:43.478361:	Validation iteration: 4800, Loss: 0.1602780967950821
2018-10-22 12:26:57.940417:	Validation iteration: 5000, Loss: 0.16074824333190918
2018-10-22 12:27:11.876950:	Validation iteration: 5200, Loss: 0.17211997509002686
2018-10-22 12:27:25.684423:	Validation iteration: 5400, Loss: 0.28761589527130127
2018-10-22 12:27:39.308707:	Validation iteration: 5600, Loss: 0.16993752121925354
2018-10-22 12:27:53.220668:	Validation iteration: 5800, Loss: 0.14534533023834229
2018-10-22 12:28:07.032207:	Validation iteration: 6000, Loss: 0.21452224254608154
2018-10-22 12:28:21.246439:	Validation iteration: 6200, Loss: 0.20489592850208282
2018-10-22 12:28:35.641327: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 688 of 1000
2018-10-22 12:28:39.763908: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 12:28:48.506817:	Validation iteration: 6400, Loss: 0.18104641139507294
2018-10-22 12:29:01.667695:	Validation iteration: 6600, Loss: 0.2724927067756653
2018-10-22 12:29:15.288824:	Validation iteration: 6800, Loss: 0.1800445169210434
2018-10-22 12:29:29.239399:	Validation iteration: 7000, Loss: 0.1665324568748474
2018-10-22 12:29:43.309076:	Validation iteration: 7200, Loss: 0.19724684953689575
2018-10-22 12:29:57.364852:	Validation iteration: 7400, Loss: 0.2239198386669159
2018-10-22 12:30:14.358541:	Validation iteration: 7600, Loss: 0.18358635902404785
2018-10-22 12:30:28.305521:	Validation iteration: 7800, Loss: 0.22383292019367218
2018-10-22 12:30:42.567474:	Validation iteration: 8000, Loss: 0.3158997595310211
2018-10-22 12:30:56.753882:	Validation iteration: 8200, Loss: 0.2158472239971161
2018-10-22 12:31:17.485869: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 641 of 1000
2018-10-22 12:31:23.023470: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 12:31:26.239996:	Validation iteration: 8400, Loss: 0.12434283643960953
2018-10-22 12:31:39.799354:	Validation iteration: 8600, Loss: 0.11860929429531097
2018-10-22 12:31:53.164850:	Validation iteration: 8800, Loss: 0.1096850037574768
2018-10-22 12:32:07.064931:	Validation iteration: 9000, Loss: 0.12043708562850952
2018-10-22 12:32:21.389801:	Validation iteration: 9200, Loss: 0.14639315009117126
2018-10-22 12:32:35.497429:	Validation iteration: 9400, Loss: 0.19142740964889526
2018-10-22 12:32:49.756054:	Validation iteration: 9600, Loss: 0.1474359929561615
2018-10-22 12:33:03.521110:	Validation iteration: 9800, Loss: 0.20549935102462769
2018-10-22 12:33:17.639744:	Validation iteration: 10000, Loss: 0.15347060561180115
2018-10-22 12:33:31.558741:	Validation iteration: 10200, Loss: 0.15600211918354034
2018-10-22 12:33:45.332389:	Validation iteration: 10400, Loss: 0.26157498359680176
2018-10-22 12:33:59.703049:	Validation iteration: 10600, Loss: 0.15982672572135925
2018-10-22 12:34:13.585988:	Validation iteration: 10800, Loss: 0.16478762030601501
2018-10-22 12:34:27.796242:	Validation iteration: 11000, Loss: 0.18397125601768494
2018-10-22 12:34:41.701212:	Validation iteration: 11200, Loss: 0.20467185974121094
Validation check mean loss: 0.19503015461231218
Validation loss has worsened. worse_val_checks = 1
Checkpoint
2018-10-22 12:35:25.150944: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 746 of 1000
2018-10-22 12:35:35.454611: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 882 of 1000
2018-10-22 12:35:58.235929: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 946 of 1000
2018-10-22 12:35:58.274868: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 947 of 1000
2018-10-22 12:35:59.030425: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 12:36:10.532580:	Training iteration: 719800, Loss: 0.1513441801071167
2018-10-22 12:36:34.891896:	Training iteration: 720000, Loss: 0.11610597372055054
2018-10-22 12:37:01.138382:	Training iteration: 720200, Loss: 0.12286551296710968
2018-10-22 12:37:28.052044:	Training iteration: 720400, Loss: 0.0961325615644455
2018-10-22 12:37:56.285111:	Training iteration: 720600, Loss: 0.12866908311843872
2018-10-22 12:38:25.215104:	Training iteration: 720800, Loss: 0.08421354740858078
2018-10-22 12:38:54.208519:	Training iteration: 721000, Loss: 0.14951342344284058
2018-10-22 12:39:23.221709:	Training iteration: 721200, Loss: 0.10029686987400055
2018-10-22 12:39:52.441297:	Training iteration: 721400, Loss: 0.13070112466812134
2018-10-22 12:40:21.592732:	Training iteration: 721600, Loss: 0.09739325940608978
2018-10-22 12:40:51.288800:	Training iteration: 721800, Loss: 0.16704365611076355
2018-10-22 12:41:20.890561:	Training iteration: 722000, Loss: 0.12078475207090378
2018-10-22 12:41:50.228504:	Training iteration: 722200, Loss: 0.12272629886865616
2018-10-22 12:42:18.913434:	Training iteration: 722400, Loss: 0.10693000257015228
2018-10-22 12:42:48.162850:	Training iteration: 722600, Loss: 0.112341970205307
2018-10-22 12:43:18.233330:	Training iteration: 722800, Loss: 0.14494381844997406
2018-10-22 12:43:47.612657:	Training iteration: 723000, Loss: 0.11979005485773087
2018-10-22 12:44:16.580223:	Training iteration: 723200, Loss: 0.11705933511257172
2018-10-22 12:44:46.516506:	Training iteration: 723400, Loss: 0.09978391230106354
2018-10-22 12:45:15.417950:	Training iteration: 723600, Loss: 0.18786057829856873
2018-10-22 12:45:44.322982:	Training iteration: 723800, Loss: 0.11452329158782959
2018-10-22 12:46:14.016168:	Training iteration: 724000, Loss: 0.08460168540477753
2018-10-22 12:46:43.396936:	Training iteration: 724200, Loss: 0.20371243357658386
2018-10-22 12:47:12.739139:	Training iteration: 724400, Loss: 0.12226655334234238
2018-10-22 12:47:41.425148:	Training iteration: 724600, Loss: 0.20062904059886932
2018-10-22 12:48:10.979134:	Training iteration: 724800, Loss: 0.12395630776882172
2018-10-22 12:48:40.424143:	Training iteration: 725000, Loss: 0.08498349785804749
2018-10-22 12:49:10.297024:	Training iteration: 725200, Loss: 0.09758670628070831
2018-10-22 12:49:39.728899:	Training iteration: 725400, Loss: 0.09839783608913422
2018-10-22 12:50:14.002795:	Training iteration: 725600, Loss: 0.18636822700500488
2018-10-22 12:50:43.007235:	Training iteration: 725800, Loss: 0.09946411848068237
2018-10-22 12:51:12.418532:	Training iteration: 726000, Loss: 0.11542487889528275
2018-10-22 12:51:42.004263:	Training iteration: 726200, Loss: 0.10679648816585541
2018-10-22 12:52:11.315929:	Training iteration: 726400, Loss: 0.1404031217098236
2018-10-22 12:52:40.801159:	Training iteration: 726600, Loss: 0.11693091690540314
2018-10-22 12:53:10.413096:	Training iteration: 726800, Loss: 0.1272158920764923
2018-10-22 12:53:40.405519:	Training iteration: 727000, Loss: 0.13933305442333221
2018-10-22 12:54:09.883485:	Training iteration: 727200, Loss: 0.09870221465826035
2018-10-22 12:54:38.945673:	Training iteration: 727400, Loss: 0.14630483090877533
2018-10-22 12:55:08.849800:	Training iteration: 727600, Loss: 0.15077176690101624
2018-10-22 12:55:37.778484:	Training iteration: 727800, Loss: 0.1597292721271515
2018-10-22 12:56:08.404033:	Training iteration: 728000, Loss: 0.1470111757516861
2018-10-22 12:56:38.016532:	Training iteration: 728200, Loss: 0.10034555196762085
2018-10-22 12:57:06.836188:	Training iteration: 728400, Loss: 0.14278043806552887
2018-10-22 12:57:36.475749:	Training iteration: 728600, Loss: 0.13326916098594666
2018-10-22 12:58:06.068203:	Training iteration: 728800, Loss: 0.10114504396915436
2018-10-22 12:58:35.483499:	Training iteration: 729000, Loss: 0.14626488089561462
2018-10-22 12:59:04.895716:	Training iteration: 729200, Loss: 0.10261038690805435
2018-10-22 12:59:34.913068:	Training iteration: 729400, Loss: 0.17999371886253357
2018-10-22 13:00:04.221999:	Training iteration: 729600, Loss: 0.10112409293651581
2018-10-22 13:00:33.613721:	Training iteration: 729800, Loss: 0.10408870875835419
2018-10-22 13:01:03.401903:	Training iteration: 730000, Loss: 0.1819930374622345
2018-10-22 13:01:33.164237:	Training iteration: 730200, Loss: 0.16398873925209045
2018-10-22 13:02:01.890571:	Training iteration: 730400, Loss: 0.09831465780735016
2018-10-22 13:02:31.646047:	Training iteration: 730600, Loss: 0.1321856528520584
2018-10-22 13:03:00.795821:	Training iteration: 730800, Loss: 0.13719165325164795
2018-10-22 13:03:30.905074:	Training iteration: 731000, Loss: 0.10962013155221939
2018-10-22 13:04:00.867375:	Training iteration: 731200, Loss: 0.09774526953697205
2018-10-22 13:04:34.324888:	Training iteration: 731400, Loss: 0.1425170600414276
2018-10-22 13:05:06.081815:	Training iteration: 731600, Loss: 0.13915760815143585
2018-10-22 13:05:35.039558:	Training iteration: 731800, Loss: 0.1518242061138153
2018-10-22 13:06:04.380422:	Training iteration: 732000, Loss: 0.12012939155101776
2018-10-22 13:06:29.413769: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 697 of 1000
2018-10-22 13:06:33.443206: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 13:06:47.161358:	Training iteration: 732200, Loss: 0.07549656927585602
2018-10-22 13:07:16.105494:	Training iteration: 732400, Loss: 0.13825416564941406
2018-10-22 13:07:45.521948:	Training iteration: 732600, Loss: 0.10172532498836517
2018-10-22 13:08:15.015718:	Training iteration: 732800, Loss: 0.1115216612815857
2018-10-22 13:08:44.284776:	Training iteration: 733000, Loss: 0.06971704214811325
2018-10-22 13:09:14.510015:	Training iteration: 733200, Loss: 0.12832485139369965
2018-10-22 13:09:43.629663:	Training iteration: 733400, Loss: 0.15294624865055084
2018-10-22 13:10:12.886589:	Training iteration: 733600, Loss: 0.1475163996219635
2018-10-22 13:10:41.650780:	Training iteration: 733800, Loss: 0.10590079426765442
2018-10-22 13:11:11.041756:	Training iteration: 734000, Loss: 0.11650758981704712
2018-10-22 13:11:40.466710:	Training iteration: 734200, Loss: 0.13720619678497314
2018-10-22 13:12:10.489219:	Training iteration: 734400, Loss: 0.1645640879869461
2018-10-22 13:12:40.172668:	Training iteration: 734600, Loss: 0.09030518680810928
2018-10-22 13:13:10.115090:	Training iteration: 734800, Loss: 0.13379846513271332
2018-10-22 13:13:39.882668:	Training iteration: 735000, Loss: 0.1092500388622284
2018-10-22 13:14:09.214667:	Training iteration: 735200, Loss: 0.12273930758237839
2018-10-22 13:14:38.373926:	Training iteration: 735400, Loss: 0.10490597784519196
2018-10-22 13:15:08.567167:	Training iteration: 735600, Loss: 0.10233324766159058
2018-10-22 13:15:38.815876:	Training iteration: 735800, Loss: 0.08955224603414536
2018-10-22 13:16:07.949741:	Training iteration: 736000, Loss: 0.11670319736003876
2018-10-22 13:16:37.389396:	Training iteration: 736200, Loss: 0.09457184374332428
2018-10-22 13:17:06.621544:	Training iteration: 736400, Loss: 0.1094098761677742
2018-10-22 13:17:36.587107:	Training iteration: 736600, Loss: 0.11297321319580078
2018-10-22 13:18:06.200354:	Training iteration: 736800, Loss: 0.12536117434501648
2018-10-22 13:18:35.400447:	Training iteration: 737000, Loss: 0.08172206580638885
2018-10-22 13:19:06.574269:	Training iteration: 737200, Loss: 0.11035218089818954
2018-10-22 13:19:36.215943:	Training iteration: 737400, Loss: 0.0789276510477066
2018-10-22 13:20:05.533763:	Training iteration: 737600, Loss: 0.09528511762619019
2018-10-22 13:20:35.953837:	Training iteration: 737800, Loss: 0.10456573963165283
2018-10-22 13:21:05.247539:	Training iteration: 738000, Loss: 0.12688183784484863
2018-10-22 13:21:35.360423:	Training iteration: 738200, Loss: 0.10559220612049103
2018-10-22 13:22:04.462475:	Training iteration: 738400, Loss: 0.10897068679332733
2018-10-22 13:22:33.776680:	Training iteration: 738600, Loss: 0.10310940444469452
2018-10-22 13:23:03.385618:	Training iteration: 738800, Loss: 0.07991500198841095
2018-10-22 13:23:33.005789:	Training iteration: 739000, Loss: 0.12946568429470062
2018-10-22 13:24:02.497394:	Training iteration: 739200, Loss: 0.12443804740905762
2018-10-22 13:24:31.870541:	Training iteration: 739400, Loss: 0.14400239288806915
2018-10-22 13:25:01.034391:	Training iteration: 739600, Loss: 0.11722743511199951
2018-10-22 13:25:30.108859:	Training iteration: 739800, Loss: 0.12820687890052795
2018-10-22 13:26:00.498023:	Training iteration: 740000, Loss: 0.10274739563465118
2018-10-22 13:26:30.219908:	Training iteration: 740200, Loss: 0.10517091304063797
2018-10-22 13:26:59.917381:	Training iteration: 740400, Loss: 0.10732077807188034
2018-10-22 13:27:29.716482:	Training iteration: 740600, Loss: 0.14170050621032715
2018-10-22 13:27:59.385508:	Training iteration: 740800, Loss: 0.1472524106502533
2018-10-22 13:28:28.801676:	Training iteration: 741000, Loss: 0.1025562509894371
2018-10-22 13:28:58.564077:	Training iteration: 741200, Loss: 0.11726818978786469
2018-10-22 13:29:28.508100:	Training iteration: 741400, Loss: 0.13186028599739075
2018-10-22 13:29:58.044532:	Training iteration: 741600, Loss: 0.1416562795639038
2018-10-22 13:30:27.710009:	Training iteration: 741800, Loss: 0.12165225297212601
2018-10-22 13:30:57.436329:	Training iteration: 742000, Loss: 0.1177583783864975
2018-10-22 13:31:26.782906:	Training iteration: 742200, Loss: 0.12964797019958496
2018-10-22 13:31:55.887052:	Training iteration: 742400, Loss: 0.1818329393863678
2018-10-22 13:32:25.656881:	Training iteration: 742600, Loss: 0.1385953724384308
2018-10-22 13:32:53.931778:	Training iteration: 742800, Loss: 0.08124382793903351
2018-10-22 13:33:24.471645:	Training iteration: 743000, Loss: 0.0941193699836731
2018-10-22 13:33:54.002057:	Training iteration: 743200, Loss: 0.09538272023200989
2018-10-22 13:34:22.993409:	Training iteration: 743400, Loss: 0.10744358599185944
2018-10-22 13:34:52.497987:	Training iteration: 743600, Loss: 0.08585398644208908
2018-10-22 13:35:22.202386:	Training iteration: 743800, Loss: 0.14666128158569336
2018-10-22 13:35:51.603647:	Training iteration: 744000, Loss: 0.14063532650470734
2018-10-22 13:36:21.902059:	Training iteration: 744200, Loss: 0.11419377475976944
2018-10-22 13:36:51.764988:	Training iteration: 744400, Loss: 0.12952029705047607
2018-10-22 13:37:27.004017: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 715 of 1000
2018-10-22 13:37:30.798101: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 13:37:35.124990:	Training iteration: 744600, Loss: 0.09511102735996246
2018-10-22 13:38:03.351956:	Training iteration: 744800, Loss: 0.12402625381946564
2018-10-22 13:38:32.211340:	Training iteration: 745000, Loss: 0.09298324584960938
2018-10-22 13:39:02.491186:	Training iteration: 745200, Loss: 0.10209918767213821
2018-10-22 13:39:31.946210:	Training iteration: 745400, Loss: 0.10243433713912964
2018-10-22 13:40:01.440786:	Training iteration: 745600, Loss: 0.10712011158466339
2018-10-22 13:40:30.835845:	Training iteration: 745800, Loss: 0.08706632256507874
2018-10-22 13:40:59.581018:	Training iteration: 746000, Loss: 0.08457469940185547
2018-10-22 13:41:28.817597:	Training iteration: 746200, Loss: 0.10236550867557526
2018-10-22 13:41:59.253011:	Training iteration: 746400, Loss: 0.08164030313491821
2018-10-22 13:42:28.937199:	Training iteration: 746600, Loss: 0.06363862007856369
2018-10-22 13:42:58.657525:	Training iteration: 746800, Loss: 0.07868437469005585
2018-10-22 13:43:27.986989:	Training iteration: 747000, Loss: 0.12224359810352325
2018-10-22 13:43:57.259372:	Training iteration: 747200, Loss: 0.123585544526577
2018-10-22 13:44:27.127754:	Training iteration: 747400, Loss: 0.10099063068628311
2018-10-22 13:44:56.495995:	Training iteration: 747600, Loss: 0.1284855604171753
2018-10-22 13:45:26.303710:	Training iteration: 747800, Loss: 0.09590199589729309
2018-10-22 13:45:56.106384:	Training iteration: 748000, Loss: 0.10599412769079208
2018-10-22 13:46:25.123777:	Training iteration: 748200, Loss: 0.08994649350643158
2018-10-22 13:46:54.703289:	Training iteration: 748400, Loss: 0.06728355586528778
2018-10-22 13:47:24.135720:	Training iteration: 748600, Loss: 0.08571739494800568
2018-10-22 13:47:54.079690:	Training iteration: 748800, Loss: 0.10889163613319397
2018-10-22 13:48:23.009890:	Training iteration: 749000, Loss: 0.07829601317644119
2018-10-22 13:48:52.489011:	Training iteration: 749200, Loss: 0.14723935723304749
2018-10-22 13:49:22.344425:	Training iteration: 749400, Loss: 0.0980662852525711
2018-10-22 13:49:51.891671:	Training iteration: 749600, Loss: 0.08474323898553848
2018-10-22 13:50:20.449931:	Training iteration: 749800, Loss: 0.1289345622062683
2018-10-22 13:50:49.779472:	Training iteration: 750000, Loss: 0.07455140352249146
2018-10-22 13:51:19.301808:	Training iteration: 750200, Loss: 0.10380758345127106
2018-10-22 13:51:48.789194:	Training iteration: 750400, Loss: 0.0928720086812973
2018-10-22 13:52:18.244745:	Training iteration: 750600, Loss: 0.09527057409286499
2018-10-22 13:52:47.357986:	Training iteration: 750800, Loss: 0.12987712025642395
2018-10-22 13:53:17.438860:	Training iteration: 751000, Loss: 0.13085974752902985
2018-10-22 13:53:47.074021:	Training iteration: 751200, Loss: 0.08291883766651154
2018-10-22 13:54:15.953649:	Training iteration: 751400, Loss: 0.0896148532629013
2018-10-22 13:54:45.092875:	Training iteration: 751600, Loss: 0.09707462042570114
2018-10-22 13:55:14.805412:	Training iteration: 751800, Loss: 0.05578518658876419
2018-10-22 13:55:44.366590:	Training iteration: 752000, Loss: 0.0985286682844162
2018-10-22 13:56:13.956699:	Training iteration: 752200, Loss: 0.07667621970176697
2018-10-22 13:56:43.540524:	Training iteration: 752400, Loss: 0.0748562440276146
2018-10-22 13:57:13.297355:	Training iteration: 752600, Loss: 0.08784171938896179
2018-10-22 13:57:42.652598:	Training iteration: 752800, Loss: 0.07089029252529144
2018-10-22 13:58:11.131732:	Training iteration: 753000, Loss: 0.0853663831949234
2018-10-22 13:58:40.581161:	Training iteration: 753200, Loss: 0.06877636164426804
2018-10-22 13:59:09.735074:	Training iteration: 753400, Loss: 0.0939507782459259
2018-10-22 13:59:40.666681:	Training iteration: 753600, Loss: 0.10498794168233871
2018-10-22 14:00:09.069392:	Training iteration: 753800, Loss: 0.16311876475811005
2018-10-22 14:00:37.390151:	Training iteration: 754000, Loss: 0.10171008110046387
2018-10-22 14:01:06.777461:	Training iteration: 754200, Loss: 0.12356653809547424
2018-10-22 14:01:36.492104:	Training iteration: 754400, Loss: 0.13700641691684723
2018-10-22 14:02:05.373013:	Training iteration: 754600, Loss: 0.09909141063690186
2018-10-22 14:02:35.037396:	Training iteration: 754800, Loss: 0.09868612140417099
2018-10-22 14:03:06.751191:	Training iteration: 755000, Loss: 0.10283675789833069
2018-10-22 14:03:35.957203:	Training iteration: 755200, Loss: 0.09320980310440063
2018-10-22 14:04:05.519481:	Training iteration: 755400, Loss: 0.07974400371313095
2018-10-22 14:04:33.968498:	Training iteration: 755600, Loss: 0.09017467498779297
2018-10-22 14:05:04.493277:	Training iteration: 755800, Loss: 0.14292652904987335
2018-10-22 14:05:32.716266:	Training iteration: 756000, Loss: 0.087556391954422
2018-10-22 14:06:02.132227:	Training iteration: 756200, Loss: 0.06770794093608856
2018-10-22 14:06:31.056164:	Training iteration: 756400, Loss: 0.09311900287866592
2018-10-22 14:07:00.365359:	Training iteration: 756600, Loss: 0.14746004343032837
2018-10-22 14:07:29.104478:	Training iteration: 756800, Loss: 0.07957267761230469
2018-10-22 14:07:58.333234:	Training iteration: 757000, Loss: 0.1139729917049408
2018-10-22 14:08:12.604842: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 707 of 1000
2018-10-22 14:08:16.373078: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:08:40.218290:	Training iteration: 757200, Loss: 0.10937970876693726
2018-10-22 14:09:09.239090:	Training iteration: 757400, Loss: 0.09215851128101349
2018-10-22 14:09:38.275338:	Training iteration: 757600, Loss: 0.1141766831278801
2018-10-22 14:10:08.909027:	Training iteration: 757800, Loss: 0.12700198590755463
2018-10-22 14:10:37.090847:	Training iteration: 758000, Loss: 0.11378664523363113
2018-10-22 14:11:05.889390:	Training iteration: 758200, Loss: 0.16511818766593933
2018-10-22 14:11:34.949577:	Training iteration: 758400, Loss: 0.12958954274654388
2018-10-22 14:12:04.197973:	Training iteration: 758600, Loss: 0.22959285974502563
2018-10-22 14:12:34.040503:	Training iteration: 758800, Loss: 0.14642149209976196
2018-10-22 14:13:03.021648:	Training iteration: 759000, Loss: 0.10343945771455765
2018-10-22 14:13:31.772819:	Training iteration: 759200, Loss: 0.19313837587833405
2018-10-22 14:14:01.008373:	Training iteration: 759400, Loss: 0.08704207837581635
2018-10-22 14:14:30.329299:	Training iteration: 759600, Loss: 0.12693831324577332
2018-10-22 14:14:59.219643:	Training iteration: 759800, Loss: 0.18103596568107605
2018-10-22 14:15:29.431859:	Training iteration: 760000, Loss: 0.14040960371494293
2018-10-22 14:15:57.329333:	Training iteration: 760200, Loss: 0.17518259584903717
2018-10-22 14:16:27.174515:	Training iteration: 760400, Loss: 0.14691594243049622
2018-10-22 14:16:56.527580:	Training iteration: 760600, Loss: 0.11401407420635223
2018-10-22 14:17:25.338991:	Training iteration: 760800, Loss: 0.11922495812177658
2018-10-22 14:17:54.827105:	Training iteration: 761000, Loss: 0.09440064430236816
2018-10-22 14:18:23.967696:	Training iteration: 761200, Loss: 0.11477438360452652
2018-10-22 14:18:53.490618:	Training iteration: 761400, Loss: 0.12029913067817688
2018-10-22 14:19:23.606800:	Training iteration: 761600, Loss: 0.09973447024822235
2018-10-22 14:19:53.101823:	Training iteration: 761800, Loss: 0.1439417451620102
2018-10-22 14:20:23.953682:	Training iteration: 762000, Loss: 0.10602974891662598
2018-10-22 14:20:53.705594:	Training iteration: 762200, Loss: 0.12986187636852264
2018-10-22 14:21:22.542488:	Training iteration: 762400, Loss: 0.1738555133342743
2018-10-22 14:21:51.588639:	Training iteration: 762600, Loss: 0.16566619277000427
2018-10-22 14:22:21.127591:	Training iteration: 762800, Loss: 0.11279262602329254
2018-10-22 14:22:50.603742:	Training iteration: 763000, Loss: 0.13693755865097046
2018-10-22 14:23:19.867562:	Training iteration: 763200, Loss: 0.19123411178588867
2018-10-22 14:23:48.664338:	Training iteration: 763400, Loss: 0.07560352981090546
2018-10-22 14:24:18.185865:	Training iteration: 763600, Loss: 0.11894887685775757
2018-10-22 14:24:47.464767:	Training iteration: 763800, Loss: 0.1429661214351654
2018-10-22 14:25:16.894859:	Training iteration: 764000, Loss: 0.18367241322994232
2018-10-22 14:25:46.043874:	Training iteration: 764200, Loss: 0.1906794011592865
2018-10-22 14:26:16.116137:	Training iteration: 764400, Loss: 0.10387273132801056
2018-10-22 14:26:45.877996:	Training iteration: 764600, Loss: 0.11216290295124054
2018-10-22 14:27:14.811411:	Training iteration: 764800, Loss: 0.14662179350852966
2018-10-22 14:27:45.247991:	Training iteration: 765000, Loss: 0.17136162519454956
2018-10-22 14:28:13.847274:	Training iteration: 765200, Loss: 0.10742618143558502
2018-10-22 14:28:42.571540:	Training iteration: 765400, Loss: 0.16498886048793793
2018-10-22 14:29:11.979941:	Training iteration: 765600, Loss: 0.13999132812023163
2018-10-22 14:29:41.413890:	Training iteration: 765800, Loss: 0.08529600501060486
2018-10-22 14:30:10.821754:	Training iteration: 766000, Loss: 0.09836433827877045
2018-10-22 14:30:40.829603:	Training iteration: 766200, Loss: 0.07848729938268661
2018-10-22 14:31:10.217458:	Training iteration: 766400, Loss: 0.16427192091941833
2018-10-22 14:31:39.780086:	Training iteration: 766600, Loss: 0.1734236478805542
2018-10-22 14:32:09.567802:	Training iteration: 766800, Loss: 0.1454702615737915
2018-10-22 14:32:39.400081:	Training iteration: 767000, Loss: 0.13687798380851746
2018-10-22 14:33:08.609387:	Training iteration: 767200, Loss: 0.11404868960380554
2018-10-22 14:33:38.168933:	Training iteration: 767400, Loss: 0.16547739505767822
2018-10-22 14:34:07.822988:	Training iteration: 767600, Loss: 0.16562765836715698
2018-10-22 14:34:37.457575:	Training iteration: 767800, Loss: 0.1959504634141922
2018-10-22 14:35:06.467982:	Training iteration: 768000, Loss: 0.1093713641166687
2018-10-22 14:35:35.708719:	Training iteration: 768200, Loss: 0.13801619410514832
2018-10-22 14:36:04.405241:	Training iteration: 768400, Loss: 0.1372932344675064
2018-10-22 14:36:33.183985:	Training iteration: 768600, Loss: 0.08473756909370422
2018-10-22 14:37:03.156018:	Training iteration: 768800, Loss: 0.11874949932098389
2018-10-22 14:37:33.259444:	Training iteration: 769000, Loss: 0.10048200935125351
2018-10-22 14:38:03.092956:	Training iteration: 769200, Loss: 0.09491154551506042
2018-10-22 14:38:32.253782:	Training iteration: 769400, Loss: 0.08213464915752411
2018-10-22 14:39:02.643362:	Training iteration: 769600, Loss: 0.13470624387264252
2018-10-22 14:39:31.371774:	Training iteration: 769800, Loss: 0.1441320776939392
2018-10-22 14:40:01.058301:	Training iteration: 770000, Loss: 0.17326763272285461
2018-10-22 14:40:18.222869: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 827 of 1000
2018-10-22 14:40:20.030344: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 14:40:41.558534:	Training iteration: 770200, Loss: 0.3029760718345642
2018-10-22 14:41:10.705317:	Training iteration: 770400, Loss: 0.16187942028045654
2018-10-22 14:41:39.944645:	Training iteration: 770600, Loss: 0.37654566764831543
2018-10-22 14:42:09.409251:	Training iteration: 770800, Loss: 0.11525477468967438
2018-10-22 14:42:38.902845:	Training iteration: 771000, Loss: 0.10576210170984268
2018-10-22 14:43:07.599472:	Training iteration: 771200, Loss: 0.20116901397705078
2018-10-22 14:43:37.192434:	Training iteration: 771400, Loss: 0.181939035654068
2018-10-22 14:44:06.539621:	Training iteration: 771600, Loss: 0.19959038496017456
2018-10-22 14:44:35.708457:	Training iteration: 771800, Loss: 0.1304958462715149
2018-10-22 14:45:05.206290:	Training iteration: 772000, Loss: 0.09885427355766296
2018-10-22 14:45:34.254085:	Training iteration: 772200, Loss: 0.19364774227142334
2018-10-22 14:46:03.927629:	Training iteration: 772400, Loss: 0.14255845546722412
2018-10-22 14:46:33.399596:	Training iteration: 772600, Loss: 0.14077407121658325
2018-10-22 14:47:03.551593:	Training iteration: 772800, Loss: 0.12455984950065613
2018-10-22 14:47:33.514919:	Training iteration: 773000, Loss: 0.13432231545448303
2018-10-22 14:48:02.406680:	Training iteration: 773200, Loss: 0.16620805859565735
2018-10-22 14:48:32.432159:	Training iteration: 773400, Loss: 0.18222057819366455
2018-10-22 14:49:02.334812:	Training iteration: 773600, Loss: 0.11325877904891968
2018-10-22 14:49:31.530993:	Training iteration: 773800, Loss: 0.1188647449016571
2018-10-22 14:50:01.190646:	Training iteration: 774000, Loss: 0.1649390161037445
2018-10-22 14:50:30.382216:	Training iteration: 774200, Loss: 0.14282234013080597
2018-10-22 14:50:59.577802:	Training iteration: 774400, Loss: 0.1125459223985672
2018-10-22 14:51:28.849364:	Training iteration: 774600, Loss: 0.10326631367206573
2018-10-22 14:51:57.742861:	Training iteration: 774800, Loss: 0.15174293518066406
2018-10-22 14:52:27.666990:	Training iteration: 775000, Loss: 0.21916848421096802
2018-10-22 14:52:56.192887:	Training iteration: 775200, Loss: 0.19251221418380737
2018-10-22 14:53:26.190417:	Training iteration: 775400, Loss: 0.12495216727256775
2018-10-22 14:53:55.075423:	Training iteration: 775600, Loss: 0.14728111028671265
2018-10-22 14:54:24.200609:	Training iteration: 775800, Loss: 0.22282755374908447
2018-10-22 14:54:53.420366:	Training iteration: 776000, Loss: 0.1563253402709961
2018-10-22 14:55:23.889414:	Training iteration: 776200, Loss: 0.1495843529701233
2018-10-22 14:55:53.714842:	Training iteration: 776400, Loss: 0.1644047200679779
2018-10-22 14:56:23.515192:	Training iteration: 776600, Loss: 0.25238099694252014
2018-10-22 14:56:52.783818:	Training iteration: 776800, Loss: 0.17149235308170319
2018-10-22 14:57:21.404285:	Training iteration: 777000, Loss: 0.18781778216362
2018-10-22 14:57:51.474072:	Training iteration: 777200, Loss: 0.22039595246315002
2018-10-22 14:58:21.316985:	Training iteration: 777400, Loss: 0.2374182790517807
2018-10-22 14:58:50.029654:	Training iteration: 777600, Loss: 0.21390217542648315
2018-10-22 14:59:19.436297:	Training iteration: 777800, Loss: 0.2029803991317749
2018-10-22 14:59:48.395970:	Training iteration: 778000, Loss: 0.14127299189567566
2018-10-22 15:00:17.352119:	Training iteration: 778200, Loss: 0.1476116180419922
2018-10-22 15:00:47.798891:	Training iteration: 778400, Loss: 0.18101119995117188
2018-10-22 15:01:17.474294:	Training iteration: 778600, Loss: 0.1494421511888504
2018-10-22 15:01:46.464003:	Training iteration: 778800, Loss: 0.153642475605011
2018-10-22 15:02:16.041873:	Training iteration: 779000, Loss: 0.1097317785024643
2018-10-22 15:02:45.584903:	Training iteration: 779200, Loss: 0.18068554997444153
2018-10-22 15:03:15.077511:	Training iteration: 779400, Loss: 0.1467897891998291
2018-10-22 15:03:44.654120:	Training iteration: 779600, Loss: 0.13137608766555786
2018-10-22 15:04:14.068902:	Training iteration: 779800, Loss: 0.13982626795768738
2018-10-22 15:04:43.641392:	Training iteration: 780000, Loss: 0.19491718709468842
2018-10-22 15:05:13.524235:	Training iteration: 780200, Loss: 0.1688876450061798
2018-10-22 15:05:43.185678:	Training iteration: 780400, Loss: 0.16354304552078247
2018-10-22 15:06:12.893891:	Training iteration: 780600, Loss: 0.18205560743808746
2018-10-22 15:06:42.418374:	Training iteration: 780800, Loss: 0.09835240244865417
2018-10-22 15:07:12.560417:	Training iteration: 781000, Loss: 0.16596266627311707
2018-10-22 15:07:41.854320:	Training iteration: 781200, Loss: 0.1853005737066269
2018-10-22 15:08:11.229816:	Training iteration: 781400, Loss: 0.226979598402977
2018-10-22 15:08:40.719383:	Training iteration: 781600, Loss: 0.19465018808841705
2018-10-22 15:09:10.067252:	Training iteration: 781800, Loss: 0.12577910721302032
2018-10-22 15:09:39.417940:	Training iteration: 782000, Loss: 0.16166964173316956
2018-10-22 15:10:08.782737:	Training iteration: 782200, Loss: 0.17486993968486786
2018-10-22 15:10:37.694730:	Training iteration: 782400, Loss: 0.20020650327205658
2018-10-22 15:11:07.341693:	Training iteration: 782600, Loss: 0.16275878250598907
2018-10-22 15:11:36.314200:	Training iteration: 782800, Loss: 0.20095017552375793
2018-10-22 15:12:06.208873:	Training iteration: 783000, Loss: 0.14884264767169952
2018-10-22 15:12:36.025789:	Training iteration: 783200, Loss: 0.19070535898208618
2018-10-22 15:13:05.782558:	Training iteration: 783400, Loss: 0.15531906485557556
2018-10-22 15:13:35.603849:	Training iteration: 783600, Loss: 0.14146584272384644
2018-10-22 15:14:04.711110:	Training iteration: 783800, Loss: 0.2211628258228302
2018-10-22 15:14:34.269886:	Training iteration: 784000, Loss: 0.20489534735679626
2018-10-22 15:15:03.213703:	Training iteration: 784200, Loss: 0.09933507442474365
2018-10-22 15:15:33.002365:	Training iteration: 784400, Loss: 0.16044044494628906
2018-10-22 15:16:02.780613:	Training iteration: 784600, Loss: 0.12076956033706665
2018-10-22 15:16:32.229419:	Training iteration: 784800, Loss: 0.18393823504447937
2018-10-22 15:17:02.098281:	Training iteration: 785000, Loss: 0.12283477187156677
2018-10-22 15:17:31.631693:	Training iteration: 785200, Loss: 0.11838741600513458
2018-10-22 15:18:00.442987:	Training iteration: 785400, Loss: 0.17562149465084076
2018-10-22 15:18:30.164809:	Training iteration: 785600, Loss: 0.12898355722427368
2018-10-22 15:19:00.546960:	Training iteration: 785800, Loss: 0.1673191785812378
2018-10-22 15:19:30.252573:	Training iteration: 786000, Loss: 0.14666183292865753
2018-10-22 15:19:59.574934:	Training iteration: 786200, Loss: 0.21948307752609253
2018-10-22 15:20:29.538179:	Training iteration: 786400, Loss: 0.17654699087142944
2018-10-22 15:20:58.267492:	Training iteration: 786600, Loss: 0.32668930292129517
2018-10-22 15:21:27.556618:	Training iteration: 786800, Loss: 0.18110617995262146
2018-10-22 15:21:56.923731:	Training iteration: 787000, Loss: 0.2551043927669525
2018-10-22 15:22:26.771073:	Training iteration: 787200, Loss: 0.18425299227237701
2018-10-22 15:22:55.810636:	Training iteration: 787400, Loss: 0.3134746253490448
2018-10-22 15:23:25.683492:	Training iteration: 787600, Loss: 0.1855890303850174
2018-10-22 15:23:54.550064:	Training iteration: 787800, Loss: 0.13572433590888977
2018-10-22 15:24:23.858565:	Training iteration: 788000, Loss: 0.2295110523700714
2018-10-22 15:24:53.841418:	Training iteration: 788200, Loss: 0.17955049872398376
2018-10-22 15:25:23.421763:	Training iteration: 788400, Loss: 0.1381511241197586
2018-10-22 15:25:52.630117:	Training iteration: 788600, Loss: 0.1608564257621765
2018-10-22 15:26:21.957569:	Training iteration: 788800, Loss: 0.17164508998394012
2018-10-22 15:26:52.077214:	Training iteration: 789000, Loss: 0.1548852026462555
2018-10-22 15:27:21.254719:	Training iteration: 789200, Loss: 0.1538676917552948
2018-10-22 15:27:50.269106:	Training iteration: 789400, Loss: 0.08682607114315033
2018-10-22 15:28:18.852704:	Training iteration: 789600, Loss: 0.1259651482105255
2018-10-22 15:28:48.202638:	Training iteration: 789800, Loss: 0.15934543311595917
2018-10-22 15:29:18.385602:	Training iteration: 790000, Loss: 0.20009569823741913
2018-10-22 15:29:48.025950:	Training iteration: 790200, Loss: 0.12955203652381897
2018-10-22 15:30:17.219056:	Training iteration: 790400, Loss: 0.13041463494300842
2018-10-22 15:30:47.244834:	Training iteration: 790600, Loss: 0.22890299558639526
2018-10-22 15:31:16.749709:	Training iteration: 790800, Loss: 0.14870800077915192
2018-10-22 15:31:46.199413:	Training iteration: 791000, Loss: 0.23948565125465393
2018-10-22 15:32:15.964442:	Training iteration: 791200, Loss: 0.18309533596038818
2018-10-22 15:32:45.315437:	Training iteration: 791400, Loss: 0.18605995178222656
2018-10-22 15:33:14.491908:	Training iteration: 791600, Loss: 0.11177505552768707
2018-10-22 15:33:43.733594:	Training iteration: 791800, Loss: 0.12336555123329163
2018-10-22 15:34:13.614176:	Training iteration: 792000, Loss: 0.14569821953773499
2018-10-22 15:34:43.223262:	Training iteration: 792200, Loss: 0.15968170762062073
2018-10-22 15:35:11.825523:	Training iteration: 792400, Loss: 0.14081257581710815
2018-10-22 15:35:40.665417:	Training iteration: 792600, Loss: 0.20721317827701569
2018-10-22 15:36:09.539909:	Training iteration: 792800, Loss: 0.18386662006378174
2018-10-22 15:36:39.466819:	Training iteration: 793000, Loss: 0.18550732731819153
2018-10-22 15:37:09.167329:	Training iteration: 793200, Loss: 0.17000475525856018
2018-10-22 15:37:38.192694:	Training iteration: 793400, Loss: 0.1686510145664215
2018-10-22 15:38:07.522698:	Training iteration: 793600, Loss: 0.17337250709533691
2018-10-22 15:38:36.508409:	Training iteration: 793800, Loss: 0.1512296497821808
2018-10-22 15:39:05.828742:	Training iteration: 794000, Loss: 0.15693500638008118
2018-10-22 15:39:36.901553:	Training iteration: 794200, Loss: 0.15196210145950317
2018-10-22 15:40:06.439580:	Training iteration: 794400, Loss: 0.16756264865398407
2018-10-22 15:40:35.218740:	Training iteration: 794600, Loss: 0.14659111201763153
2018-10-22 15:41:04.917261:	Training iteration: 794800, Loss: 0.13607509434223175
2018-10-22 15:41:34.125951:	Training iteration: 795000, Loss: 0.16458576917648315
2018-10-22 15:42:03.763899:	Training iteration: 795200, Loss: 0.135446235537529
2018-10-22 15:42:32.977988:	Training iteration: 795400, Loss: 0.14956006407737732
2018-10-22 15:43:02.707354:	Training iteration: 795600, Loss: 0.12932167947292328
2018-10-22 15:43:32.326695:	Training iteration: 795800, Loss: 0.11177481710910797
2018-10-22 15:44:02.691935:	Training iteration: 796000, Loss: 0.12980961799621582
2018-10-22 15:44:32.908004:	Training iteration: 796200, Loss: 0.12673594057559967
2018-10-22 15:45:02.522646:	Training iteration: 796400, Loss: 0.1963016390800476
2018-10-22 15:45:32.137492:	Training iteration: 796600, Loss: 0.1676730513572693
2018-10-22 15:46:01.330286:	Training iteration: 796800, Loss: 0.14791437983512878
2018-10-22 15:46:30.792492:	Training iteration: 797000, Loss: 0.15316593647003174
2018-10-22 15:47:00.399821:	Training iteration: 797200, Loss: 0.14140430092811584
2018-10-22 15:47:29.907586:	Training iteration: 797400, Loss: 0.14677654206752777
2018-10-22 15:47:59.548402:	Training iteration: 797600, Loss: 0.14871804416179657
2018-10-22 15:48:28.774062:	Training iteration: 797800, Loss: 0.097505122423172
2018-10-22 15:48:57.406986:	Training iteration: 798000, Loss: 0.12719719111919403
2018-10-22 15:49:26.437373:	Training iteration: 798200, Loss: 0.18174412846565247
2018-10-22 15:49:55.455579:	Training iteration: 798400, Loss: 0.12476416677236557
2018-10-22 15:50:24.845172:	Training iteration: 798600, Loss: 0.21704062819480896
2018-10-22 15:50:54.076613:	Training iteration: 798800, Loss: 0.13614971935749054
2018-10-22 15:51:24.261711:	Training iteration: 799000, Loss: 0.14901533722877502
2018-10-22 15:51:53.312509:	Training iteration: 799200, Loss: 0.16952604055404663
2018-10-22 15:52:22.361238:	Training iteration: 799400, Loss: 0.17752961814403534
2018-10-22 15:52:52.029501:	Training iteration: 799600, Loss: 0.19650182127952576
2018-10-22 15:53:21.323566:	Training iteration: 799800, Loss: 0.18164992332458496
2018-10-22 15:53:50.801266:	Training iteration: 800000, Loss: 0.10652617365121841
2018-10-22 15:54:20.771845:	Training iteration: 800200, Loss: 0.18423865735530853
2018-10-22 15:54:50.283559:	Training iteration: 800400, Loss: 0.15540505945682526
2018-10-22 15:55:20.114942:	Training iteration: 800600, Loss: 0.14763981103897095
2018-10-22 15:55:49.628811:	Training iteration: 800800, Loss: 0.1655072122812271
2018-10-22 15:56:19.501306:	Training iteration: 801000, Loss: 0.15264892578125
2018-10-22 15:56:48.751617:	Training iteration: 801200, Loss: 0.17229783535003662
2018-10-22 15:57:17.995869:	Training iteration: 801400, Loss: 0.14376837015151978
2018-10-22 15:57:47.499303:	Training iteration: 801600, Loss: 0.16948288679122925
2018-10-22 15:58:17.007601:	Training iteration: 801800, Loss: 0.19281736016273499
2018-10-22 15:58:45.829435:	Training iteration: 802000, Loss: 0.13281592726707458
2018-10-22 15:59:15.929222:	Training iteration: 802200, Loss: 0.12416248023509979
2018-10-22 15:59:46.138027:	Training iteration: 802400, Loss: 0.15374347567558289
2018-10-22 16:00:15.227154:	Training iteration: 802600, Loss: 0.12312179058790207
2018-10-22 16:00:44.564177:	Training iteration: 802800, Loss: 0.17264322936534882
2018-10-22 16:01:14.163743:	Training iteration: 803000, Loss: 0.15962615609169006
2018-10-22 16:01:43.923145:	Training iteration: 803200, Loss: 0.16772711277008057
2018-10-22 16:02:13.875987:	Training iteration: 803400, Loss: 0.21421989798545837
2018-10-22 16:02:43.444568:	Training iteration: 803600, Loss: 0.17319463193416595
2018-10-22 16:03:12.923402:	Training iteration: 803800, Loss: 0.17257799208164215
2018-10-22 16:03:42.638408:	Training iteration: 804000, Loss: 0.12295190244913101
2018-10-22 16:04:12.366508:	Training iteration: 804200, Loss: 0.1572292447090149
2018-10-22 16:04:42.468001:	Training iteration: 804400, Loss: 0.17628571391105652
2018-10-22 16:05:12.206799:	Training iteration: 804600, Loss: 0.14222849905490875
2018-10-22 16:05:42.076270:	Training iteration: 804800, Loss: 0.12751571834087372
2018-10-22 16:06:11.375105:	Training iteration: 805000, Loss: 0.1910804957151413
2018-10-22 16:06:41.209636:	Training iteration: 805200, Loss: 0.12227518111467361
2018-10-22 16:07:10.213824:	Training iteration: 805400, Loss: 0.114140585064888
2018-10-22 16:07:39.631849:	Training iteration: 805600, Loss: 0.18033123016357422
2018-10-22 16:08:08.759067:	Training iteration: 805800, Loss: 0.1567884385585785
2018-10-22 16:08:38.377673:	Training iteration: 806000, Loss: 0.20218390226364136
2018-10-22 16:09:08.042311:	Training iteration: 806200, Loss: 0.14145143330097198
2018-10-22 16:09:38.232220:	Training iteration: 806400, Loss: 0.1398484706878662
2018-10-22 16:10:06.786563:	Training iteration: 806600, Loss: 0.15059779584407806
2018-10-22 16:10:36.576836:	Training iteration: 806800, Loss: 0.1621204912662506
2018-10-22 16:11:06.219989:	Training iteration: 807000, Loss: 0.17341375350952148
2018-10-22 16:11:35.544621:	Training iteration: 807200, Loss: 0.15481874346733093
2018-10-22 16:12:04.904336:	Training iteration: 807400, Loss: 0.21836824715137482
2018-10-22 16:12:34.211668:	Training iteration: 807600, Loss: 0.16930542886257172
2018-10-22 16:13:04.195655:	Training iteration: 807800, Loss: 0.1434253454208374
2018-10-22 16:13:33.340133:	Training iteration: 808000, Loss: 0.18262925744056702
2018-10-22 16:14:02.042134:	Training iteration: 808200, Loss: 0.16689376533031464
2018-10-22 16:14:32.407906:	Training iteration: 808400, Loss: 0.1586206704378128
2018-10-22 16:15:02.381765:	Training iteration: 808600, Loss: 0.13579915463924408
2018-10-22 16:15:31.838151:	Training iteration: 808800, Loss: 0.15458478033542633
2018-10-22 16:16:01.493213:	Training iteration: 809000, Loss: 0.09517301619052887
2018-10-22 16:16:31.511780:	Training iteration: 809200, Loss: 0.18773628771305084
2018-10-22 16:17:00.446426:	Training iteration: 809400, Loss: 0.14599169790744781
2018-10-22 16:17:28.826014:	Training iteration: 809600, Loss: 0.16864019632339478
2018-10-22 16:17:58.383356:	Training iteration: 809800, Loss: 0.12638750672340393
2018-10-22 16:18:27.810117:	Training iteration: 810000, Loss: 0.1289788782596588
2018-10-22 16:18:58.063925:	Training iteration: 810200, Loss: 0.10632950812578201
2018-10-22 16:19:27.864684:	Training iteration: 810400, Loss: 0.12664347887039185
2018-10-22 16:19:57.548405:	Training iteration: 810600, Loss: 0.13932161033153534
2018-10-22 16:20:26.979394:	Training iteration: 810800, Loss: 0.11364062875509262
2018-10-22 16:20:56.526384:	Training iteration: 811000, Loss: 0.17197969555854797
2018-10-22 16:21:25.759050:	Training iteration: 811200, Loss: 0.12701353430747986
2018-10-22 16:21:55.455350:	Training iteration: 811400, Loss: 0.16848857700824738
2018-10-22 16:22:25.385190:	Training iteration: 811600, Loss: 0.1828790009021759
2018-10-22 16:22:55.368484:	Training iteration: 811800, Loss: 0.15243971347808838
2018-10-22 16:23:24.621524:	Training iteration: 812000, Loss: 0.12433083355426788
2018-10-22 16:23:53.655322:	Training iteration: 812200, Loss: 0.22677473723888397
2018-10-22 16:24:23.387704:	Training iteration: 812400, Loss: 0.17249083518981934
2018-10-22 16:24:53.517869:	Training iteration: 812600, Loss: 0.14847993850708008
2018-10-22 16:25:22.770132:	Training iteration: 812800, Loss: 0.1790294498205185
2018-10-22 16:25:52.995756:	Training iteration: 813000, Loss: 0.14339525997638702
2018-10-22 16:26:22.580923:	Training iteration: 813200, Loss: 0.14392617344856262
2018-10-22 16:26:52.199741:	Training iteration: 813400, Loss: 0.13175693154335022
2018-10-22 16:27:21.882268:	Training iteration: 813600, Loss: 0.0753859430551529
2018-10-22 16:27:50.717711:	Training iteration: 813800, Loss: 0.1348467767238617
2018-10-22 16:28:20.056742:	Training iteration: 814000, Loss: 0.2001623511314392
2018-10-22 16:28:49.276588:	Training iteration: 814200, Loss: 0.1305139660835266
2018-10-22 16:29:18.291893:	Training iteration: 814400, Loss: 0.18678432703018188
2018-10-22 16:29:48.065278:	Training iteration: 814600, Loss: 0.12820570170879364
2018-10-22 16:30:17.483324:	Training iteration: 814800, Loss: 0.15920251607894897
2018-10-22 16:30:47.461963:	Training iteration: 815000, Loss: 0.14959856867790222
2018-10-22 16:31:16.011844:	Training iteration: 815200, Loss: 0.153146892786026
2018-10-22 16:31:45.832425:	Training iteration: 815400, Loss: 0.15185068547725677
2018-10-22 16:32:15.694019:	Training iteration: 815600, Loss: 0.16699685156345367
2018-10-22 16:32:45.661578:	Training iteration: 815800, Loss: 0.19655461609363556
2018-10-22 16:33:14.462942:	Training iteration: 816000, Loss: 0.16394132375717163
2018-10-22 16:33:44.387619:	Training iteration: 816200, Loss: 0.13268901407718658
2018-10-22 16:34:12.992222:	Training iteration: 816400, Loss: 0.13865625858306885
2018-10-22 16:34:43.163018:	Training iteration: 816600, Loss: 0.1259954571723938
2018-10-22 16:35:12.516938:	Training iteration: 816800, Loss: 0.13698171079158783
2018-10-22 16:35:41.429361:	Training iteration: 817000, Loss: 0.09805640578269958
2018-10-22 16:36:10.075766:	Training iteration: 817200, Loss: 0.09335412085056305
2018-10-22 16:36:40.166172:	Training iteration: 817400, Loss: 0.10400214791297913
2018-10-22 16:37:09.427327:	Training iteration: 817600, Loss: 0.14790776371955872
2018-10-22 16:37:39.093397:	Training iteration: 817800, Loss: 0.11569343507289886
2018-10-22 16:38:09.272425:	Training iteration: 818000, Loss: 0.15507270395755768
2018-10-22 16:38:38.203112:	Training iteration: 818200, Loss: 0.13923494517803192
2018-10-22 16:39:07.644809:	Training iteration: 818400, Loss: 0.20388060808181763
2018-10-22 16:39:36.579910:	Training iteration: 818600, Loss: 0.15078160166740417
2018-10-22 16:40:06.757669:	Training iteration: 818800, Loss: 0.21335044503211975
2018-10-22 16:40:36.278790:	Training iteration: 819000, Loss: 0.16131511330604553
2018-10-22 16:41:05.323913:	Training iteration: 819200, Loss: 0.1260320544242859
2018-10-22 16:41:34.798698:	Training iteration: 819400, Loss: 0.2388012409210205
2018-10-22 16:42:04.097123:	Training iteration: 819600, Loss: 0.14659515023231506
2018-10-22 16:42:33.801673:	Training iteration: 819800, Loss: 0.17878475785255432
2018-10-22 16:43:03.713561:	Training iteration: 820000, Loss: 0.1737917959690094
2018-10-22 16:43:33.148346:	Training iteration: 820200, Loss: 0.18402008712291718
2018-10-22 16:44:02.756789:	Training iteration: 820400, Loss: 0.11485995352268219
2018-10-22 16:44:32.782441:	Training iteration: 820600, Loss: 0.14399608969688416
2018-10-22 16:45:02.615756:	Training iteration: 820800, Loss: 0.14856328070163727
2018-10-22 16:45:32.362990:	Training iteration: 821000, Loss: 0.1358325183391571
2018-10-22 16:46:01.666716:	Training iteration: 821200, Loss: 0.19278690218925476
2018-10-22 16:46:31.924503:	Training iteration: 821400, Loss: 0.12437565624713898
2018-10-22 16:47:01.870783:	Training iteration: 821600, Loss: 0.17831501364707947
2018-10-22 16:47:31.828528:	Training iteration: 821800, Loss: 0.15272609889507294
2018-10-22 16:48:01.255895:	Training iteration: 822000, Loss: 0.16598549485206604
2018-10-22 16:48:30.710447:	Training iteration: 822200, Loss: 0.19194386899471283
2018-10-22 16:49:00.356897:	Training iteration: 822400, Loss: 0.1173635795712471
2018-10-22 16:49:29.886167:	Training iteration: 822600, Loss: 0.23174957931041718
2018-10-22 16:49:59.397855:	Training iteration: 822800, Loss: 0.18922992050647736
2018-10-22 16:50:29.098180:	Training iteration: 823000, Loss: 0.1718418002128601
2018-10-22 16:50:58.115118:	Training iteration: 823200, Loss: 0.16191136837005615
2018-10-22 16:51:27.724429:	Training iteration: 823400, Loss: 0.14351794123649597
2018-10-22 16:51:57.465839:	Training iteration: 823600, Loss: 0.13154913485050201
2018-10-22 16:52:27.793577:	Training iteration: 823800, Loss: 0.1302400827407837
2018-10-22 16:52:56.462660:	Training iteration: 824000, Loss: 0.14379970729351044
2018-10-22 16:53:25.900574:	Training iteration: 824200, Loss: 0.15280427038669586
2018-10-22 16:53:54.921873:	Training iteration: 824400, Loss: 0.13624195754528046
2018-10-22 16:54:23.931836:	Training iteration: 824600, Loss: 0.15575236082077026
2018-10-22 16:54:52.997449:	Training iteration: 824800, Loss: 0.2007000595331192
2018-10-22 16:55:22.869494:	Training iteration: 825000, Loss: 0.14886203408241272
2018-10-22 16:55:52.244875:	Training iteration: 825200, Loss: 0.18916380405426025
2018-10-22 16:56:21.663170:	Training iteration: 825400, Loss: 0.17156513035297394
2018-10-22 16:56:51.617291:	Training iteration: 825600, Loss: 0.19427470862865448
2018-10-22 16:57:20.945584:	Training iteration: 825800, Loss: 0.20171354711055756
2018-10-22 16:57:50.667514:	Training iteration: 826000, Loss: 0.1595543920993805
2018-10-22 16:58:19.765403:	Training iteration: 826200, Loss: 0.18229368329048157
2018-10-22 16:58:49.365502:	Training iteration: 826400, Loss: 0.20836234092712402
2018-10-22 16:59:18.313584:	Training iteration: 826600, Loss: 0.15883171558380127
2018-10-22 16:59:48.127518:	Training iteration: 826800, Loss: 0.15449467301368713
2018-10-22 17:00:17.419088:	Training iteration: 827000, Loss: 0.14797796308994293
2018-10-22 17:00:47.069430:	Training iteration: 827200, Loss: 0.1707884669303894
2018-10-22 17:01:16.185042:	Training iteration: 827400, Loss: 0.17348191142082214
2018-10-22 17:01:45.484304:	Training iteration: 827600, Loss: 0.16003844141960144
2018-10-22 17:02:15.153084:	Training iteration: 827800, Loss: 0.17089949548244476
2018-10-22 17:02:45.139944:	Training iteration: 828000, Loss: 0.4196338653564453
2018-10-22 17:03:15.080348:	Training iteration: 828200, Loss: 0.17984144389629364
2018-10-22 17:03:44.497179:	Training iteration: 828400, Loss: 0.1823493242263794
2018-10-22 17:04:13.851733:	Training iteration: 828600, Loss: 0.16164252161979675
2018-10-22 17:04:43.437365:	Training iteration: 828800, Loss: 0.1305905282497406
2018-10-22 17:05:12.621272:	Training iteration: 829000, Loss: 0.17224329710006714
2018-10-22 17:05:42.254954:	Training iteration: 829200, Loss: 0.15461963415145874
2018-10-22 17:06:11.553414:	Training iteration: 829400, Loss: 0.11759331822395325
2018-10-22 17:06:40.439097:	Training iteration: 829600, Loss: 0.14007152616977692
2018-10-22 17:07:10.264214:	Training iteration: 829800, Loss: 0.1516665369272232
2018-10-22 17:07:42.454660:	Training iteration: 830000, Loss: 0.12082439661026001
2018-10-22 17:08:11.897562:	Training iteration: 830200, Loss: 0.1333484649658203
2018-10-22 17:08:41.044708:	Training iteration: 830400, Loss: 0.10887085646390915
2018-10-22 17:09:10.457669:	Training iteration: 830600, Loss: 0.12213568389415741
2018-10-22 17:09:39.892777:	Training iteration: 830800, Loss: 0.14914001524448395
2018-10-22 17:10:09.439293:	Training iteration: 831000, Loss: 0.12012693285942078
2018-10-22 17:10:39.194890:	Training iteration: 831200, Loss: 0.13398541510105133
2018-10-22 17:11:09.132737:	Training iteration: 831400, Loss: 0.11554700136184692
2018-10-22 17:11:38.805567:	Training iteration: 831600, Loss: 0.17824125289916992
2018-10-22 17:12:08.549885:	Training iteration: 831800, Loss: 0.15457770228385925
2018-10-22 17:12:40.636576:	Training iteration: 832000, Loss: 0.16239336133003235
2018-10-22 17:13:10.250861:	Training iteration: 832200, Loss: 0.14521445333957672
2018-10-22 17:13:39.721351:	Training iteration: 832400, Loss: 0.16473785042762756
2018-10-22 17:14:09.210883:	Training iteration: 832600, Loss: 0.12997764348983765
2018-10-22 17:14:38.711418:	Training iteration: 832800, Loss: 0.18062783777713776
2018-10-22 17:15:08.896658:	Training iteration: 833000, Loss: 0.16639414429664612
2018-10-22 17:15:38.612342:	Training iteration: 833200, Loss: 0.1340700089931488
2018-10-22 17:16:08.105658:	Training iteration: 833400, Loss: 0.14008565247058868
2018-10-22 17:16:37.663600:	Training iteration: 833600, Loss: 0.1894700825214386
2018-10-22 17:17:07.793228:	Training iteration: 833800, Loss: 0.12405319511890411
2018-10-22 17:17:37.156754:	Training iteration: 834000, Loss: 0.179531067609787
2018-10-22 17:18:06.902233:	Training iteration: 834200, Loss: 0.24746611714363098
2018-10-22 17:18:36.829938:	Training iteration: 834400, Loss: 0.16083498299121857
2018-10-22 17:19:06.534460:	Training iteration: 834600, Loss: 0.12809999287128448
2018-10-22 17:19:35.963400:	Training iteration: 834800, Loss: 0.15028342604637146
2018-10-22 17:20:04.774576:	Training iteration: 835000, Loss: 0.15993988513946533
2018-10-22 17:20:34.619515:	Training iteration: 835200, Loss: 0.1447472870349884
2018-10-22 17:21:04.385605:	Training iteration: 835400, Loss: 0.1572166085243225
2018-10-22 17:21:34.011495:	Training iteration: 835600, Loss: 0.2653469443321228
2018-10-22 17:22:02.706420:	Training iteration: 835800, Loss: 0.16105228662490845
2018-10-22 17:22:32.839004:	Training iteration: 836000, Loss: 0.18108172714710236
2018-10-22 17:23:02.397231:	Training iteration: 836200, Loss: 0.21238550543785095
2018-10-22 17:23:32.555840:	Training iteration: 836400, Loss: 0.1250527799129486
2018-10-22 17:24:02.008183:	Training iteration: 836600, Loss: 0.15130344033241272
2018-10-22 17:24:31.592262:	Training iteration: 836800, Loss: 0.1596648395061493
2018-10-22 17:25:01.707456:	Training iteration: 837000, Loss: 0.13706037402153015
2018-10-22 17:25:31.517244:	Training iteration: 837200, Loss: 0.10852222144603729
2018-10-22 17:26:01.227774:	Training iteration: 837400, Loss: 0.11819152534008026
2018-10-22 17:26:31.874174:	Training iteration: 837600, Loss: 0.1009678989648819
2018-10-22 17:27:01.320797:	Training iteration: 837800, Loss: 0.24723955988883972
2018-10-22 17:27:31.547249:	Training iteration: 838000, Loss: 0.1639554500579834
2018-10-22 17:28:01.030580:	Training iteration: 838200, Loss: 0.1612015664577484
2018-10-22 17:28:30.752001:	Training iteration: 838400, Loss: 0.38477209210395813
2018-10-22 17:29:00.134012:	Training iteration: 838600, Loss: 0.15579482913017273
2018-10-22 17:29:29.190499:	Training iteration: 838800, Loss: 0.1873956173658371
2018-10-22 17:29:58.922716:	Training iteration: 839000, Loss: 0.16857723891735077
2018-10-22 17:30:28.743191:	Training iteration: 839200, Loss: 0.1387639045715332
2018-10-22 17:30:58.331804:	Training iteration: 839400, Loss: 0.14985164999961853
2018-10-22 17:31:27.552254:	Training iteration: 839600, Loss: 0.18212127685546875
2018-10-22 17:31:35.659557:	Epoch 6 finished after 839658 iterations.
No images to record
Validating
2018-10-22 17:31:35.819273:	Entering validation loop
2018-10-22 17:31:45.870252: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 641 of 1000
2018-10-22 17:31:50.813225: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 17:32:04.488565:	Validation iteration: 200, Loss: 0.14709892868995667
2018-10-22 17:32:18.146602:	Validation iteration: 400, Loss: 0.20541676878929138
2018-10-22 17:32:32.446881:	Validation iteration: 600, Loss: 0.23096218705177307
2018-10-22 17:32:46.445816:	Validation iteration: 800, Loss: 0.1366981565952301
2018-10-22 17:33:00.914757:	Validation iteration: 1000, Loss: 0.3101270794868469
2018-10-22 17:33:15.824339:	Validation iteration: 1200, Loss: 0.2554713487625122
2018-10-22 17:33:30.225977:	Validation iteration: 1400, Loss: 0.18606936931610107
2018-10-22 17:33:44.185551:	Validation iteration: 1600, Loss: 0.15466928482055664
2018-10-22 17:33:58.381097:	Validation iteration: 1800, Loss: 0.12959444522857666
2018-10-22 17:34:12.365752:	Validation iteration: 2000, Loss: 0.20760969817638397
2018-10-22 17:34:28.594346: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 679 of 1000
2018-10-22 17:34:32.997111: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 17:34:40.736115:	Validation iteration: 2200, Loss: 0.25371071696281433
2018-10-22 17:34:54.222041:	Validation iteration: 2400, Loss: 0.2387738823890686
2018-10-22 17:35:07.942510:	Validation iteration: 2600, Loss: 0.2647242546081543
2018-10-22 17:35:21.945940:	Validation iteration: 2800, Loss: 0.269464373588562
2018-10-22 17:35:35.880062:	Validation iteration: 3000, Loss: 0.20634226500988007
2018-10-22 17:35:50.060948:	Validation iteration: 3200, Loss: 0.28888219594955444
2018-10-22 17:36:04.238005:	Validation iteration: 3400, Loss: 0.19249653816223145
2018-10-22 17:36:18.430348:	Validation iteration: 3600, Loss: 0.2238721251487732
2018-10-22 17:36:32.606757:	Validation iteration: 3800, Loss: 0.2139003574848175
2018-10-22 17:36:46.654986:	Validation iteration: 4000, Loss: 0.17831051349639893
2018-10-22 17:37:09.214234: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 648 of 1000
2018-10-22 17:37:13.966852: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 17:37:15.576406:	Validation iteration: 4200, Loss: 0.15680590271949768
2018-10-22 17:37:29.106016:	Validation iteration: 4400, Loss: 0.1447058469057083
2018-10-22 17:37:42.646598:	Validation iteration: 4600, Loss: 0.15363627672195435
2018-10-22 17:37:56.469392:	Validation iteration: 4800, Loss: 0.09795030206441879
2018-10-22 17:38:10.394610:	Validation iteration: 5000, Loss: 0.16956232488155365
2018-10-22 17:38:24.513270:	Validation iteration: 5200, Loss: 0.15966829657554626
2018-10-22 17:38:38.740326:	Validation iteration: 5400, Loss: 0.11529717594385147
2018-10-22 17:38:52.225915:	Validation iteration: 5600, Loss: 0.2068360447883606
2018-10-22 17:39:05.981251:	Validation iteration: 5800, Loss: 0.24979168176651
2018-10-22 17:39:20.109207:	Validation iteration: 6000, Loss: 0.2329753041267395
2018-10-22 17:39:34.144326:	Validation iteration: 6200, Loss: 0.14292940497398376
2018-10-22 17:39:48.416156: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 670 of 1000
2018-10-22 17:39:52.959556: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 17:40:02.296156:	Validation iteration: 6400, Loss: 0.27203112840652466
2018-10-22 17:40:15.453907:	Validation iteration: 6600, Loss: 0.2404724806547165
2018-10-22 17:40:29.251458:	Validation iteration: 6800, Loss: 0.30243197083473206
2018-10-22 17:40:43.433396:	Validation iteration: 7000, Loss: 0.2718617916107178
2018-10-22 17:40:57.839421:	Validation iteration: 7200, Loss: 0.2764701247215271
2018-10-22 17:41:12.283676:	Validation iteration: 7400, Loss: 0.19634494185447693
2018-10-22 17:41:26.254870:	Validation iteration: 7600, Loss: 0.2014092057943344
2018-10-22 17:41:50.081694:	Validation iteration: 7800, Loss: 0.20266816020011902
2018-10-22 17:42:02.989964:	Validation iteration: 8000, Loss: 0.17550507187843323
2018-10-22 17:42:17.028628:	Validation iteration: 8200, Loss: 0.23461362719535828
2018-10-22 17:42:37.700672: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 641 of 1000
2018-10-22 17:42:43.115782: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 17:42:46.276060:	Validation iteration: 8400, Loss: 0.16192156076431274
2018-10-22 17:42:59.392989:	Validation iteration: 8600, Loss: 0.1763399839401245
2018-10-22 17:43:13.555360:	Validation iteration: 8800, Loss: 0.1693401336669922
2018-10-22 17:43:27.354673:	Validation iteration: 9000, Loss: 0.16862916946411133
2018-10-22 17:43:41.797136:	Validation iteration: 9200, Loss: 0.17504818737506866
2018-10-22 17:43:56.455107:	Validation iteration: 9400, Loss: 0.1901703029870987
2018-10-22 17:44:11.010779:	Validation iteration: 9600, Loss: 0.16124922037124634
2018-10-22 17:44:25.252944:	Validation iteration: 9800, Loss: 0.18947184085845947
2018-10-22 17:44:39.714496:	Validation iteration: 10000, Loss: 0.11993665248155594
2018-10-22 17:44:54.530751:	Validation iteration: 10200, Loss: 0.18478813767433167
2018-10-22 17:45:08.526401:	Validation iteration: 10400, Loss: 0.17814216017723083
2018-10-22 17:45:22.774944:	Validation iteration: 10600, Loss: 0.1808360517024994
2018-10-22 17:45:37.002420:	Validation iteration: 10800, Loss: 0.22114336490631104
2018-10-22 17:45:51.246522:	Validation iteration: 11000, Loss: 0.19661441445350647
2018-10-22 17:46:05.663168:	Validation iteration: 11200, Loss: 0.12530851364135742
Validation check mean loss: 0.20030102279436732
Validation loss has worsened. worse_val_checks = 2
Checkpoint
Training complete after 7 epochs.
Finished requested number of epochs.
Final validation loss: 0.20030102279436732
Best validation loss (0.18700539684954984) achieved at validation check 5
Starting testing
2018-10-22 17:48:52.955571:	Entering test loop
2018-10-22 17:50:20.171419: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 388 of 1000
2018-10-22 17:50:29.725860: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 458 of 1000
2018-10-22 17:50:38.940736: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 17:50:39.311568:	Testing iteration: 0, Loss: 0.18466529250144958
2018-10-22 18:14:00.973239:	Testing iteration: 200, Loss: 0.282728374004364
2018-10-22 18:37:36.125565:	Testing iteration: 400, Loss: 0.12002622336149216
2018-10-22 19:01:40.056229:	Testing iteration: 600, Loss: 0.24521584808826447
2018-10-22 19:26:16.897029:	Testing iteration: 800, Loss: 0.21470552682876587
2018-10-22 19:51:07.883052:	Testing iteration: 1000, Loss: 0.16044804453849792
2018-10-22 20:16:30.254288:	Testing iteration: 1200, Loss: 0.11811189353466034
2018-10-22 20:42:12.691784:	Testing iteration: 1400, Loss: 0.1749700903892517
2018-10-22 21:08:36.350854:	Testing iteration: 1600, Loss: 0.24190938472747803
2018-10-22 21:10:37.379984: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 637 of 1000
2018-10-22 21:10:42.677185: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-22 21:35:32.270168:	Testing iteration: 1800, Loss: 0.2073182314634323
2018-10-22 22:02:44.957280:	Testing iteration: 2000, Loss: 0.22933398187160492
2018-10-22 22:30:24.000647:	Testing iteration: 2200, Loss: 0.24184300005435944
2018-10-22 22:58:24.700546:	Testing iteration: 2400, Loss: 0.2634005546569824
2018-10-22 23:26:56.593479:	Testing iteration: 2600, Loss: 0.3959196209907532
2018-10-22 23:55:53.195197:	Testing iteration: 2800, Loss: 0.2903522253036499
2018-10-23 00:25:18.475623:	Testing iteration: 3000, Loss: 0.30464065074920654
2018-10-23 00:55:39.495089:	Testing iteration: 3200, Loss: 0.1712304949760437
2018-10-23 01:00:04.178025: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 645 of 1000
2018-10-23 01:00:09.254233: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 01:26:19.885906:	Testing iteration: 3400, Loss: 0.17054492235183716
2018-10-23 01:57:12.637480:	Testing iteration: 3600, Loss: 0.262259840965271
2018-10-23 02:28:38.323065:	Testing iteration: 3800, Loss: 0.28242576122283936
2018-10-23 03:00:27.642054:	Testing iteration: 4000, Loss: 0.2508716285228729
2018-10-23 03:32:51.171019:	Testing iteration: 4200, Loss: 0.16576871275901794
2018-10-23 04:05:53.418206:	Testing iteration: 4400, Loss: 0.2950698733329773
2018-10-23 04:39:07.409097:	Testing iteration: 4600, Loss: 0.23600293695926666
2018-10-23 05:12:46.688645:	Testing iteration: 4800, Loss: 0.2013777792453766
2018-10-23 05:20:02.308336: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 633 of 1000
2018-10-23 05:20:07.432389: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-23 05:47:00.705939:	Testing iteration: 5000, Loss: 0.246711865067482
2018-10-23 06:21:32.435027:	Testing iteration: 5200, Loss: 0.41058149933815
