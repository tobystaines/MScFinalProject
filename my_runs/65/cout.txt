INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "65"
Experiment ID: 65
Preparing dataset
Dataset ready
2018-09-30 16:28:29.850795: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-09-30 16:28:31.078039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-30 16:28:31.078564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:27:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-09-30 16:28:31.078581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:27:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Running initialisation test
Starting testing
2018-09-30 16:28:39.709989:	Entering test loop
2018-09-30 16:28:50.119294: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 640 of 1000
2018-09-30 16:28:55.144731: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 16:28:56.756657:	Testing iteration: 0, Loss: 0.28548842668533325
2018-09-30 16:32:09.781423:	Testing iteration: 200, Loss: 0.24923861026763916
2018-09-30 16:35:39.556236:	Testing iteration: 400, Loss: 0.17432966828346252
2018-09-30 16:39:31.374665:	Testing iteration: 600, Loss: 0.18616649508476257
2018-09-30 16:40:37.416174: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 701 of 1000
2018-09-30 16:40:41.607452: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 16:44:01.768998:	Testing iteration: 800, Loss: 0.26032423973083496
2018-09-30 16:48:42.208971:	Testing iteration: 1000, Loss: 0.3899274468421936
2018-09-30 16:53:45.771208:	Testing iteration: 1200, Loss: 0.271838515996933
2018-09-30 16:56:22.331061: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 713 of 1000
2018-09-30 16:56:26.268036: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 16:59:26.137848:	Testing iteration: 1400, Loss: 0.26822298765182495
2018-09-30 17:05:35.481543:	Testing iteration: 1600, Loss: 0.3045751452445984
2018-09-30 17:12:11.144529:	Testing iteration: 1800, Loss: 0.3108639717102051
2018-09-30 17:16:51.209401: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 694 of 1000
2018-09-30 17:16:55.413073: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 17:19:02.246927:	Testing iteration: 2000, Loss: 0.2988148331642151
2018-09-30 17:26:02.789421:	Testing iteration: 2200, Loss: 0.25541073083877563
2018-09-30 17:33:28.289864:	Testing iteration: 2400, Loss: 0.30710476636886597
2018-09-30 17:40:48.315755: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 691 of 1000
2018-09-30 17:40:52.151556: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 17:41:31.316620:	Testing iteration: 2600, Loss: 0.19914019107818604
2018-09-30 17:49:43.980419:	Testing iteration: 2800, Loss: 0.24512656033039093
2018-09-30 17:58:19.723871:	Testing iteration: 3000, Loss: 0.22513341903686523
2018-09-30 18:07:38.948038:	Testing iteration: 3200, Loss: 0.19921964406967163
2018-09-30 18:17:06.759293:	Testing iteration: 3400, Loss: 0.216729998588562
2018-09-30 18:26:55.723748:	Testing iteration: 3600, Loss: 0.1576438695192337
Test pass complete
Mean loss over test set: 0.2682114173706377
Data saved to dumps/65 for later audio metric calculation
Starting training
2018-09-30 18:35:45.989924: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 764 of 1000
2018-09-30 18:35:48.742694: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 18:38:12.296766:	Training iteration: 200, Loss: 0.2785923480987549
2018-09-30 18:40:39.101640:	Training iteration: 400, Loss: 0.1704980432987213
2018-09-30 18:43:07.490483:	Training iteration: 600, Loss: 0.2020706832408905
2018-09-30 18:45:34.835624:	Training iteration: 800, Loss: 0.16958986222743988
2018-09-30 18:48:01.934658:	Training iteration: 1000, Loss: 0.18032202124595642
2018-09-30 18:50:29.305323:	Training iteration: 1200, Loss: 0.18915048241615295
2018-09-30 18:52:57.096929:	Training iteration: 1400, Loss: 0.1776573657989502
2018-09-30 18:55:25.102040:	Training iteration: 1600, Loss: 0.16667014360427856
2018-09-30 18:57:52.779361:	Training iteration: 1800, Loss: 0.1634405553340912
2018-09-30 19:00:20.586420:	Training iteration: 2000, Loss: 0.18626613914966583
2018-09-30 19:02:48.040980:	Training iteration: 2200, Loss: 0.17524531483650208
2018-09-30 19:05:15.685429:	Training iteration: 2400, Loss: 0.16043242812156677
2018-09-30 19:07:44.061004:	Training iteration: 2600, Loss: 0.2005857229232788
2018-09-30 19:10:11.848158:	Training iteration: 2800, Loss: 0.11402922868728638
2018-09-30 19:12:39.891283:	Training iteration: 3000, Loss: 0.15843646228313446
2018-09-30 19:15:07.686926:	Training iteration: 3200, Loss: 0.21461033821105957
2018-09-30 19:17:36.089387:	Training iteration: 3400, Loss: 0.1893700510263443
2018-09-30 19:20:03.957057:	Training iteration: 3600, Loss: 0.11941491067409515
2018-09-30 19:22:32.363405:	Training iteration: 3800, Loss: 0.19994071125984192
2018-09-30 19:25:00.392105:	Training iteration: 4000, Loss: 0.15497273206710815
2018-09-30 19:27:28.460564:	Training iteration: 4200, Loss: 0.17747750878334045
2018-09-30 19:29:56.455116:	Training iteration: 4400, Loss: 0.12086919695138931
2018-09-30 19:32:24.964453:	Training iteration: 4600, Loss: 0.19171544909477234
2018-09-30 19:34:52.522022:	Training iteration: 4800, Loss: 0.1474720537662506
2018-09-30 19:36:59.864566: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 753 of 1000
2018-09-30 19:37:02.961344: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 19:37:32.551764:	Training iteration: 5000, Loss: 0.15392489731311798
2018-09-30 19:39:59.885022:	Training iteration: 5200, Loss: 0.15708985924720764
2018-09-30 19:42:27.891292:	Training iteration: 5400, Loss: 0.12969417870044708
2018-09-30 19:44:55.788198:	Training iteration: 5600, Loss: 0.13786564767360687
2018-09-30 19:47:23.660509:	Training iteration: 5800, Loss: 0.15927204489707947
2018-09-30 19:49:51.998115:	Training iteration: 6000, Loss: 0.1410192996263504
2018-09-30 19:52:20.417364:	Training iteration: 6200, Loss: 0.13236400485038757
2018-09-30 19:54:48.801572:	Training iteration: 6400, Loss: 0.12431351095438004
2018-09-30 19:57:16.721812:	Training iteration: 6600, Loss: 0.12907159328460693
2018-09-30 19:59:44.109202:	Training iteration: 6800, Loss: 0.14350023865699768
2018-09-30 20:02:12.529627:	Training iteration: 7000, Loss: 0.1302269995212555
2018-09-30 20:04:40.493593:	Training iteration: 7200, Loss: 0.14979706704616547
2018-09-30 20:07:08.598735:	Training iteration: 7400, Loss: 0.1638888418674469
2018-09-30 20:09:36.393240:	Training iteration: 7600, Loss: 0.13377080857753754
2018-09-30 20:12:04.657311:	Training iteration: 7800, Loss: 0.13025245070457458
2018-09-30 20:14:33.194459:	Training iteration: 8000, Loss: 0.16072459518909454
2018-09-30 20:17:01.355702:	Training iteration: 8200, Loss: 0.14526957273483276
2018-09-30 20:19:29.015634:	Training iteration: 8400, Loss: 0.1289776712656021
2018-09-30 20:21:56.914868:	Training iteration: 8600, Loss: 0.14775016903877258
2018-09-30 20:24:25.320787:	Training iteration: 8800, Loss: 0.09339907765388489
2018-09-30 20:26:53.585043:	Training iteration: 9000, Loss: 0.12041854858398438
2018-09-30 20:29:21.557681:	Training iteration: 9200, Loss: 0.14431637525558472
2018-09-30 20:31:50.118730:	Training iteration: 9400, Loss: 0.13710995018482208
2018-09-30 20:34:18.249562:	Training iteration: 9600, Loss: 0.1432729810476303
2018-09-30 20:36:46.223729:	Training iteration: 9800, Loss: 0.11898299306631088
2018-09-30 20:38:44.012584: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 747 of 1000
2018-09-30 20:38:47.154753: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 20:39:25.385632:	Training iteration: 10000, Loss: 0.08479908853769302
2018-09-30 20:41:53.085696:	Training iteration: 10200, Loss: 0.08580569177865982
2018-09-30 20:44:21.430933:	Training iteration: 10400, Loss: 0.06713750213384628
2018-09-30 20:46:49.511967:	Training iteration: 10600, Loss: 0.11332179605960846
2018-09-30 20:49:17.162240:	Training iteration: 10800, Loss: 0.1161084771156311
2018-09-30 20:51:45.471649:	Training iteration: 11000, Loss: 0.07342439889907837
2018-09-30 20:54:13.180643:	Training iteration: 11200, Loss: 0.1091146245598793
2018-09-30 20:56:40.752429:	Training iteration: 11400, Loss: 0.11766690015792847
2018-09-30 20:59:08.491546:	Training iteration: 11600, Loss: 0.09717988222837448
2018-09-30 21:01:36.180059:	Training iteration: 11800, Loss: 0.06790051609277725
2018-09-30 21:04:04.062873:	Training iteration: 12000, Loss: 0.13884615898132324
2018-09-30 21:06:31.213204:	Training iteration: 12200, Loss: 0.10109400749206543
2018-09-30 21:09:00.055739:	Training iteration: 12400, Loss: 0.09688787162303925
2018-09-30 21:11:28.052467:	Training iteration: 12600, Loss: 0.10975624620914459
2018-09-30 21:13:56.066592:	Training iteration: 12800, Loss: 0.09707532078027725
2018-09-30 21:16:24.581573:	Training iteration: 13000, Loss: 0.07776603102684021
2018-09-30 21:18:52.869244:	Training iteration: 13200, Loss: 0.12049319595098495
2018-09-30 21:21:21.059290:	Training iteration: 13400, Loss: 0.08182694017887115
2018-09-30 21:23:49.192670:	Training iteration: 13600, Loss: 0.08345713466405869
2018-09-30 21:26:17.148962:	Training iteration: 13800, Loss: 0.1207585483789444
2018-09-30 21:28:45.235738:	Training iteration: 14000, Loss: 0.09655682742595673
2018-09-30 21:31:12.887020:	Training iteration: 14200, Loss: 0.07808855175971985
2018-09-30 21:33:41.346208:	Training iteration: 14400, Loss: 0.11253142356872559
2018-09-30 21:36:09.926278:	Training iteration: 14600, Loss: 0.0711064338684082
2018-09-30 21:38:37.833057:	Training iteration: 14800, Loss: 0.11493577808141708
2018-09-30 21:40:24.451422: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 789 of 1000
2018-09-30 21:40:27.092168: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 21:41:17.115689:	Training iteration: 15000, Loss: 0.17741768062114716
2018-09-30 21:43:44.715039:	Training iteration: 15200, Loss: 0.12046582996845245
2018-09-30 21:46:12.784604:	Training iteration: 15400, Loss: 0.1340697705745697
2018-09-30 21:48:41.082282:	Training iteration: 15600, Loss: 0.08773218095302582
2018-09-30 21:51:08.746011:	Training iteration: 15800, Loss: 0.12925416231155396
2018-09-30 21:53:36.624765:	Training iteration: 16000, Loss: 0.16713008284568787
2018-09-30 21:56:04.929819:	Training iteration: 16200, Loss: 0.12008688598871231
2018-09-30 21:58:32.878457:	Training iteration: 16400, Loss: 0.12749138474464417
2018-09-30 22:01:00.810078:	Training iteration: 16600, Loss: 0.14839479327201843
2018-09-30 22:03:28.914813:	Training iteration: 16800, Loss: 0.1639430820941925
2018-09-30 22:05:56.452726:	Training iteration: 17000, Loss: 0.15497618913650513
2018-09-30 22:08:24.562135:	Training iteration: 17200, Loss: 0.11257161945104599
2018-09-30 22:10:52.430660:	Training iteration: 17400, Loss: 0.17808078229427338
2018-09-30 22:13:20.555572:	Training iteration: 17600, Loss: 0.1262541115283966
2018-09-30 22:15:48.606465:	Training iteration: 17800, Loss: 0.14692655205726624
2018-09-30 22:18:16.536305:	Training iteration: 18000, Loss: 0.1434423178434372
2018-09-30 22:20:44.642052:	Training iteration: 18200, Loss: 0.17256000638008118
2018-09-30 22:23:13.058488:	Training iteration: 18400, Loss: 0.11389286816120148
2018-09-30 22:25:41.506965:	Training iteration: 18600, Loss: 0.16850443184375763
2018-09-30 22:28:09.863876:	Training iteration: 18800, Loss: 0.12482231855392456
2018-09-30 22:30:38.023192:	Training iteration: 19000, Loss: 0.16388534009456635
2018-09-30 22:33:06.516765:	Training iteration: 19200, Loss: 0.13366766273975372
2018-09-30 22:35:34.664910:	Training iteration: 19400, Loss: 0.16899727284908295
2018-09-30 22:38:03.036992:	Training iteration: 19600, Loss: 0.1329905390739441
2018-09-30 22:40:30.991651:	Training iteration: 19800, Loss: 0.1450626105070114
2018-09-30 22:42:58.748145:	Training iteration: 20000, Loss: 0.1342940479516983
2018-09-30 22:44:51.556292: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 853 of 1000
2018-09-30 22:44:53.163918: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 22:45:37.293130:	Training iteration: 20200, Loss: 0.12930041551589966
2018-09-30 22:48:04.504539:	Training iteration: 20400, Loss: 0.12533403933048248
2018-09-30 22:50:32.234206:	Training iteration: 20600, Loss: 0.23495884239673615
2018-09-30 22:52:59.627267:	Training iteration: 20800, Loss: 0.23661084473133087
2018-09-30 22:55:26.967512:	Training iteration: 21000, Loss: 0.15230469405651093
2018-09-30 22:57:54.270502:	Training iteration: 21200, Loss: 0.13309931755065918
2018-09-30 23:00:21.797087:	Training iteration: 21400, Loss: 0.19113418459892273
2018-09-30 23:02:49.099057:	Training iteration: 21600, Loss: 0.17652755975723267
2018-09-30 23:05:16.873458:	Training iteration: 21800, Loss: 0.13541486859321594
2018-09-30 23:07:44.336200:	Training iteration: 22000, Loss: 0.23117494583129883
2018-09-30 23:10:11.810098:	Training iteration: 22200, Loss: 0.16130705177783966
2018-09-30 23:12:38.992793:	Training iteration: 22400, Loss: 0.15881982445716858
2018-09-30 23:15:06.821981:	Training iteration: 22600, Loss: 0.16178688406944275
2018-09-30 23:17:34.216526:	Training iteration: 22800, Loss: 0.15922975540161133
2018-09-30 23:20:01.646185:	Training iteration: 23000, Loss: 0.17582044005393982
2018-09-30 23:22:29.207844:	Training iteration: 23200, Loss: 0.18979144096374512
2018-09-30 23:24:56.967758:	Training iteration: 23400, Loss: 0.19509242475032806
2018-09-30 23:27:24.389805:	Training iteration: 23600, Loss: 0.15960372984409332
2018-09-30 23:29:51.706812:	Training iteration: 23800, Loss: 0.16639170050621033
2018-09-30 23:32:19.288138:	Training iteration: 24000, Loss: 0.1368679702281952
2018-09-30 23:34:47.041301:	Training iteration: 24200, Loss: 0.16461752355098724
2018-09-30 23:37:14.461217:	Training iteration: 24400, Loss: 0.1970209777355194
2018-09-30 23:39:42.500103:	Training iteration: 24600, Loss: 0.1764189749956131
2018-09-30 23:42:10.372439:	Training iteration: 24800, Loss: 0.14612898230552673
2018-09-30 23:44:37.883119:	Training iteration: 25000, Loss: 0.1427546739578247
2018-09-30 23:47:04.961121:	Training iteration: 25200, Loss: 0.12236727774143219
2018-09-30 23:49:32.713581:	Training iteration: 25400, Loss: 0.1901313215494156
2018-09-30 23:52:00.427474:	Training iteration: 25600, Loss: 0.18731603026390076
2018-09-30 23:54:28.046098:	Training iteration: 25800, Loss: 0.16002357006072998
2018-09-30 23:56:55.353487:	Training iteration: 26000, Loss: 0.1399284303188324
2018-09-30 23:59:22.869906:	Training iteration: 26200, Loss: 0.16173169016838074
2018-10-01 00:01:50.123360:	Training iteration: 26400, Loss: 0.16824014484882355
2018-10-01 00:04:17.881512:	Training iteration: 26600, Loss: 0.19441981613636017
2018-10-01 00:06:45.469001:	Training iteration: 26800, Loss: 0.14668290317058563
2018-10-01 00:09:13.244847:	Training iteration: 27000, Loss: 0.13354355096817017
2018-10-01 00:11:40.977957:	Training iteration: 27200, Loss: 0.1656661331653595
2018-10-01 00:14:08.448484:	Training iteration: 27400, Loss: 0.13565310835838318
2018-10-01 00:16:35.790733:	Training iteration: 27600, Loss: 0.17622342705726624
2018-10-01 00:19:03.359117:	Training iteration: 27800, Loss: 0.16490887105464935
2018-10-01 00:21:30.952049:	Training iteration: 28000, Loss: 0.1470469832420349
2018-10-01 00:23:58.314351:	Training iteration: 28200, Loss: 0.19839254021644592
2018-10-01 00:26:26.177317:	Training iteration: 28400, Loss: 0.1718001663684845
2018-10-01 00:28:53.081474:	Training iteration: 28600, Loss: 0.1758023202419281
2018-10-01 00:31:20.686772:	Training iteration: 28800, Loss: 0.13914456963539124
2018-10-01 00:33:48.149339:	Training iteration: 29000, Loss: 0.16278406977653503
2018-10-01 00:36:15.322052:	Training iteration: 29200, Loss: 0.16698741912841797
2018-10-01 00:38:43.161548:	Training iteration: 29400, Loss: 0.18647554516792297
2018-10-01 00:41:11.112756:	Training iteration: 29600, Loss: 0.19275996088981628
2018-10-01 00:43:38.597227:	Training iteration: 29800, Loss: 0.14081928133964539
2018-10-01 00:46:05.934411:	Training iteration: 30000, Loss: 0.16115984320640564
2018-10-01 00:48:33.009140:	Training iteration: 30200, Loss: 0.16833582520484924
2018-10-01 00:51:00.687535:	Training iteration: 30400, Loss: 0.188092440366745
2018-10-01 00:53:28.206597:	Training iteration: 30600, Loss: 0.17927730083465576
2018-10-01 00:55:55.966415:	Training iteration: 30800, Loss: 0.15158233046531677
2018-10-01 00:58:23.390277:	Training iteration: 31000, Loss: 0.1619580239057541
2018-10-01 01:00:50.750477:	Training iteration: 31200, Loss: 0.16465817391872406
2018-10-01 01:03:18.400054:	Training iteration: 31400, Loss: 0.17739936709403992
2018-10-01 01:05:45.968379:	Training iteration: 31600, Loss: 0.15004611015319824
2018-10-01 01:08:13.660683:	Training iteration: 31800, Loss: 0.1764136105775833
2018-10-01 01:10:41.160149:	Training iteration: 32000, Loss: 0.191621795296669
2018-10-01 01:13:08.590003:	Training iteration: 32200, Loss: 0.18408402800559998
2018-10-01 01:15:36.312601:	Training iteration: 32400, Loss: 0.1725129634141922
2018-10-01 01:18:03.806035:	Training iteration: 32600, Loss: 0.24093523621559143
2018-10-01 01:20:31.854938:	Training iteration: 32800, Loss: 0.15620754659175873
2018-10-01 01:22:59.089692:	Training iteration: 33000, Loss: 0.1677788943052292
2018-10-01 01:25:26.916294:	Training iteration: 33200, Loss: 0.14170122146606445
2018-10-01 01:27:54.251934:	Training iteration: 33400, Loss: 0.15840111672878265
2018-10-01 01:30:21.811266:	Training iteration: 33600, Loss: 0.14071159064769745
2018-10-01 01:32:49.245485:	Training iteration: 33800, Loss: 0.14521363377571106
2018-10-01 01:35:16.591641:	Training iteration: 34000, Loss: 0.18972346186637878
2018-10-01 01:37:44.297310:	Training iteration: 34200, Loss: 0.29579657316207886
2018-10-01 01:40:11.743862:	Training iteration: 34400, Loss: 0.1757861077785492
2018-10-01 01:42:39.389447:	Training iteration: 34600, Loss: 0.15638160705566406
2018-10-01 01:45:06.568091:	Training iteration: 34800, Loss: 0.1499163955450058
2018-10-01 01:47:33.730591:	Training iteration: 35000, Loss: 0.15786579251289368
2018-10-01 01:50:01.073132:	Training iteration: 35200, Loss: 0.1633695363998413
2018-10-01 01:52:28.727433:	Training iteration: 35400, Loss: 0.1622721254825592
2018-10-01 01:54:56.054638:	Training iteration: 35600, Loss: 0.16925711929798126
2018-10-01 01:57:23.713152:	Training iteration: 35800, Loss: 0.17308445274829865
2018-10-01 01:59:51.764607:	Training iteration: 36000, Loss: 0.21495351195335388
2018-10-01 02:02:19.719789:	Training iteration: 36200, Loss: 0.15364737808704376
2018-10-01 02:04:47.308777:	Training iteration: 36400, Loss: 0.18038532137870789
2018-10-01 02:07:14.828731:	Training iteration: 36600, Loss: 0.15835843980312347
2018-10-01 02:09:42.622823:	Training iteration: 36800, Loss: 0.19533690810203552
2018-10-01 02:12:09.579369:	Training iteration: 37000, Loss: 0.15775573253631592
2018-10-01 02:14:37.460695:	Training iteration: 37200, Loss: 0.13222074508666992
2018-10-01 02:17:05.262465:	Training iteration: 37400, Loss: 0.1384740173816681
2018-10-01 02:19:32.835645:	Training iteration: 37600, Loss: 0.18194933235645294
2018-10-01 02:22:00.303355:	Training iteration: 37800, Loss: 0.16798202693462372
2018-10-01 02:24:28.020084:	Training iteration: 38000, Loss: 0.1850244104862213
2018-10-01 02:26:55.687746:	Training iteration: 38200, Loss: 0.16858401894569397
2018-10-01 02:29:22.737790:	Training iteration: 38400, Loss: 0.16995379328727722
2018-10-01 02:31:50.755456:	Training iteration: 38600, Loss: 0.16621895134449005
2018-10-01 02:34:18.346768:	Training iteration: 38800, Loss: 0.1768273264169693
2018-10-01 02:36:45.990900:	Training iteration: 39000, Loss: 0.1251404732465744
2018-10-01 02:39:12.794490:	Training iteration: 39200, Loss: 0.13147693872451782
2018-10-01 02:41:40.519819:	Training iteration: 39400, Loss: 0.14284774661064148
2018-10-01 02:44:07.737935:	Training iteration: 39600, Loss: 0.20006227493286133
2018-10-01 02:46:35.311597:	Training iteration: 39800, Loss: 0.1624414026737213
2018-10-01 02:49:02.926294:	Training iteration: 40000, Loss: 0.1590300351381302
2018-10-01 02:51:30.264496:	Training iteration: 40200, Loss: 0.16842374205589294
2018-10-01 02:53:57.467520:	Training iteration: 40400, Loss: 0.16957613825798035
2018-10-01 02:56:25.100837:	Training iteration: 40600, Loss: 0.17377997934818268
2018-10-01 02:58:52.619310:	Training iteration: 40800, Loss: 0.153082013130188
2018-10-01 03:01:19.639111:	Training iteration: 41000, Loss: 0.1524786353111267
2018-10-01 03:03:47.363238:	Training iteration: 41200, Loss: 0.14816723763942719
2018-10-01 03:06:15.022529:	Training iteration: 41400, Loss: 0.16795745491981506
2018-10-01 03:08:42.624220:	Training iteration: 41600, Loss: 0.1821283996105194
2018-10-01 03:11:09.670364:	Training iteration: 41800, Loss: 0.1561817079782486
2018-10-01 03:13:37.392474:	Training iteration: 42000, Loss: 0.1434328556060791
2018-10-01 03:16:05.439324:	Training iteration: 42200, Loss: 0.19203732907772064
2018-10-01 03:18:33.108544:	Training iteration: 42400, Loss: 0.32249340415000916
2018-10-01 03:21:00.610713:	Training iteration: 42600, Loss: 0.19240806996822357
2018-10-01 03:23:27.414380:	Training iteration: 42800, Loss: 0.1334388256072998
2018-10-01 03:25:55.082938:	Training iteration: 43000, Loss: 0.16158607602119446
2018-10-01 03:28:22.620968:	Training iteration: 43200, Loss: 0.1340337097644806
2018-10-01 03:30:49.778740:	Training iteration: 43400, Loss: 0.19450592994689941
2018-10-01 03:33:17.644026:	Training iteration: 43600, Loss: 0.19882896542549133
2018-10-01 03:35:45.309824:	Training iteration: 43800, Loss: 0.15894198417663574
2018-10-01 03:38:12.833207:	Training iteration: 44000, Loss: 0.14164283871650696
2018-10-01 03:40:39.644846:	Training iteration: 44200, Loss: 0.15853014588356018
2018-10-01 03:43:07.332840:	Training iteration: 44400, Loss: 0.20301060378551483
2018-10-01 03:45:34.828649:	Training iteration: 44600, Loss: 0.15532377362251282
2018-10-01 03:48:02.302156:	Training iteration: 44800, Loss: 0.2205936163663864
2018-10-01 03:50:30.029939:	Training iteration: 45000, Loss: 0.14974069595336914
2018-10-01 03:52:58.024401:	Training iteration: 45200, Loss: 0.20510122179985046
2018-10-01 03:55:25.179974:	Training iteration: 45400, Loss: 0.2023247480392456
2018-10-01 03:57:52.458466:	Training iteration: 45600, Loss: 0.17948974668979645
2018-10-01 04:00:19.789529:	Training iteration: 45800, Loss: 0.20191173255443573
2018-10-01 04:02:47.827572:	Training iteration: 46000, Loss: 0.14931759238243103
2018-10-01 04:05:15.580369:	Training iteration: 46200, Loss: 0.18234172463417053
2018-10-01 04:07:43.046999:	Training iteration: 46400, Loss: 0.17479482293128967
2018-10-01 04:10:10.748657:	Training iteration: 46600, Loss: 0.1455042064189911
2018-10-01 04:12:38.517044:	Training iteration: 46800, Loss: 0.15710732340812683
2018-10-01 04:15:05.935755:	Training iteration: 47000, Loss: 0.15612682700157166
2018-10-01 04:17:33.795686:	Training iteration: 47200, Loss: 0.1623525619506836
2018-10-01 04:20:01.156900:	Training iteration: 47400, Loss: 0.20526190102100372
2018-10-01 04:22:28.706354:	Training iteration: 47600, Loss: 0.20815277099609375
2018-10-01 04:24:56.232609:	Training iteration: 47800, Loss: 0.1661551296710968
2018-10-01 04:27:23.463810:	Training iteration: 48000, Loss: 0.16414573788642883
2018-10-01 04:29:50.709200:	Training iteration: 48200, Loss: 0.14075914025306702
2018-10-01 04:32:18.641671:	Training iteration: 48400, Loss: 0.17716087400913239
2018-10-01 04:34:46.142444:	Training iteration: 48600, Loss: 0.16737130284309387
2018-10-01 04:37:13.897588:	Training iteration: 48800, Loss: 0.17973080277442932
2018-10-01 04:39:41.394544:	Training iteration: 49000, Loss: 0.1774768978357315
2018-10-01 04:42:08.634020:	Training iteration: 49200, Loss: 0.14454717934131622
2018-10-01 04:44:36.069477:	Training iteration: 49400, Loss: 0.1564764529466629
2018-10-01 04:47:03.768653:	Training iteration: 49600, Loss: 0.12144573777914047
2018-10-01 04:49:31.576264:	Training iteration: 49800, Loss: 0.16427819430828094
2018-10-01 04:51:58.771113:	Training iteration: 50000, Loss: 0.18939615786075592
2018-10-01 04:54:26.435878:	Training iteration: 50200, Loss: 0.20060627162456512
2018-10-01 04:56:54.065250:	Training iteration: 50400, Loss: 0.13381260633468628
2018-10-01 04:59:21.807457:	Training iteration: 50600, Loss: 0.1787852942943573
2018-10-01 05:01:49.486193:	Training iteration: 50800, Loss: 0.21401970088481903
2018-10-01 05:04:17.007736:	Training iteration: 51000, Loss: 0.23407652974128723
2018-10-01 05:06:44.219463:	Training iteration: 51200, Loss: 0.15347342193126678
2018-10-01 05:09:11.606740:	Training iteration: 51400, Loss: 0.13371916115283966
2018-10-01 05:11:39.024290:	Training iteration: 51600, Loss: 0.15524789690971375
2018-10-01 05:14:06.294031:	Training iteration: 51800, Loss: 0.1465391218662262
2018-10-01 05:16:34.064271:	Training iteration: 52000, Loss: 0.1748056709766388
2018-10-01 05:19:01.455068:	Training iteration: 52200, Loss: 0.2347269058227539
2018-10-01 05:21:28.883793:	Training iteration: 52400, Loss: 0.18366427719593048
2018-10-01 05:23:56.095006:	Training iteration: 52600, Loss: 0.17037735879421234
2018-10-01 05:26:23.328300:	Training iteration: 52800, Loss: 0.1467462033033371
2018-10-01 05:28:50.905474:	Training iteration: 53000, Loss: 0.18876178562641144
2018-10-01 05:31:18.342207:	Training iteration: 53200, Loss: 0.15481352806091309
2018-10-01 05:33:45.839064:	Training iteration: 53400, Loss: 0.1750767081975937
2018-10-01 05:36:13.127564:	Training iteration: 53600, Loss: 0.1425030678510666
2018-10-01 05:38:41.156973:	Training iteration: 53800, Loss: 0.13637329638004303
2018-10-01 05:41:08.685765:	Training iteration: 54000, Loss: 0.188416987657547
2018-10-01 05:43:36.337990:	Training iteration: 54200, Loss: 0.15770265460014343
2018-10-01 05:46:02.398841:	Training iteration: 54400, Loss: 0.175687775015831
2018-10-01 05:48:30.387766:	Training iteration: 54600, Loss: 0.14923718571662903
2018-10-01 05:50:58.029702:	Training iteration: 54800, Loss: 0.16488590836524963
2018-10-01 05:53:25.962359:	Training iteration: 55000, Loss: 0.15436334908008575
2018-10-01 05:55:53.394588:	Training iteration: 55200, Loss: 0.1444128304719925
2018-10-01 05:58:21.219680:	Training iteration: 55400, Loss: 0.2328883707523346
2018-10-01 06:00:48.548981:	Training iteration: 55600, Loss: 0.1983342468738556
2018-10-01 06:03:16.046150:	Training iteration: 55800, Loss: 0.2441726177930832
2018-10-01 06:05:43.792856:	Training iteration: 56000, Loss: 0.1968483328819275
2018-10-01 06:08:11.063701:	Training iteration: 56200, Loss: 0.1140742301940918
2018-10-01 06:10:38.937119:	Training iteration: 56400, Loss: 0.13156571984291077
2018-10-01 06:13:05.987086:	Training iteration: 56600, Loss: 0.1763749122619629
2018-10-01 06:15:33.325812:	Training iteration: 56800, Loss: 0.1932714283466339
2018-10-01 06:18:00.544920:	Training iteration: 57000, Loss: 0.1644897311925888
2018-10-01 06:20:28.009050:	Training iteration: 57200, Loss: 0.17195911705493927
2018-10-01 06:22:55.149359:	Training iteration: 57400, Loss: 0.1575966626405716
2018-10-01 06:25:22.807117:	Training iteration: 57600, Loss: 0.19660916924476624
2018-10-01 06:27:49.832032:	Training iteration: 57800, Loss: 0.20420663058757782
2018-10-01 06:30:17.756011:	Training iteration: 58000, Loss: 0.20065517723560333
2018-10-01 06:32:45.301278:	Training iteration: 58200, Loss: 0.17287617921829224
2018-10-01 06:35:12.525090:	Training iteration: 58400, Loss: 0.1394955813884735
2018-10-01 06:37:39.483337:	Training iteration: 58600, Loss: 0.18338489532470703
2018-10-01 06:40:07.484967:	Training iteration: 58800, Loss: 0.1466401219367981
2018-10-01 06:42:35.267104:	Training iteration: 59000, Loss: 0.15139944851398468
2018-10-01 06:45:02.994678:	Training iteration: 59200, Loss: 0.1583680808544159
2018-10-01 06:47:30.879202:	Training iteration: 59400, Loss: 0.16490115225315094
2018-10-01 06:49:58.147541:	Training iteration: 59600, Loss: 0.15870067477226257
2018-10-01 06:52:26.035635:	Training iteration: 59800, Loss: 0.15460002422332764
2018-10-01 06:54:53.704259:	Training iteration: 60000, Loss: 0.192198246717453
2018-10-01 06:57:21.525698:	Training iteration: 60200, Loss: 0.18386709690093994
2018-10-01 06:59:49.297610:	Training iteration: 60400, Loss: 0.13682986795902252
2018-10-01 07:02:16.860828:	Training iteration: 60600, Loss: 0.1675000935792923
2018-10-01 07:04:44.233842:	Training iteration: 60800, Loss: 0.184278666973114
2018-10-01 07:07:11.613656:	Training iteration: 61000, Loss: 0.11193466186523438
2018-10-01 07:09:39.424291:	Training iteration: 61200, Loss: 0.15130281448364258
2018-10-01 07:12:07.048278:	Training iteration: 61400, Loss: 0.1414221078157425
2018-10-01 07:14:34.430837:	Training iteration: 61600, Loss: 0.15344120562076569
2018-10-01 07:17:02.126139:	Training iteration: 61800, Loss: 0.1630500853061676
2018-10-01 07:19:29.861816:	Training iteration: 62000, Loss: 0.15263178944587708
2018-10-01 07:21:57.854686:	Training iteration: 62200, Loss: 0.18208667635917664
2018-10-01 07:24:25.184899:	Training iteration: 62400, Loss: 0.16456632316112518
2018-10-01 07:26:52.855331:	Training iteration: 62600, Loss: 0.16918909549713135
2018-10-01 07:29:20.602998:	Training iteration: 62800, Loss: 0.17555123567581177
2018-10-01 07:31:48.346561:	Training iteration: 63000, Loss: 0.17050687968730927
2018-10-01 07:34:15.803547:	Training iteration: 63200, Loss: 0.1779402792453766
2018-10-01 07:36:43.185982:	Training iteration: 63400, Loss: 0.18410423398017883
2018-10-01 07:39:10.594213:	Training iteration: 63600, Loss: 0.16688689589500427
2018-10-01 07:41:37.833825:	Training iteration: 63800, Loss: 0.17258667945861816
2018-10-01 07:44:05.761408:	Training iteration: 64000, Loss: 0.17027083039283752
2018-10-01 07:46:33.702140:	Training iteration: 64200, Loss: 0.1625906229019165
2018-10-01 07:49:01.485401:	Training iteration: 64400, Loss: 0.1596822887659073
2018-10-01 07:51:29.408367:	Training iteration: 64600, Loss: 0.13331790268421173
2018-10-01 07:53:57.188362:	Training iteration: 64800, Loss: 0.17418578267097473
2018-10-01 07:56:24.895922:	Training iteration: 65000, Loss: 0.17841629683971405
2018-10-01 07:58:52.520177:	Training iteration: 65200, Loss: 0.18661880493164062
2018-10-01 08:01:19.823831:	Training iteration: 65400, Loss: 0.17197255790233612
2018-10-01 08:03:47.543392:	Training iteration: 65600, Loss: 0.15330800414085388
2018-10-01 08:06:14.925880:	Training iteration: 65800, Loss: 0.18645717203617096
2018-10-01 08:08:42.458244:	Training iteration: 66000, Loss: 0.26108261942863464
2018-10-01 08:11:09.819176:	Training iteration: 66200, Loss: 0.40943050384521484
2018-10-01 08:13:36.840386:	Training iteration: 66400, Loss: 0.20232105255126953
2018-10-01 08:16:04.113754:	Training iteration: 66600, Loss: 0.15473301708698273
2018-10-01 08:18:31.886735:	Training iteration: 66800, Loss: 0.15676039457321167
2018-10-01 08:20:58.537538:	Training iteration: 67000, Loss: 0.18517360091209412
2018-10-01 08:23:26.288539:	Training iteration: 67200, Loss: 0.18777358531951904
2018-10-01 08:25:53.581302:	Training iteration: 67400, Loss: 0.16311532258987427
2018-10-01 08:28:21.269561:	Training iteration: 67600, Loss: 0.16630269587039948
2018-10-01 08:30:48.244124:	Training iteration: 67800, Loss: 0.19539594650268555
2018-10-01 08:33:15.746312:	Training iteration: 68000, Loss: 0.19628135859966278
2018-10-01 08:35:42.965200:	Training iteration: 68200, Loss: 0.18277163803577423
2018-10-01 08:38:10.245536:	Training iteration: 68400, Loss: 0.17014269530773163
2018-10-01 08:40:37.307424:	Training iteration: 68600, Loss: 0.16792893409729004
2018-10-01 08:43:04.459867:	Training iteration: 68800, Loss: 0.16662849485874176
2018-10-01 08:45:31.373264:	Training iteration: 69000, Loss: 0.1704034060239792
2018-10-01 08:47:58.655845:	Training iteration: 69200, Loss: 0.19883035123348236
2018-10-01 08:50:26.183479:	Training iteration: 69400, Loss: 0.21243807673454285
2018-10-01 08:52:53.320396:	Training iteration: 69600, Loss: 0.14296849071979523
2018-10-01 08:55:20.514319:	Training iteration: 69800, Loss: 0.17303094267845154
2018-10-01 08:57:47.711262:	Training iteration: 70000, Loss: 0.12716791033744812
2018-10-01 09:00:15.112798:	Training iteration: 70200, Loss: 0.15537628531455994
2018-10-01 09:02:42.727131:	Training iteration: 70400, Loss: 0.21247705817222595
2018-10-01 09:05:09.465151:	Training iteration: 70600, Loss: 0.14691446721553802
2018-10-01 09:07:36.872542:	Training iteration: 70800, Loss: 0.16312041878700256
2018-10-01 09:10:03.864466:	Training iteration: 71000, Loss: 0.19966165721416473
2018-10-01 09:12:31.394282:	Training iteration: 71200, Loss: 0.17510256171226501
2018-10-01 09:14:58.623521:	Training iteration: 71400, Loss: 0.13835150003433228
2018-10-01 09:17:25.841846:	Training iteration: 71600, Loss: 0.1395830512046814
2018-10-01 09:19:53.388173:	Training iteration: 71800, Loss: 0.16470304131507874
2018-10-01 09:22:20.560654:	Training iteration: 72000, Loss: 0.1708909124135971
2018-10-01 09:24:47.645922:	Training iteration: 72200, Loss: 0.1593324989080429
2018-10-01 09:27:14.772424:	Training iteration: 72400, Loss: 0.14194434881210327
2018-10-01 09:29:42.293592:	Training iteration: 72600, Loss: 0.11547289788722992
2018-10-01 09:32:09.466354:	Training iteration: 72800, Loss: 0.19762367010116577
2018-10-01 09:34:36.680994:	Training iteration: 73000, Loss: 0.18717032670974731
2018-10-01 09:37:03.905572:	Training iteration: 73200, Loss: 0.1543893665075302
2018-10-01 09:39:31.617755:	Training iteration: 73400, Loss: 0.1611367166042328
2018-10-01 09:41:59.054303:	Training iteration: 73600, Loss: 0.1743265986442566
2018-10-01 09:44:26.569867:	Training iteration: 73800, Loss: 0.14630591869354248
2018-10-01 09:46:53.357367:	Training iteration: 74000, Loss: 0.14985080063343048
2018-10-01 09:49:20.909504:	Training iteration: 74200, Loss: 0.17185130715370178
2018-10-01 09:51:48.167277:	Training iteration: 74400, Loss: 0.1318550705909729
2018-10-01 09:54:15.721754:	Training iteration: 74600, Loss: 0.13955217599868774
2018-10-01 09:56:42.949106:	Training iteration: 74800, Loss: 0.21459481120109558
2018-10-01 09:59:09.661374:	Training iteration: 75000, Loss: 0.17601145803928375
2018-10-01 10:01:37.388624:	Training iteration: 75200, Loss: 0.11390695720911026
2018-10-01 10:04:04.754114:	Training iteration: 75400, Loss: 0.1606692373752594
2018-10-01 10:06:32.118329:	Training iteration: 75600, Loss: 0.24344778060913086
2018-10-01 10:08:59.365015:	Training iteration: 75800, Loss: 0.14901021122932434
2018-10-01 10:11:26.906442:	Training iteration: 76000, Loss: 0.27845150232315063
2018-10-01 10:13:54.004428:	Training iteration: 76200, Loss: 0.22509567439556122
2018-10-01 10:16:21.240644:	Training iteration: 76400, Loss: 0.1858014464378357
2018-10-01 10:18:48.247201:	Training iteration: 76600, Loss: 0.1715000867843628
2018-10-01 10:21:15.412512:	Training iteration: 76800, Loss: 0.13949117064476013
2018-10-01 10:23:42.765520:	Training iteration: 77000, Loss: 0.20463700592517853
2018-10-01 10:26:09.749468:	Training iteration: 77200, Loss: 0.19689083099365234
2018-10-01 10:28:37.396595:	Training iteration: 77400, Loss: 0.1498260647058487
2018-10-01 10:31:04.677167:	Training iteration: 77600, Loss: 0.21299490332603455
2018-10-01 10:33:32.471155:	Training iteration: 77800, Loss: 0.1583726406097412
2018-10-01 10:36:00.111667:	Training iteration: 78000, Loss: 0.1415325105190277
2018-10-01 10:38:27.853451:	Training iteration: 78200, Loss: 0.12825679779052734
2018-10-01 10:40:55.142571:	Training iteration: 78400, Loss: 0.16321566700935364
2018-10-01 10:43:23.022719:	Training iteration: 78600, Loss: 0.15357403457164764
2018-10-01 10:45:49.977802:	Training iteration: 78800, Loss: 0.14865383505821228
2018-10-01 10:48:17.730772:	Training iteration: 79000, Loss: 0.14602099359035492
2018-10-01 10:50:45.015309:	Training iteration: 79200, Loss: 0.18129801750183105
2018-10-01 10:53:12.766538:	Training iteration: 79400, Loss: 0.18073567748069763
2018-10-01 10:55:40.535970:	Training iteration: 79600, Loss: 0.1605789065361023
2018-10-01 10:58:07.631381:	Training iteration: 79800, Loss: 0.1532302051782608
2018-10-01 11:00:35.139666:	Training iteration: 80000, Loss: 0.1550830453634262
2018-10-01 11:03:02.953588:	Training iteration: 80200, Loss: 0.18298766016960144
2018-10-01 11:05:30.461439:	Training iteration: 80400, Loss: 0.1575934886932373
2018-10-01 11:07:58.202321:	Training iteration: 80600, Loss: 0.1719246655702591
