INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "65"
Experiment ID: 65
Preparing dataset
Dataset ready
2018-09-30 16:28:29.850795: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-09-30 16:28:31.078039: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-30 16:28:31.078564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:27:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-09-30 16:28:31.078581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:27:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Running initialisation test
Starting testing
2018-09-30 16:28:39.709989:	Entering test loop
2018-09-30 16:28:50.119294: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 640 of 1000
2018-09-30 16:28:55.144731: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 16:28:56.756657:	Testing iteration: 0, Loss: 0.28548842668533325
2018-09-30 16:32:09.781423:	Testing iteration: 200, Loss: 0.24923861026763916
2018-09-30 16:35:39.556236:	Testing iteration: 400, Loss: 0.17432966828346252
2018-09-30 16:39:31.374665:	Testing iteration: 600, Loss: 0.18616649508476257
2018-09-30 16:40:37.416174: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 701 of 1000
2018-09-30 16:40:41.607452: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 16:44:01.768998:	Testing iteration: 800, Loss: 0.26032423973083496
2018-09-30 16:48:42.208971:	Testing iteration: 1000, Loss: 0.3899274468421936
2018-09-30 16:53:45.771208:	Testing iteration: 1200, Loss: 0.271838515996933
2018-09-30 16:56:22.331061: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 713 of 1000
2018-09-30 16:56:26.268036: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 16:59:26.137848:	Testing iteration: 1400, Loss: 0.26822298765182495
2018-09-30 17:05:35.481543:	Testing iteration: 1600, Loss: 0.3045751452445984
2018-09-30 17:12:11.144529:	Testing iteration: 1800, Loss: 0.3108639717102051
2018-09-30 17:16:51.209401: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 694 of 1000
2018-09-30 17:16:55.413073: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 17:19:02.246927:	Testing iteration: 2000, Loss: 0.2988148331642151
2018-09-30 17:26:02.789421:	Testing iteration: 2200, Loss: 0.25541073083877563
2018-09-30 17:33:28.289864:	Testing iteration: 2400, Loss: 0.30710476636886597
2018-09-30 17:40:48.315755: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 691 of 1000
2018-09-30 17:40:52.151556: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 17:41:31.316620:	Testing iteration: 2600, Loss: 0.19914019107818604
2018-09-30 17:49:43.980419:	Testing iteration: 2800, Loss: 0.24512656033039093
2018-09-30 17:58:19.723871:	Testing iteration: 3000, Loss: 0.22513341903686523
2018-09-30 18:07:38.948038:	Testing iteration: 3200, Loss: 0.19921964406967163
2018-09-30 18:17:06.759293:	Testing iteration: 3400, Loss: 0.216729998588562
2018-09-30 18:26:55.723748:	Testing iteration: 3600, Loss: 0.1576438695192337
Test pass complete
Mean loss over test set: 0.2682114173706377
Data saved to dumps/65 for later audio metric calculation
Starting training
2018-09-30 18:35:45.989924: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 764 of 1000
2018-09-30 18:35:48.742694: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 18:38:12.296766:	Training iteration: 200, Loss: 0.2785923480987549
2018-09-30 18:40:39.101640:	Training iteration: 400, Loss: 0.1704980432987213
2018-09-30 18:43:07.490483:	Training iteration: 600, Loss: 0.2020706832408905
2018-09-30 18:45:34.835624:	Training iteration: 800, Loss: 0.16958986222743988
2018-09-30 18:48:01.934658:	Training iteration: 1000, Loss: 0.18032202124595642
2018-09-30 18:50:29.305323:	Training iteration: 1200, Loss: 0.18915048241615295
2018-09-30 18:52:57.096929:	Training iteration: 1400, Loss: 0.1776573657989502
2018-09-30 18:55:25.102040:	Training iteration: 1600, Loss: 0.16667014360427856
2018-09-30 18:57:52.779361:	Training iteration: 1800, Loss: 0.1634405553340912
2018-09-30 19:00:20.586420:	Training iteration: 2000, Loss: 0.18626613914966583
2018-09-30 19:02:48.040980:	Training iteration: 2200, Loss: 0.17524531483650208
2018-09-30 19:05:15.685429:	Training iteration: 2400, Loss: 0.16043242812156677
2018-09-30 19:07:44.061004:	Training iteration: 2600, Loss: 0.2005857229232788
2018-09-30 19:10:11.848158:	Training iteration: 2800, Loss: 0.11402922868728638
2018-09-30 19:12:39.891283:	Training iteration: 3000, Loss: 0.15843646228313446
2018-09-30 19:15:07.686926:	Training iteration: 3200, Loss: 0.21461033821105957
2018-09-30 19:17:36.089387:	Training iteration: 3400, Loss: 0.1893700510263443
2018-09-30 19:20:03.957057:	Training iteration: 3600, Loss: 0.11941491067409515
2018-09-30 19:22:32.363405:	Training iteration: 3800, Loss: 0.19994071125984192
2018-09-30 19:25:00.392105:	Training iteration: 4000, Loss: 0.15497273206710815
2018-09-30 19:27:28.460564:	Training iteration: 4200, Loss: 0.17747750878334045
2018-09-30 19:29:56.455116:	Training iteration: 4400, Loss: 0.12086919695138931
2018-09-30 19:32:24.964453:	Training iteration: 4600, Loss: 0.19171544909477234
2018-09-30 19:34:52.522022:	Training iteration: 4800, Loss: 0.1474720537662506
2018-09-30 19:36:59.864566: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 753 of 1000
2018-09-30 19:37:02.961344: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 19:37:32.551764:	Training iteration: 5000, Loss: 0.15392489731311798
2018-09-30 19:39:59.885022:	Training iteration: 5200, Loss: 0.15708985924720764
2018-09-30 19:42:27.891292:	Training iteration: 5400, Loss: 0.12969417870044708
2018-09-30 19:44:55.788198:	Training iteration: 5600, Loss: 0.13786564767360687
2018-09-30 19:47:23.660509:	Training iteration: 5800, Loss: 0.15927204489707947
2018-09-30 19:49:51.998115:	Training iteration: 6000, Loss: 0.1410192996263504
2018-09-30 19:52:20.417364:	Training iteration: 6200, Loss: 0.13236400485038757
2018-09-30 19:54:48.801572:	Training iteration: 6400, Loss: 0.12431351095438004
2018-09-30 19:57:16.721812:	Training iteration: 6600, Loss: 0.12907159328460693
2018-09-30 19:59:44.109202:	Training iteration: 6800, Loss: 0.14350023865699768
2018-09-30 20:02:12.529627:	Training iteration: 7000, Loss: 0.1302269995212555
2018-09-30 20:04:40.493593:	Training iteration: 7200, Loss: 0.14979706704616547
2018-09-30 20:07:08.598735:	Training iteration: 7400, Loss: 0.1638888418674469
2018-09-30 20:09:36.393240:	Training iteration: 7600, Loss: 0.13377080857753754
2018-09-30 20:12:04.657311:	Training iteration: 7800, Loss: 0.13025245070457458
2018-09-30 20:14:33.194459:	Training iteration: 8000, Loss: 0.16072459518909454
2018-09-30 20:17:01.355702:	Training iteration: 8200, Loss: 0.14526957273483276
2018-09-30 20:19:29.015634:	Training iteration: 8400, Loss: 0.1289776712656021
2018-09-30 20:21:56.914868:	Training iteration: 8600, Loss: 0.14775016903877258
2018-09-30 20:24:25.320787:	Training iteration: 8800, Loss: 0.09339907765388489
2018-09-30 20:26:53.585043:	Training iteration: 9000, Loss: 0.12041854858398438
2018-09-30 20:29:21.557681:	Training iteration: 9200, Loss: 0.14431637525558472
2018-09-30 20:31:50.118730:	Training iteration: 9400, Loss: 0.13710995018482208
2018-09-30 20:34:18.249562:	Training iteration: 9600, Loss: 0.1432729810476303
2018-09-30 20:36:46.223729:	Training iteration: 9800, Loss: 0.11898299306631088
2018-09-30 20:38:44.012584: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 747 of 1000
2018-09-30 20:38:47.154753: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 20:39:25.385632:	Training iteration: 10000, Loss: 0.08479908853769302
2018-09-30 20:41:53.085696:	Training iteration: 10200, Loss: 0.08580569177865982
2018-09-30 20:44:21.430933:	Training iteration: 10400, Loss: 0.06713750213384628
2018-09-30 20:46:49.511967:	Training iteration: 10600, Loss: 0.11332179605960846
2018-09-30 20:49:17.162240:	Training iteration: 10800, Loss: 0.1161084771156311
2018-09-30 20:51:45.471649:	Training iteration: 11000, Loss: 0.07342439889907837
2018-09-30 20:54:13.180643:	Training iteration: 11200, Loss: 0.1091146245598793
2018-09-30 20:56:40.752429:	Training iteration: 11400, Loss: 0.11766690015792847
2018-09-30 20:59:08.491546:	Training iteration: 11600, Loss: 0.09717988222837448
2018-09-30 21:01:36.180059:	Training iteration: 11800, Loss: 0.06790051609277725
2018-09-30 21:04:04.062873:	Training iteration: 12000, Loss: 0.13884615898132324
2018-09-30 21:06:31.213204:	Training iteration: 12200, Loss: 0.10109400749206543
2018-09-30 21:09:00.055739:	Training iteration: 12400, Loss: 0.09688787162303925
2018-09-30 21:11:28.052467:	Training iteration: 12600, Loss: 0.10975624620914459
2018-09-30 21:13:56.066592:	Training iteration: 12800, Loss: 0.09707532078027725
2018-09-30 21:16:24.581573:	Training iteration: 13000, Loss: 0.07776603102684021
2018-09-30 21:18:52.869244:	Training iteration: 13200, Loss: 0.12049319595098495
2018-09-30 21:21:21.059290:	Training iteration: 13400, Loss: 0.08182694017887115
2018-09-30 21:23:49.192670:	Training iteration: 13600, Loss: 0.08345713466405869
2018-09-30 21:26:17.148962:	Training iteration: 13800, Loss: 0.1207585483789444
2018-09-30 21:28:45.235738:	Training iteration: 14000, Loss: 0.09655682742595673
2018-09-30 21:31:12.887020:	Training iteration: 14200, Loss: 0.07808855175971985
2018-09-30 21:33:41.346208:	Training iteration: 14400, Loss: 0.11253142356872559
2018-09-30 21:36:09.926278:	Training iteration: 14600, Loss: 0.0711064338684082
2018-09-30 21:38:37.833057:	Training iteration: 14800, Loss: 0.11493577808141708
2018-09-30 21:40:24.451422: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 789 of 1000
2018-09-30 21:40:27.092168: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 21:41:17.115689:	Training iteration: 15000, Loss: 0.17741768062114716
2018-09-30 21:43:44.715039:	Training iteration: 15200, Loss: 0.12046582996845245
2018-09-30 21:46:12.784604:	Training iteration: 15400, Loss: 0.1340697705745697
2018-09-30 21:48:41.082282:	Training iteration: 15600, Loss: 0.08773218095302582
2018-09-30 21:51:08.746011:	Training iteration: 15800, Loss: 0.12925416231155396
2018-09-30 21:53:36.624765:	Training iteration: 16000, Loss: 0.16713008284568787
2018-09-30 21:56:04.929819:	Training iteration: 16200, Loss: 0.12008688598871231
2018-09-30 21:58:32.878457:	Training iteration: 16400, Loss: 0.12749138474464417
2018-09-30 22:01:00.810078:	Training iteration: 16600, Loss: 0.14839479327201843
2018-09-30 22:03:28.914813:	Training iteration: 16800, Loss: 0.1639430820941925
2018-09-30 22:05:56.452726:	Training iteration: 17000, Loss: 0.15497618913650513
2018-09-30 22:08:24.562135:	Training iteration: 17200, Loss: 0.11257161945104599
2018-09-30 22:10:52.430660:	Training iteration: 17400, Loss: 0.17808078229427338
2018-09-30 22:13:20.555572:	Training iteration: 17600, Loss: 0.1262541115283966
2018-09-30 22:15:48.606465:	Training iteration: 17800, Loss: 0.14692655205726624
2018-09-30 22:18:16.536305:	Training iteration: 18000, Loss: 0.1434423178434372
2018-09-30 22:20:44.642052:	Training iteration: 18200, Loss: 0.17256000638008118
2018-09-30 22:23:13.058488:	Training iteration: 18400, Loss: 0.11389286816120148
2018-09-30 22:25:41.506965:	Training iteration: 18600, Loss: 0.16850443184375763
2018-09-30 22:28:09.863876:	Training iteration: 18800, Loss: 0.12482231855392456
2018-09-30 22:30:38.023192:	Training iteration: 19000, Loss: 0.16388534009456635
2018-09-30 22:33:06.516765:	Training iteration: 19200, Loss: 0.13366766273975372
2018-09-30 22:35:34.664910:	Training iteration: 19400, Loss: 0.16899727284908295
2018-09-30 22:38:03.036992:	Training iteration: 19600, Loss: 0.1329905390739441
2018-09-30 22:40:30.991651:	Training iteration: 19800, Loss: 0.1450626105070114
2018-09-30 22:42:58.748145:	Training iteration: 20000, Loss: 0.1342940479516983
2018-09-30 22:44:51.556292: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 853 of 1000
2018-09-30 22:44:53.163918: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 22:45:37.293130:	Training iteration: 20200, Loss: 0.12930041551589966
2018-09-30 22:48:04.504539:	Training iteration: 20400, Loss: 0.12533403933048248
2018-09-30 22:50:32.234206:	Training iteration: 20600, Loss: 0.23495884239673615
2018-09-30 22:52:59.627267:	Training iteration: 20800, Loss: 0.23661084473133087
2018-09-30 22:55:26.967512:	Training iteration: 21000, Loss: 0.15230469405651093
2018-09-30 22:57:54.270502:	Training iteration: 21200, Loss: 0.13309931755065918
2018-09-30 23:00:21.797087:	Training iteration: 21400, Loss: 0.19113418459892273
2018-09-30 23:02:49.099057:	Training iteration: 21600, Loss: 0.17652755975723267
2018-09-30 23:05:16.873458:	Training iteration: 21800, Loss: 0.13541486859321594
2018-09-30 23:07:44.336200:	Training iteration: 22000, Loss: 0.23117494583129883
2018-09-30 23:10:11.810098:	Training iteration: 22200, Loss: 0.16130705177783966
2018-09-30 23:12:38.992793:	Training iteration: 22400, Loss: 0.15881982445716858
2018-09-30 23:15:06.821981:	Training iteration: 22600, Loss: 0.16178688406944275
2018-09-30 23:17:34.216526:	Training iteration: 22800, Loss: 0.15922975540161133
2018-09-30 23:20:01.646185:	Training iteration: 23000, Loss: 0.17582044005393982
2018-09-30 23:22:29.207844:	Training iteration: 23200, Loss: 0.18979144096374512
2018-09-30 23:24:56.967758:	Training iteration: 23400, Loss: 0.19509242475032806
2018-09-30 23:27:24.389805:	Training iteration: 23600, Loss: 0.15960372984409332
2018-09-30 23:29:51.706812:	Training iteration: 23800, Loss: 0.16639170050621033
2018-09-30 23:32:19.288138:	Training iteration: 24000, Loss: 0.1368679702281952
2018-09-30 23:34:47.041301:	Training iteration: 24200, Loss: 0.16461752355098724
2018-09-30 23:37:14.461217:	Training iteration: 24400, Loss: 0.1970209777355194
2018-09-30 23:39:42.500103:	Training iteration: 24600, Loss: 0.1764189749956131
2018-09-30 23:42:10.372439:	Training iteration: 24800, Loss: 0.14612898230552673
2018-09-30 23:44:37.883119:	Training iteration: 25000, Loss: 0.1427546739578247
2018-09-30 23:47:04.961121:	Training iteration: 25200, Loss: 0.12236727774143219
2018-09-30 23:49:32.713581:	Training iteration: 25400, Loss: 0.1901313215494156
2018-09-30 23:52:00.427474:	Training iteration: 25600, Loss: 0.18731603026390076
2018-09-30 23:54:28.046098:	Training iteration: 25800, Loss: 0.16002357006072998
2018-09-30 23:56:55.353487:	Training iteration: 26000, Loss: 0.1399284303188324
2018-09-30 23:59:22.869906:	Training iteration: 26200, Loss: 0.16173169016838074
2018-10-01 00:01:50.123360:	Training iteration: 26400, Loss: 0.16824014484882355
2018-10-01 00:04:17.881512:	Training iteration: 26600, Loss: 0.19441981613636017
2018-10-01 00:06:45.469001:	Training iteration: 26800, Loss: 0.14668290317058563
2018-10-01 00:09:13.244847:	Training iteration: 27000, Loss: 0.13354355096817017
2018-10-01 00:11:40.977957:	Training iteration: 27200, Loss: 0.1656661331653595
2018-10-01 00:14:08.448484:	Training iteration: 27400, Loss: 0.13565310835838318
2018-10-01 00:16:35.790733:	Training iteration: 27600, Loss: 0.17622342705726624
2018-10-01 00:19:03.359117:	Training iteration: 27800, Loss: 0.16490887105464935
2018-10-01 00:21:30.952049:	Training iteration: 28000, Loss: 0.1470469832420349
2018-10-01 00:23:58.314351:	Training iteration: 28200, Loss: 0.19839254021644592
2018-10-01 00:26:26.177317:	Training iteration: 28400, Loss: 0.1718001663684845
2018-10-01 00:28:53.081474:	Training iteration: 28600, Loss: 0.1758023202419281
2018-10-01 00:31:20.686772:	Training iteration: 28800, Loss: 0.13914456963539124
2018-10-01 00:33:48.149339:	Training iteration: 29000, Loss: 0.16278406977653503
2018-10-01 00:36:15.322052:	Training iteration: 29200, Loss: 0.16698741912841797
2018-10-01 00:38:43.161548:	Training iteration: 29400, Loss: 0.18647554516792297
2018-10-01 00:41:11.112756:	Training iteration: 29600, Loss: 0.19275996088981628
2018-10-01 00:43:38.597227:	Training iteration: 29800, Loss: 0.14081928133964539
2018-10-01 00:46:05.934411:	Training iteration: 30000, Loss: 0.16115984320640564
2018-10-01 00:48:33.009140:	Training iteration: 30200, Loss: 0.16833582520484924
2018-10-01 00:51:00.687535:	Training iteration: 30400, Loss: 0.188092440366745
2018-10-01 00:53:28.206597:	Training iteration: 30600, Loss: 0.17927730083465576
2018-10-01 00:55:55.966415:	Training iteration: 30800, Loss: 0.15158233046531677
2018-10-01 00:58:23.390277:	Training iteration: 31000, Loss: 0.1619580239057541
2018-10-01 01:00:50.750477:	Training iteration: 31200, Loss: 0.16465817391872406
2018-10-01 01:03:18.400054:	Training iteration: 31400, Loss: 0.17739936709403992
2018-10-01 01:05:45.968379:	Training iteration: 31600, Loss: 0.15004611015319824
2018-10-01 01:08:13.660683:	Training iteration: 31800, Loss: 0.1764136105775833
2018-10-01 01:10:41.160149:	Training iteration: 32000, Loss: 0.191621795296669
2018-10-01 01:13:08.590003:	Training iteration: 32200, Loss: 0.18408402800559998
2018-10-01 01:15:36.312601:	Training iteration: 32400, Loss: 0.1725129634141922
2018-10-01 01:18:03.806035:	Training iteration: 32600, Loss: 0.24093523621559143
2018-10-01 01:20:31.854938:	Training iteration: 32800, Loss: 0.15620754659175873
2018-10-01 01:22:59.089692:	Training iteration: 33000, Loss: 0.1677788943052292
2018-10-01 01:25:26.916294:	Training iteration: 33200, Loss: 0.14170122146606445
2018-10-01 01:27:54.251934:	Training iteration: 33400, Loss: 0.15840111672878265
2018-10-01 01:30:21.811266:	Training iteration: 33600, Loss: 0.14071159064769745
2018-10-01 01:32:49.245485:	Training iteration: 33800, Loss: 0.14521363377571106
2018-10-01 01:35:16.591641:	Training iteration: 34000, Loss: 0.18972346186637878
2018-10-01 01:37:44.297310:	Training iteration: 34200, Loss: 0.29579657316207886
2018-10-01 01:40:11.743862:	Training iteration: 34400, Loss: 0.1757861077785492
2018-10-01 01:42:39.389447:	Training iteration: 34600, Loss: 0.15638160705566406
2018-10-01 01:45:06.568091:	Training iteration: 34800, Loss: 0.1499163955450058
2018-10-01 01:47:33.730591:	Training iteration: 35000, Loss: 0.15786579251289368
2018-10-01 01:50:01.073132:	Training iteration: 35200, Loss: 0.1633695363998413
2018-10-01 01:52:28.727433:	Training iteration: 35400, Loss: 0.1622721254825592
2018-10-01 01:54:56.054638:	Training iteration: 35600, Loss: 0.16925711929798126
2018-10-01 01:57:23.713152:	Training iteration: 35800, Loss: 0.17308445274829865
2018-10-01 01:59:51.764607:	Training iteration: 36000, Loss: 0.21495351195335388
2018-10-01 02:02:19.719789:	Training iteration: 36200, Loss: 0.15364737808704376
2018-10-01 02:04:47.308777:	Training iteration: 36400, Loss: 0.18038532137870789
2018-10-01 02:07:14.828731:	Training iteration: 36600, Loss: 0.15835843980312347
2018-10-01 02:09:42.622823:	Training iteration: 36800, Loss: 0.19533690810203552
2018-10-01 02:12:09.579369:	Training iteration: 37000, Loss: 0.15775573253631592
2018-10-01 02:14:37.460695:	Training iteration: 37200, Loss: 0.13222074508666992
2018-10-01 02:17:05.262465:	Training iteration: 37400, Loss: 0.1384740173816681
2018-10-01 02:19:32.835645:	Training iteration: 37600, Loss: 0.18194933235645294
2018-10-01 02:22:00.303355:	Training iteration: 37800, Loss: 0.16798202693462372
2018-10-01 02:24:28.020084:	Training iteration: 38000, Loss: 0.1850244104862213
2018-10-01 02:26:55.687746:	Training iteration: 38200, Loss: 0.16858401894569397
2018-10-01 02:29:22.737790:	Training iteration: 38400, Loss: 0.16995379328727722
2018-10-01 02:31:50.755456:	Training iteration: 38600, Loss: 0.16621895134449005
2018-10-01 02:34:18.346768:	Training iteration: 38800, Loss: 0.1768273264169693
2018-10-01 02:36:45.990900:	Training iteration: 39000, Loss: 0.1251404732465744
2018-10-01 02:39:12.794490:	Training iteration: 39200, Loss: 0.13147693872451782
2018-10-01 02:41:40.519819:	Training iteration: 39400, Loss: 0.14284774661064148
2018-10-01 02:44:07.737935:	Training iteration: 39600, Loss: 0.20006227493286133
2018-10-01 02:46:35.311597:	Training iteration: 39800, Loss: 0.1624414026737213
2018-10-01 02:49:02.926294:	Training iteration: 40000, Loss: 0.1590300351381302
2018-10-01 02:51:30.264496:	Training iteration: 40200, Loss: 0.16842374205589294
2018-10-01 02:53:57.467520:	Training iteration: 40400, Loss: 0.16957613825798035
2018-10-01 02:56:25.100837:	Training iteration: 40600, Loss: 0.17377997934818268
2018-10-01 02:58:52.619310:	Training iteration: 40800, Loss: 0.153082013130188
2018-10-01 03:01:19.639111:	Training iteration: 41000, Loss: 0.1524786353111267
2018-10-01 03:03:47.363238:	Training iteration: 41200, Loss: 0.14816723763942719
2018-10-01 03:06:15.022529:	Training iteration: 41400, Loss: 0.16795745491981506
2018-10-01 03:08:42.624220:	Training iteration: 41600, Loss: 0.1821283996105194
2018-10-01 03:11:09.670364:	Training iteration: 41800, Loss: 0.1561817079782486
2018-10-01 03:13:37.392474:	Training iteration: 42000, Loss: 0.1434328556060791
2018-10-01 03:16:05.439324:	Training iteration: 42200, Loss: 0.19203732907772064
2018-10-01 03:18:33.108544:	Training iteration: 42400, Loss: 0.32249340415000916
2018-10-01 03:21:00.610713:	Training iteration: 42600, Loss: 0.19240806996822357
2018-10-01 03:23:27.414380:	Training iteration: 42800, Loss: 0.1334388256072998
2018-10-01 03:25:55.082938:	Training iteration: 43000, Loss: 0.16158607602119446
2018-10-01 03:28:22.620968:	Training iteration: 43200, Loss: 0.1340337097644806
2018-10-01 03:30:49.778740:	Training iteration: 43400, Loss: 0.19450592994689941
2018-10-01 03:33:17.644026:	Training iteration: 43600, Loss: 0.19882896542549133
2018-10-01 03:35:45.309824:	Training iteration: 43800, Loss: 0.15894198417663574
2018-10-01 03:38:12.833207:	Training iteration: 44000, Loss: 0.14164283871650696
2018-10-01 03:40:39.644846:	Training iteration: 44200, Loss: 0.15853014588356018
2018-10-01 03:43:07.332840:	Training iteration: 44400, Loss: 0.20301060378551483
2018-10-01 03:45:34.828649:	Training iteration: 44600, Loss: 0.15532377362251282
2018-10-01 03:48:02.302156:	Training iteration: 44800, Loss: 0.2205936163663864
2018-10-01 03:50:30.029939:	Training iteration: 45000, Loss: 0.14974069595336914
2018-10-01 03:52:58.024401:	Training iteration: 45200, Loss: 0.20510122179985046
2018-10-01 03:55:25.179974:	Training iteration: 45400, Loss: 0.2023247480392456
2018-10-01 03:57:52.458466:	Training iteration: 45600, Loss: 0.17948974668979645
2018-10-01 04:00:19.789529:	Training iteration: 45800, Loss: 0.20191173255443573
2018-10-01 04:02:47.827572:	Training iteration: 46000, Loss: 0.14931759238243103
2018-10-01 04:05:15.580369:	Training iteration: 46200, Loss: 0.18234172463417053
2018-10-01 04:07:43.046999:	Training iteration: 46400, Loss: 0.17479482293128967
2018-10-01 04:10:10.748657:	Training iteration: 46600, Loss: 0.1455042064189911
2018-10-01 04:12:38.517044:	Training iteration: 46800, Loss: 0.15710732340812683
2018-10-01 04:15:05.935755:	Training iteration: 47000, Loss: 0.15612682700157166
2018-10-01 04:17:33.795686:	Training iteration: 47200, Loss: 0.1623525619506836
2018-10-01 04:20:01.156900:	Training iteration: 47400, Loss: 0.20526190102100372
2018-10-01 04:22:28.706354:	Training iteration: 47600, Loss: 0.20815277099609375
2018-10-01 04:24:56.232609:	Training iteration: 47800, Loss: 0.1661551296710968
2018-10-01 04:27:23.463810:	Training iteration: 48000, Loss: 0.16414573788642883
2018-10-01 04:29:50.709200:	Training iteration: 48200, Loss: 0.14075914025306702
2018-10-01 04:32:18.641671:	Training iteration: 48400, Loss: 0.17716087400913239
2018-10-01 04:34:46.142444:	Training iteration: 48600, Loss: 0.16737130284309387
2018-10-01 04:37:13.897588:	Training iteration: 48800, Loss: 0.17973080277442932
2018-10-01 04:39:41.394544:	Training iteration: 49000, Loss: 0.1774768978357315
2018-10-01 04:42:08.634020:	Training iteration: 49200, Loss: 0.14454717934131622
2018-10-01 04:44:36.069477:	Training iteration: 49400, Loss: 0.1564764529466629
2018-10-01 04:47:03.768653:	Training iteration: 49600, Loss: 0.12144573777914047
2018-10-01 04:49:31.576264:	Training iteration: 49800, Loss: 0.16427819430828094
2018-10-01 04:51:58.771113:	Training iteration: 50000, Loss: 0.18939615786075592
2018-10-01 04:54:26.435878:	Training iteration: 50200, Loss: 0.20060627162456512
2018-10-01 04:56:54.065250:	Training iteration: 50400, Loss: 0.13381260633468628
2018-10-01 04:59:21.807457:	Training iteration: 50600, Loss: 0.1787852942943573
2018-10-01 05:01:49.486193:	Training iteration: 50800, Loss: 0.21401970088481903
2018-10-01 05:04:17.007736:	Training iteration: 51000, Loss: 0.23407652974128723
2018-10-01 05:06:44.219463:	Training iteration: 51200, Loss: 0.15347342193126678
2018-10-01 05:09:11.606740:	Training iteration: 51400, Loss: 0.13371916115283966
2018-10-01 05:11:39.024290:	Training iteration: 51600, Loss: 0.15524789690971375
2018-10-01 05:14:06.294031:	Training iteration: 51800, Loss: 0.1465391218662262
2018-10-01 05:16:34.064271:	Training iteration: 52000, Loss: 0.1748056709766388
2018-10-01 05:19:01.455068:	Training iteration: 52200, Loss: 0.2347269058227539
2018-10-01 05:21:28.883793:	Training iteration: 52400, Loss: 0.18366427719593048
2018-10-01 05:23:56.095006:	Training iteration: 52600, Loss: 0.17037735879421234
2018-10-01 05:26:23.328300:	Training iteration: 52800, Loss: 0.1467462033033371
2018-10-01 05:28:50.905474:	Training iteration: 53000, Loss: 0.18876178562641144
2018-10-01 05:31:18.342207:	Training iteration: 53200, Loss: 0.15481352806091309
2018-10-01 05:33:45.839064:	Training iteration: 53400, Loss: 0.1750767081975937
2018-10-01 05:36:13.127564:	Training iteration: 53600, Loss: 0.1425030678510666
2018-10-01 05:38:41.156973:	Training iteration: 53800, Loss: 0.13637329638004303
2018-10-01 05:41:08.685765:	Training iteration: 54000, Loss: 0.188416987657547
2018-10-01 05:43:36.337990:	Training iteration: 54200, Loss: 0.15770265460014343
2018-10-01 05:46:02.398841:	Training iteration: 54400, Loss: 0.175687775015831
2018-10-01 05:48:30.387766:	Training iteration: 54600, Loss: 0.14923718571662903
2018-10-01 05:50:58.029702:	Training iteration: 54800, Loss: 0.16488590836524963
2018-10-01 05:53:25.962359:	Training iteration: 55000, Loss: 0.15436334908008575
2018-10-01 05:55:53.394588:	Training iteration: 55200, Loss: 0.1444128304719925
2018-10-01 05:58:21.219680:	Training iteration: 55400, Loss: 0.2328883707523346
2018-10-01 06:00:48.548981:	Training iteration: 55600, Loss: 0.1983342468738556
2018-10-01 06:03:16.046150:	Training iteration: 55800, Loss: 0.2441726177930832
2018-10-01 06:05:43.792856:	Training iteration: 56000, Loss: 0.1968483328819275
2018-10-01 06:08:11.063701:	Training iteration: 56200, Loss: 0.1140742301940918
2018-10-01 06:10:38.937119:	Training iteration: 56400, Loss: 0.13156571984291077
2018-10-01 06:13:05.987086:	Training iteration: 56600, Loss: 0.1763749122619629
2018-10-01 06:15:33.325812:	Training iteration: 56800, Loss: 0.1932714283466339
2018-10-01 06:18:00.544920:	Training iteration: 57000, Loss: 0.1644897311925888
2018-10-01 06:20:28.009050:	Training iteration: 57200, Loss: 0.17195911705493927
2018-10-01 06:22:55.149359:	Training iteration: 57400, Loss: 0.1575966626405716
2018-10-01 06:25:22.807117:	Training iteration: 57600, Loss: 0.19660916924476624
2018-10-01 06:27:49.832032:	Training iteration: 57800, Loss: 0.20420663058757782
2018-10-01 06:30:17.756011:	Training iteration: 58000, Loss: 0.20065517723560333
2018-10-01 06:32:45.301278:	Training iteration: 58200, Loss: 0.17287617921829224
2018-10-01 06:35:12.525090:	Training iteration: 58400, Loss: 0.1394955813884735
2018-10-01 06:37:39.483337:	Training iteration: 58600, Loss: 0.18338489532470703
2018-10-01 06:40:07.484967:	Training iteration: 58800, Loss: 0.1466401219367981
2018-10-01 06:42:35.267104:	Training iteration: 59000, Loss: 0.15139944851398468
2018-10-01 06:45:02.994678:	Training iteration: 59200, Loss: 0.1583680808544159
2018-10-01 06:47:30.879202:	Training iteration: 59400, Loss: 0.16490115225315094
2018-10-01 06:49:58.147541:	Training iteration: 59600, Loss: 0.15870067477226257
2018-10-01 06:52:26.035635:	Training iteration: 59800, Loss: 0.15460002422332764
2018-10-01 06:54:53.704259:	Training iteration: 60000, Loss: 0.192198246717453
2018-10-01 06:57:21.525698:	Training iteration: 60200, Loss: 0.18386709690093994
2018-10-01 06:59:49.297610:	Training iteration: 60400, Loss: 0.13682986795902252
2018-10-01 07:02:16.860828:	Training iteration: 60600, Loss: 0.1675000935792923
2018-10-01 07:04:44.233842:	Training iteration: 60800, Loss: 0.184278666973114
2018-10-01 07:07:11.613656:	Training iteration: 61000, Loss: 0.11193466186523438
2018-10-01 07:09:39.424291:	Training iteration: 61200, Loss: 0.15130281448364258
2018-10-01 07:12:07.048278:	Training iteration: 61400, Loss: 0.1414221078157425
2018-10-01 07:14:34.430837:	Training iteration: 61600, Loss: 0.15344120562076569
2018-10-01 07:17:02.126139:	Training iteration: 61800, Loss: 0.1630500853061676
2018-10-01 07:19:29.861816:	Training iteration: 62000, Loss: 0.15263178944587708
2018-10-01 07:21:57.854686:	Training iteration: 62200, Loss: 0.18208667635917664
2018-10-01 07:24:25.184899:	Training iteration: 62400, Loss: 0.16456632316112518
2018-10-01 07:26:52.855331:	Training iteration: 62600, Loss: 0.16918909549713135
2018-10-01 07:29:20.602998:	Training iteration: 62800, Loss: 0.17555123567581177
2018-10-01 07:31:48.346561:	Training iteration: 63000, Loss: 0.17050687968730927
2018-10-01 07:34:15.803547:	Training iteration: 63200, Loss: 0.1779402792453766
2018-10-01 07:36:43.185982:	Training iteration: 63400, Loss: 0.18410423398017883
2018-10-01 07:39:10.594213:	Training iteration: 63600, Loss: 0.16688689589500427
2018-10-01 07:41:37.833825:	Training iteration: 63800, Loss: 0.17258667945861816
2018-10-01 07:44:05.761408:	Training iteration: 64000, Loss: 0.17027083039283752
2018-10-01 07:46:33.702140:	Training iteration: 64200, Loss: 0.1625906229019165
2018-10-01 07:49:01.485401:	Training iteration: 64400, Loss: 0.1596822887659073
2018-10-01 07:51:29.408367:	Training iteration: 64600, Loss: 0.13331790268421173
2018-10-01 07:53:57.188362:	Training iteration: 64800, Loss: 0.17418578267097473
2018-10-01 07:56:24.895922:	Training iteration: 65000, Loss: 0.17841629683971405
2018-10-01 07:58:52.520177:	Training iteration: 65200, Loss: 0.18661880493164062
2018-10-01 08:01:19.823831:	Training iteration: 65400, Loss: 0.17197255790233612
2018-10-01 08:03:47.543392:	Training iteration: 65600, Loss: 0.15330800414085388
2018-10-01 08:06:14.925880:	Training iteration: 65800, Loss: 0.18645717203617096
2018-10-01 08:08:42.458244:	Training iteration: 66000, Loss: 0.26108261942863464
2018-10-01 08:11:09.819176:	Training iteration: 66200, Loss: 0.40943050384521484
2018-10-01 08:13:36.840386:	Training iteration: 66400, Loss: 0.20232105255126953
2018-10-01 08:16:04.113754:	Training iteration: 66600, Loss: 0.15473301708698273
2018-10-01 08:18:31.886735:	Training iteration: 66800, Loss: 0.15676039457321167
2018-10-01 08:20:58.537538:	Training iteration: 67000, Loss: 0.18517360091209412
2018-10-01 08:23:26.288539:	Training iteration: 67200, Loss: 0.18777358531951904
2018-10-01 08:25:53.581302:	Training iteration: 67400, Loss: 0.16311532258987427
2018-10-01 08:28:21.269561:	Training iteration: 67600, Loss: 0.16630269587039948
2018-10-01 08:30:48.244124:	Training iteration: 67800, Loss: 0.19539594650268555
2018-10-01 08:33:15.746312:	Training iteration: 68000, Loss: 0.19628135859966278
2018-10-01 08:35:42.965200:	Training iteration: 68200, Loss: 0.18277163803577423
2018-10-01 08:38:10.245536:	Training iteration: 68400, Loss: 0.17014269530773163
2018-10-01 08:40:37.307424:	Training iteration: 68600, Loss: 0.16792893409729004
2018-10-01 08:43:04.459867:	Training iteration: 68800, Loss: 0.16662849485874176
2018-10-01 08:45:31.373264:	Training iteration: 69000, Loss: 0.1704034060239792
2018-10-01 08:47:58.655845:	Training iteration: 69200, Loss: 0.19883035123348236
2018-10-01 08:50:26.183479:	Training iteration: 69400, Loss: 0.21243807673454285
2018-10-01 08:52:53.320396:	Training iteration: 69600, Loss: 0.14296849071979523
2018-10-01 08:55:20.514319:	Training iteration: 69800, Loss: 0.17303094267845154
2018-10-01 08:57:47.711262:	Training iteration: 70000, Loss: 0.12716791033744812
2018-10-01 09:00:15.112798:	Training iteration: 70200, Loss: 0.15537628531455994
2018-10-01 09:02:42.727131:	Training iteration: 70400, Loss: 0.21247705817222595
2018-10-01 09:05:09.465151:	Training iteration: 70600, Loss: 0.14691446721553802
2018-10-01 09:07:36.872542:	Training iteration: 70800, Loss: 0.16312041878700256
2018-10-01 09:10:03.864466:	Training iteration: 71000, Loss: 0.19966165721416473
2018-10-01 09:12:31.394282:	Training iteration: 71200, Loss: 0.17510256171226501
2018-10-01 09:14:58.623521:	Training iteration: 71400, Loss: 0.13835150003433228
2018-10-01 09:17:25.841846:	Training iteration: 71600, Loss: 0.1395830512046814
2018-10-01 09:19:53.388173:	Training iteration: 71800, Loss: 0.16470304131507874
2018-10-01 09:22:20.560654:	Training iteration: 72000, Loss: 0.1708909124135971
2018-10-01 09:24:47.645922:	Training iteration: 72200, Loss: 0.1593324989080429
2018-10-01 09:27:14.772424:	Training iteration: 72400, Loss: 0.14194434881210327
2018-10-01 09:29:42.293592:	Training iteration: 72600, Loss: 0.11547289788722992
2018-10-01 09:32:09.466354:	Training iteration: 72800, Loss: 0.19762367010116577
2018-10-01 09:34:36.680994:	Training iteration: 73000, Loss: 0.18717032670974731
2018-10-01 09:37:03.905572:	Training iteration: 73200, Loss: 0.1543893665075302
2018-10-01 09:39:31.617755:	Training iteration: 73400, Loss: 0.1611367166042328
2018-10-01 09:41:59.054303:	Training iteration: 73600, Loss: 0.1743265986442566
2018-10-01 09:44:26.569867:	Training iteration: 73800, Loss: 0.14630591869354248
2018-10-01 09:46:53.357367:	Training iteration: 74000, Loss: 0.14985080063343048
2018-10-01 09:49:20.909504:	Training iteration: 74200, Loss: 0.17185130715370178
2018-10-01 09:51:48.167277:	Training iteration: 74400, Loss: 0.1318550705909729
2018-10-01 09:54:15.721754:	Training iteration: 74600, Loss: 0.13955217599868774
2018-10-01 09:56:42.949106:	Training iteration: 74800, Loss: 0.21459481120109558
2018-10-01 09:59:09.661374:	Training iteration: 75000, Loss: 0.17601145803928375
2018-10-01 10:01:37.388624:	Training iteration: 75200, Loss: 0.11390695720911026
2018-10-01 10:04:04.754114:	Training iteration: 75400, Loss: 0.1606692373752594
2018-10-01 10:06:32.118329:	Training iteration: 75600, Loss: 0.24344778060913086
2018-10-01 10:08:59.365015:	Training iteration: 75800, Loss: 0.14901021122932434
2018-10-01 10:11:26.906442:	Training iteration: 76000, Loss: 0.27845150232315063
2018-10-01 10:13:54.004428:	Training iteration: 76200, Loss: 0.22509567439556122
2018-10-01 10:16:21.240644:	Training iteration: 76400, Loss: 0.1858014464378357
2018-10-01 10:18:48.247201:	Training iteration: 76600, Loss: 0.1715000867843628
2018-10-01 10:21:15.412512:	Training iteration: 76800, Loss: 0.13949117064476013
2018-10-01 10:23:42.765520:	Training iteration: 77000, Loss: 0.20463700592517853
2018-10-01 10:26:09.749468:	Training iteration: 77200, Loss: 0.19689083099365234
2018-10-01 10:28:37.396595:	Training iteration: 77400, Loss: 0.1498260647058487
2018-10-01 10:31:04.677167:	Training iteration: 77600, Loss: 0.21299490332603455
2018-10-01 10:33:32.471155:	Training iteration: 77800, Loss: 0.1583726406097412
2018-10-01 10:36:00.111667:	Training iteration: 78000, Loss: 0.1415325105190277
2018-10-01 10:38:27.853451:	Training iteration: 78200, Loss: 0.12825679779052734
2018-10-01 10:40:55.142571:	Training iteration: 78400, Loss: 0.16321566700935364
2018-10-01 10:43:23.022719:	Training iteration: 78600, Loss: 0.15357403457164764
2018-10-01 10:45:49.977802:	Training iteration: 78800, Loss: 0.14865383505821228
2018-10-01 10:48:17.730772:	Training iteration: 79000, Loss: 0.14602099359035492
2018-10-01 10:50:45.015309:	Training iteration: 79200, Loss: 0.18129801750183105
2018-10-01 10:53:12.766538:	Training iteration: 79400, Loss: 0.18073567748069763
2018-10-01 10:55:40.535970:	Training iteration: 79600, Loss: 0.1605789065361023
2018-10-01 10:58:07.631381:	Training iteration: 79800, Loss: 0.1532302051782608
2018-10-01 11:00:35.139666:	Training iteration: 80000, Loss: 0.1550830453634262
2018-10-01 11:03:02.953588:	Training iteration: 80200, Loss: 0.18298766016960144
2018-10-01 11:05:30.461439:	Training iteration: 80400, Loss: 0.1575934886932373
2018-10-01 11:07:58.202321:	Training iteration: 80600, Loss: 0.1719246655702591
2018-10-01 11:10:25.272061:	Training iteration: 80800, Loss: 0.14310869574546814
2018-10-01 11:12:53.131683:	Training iteration: 81000, Loss: 0.1760450005531311
2018-10-01 11:15:20.223003:	Training iteration: 81200, Loss: 0.17338871955871582
2018-10-01 11:17:47.581863:	Training iteration: 81400, Loss: 0.15616349875926971
2018-10-01 11:20:15.194864:	Training iteration: 81600, Loss: 0.15604190528392792
2018-10-01 11:22:42.404207:	Training iteration: 81800, Loss: 0.16013483703136444
2018-10-01 11:25:09.663275:	Training iteration: 82000, Loss: 0.142007976770401
2018-10-01 11:27:36.884450:	Training iteration: 82200, Loss: 0.16198304295539856
2018-10-01 11:30:04.263467:	Training iteration: 82400, Loss: 0.14563341438770294
2018-10-01 11:32:31.675477:	Training iteration: 82600, Loss: 0.1823047697544098
2018-10-01 11:34:59.419390:	Training iteration: 82800, Loss: 0.21003301441669464
2018-10-01 11:37:26.972280:	Training iteration: 83000, Loss: 0.1641758233308792
2018-10-01 11:39:54.662466:	Training iteration: 83200, Loss: 0.1560976207256317
2018-10-01 11:42:22.144216:	Training iteration: 83400, Loss: 0.17746031284332275
2018-10-01 11:44:49.656352:	Training iteration: 83600, Loss: 0.17619358003139496
2018-10-01 11:47:17.210038:	Training iteration: 83800, Loss: 0.21518094837665558
2018-10-01 11:49:45.082258:	Training iteration: 84000, Loss: 0.1784849613904953
2018-10-01 11:52:12.194097:	Training iteration: 84200, Loss: 0.18613648414611816
2018-10-01 11:54:39.994141:	Training iteration: 84400, Loss: 0.20233149826526642
2018-10-01 11:57:07.793755:	Training iteration: 84600, Loss: 0.15150076150894165
2018-10-01 11:59:35.242442:	Training iteration: 84800, Loss: 0.19948899745941162
2018-10-01 12:02:02.715415:	Training iteration: 85000, Loss: 0.19416303932666779
2018-10-01 12:04:30.456638:	Training iteration: 85200, Loss: 0.16260573267936707
2018-10-01 12:06:57.778585:	Training iteration: 85400, Loss: 0.2129364013671875
2018-10-01 12:09:25.088003:	Training iteration: 85600, Loss: 0.19870984554290771
2018-10-01 12:11:52.371229:	Training iteration: 85800, Loss: 0.19753916561603546
2018-10-01 12:14:19.903841:	Training iteration: 86000, Loss: 0.1312646120786667
2018-10-01 12:16:47.504237:	Training iteration: 86200, Loss: 0.22591765224933624
2018-10-01 12:19:14.979459:	Training iteration: 86400, Loss: 0.20434555411338806
2018-10-01 12:21:42.408125:	Training iteration: 86600, Loss: 0.1909693032503128
2018-10-01 12:24:09.858720:	Training iteration: 86800, Loss: 0.146682471036911
2018-10-01 12:26:37.185286:	Training iteration: 87000, Loss: 0.17129965126514435
2018-10-01 12:29:04.578762:	Training iteration: 87200, Loss: 0.13920441269874573
2018-10-01 12:31:31.993647:	Training iteration: 87400, Loss: 0.24625518918037415
2018-10-01 12:33:58.782727:	Training iteration: 87600, Loss: 0.16901658475399017
2018-10-01 12:36:26.721564:	Training iteration: 87800, Loss: 0.2306949347257614
2018-10-01 12:38:54.290718:	Training iteration: 88000, Loss: 0.1535443514585495
2018-10-01 12:41:22.062453:	Training iteration: 88200, Loss: 0.17550338804721832
2018-10-01 12:43:49.342527:	Training iteration: 88400, Loss: 0.1560979187488556
2018-10-01 12:46:16.939530:	Training iteration: 88600, Loss: 0.15432703495025635
2018-10-01 12:48:44.778158:	Training iteration: 88800, Loss: 0.1687782108783722
2018-10-01 12:51:12.349257:	Training iteration: 89000, Loss: 0.17902565002441406
2018-10-01 12:53:39.475382:	Training iteration: 89200, Loss: 0.18954285979270935
2018-10-01 12:56:07.565017:	Training iteration: 89400, Loss: 0.1673125922679901
2018-10-01 12:58:34.967123:	Training iteration: 89600, Loss: 0.1698441058397293
2018-10-01 13:01:01.674822:	Training iteration: 89800, Loss: 0.16796396672725677
2018-10-01 13:03:29.471531:	Training iteration: 90000, Loss: 0.18767225742340088
2018-10-01 13:05:57.502742:	Training iteration: 90200, Loss: 0.15687304735183716
2018-10-01 13:08:25.233873:	Training iteration: 90400, Loss: 0.15841622650623322
2018-10-01 13:10:51.597984:	Training iteration: 90600, Loss: 0.1509537398815155
2018-10-01 13:13:19.821024:	Training iteration: 90800, Loss: 0.13137903809547424
2018-10-01 13:15:47.318119:	Training iteration: 91000, Loss: 0.13977667689323425
2018-10-01 13:18:14.616147:	Training iteration: 91200, Loss: 0.192839115858078
2018-10-01 13:20:42.002086:	Training iteration: 91400, Loss: 0.17877347767353058
2018-10-01 13:23:09.859458:	Training iteration: 91600, Loss: 0.18445844948291779
2018-10-01 13:25:37.504632:	Training iteration: 91800, Loss: 0.18160782754421234
2018-10-01 13:28:04.565650:	Training iteration: 92000, Loss: 0.20138809084892273
2018-10-01 13:30:32.322516:	Training iteration: 92200, Loss: 0.1744231879711151
2018-10-01 13:32:59.957301:	Training iteration: 92400, Loss: 0.2054692506790161
2018-10-01 13:35:27.145443:	Training iteration: 92600, Loss: 0.20297297835350037
2018-10-01 13:37:54.037259:	Training iteration: 92800, Loss: 0.18085217475891113
2018-10-01 13:40:21.668475:	Training iteration: 93000, Loss: 0.14955350756645203
2018-10-01 13:42:49.230914:	Training iteration: 93200, Loss: 0.14133091270923615
2018-10-01 13:45:16.506961:	Training iteration: 93400, Loss: 0.18571457266807556
2018-10-01 13:47:44.243561:	Training iteration: 93600, Loss: 0.15715640783309937
2018-10-01 13:50:12.002690:	Training iteration: 93800, Loss: 0.11956149339675903
2018-10-01 13:52:39.742135:	Training iteration: 94000, Loss: 0.1296689808368683
2018-10-01 13:55:07.889091:	Training iteration: 94200, Loss: 0.2019919455051422
2018-10-01 13:57:35.523027:	Training iteration: 94400, Loss: 0.16944915056228638
2018-10-01 14:00:03.008521:	Training iteration: 94600, Loss: 0.1562778502702713
2018-10-01 14:02:31.070107:	Training iteration: 94800, Loss: 0.15482375025749207
2018-10-01 14:04:58.716572:	Training iteration: 95000, Loss: 0.15684768557548523
2018-10-01 14:07:26.525321:	Training iteration: 95200, Loss: 0.16407261788845062
2018-10-01 14:09:53.747360:	Training iteration: 95400, Loss: 0.14908578991889954
2018-10-01 14:12:21.652575:	Training iteration: 95600, Loss: 0.19133032858371735
2018-10-01 14:14:49.284514:	Training iteration: 95800, Loss: 0.13809040188789368
2018-10-01 14:17:16.629205:	Training iteration: 96000, Loss: 0.15585288405418396
2018-10-01 14:19:44.461352:	Training iteration: 96200, Loss: 0.14444474875926971
2018-10-01 14:22:11.807206:	Training iteration: 96400, Loss: 0.14084890484809875
2018-10-01 14:24:39.660246:	Training iteration: 96600, Loss: 0.2838534414768219
2018-10-01 14:27:06.793880:	Training iteration: 96800, Loss: 0.20052313804626465
2018-10-01 14:29:34.108679:	Training iteration: 97000, Loss: 0.1890488862991333
2018-10-01 14:32:01.601760:	Training iteration: 97200, Loss: 0.25181740522384644
2018-10-01 14:34:28.691499:	Training iteration: 97400, Loss: 0.16516795754432678
2018-10-01 14:36:55.423903:	Training iteration: 97600, Loss: 0.14442893862724304
2018-10-01 14:39:22.835695:	Training iteration: 97800, Loss: 0.15922212600708008
2018-10-01 14:41:49.590081:	Training iteration: 98000, Loss: 0.18585459887981415
2018-10-01 14:44:16.799665:	Training iteration: 98200, Loss: 0.1698504090309143
2018-10-01 14:46:43.946240:	Training iteration: 98400, Loss: 0.12725858390331268
2018-10-01 14:49:10.916420:	Training iteration: 98600, Loss: 0.15969377756118774
2018-10-01 14:51:38.055456:	Training iteration: 98800, Loss: 0.15294912457466125
2018-10-01 14:54:05.089037:	Training iteration: 99000, Loss: 0.2217663824558258
2018-10-01 14:56:32.540209:	Training iteration: 99200, Loss: 0.21766848862171173
2018-10-01 14:58:59.543632:	Training iteration: 99400, Loss: 0.1558152288198471
2018-10-01 15:01:26.271353:	Training iteration: 99600, Loss: 0.1713602989912033
2018-10-01 15:03:53.741751:	Training iteration: 99800, Loss: 0.19985906779766083
2018-10-01 15:06:20.929656:	Training iteration: 100000, Loss: 0.23052296042442322
2018-10-01 15:08:47.978224:	Training iteration: 100200, Loss: 0.15121999382972717
2018-10-01 15:11:14.702743:	Training iteration: 100400, Loss: 0.17127542197704315
2018-10-01 15:13:41.672107:	Training iteration: 100600, Loss: 0.1734454333782196
2018-10-01 15:16:08.503237:	Training iteration: 100800, Loss: 0.19169758260250092
2018-10-01 15:18:35.529967:	Training iteration: 101000, Loss: 0.15837380290031433
2018-10-01 15:21:02.333325:	Training iteration: 101200, Loss: 0.15872810781002045
2018-10-01 15:23:29.473821:	Training iteration: 101400, Loss: 0.14162437617778778
2018-10-01 15:25:56.568920:	Training iteration: 101600, Loss: 0.13142701983451843
2018-10-01 15:28:23.098952:	Training iteration: 101800, Loss: 0.16929642856121063
2018-10-01 15:30:50.078670:	Training iteration: 102000, Loss: 0.2663712799549103
2018-10-01 15:33:16.982174:	Training iteration: 102200, Loss: 0.11726827919483185
2018-10-01 15:35:43.806927:	Training iteration: 102400, Loss: 0.1508827656507492
2018-10-01 15:38:10.339285:	Training iteration: 102600, Loss: 0.1363464891910553
2018-10-01 15:40:37.459029:	Training iteration: 102800, Loss: 0.15897028148174286
2018-10-01 15:43:04.156386:	Training iteration: 103000, Loss: 0.1506330519914627
2018-10-01 15:45:30.469507:	Training iteration: 103200, Loss: 0.1751505583524704
2018-10-01 15:47:56.864052:	Training iteration: 103400, Loss: 0.16421589255332947
2018-10-01 15:50:23.793276:	Training iteration: 103600, Loss: 0.11866124719381332
2018-10-01 15:52:50.455155:	Training iteration: 103800, Loss: 0.1602112054824829
2018-10-01 15:55:17.022241:	Training iteration: 104000, Loss: 0.1314936727285385
2018-10-01 15:57:43.737178:	Training iteration: 104200, Loss: 0.15381699800491333
2018-10-01 16:00:10.427166:	Training iteration: 104400, Loss: 0.17242293059825897
2018-10-01 16:02:37.252589:	Training iteration: 104600, Loss: 0.1551555097103119
2018-10-01 16:05:04.337816:	Training iteration: 104800, Loss: 0.18089552223682404
2018-10-01 16:07:31.216999:	Training iteration: 105000, Loss: 0.15917149186134338
2018-10-01 16:09:58.129913:	Training iteration: 105200, Loss: 0.18050651252269745
2018-10-01 16:12:25.042820:	Training iteration: 105400, Loss: 0.1687200367450714
2018-10-01 16:14:52.135983:	Training iteration: 105600, Loss: 0.15762516856193542
2018-10-01 16:17:18.944960:	Training iteration: 105800, Loss: 0.15447409451007843
2018-10-01 16:19:45.795645:	Training iteration: 106000, Loss: 0.16306397318840027
2018-10-01 16:22:12.815122:	Training iteration: 106200, Loss: 0.1410554200410843
2018-10-01 16:24:39.591743:	Training iteration: 106400, Loss: 0.1677873432636261
2018-10-01 16:27:06.357275:	Training iteration: 106600, Loss: 0.18229112029075623
2018-10-01 16:29:32.938911:	Training iteration: 106800, Loss: 0.15147842466831207
2018-10-01 16:31:59.417353:	Training iteration: 107000, Loss: 0.1664411872625351
2018-10-01 16:34:26.350038:	Training iteration: 107200, Loss: 0.19254061579704285
2018-10-01 16:36:53.148177:	Training iteration: 107400, Loss: 0.1715821623802185
2018-10-01 16:39:20.057108:	Training iteration: 107600, Loss: 0.15565916895866394
2018-10-01 16:41:46.782865:	Training iteration: 107800, Loss: 0.15932318568229675
2018-10-01 16:44:13.391468:	Training iteration: 108000, Loss: 0.15822641551494598
2018-10-01 16:46:39.734424:	Training iteration: 108200, Loss: 0.16946528851985931
2018-10-01 16:49:06.546385:	Training iteration: 108400, Loss: 0.1798335164785385
2018-10-01 16:51:33.069635:	Training iteration: 108600, Loss: 0.18041077256202698
2018-10-01 16:53:59.581384:	Training iteration: 108800, Loss: 0.16893112659454346
2018-10-01 16:56:26.148952:	Training iteration: 109000, Loss: 0.11659040302038193
2018-10-01 16:58:52.846654:	Training iteration: 109200, Loss: 0.1331891119480133
2018-10-01 17:01:19.808941:	Training iteration: 109400, Loss: 0.19844666123390198
2018-10-01 17:03:46.709463:	Training iteration: 109600, Loss: 0.1858435571193695
2018-10-01 17:06:13.760345:	Training iteration: 109800, Loss: 0.15597906708717346
2018-10-01 17:08:40.554929:	Training iteration: 110000, Loss: 0.1551230400800705
2018-10-01 17:11:07.293145:	Training iteration: 110200, Loss: 0.21282103657722473
2018-10-01 17:13:34.334613:	Training iteration: 110400, Loss: 0.19208474457263947
2018-10-01 17:16:01.024870:	Training iteration: 110600, Loss: 0.1256418377161026
2018-10-01 17:18:27.730015:	Training iteration: 110800, Loss: 0.1950206309556961
2018-10-01 17:20:54.419293:	Training iteration: 111000, Loss: 0.19027327001094818
2018-10-01 17:23:21.217532:	Training iteration: 111200, Loss: 0.18808500468730927
2018-10-01 17:25:48.423021:	Training iteration: 111400, Loss: 0.16249243915081024
2018-10-01 17:28:15.308358:	Training iteration: 111600, Loss: 0.1802079975605011
2018-10-01 17:30:42.243793:	Training iteration: 111800, Loss: 0.13944542407989502
2018-10-01 17:33:08.950589:	Training iteration: 112000, Loss: 0.16299135982990265
2018-10-01 17:35:35.607799:	Training iteration: 112200, Loss: 0.16762197017669678
2018-10-01 17:38:02.441666:	Training iteration: 112400, Loss: 0.15733888745307922
2018-10-01 17:40:29.387514:	Training iteration: 112600, Loss: 0.18274232745170593
2018-10-01 17:42:56.293204:	Training iteration: 112800, Loss: 0.1762007474899292
2018-10-01 17:45:23.076872:	Training iteration: 113000, Loss: 0.16820546984672546
2018-10-01 17:47:49.713584:	Training iteration: 113200, Loss: 0.18199419975280762
2018-10-01 17:50:17.123820:	Training iteration: 113400, Loss: 0.13752558827400208
2018-10-01 17:52:44.052327:	Training iteration: 113600, Loss: 0.14891545474529266
2018-10-01 17:55:10.762136:	Training iteration: 113800, Loss: 0.10281803458929062
2018-10-01 17:57:37.535186:	Training iteration: 114000, Loss: 0.16180209815502167
2018-10-01 18:00:04.354886:	Training iteration: 114200, Loss: 0.12878677248954773
2018-10-01 18:02:31.785437:	Training iteration: 114400, Loss: 0.2253452092409134
2018-10-01 18:04:59.058051:	Training iteration: 114600, Loss: 0.24776048958301544
2018-10-01 18:07:26.567036:	Training iteration: 114800, Loss: 0.11504621803760529
2018-10-01 18:09:53.801698:	Training iteration: 115000, Loss: 0.15413346886634827
2018-10-01 18:12:20.777242:	Training iteration: 115200, Loss: 0.1638956516981125
2018-10-01 18:14:48.367325:	Training iteration: 115400, Loss: 0.15884771943092346
2018-10-01 18:17:15.877189:	Training iteration: 115600, Loss: 0.14989686012268066
2018-10-01 18:19:43.474191:	Training iteration: 115800, Loss: 0.16553260385990143
2018-10-01 18:22:10.395323:	Training iteration: 116000, Loss: 0.16025635600090027
2018-10-01 18:24:37.631518:	Training iteration: 116200, Loss: 0.15169094502925873
2018-10-01 18:27:05.013639:	Training iteration: 116400, Loss: 0.143513485789299
2018-10-01 18:29:32.360386:	Training iteration: 116600, Loss: 0.18042996525764465
2018-10-01 18:31:59.761212:	Training iteration: 116800, Loss: 0.16542920470237732
2018-10-01 18:34:27.085061:	Training iteration: 117000, Loss: 0.14595335721969604
2018-10-01 18:36:54.272914:	Training iteration: 117200, Loss: 0.14914359152317047
2018-10-01 18:39:21.385657:	Training iteration: 117400, Loss: 0.15770740807056427
2018-10-01 18:41:48.522546:	Training iteration: 117600, Loss: 0.16068102419376373
2018-10-01 18:44:15.723607:	Training iteration: 117800, Loss: 0.18441660702228546
2018-10-01 18:46:43.234742:	Training iteration: 118000, Loss: 0.19039079546928406
2018-10-01 18:49:10.804814:	Training iteration: 118200, Loss: 0.17367663979530334
2018-10-01 18:51:38.073205:	Training iteration: 118400, Loss: 0.172529935836792
2018-10-01 18:54:05.240105:	Training iteration: 118600, Loss: 0.16611726582050323
2018-10-01 18:56:31.773709:	Training iteration: 118800, Loss: 0.1278872936964035
2018-10-01 18:58:58.611997:	Training iteration: 119000, Loss: 0.16647732257843018
2018-10-01 19:01:25.531615:	Training iteration: 119200, Loss: 0.16041240096092224
2018-10-01 19:03:52.527379:	Training iteration: 119400, Loss: 0.138676255941391
2018-10-01 19:06:19.551053:	Training iteration: 119600, Loss: 0.12995165586471558
2018-10-01 19:08:46.744825:	Training iteration: 119800, Loss: 0.14161871373653412
2018-10-01 19:11:14.195230:	Training iteration: 120000, Loss: 0.1549350768327713
2018-10-01 19:13:41.531792:	Training iteration: 120200, Loss: 0.1684156209230423
2018-10-01 19:16:09.036321:	Training iteration: 120400, Loss: 0.2930905520915985
2018-10-01 19:18:36.241198:	Training iteration: 120600, Loss: 0.20328731834888458
2018-10-01 19:21:03.156506:	Training iteration: 120800, Loss: 0.15253344178199768
2018-10-01 19:23:30.968663:	Training iteration: 121000, Loss: 0.15105751156806946
2018-10-01 19:25:58.253197:	Training iteration: 121200, Loss: 0.1390608549118042
2018-10-01 19:28:25.687943:	Training iteration: 121400, Loss: 0.160823255777359
2018-10-01 19:30:53.077123:	Training iteration: 121600, Loss: 0.2299397885799408
2018-10-01 19:33:20.328700:	Training iteration: 121800, Loss: 0.3077907860279083
2018-10-01 19:35:47.972168:	Training iteration: 122000, Loss: 0.22440555691719055
2018-10-01 19:38:15.389971:	Training iteration: 122200, Loss: 0.19571653008460999
2018-10-01 19:40:42.865552:	Training iteration: 122400, Loss: 0.18661034107208252
2018-10-01 19:43:10.376548:	Training iteration: 122600, Loss: 0.14627641439437866
2018-10-01 19:45:37.862211:	Training iteration: 122800, Loss: 0.1957421451807022
2018-10-01 19:48:05.167336:	Training iteration: 123000, Loss: 0.1921805441379547
2018-10-01 19:50:32.642589:	Training iteration: 123200, Loss: 0.14214979112148285
2018-10-01 19:53:00.110223:	Training iteration: 123400, Loss: 0.15099921822547913
2018-10-01 19:55:27.486051:	Training iteration: 123600, Loss: 0.18513457477092743
2018-10-01 19:57:54.767482:	Training iteration: 123800, Loss: 0.18405058979988098
2018-10-01 20:00:21.906394:	Training iteration: 124000, Loss: 0.14768250286579132
2018-10-01 20:02:49.201144:	Training iteration: 124200, Loss: 0.20551800727844238
2018-10-01 20:05:16.782430:	Training iteration: 124400, Loss: 0.17936669290065765
2018-10-01 20:07:43.560607:	Training iteration: 124600, Loss: 0.16394606232643127
2018-10-01 20:10:09.976093:	Training iteration: 124800, Loss: 0.15987351536750793
2018-10-01 20:12:37.553587:	Training iteration: 125000, Loss: 0.18004858493804932
2018-10-01 20:15:05.015121:	Training iteration: 125200, Loss: 0.1542496532201767
2018-10-01 20:17:32.591878:	Training iteration: 125400, Loss: 0.14991839230060577
2018-10-01 20:20:00.226029:	Training iteration: 125600, Loss: 0.13989347219467163
2018-10-01 20:22:27.766772:	Training iteration: 125800, Loss: 0.18872568011283875
2018-10-01 20:24:54.971092:	Training iteration: 126000, Loss: 0.17232969403266907
2018-10-01 20:27:22.432572:	Training iteration: 126200, Loss: 0.16753211617469788
2018-10-01 20:29:49.929958:	Training iteration: 126400, Loss: 0.1354624330997467
2018-10-01 20:32:17.046635:	Training iteration: 126600, Loss: 0.1755826771259308
2018-10-01 20:34:44.060219:	Training iteration: 126800, Loss: 0.14578379690647125
2018-10-01 20:37:10.923252:	Training iteration: 127000, Loss: 0.15003347396850586
2018-10-01 20:39:37.295724:	Training iteration: 127200, Loss: 0.15634259581565857
2018-10-01 20:42:04.230731:	Training iteration: 127400, Loss: 0.1556803286075592
2018-10-01 20:44:30.779366:	Training iteration: 127600, Loss: 0.14349448680877686
2018-10-01 20:46:57.173615:	Training iteration: 127800, Loss: 0.1485423594713211
2018-10-01 20:49:23.931837:	Training iteration: 128000, Loss: 0.15282316505908966
2018-10-01 20:51:50.597684:	Training iteration: 128200, Loss: 0.17979590594768524
2018-10-01 20:54:17.371875:	Training iteration: 128400, Loss: 0.1422516256570816
2018-10-01 20:56:44.213477:	Training iteration: 128600, Loss: 0.1829637885093689
2018-10-01 20:59:10.825492:	Training iteration: 128800, Loss: 0.18741419911384583
2018-10-01 21:01:37.262787:	Training iteration: 129000, Loss: 0.17319759726524353
2018-10-01 21:04:04.256264:	Training iteration: 129200, Loss: 0.16519910097122192
2018-10-01 21:06:31.015486:	Training iteration: 129400, Loss: 0.13728268444538116
2018-10-01 21:08:57.897633:	Training iteration: 129600, Loss: 0.16388128697872162
2018-10-01 21:11:24.919915:	Training iteration: 129800, Loss: 0.11561854183673859
2018-10-01 21:13:51.869010:	Training iteration: 130000, Loss: 0.15445400774478912
2018-10-01 21:16:19.028367:	Training iteration: 130200, Loss: 0.16577473282814026
2018-10-01 21:18:45.797102:	Training iteration: 130400, Loss: 0.18649020791053772
2018-10-01 21:21:12.517995:	Training iteration: 130600, Loss: 0.1531257927417755
2018-10-01 21:23:39.579776:	Training iteration: 130800, Loss: 0.14662082493305206
2018-10-01 21:26:06.657437:	Training iteration: 131000, Loss: 0.15815021097660065
2018-10-01 21:28:33.636518:	Training iteration: 131200, Loss: 0.18419483304023743
2018-10-01 21:31:00.396970:	Training iteration: 131400, Loss: 0.17590734362602234
2018-10-01 21:33:27.281044:	Training iteration: 131600, Loss: 0.18924379348754883
2018-10-01 21:35:53.665118:	Training iteration: 131800, Loss: 0.1677124798297882
2018-10-01 21:38:20.441495:	Training iteration: 132000, Loss: 0.15659864246845245
2018-10-01 21:40:47.022137:	Training iteration: 132200, Loss: 0.1665896773338318
2018-10-01 21:43:14.069209:	Training iteration: 132400, Loss: 0.15036237239837646
2018-10-01 21:45:40.869220:	Training iteration: 132600, Loss: 0.11843196302652359
2018-10-01 21:48:08.054741:	Training iteration: 132800, Loss: 0.12042279541492462
2018-10-01 21:50:34.917686:	Training iteration: 133000, Loss: 0.11987918615341187
2018-10-01 21:53:01.883013:	Training iteration: 133200, Loss: 0.15583719313144684
2018-10-01 21:55:29.027697:	Training iteration: 133400, Loss: 0.16911999881267548
2018-10-01 21:57:55.986275:	Training iteration: 133600, Loss: 0.15780194103717804
2018-10-01 22:00:22.945140:	Training iteration: 133800, Loss: 0.156532883644104
2018-10-01 22:02:49.869311:	Training iteration: 134000, Loss: 0.1751275360584259
2018-10-01 22:05:17.230479:	Training iteration: 134200, Loss: 0.16842898726463318
2018-10-01 22:07:44.193060:	Training iteration: 134400, Loss: 0.15320353209972382
2018-10-01 22:10:11.295189:	Training iteration: 134600, Loss: 0.16748769581317902
2018-10-01 22:12:38.362705:	Training iteration: 134800, Loss: 0.2029080092906952
2018-10-01 22:15:05.241781:	Training iteration: 135000, Loss: 0.16551728546619415
2018-10-01 22:17:32.586819:	Training iteration: 135200, Loss: 0.14266043901443481
2018-10-01 22:19:59.219327:	Training iteration: 135400, Loss: 0.18317480385303497
2018-10-01 22:22:26.114385:	Training iteration: 135600, Loss: 0.23746219277381897
2018-10-01 22:24:53.206772:	Training iteration: 135800, Loss: 0.3320315480232239
2018-10-01 22:27:20.394917:	Training iteration: 136000, Loss: 0.2076275795698166
2018-10-01 22:29:47.131385:	Training iteration: 136200, Loss: 0.1497681587934494
2018-10-01 22:32:14.021538:	Training iteration: 136400, Loss: 0.17132428288459778
2018-10-01 22:34:41.237205:	Training iteration: 136600, Loss: 0.12432672083377838
2018-10-01 22:37:08.185165:	Training iteration: 136800, Loss: 0.14029449224472046
2018-10-01 22:39:35.039546:	Training iteration: 137000, Loss: 0.12513944506645203
2018-10-01 22:42:01.366960:	Training iteration: 137200, Loss: 0.16700387001037598
2018-10-01 22:44:28.095726:	Training iteration: 137400, Loss: 0.1356411874294281
2018-10-01 22:46:54.889268:	Training iteration: 137600, Loss: 0.13335208594799042
2018-10-01 22:49:21.609691:	Training iteration: 137800, Loss: 0.15724438428878784
2018-10-01 22:51:48.642517:	Training iteration: 138000, Loss: 0.14287196099758148
2018-10-01 22:54:15.569533:	Training iteration: 138200, Loss: 0.19898100197315216
2018-10-01 22:56:42.360093:	Training iteration: 138400, Loss: 0.13194698095321655
2018-10-01 22:59:09.040934:	Training iteration: 138600, Loss: 0.16153794527053833
2018-10-01 23:01:35.938267:	Training iteration: 138800, Loss: 0.20066635310649872
2018-10-01 23:04:02.933802:	Training iteration: 139000, Loss: 0.16961713135242462
2018-10-01 23:06:29.877964:	Training iteration: 139200, Loss: 0.12366645038127899
2018-10-01 23:08:56.947070:	Training iteration: 139400, Loss: 0.11878825724124908
2018-10-01 23:11:24.017004:	Training iteration: 139600, Loss: 0.16014984250068665
2018-10-01 23:13:50.709676:	Training iteration: 139800, Loss: 0.16187942028045654
2018-10-01 23:16:17.827708:	Training iteration: 140000, Loss: 0.15942741930484772
2018-10-01 23:18:44.160413:	Training iteration: 140200, Loss: 0.17995980381965637
2018-10-01 23:21:11.400997:	Training iteration: 140400, Loss: 0.16326645016670227
2018-10-01 23:23:38.250898:	Training iteration: 140600, Loss: 0.12352593243122101
2018-10-01 23:26:05.449877:	Training iteration: 140800, Loss: 0.1841316968202591
2018-10-01 23:28:32.670163:	Training iteration: 141000, Loss: 0.11974991858005524
2018-10-01 23:30:59.566156:	Training iteration: 141200, Loss: 0.14992952346801758
2018-10-01 23:33:26.840871:	Training iteration: 141400, Loss: 0.15647895634174347
2018-10-01 23:35:53.777626:	Training iteration: 141600, Loss: 0.1486944705247879
2018-10-01 23:38:20.269056:	Training iteration: 141800, Loss: 0.13686059415340424
2018-10-01 23:40:47.329400:	Training iteration: 142000, Loss: 0.16050754487514496
2018-10-01 23:43:14.464018:	Training iteration: 142200, Loss: 0.19532638788223267
2018-10-01 23:45:41.235984:	Training iteration: 142400, Loss: 0.14781056344509125
2018-10-01 23:48:07.847595:	Training iteration: 142600, Loss: 0.1537957489490509
2018-10-01 23:50:34.475686:	Training iteration: 142800, Loss: 0.13884517550468445
2018-10-01 23:53:01.225111:	Training iteration: 143000, Loss: 0.14505340158939362
2018-10-01 23:55:28.466163:	Training iteration: 143200, Loss: 0.16087274253368378
2018-10-01 23:57:55.462749:	Training iteration: 143400, Loss: 0.1498395949602127
2018-10-02 00:00:22.466184:	Training iteration: 143600, Loss: 0.17240987718105316
2018-10-02 00:02:49.099286:	Training iteration: 143800, Loss: 0.17031334340572357
2018-10-02 00:05:15.689084:	Training iteration: 144000, Loss: 0.19531086087226868
2018-10-02 00:07:43.204587:	Training iteration: 144200, Loss: 0.13695690035820007
2018-10-02 00:10:10.016177:	Training iteration: 144400, Loss: 0.1375642716884613
2018-10-02 00:12:37.132095:	Training iteration: 144600, Loss: 0.1387854814529419
2018-10-02 00:15:03.950048:	Training iteration: 144800, Loss: 0.13247615098953247
2018-10-02 00:17:31.040144:	Training iteration: 145000, Loss: 0.17033742368221283
2018-10-02 00:19:57.661686:	Training iteration: 145200, Loss: 0.17228366434574127
2018-10-02 00:22:24.819320:	Training iteration: 145400, Loss: 0.1928211748600006
2018-10-02 00:24:51.943495:	Training iteration: 145600, Loss: 0.13918963074684143
2018-10-02 00:27:19.105316:	Training iteration: 145800, Loss: 0.1306389570236206
2018-10-02 00:29:46.086103:	Training iteration: 146000, Loss: 0.1619444042444229
2018-10-02 00:32:12.657933:	Training iteration: 146200, Loss: 0.21693027019500732
2018-10-02 00:34:39.785778:	Training iteration: 146400, Loss: 0.1747821867465973
2018-10-02 00:37:06.426973:	Training iteration: 146600, Loss: 0.21141675114631653
2018-10-02 00:39:32.993337:	Training iteration: 146800, Loss: 0.1424044817686081
2018-10-02 00:42:00.092262:	Training iteration: 147000, Loss: 0.13664968311786652
2018-10-02 00:44:26.895357:	Training iteration: 147200, Loss: 0.13854435086250305
2018-10-02 00:46:53.820020:	Training iteration: 147400, Loss: 0.13763843476772308
2018-10-02 00:49:20.451593:	Training iteration: 147600, Loss: 0.1437956988811493
2018-10-02 00:51:46.925438:	Training iteration: 147800, Loss: 0.16362181305885315
2018-10-02 00:54:13.603346:	Training iteration: 148000, Loss: 0.1691495180130005
2018-10-02 00:56:40.353880:	Training iteration: 148200, Loss: 0.1634618639945984
2018-10-02 00:59:07.425833:	Training iteration: 148400, Loss: 0.1640796661376953
2018-10-02 01:01:34.306545:	Training iteration: 148600, Loss: 0.16416816413402557
2018-10-02 01:04:01.174677:	Training iteration: 148800, Loss: 0.14372867345809937
2018-10-02 01:06:28.300408:	Training iteration: 149000, Loss: 0.15864405035972595
2018-10-02 01:08:55.465597:	Training iteration: 149200, Loss: 0.17907284200191498
2018-10-02 01:11:22.623214:	Training iteration: 149400, Loss: 0.15575365722179413
2018-10-02 01:13:49.107675:	Training iteration: 149600, Loss: 0.17091670632362366
2018-10-02 01:16:16.271241:	Training iteration: 149800, Loss: 0.18145795166492462
2018-10-02 01:18:43.369027:	Training iteration: 150000, Loss: 0.18911240994930267
2018-10-02 01:21:10.158283:	Training iteration: 150200, Loss: 0.1528116911649704
2018-10-02 01:23:36.889724:	Training iteration: 150400, Loss: 0.1681535691022873
2018-10-02 01:26:04.208273:	Training iteration: 150600, Loss: 0.16609416902065277
2018-10-02 01:28:31.086541:	Training iteration: 150800, Loss: 0.15476925671100616
2018-10-02 01:30:58.061314:	Training iteration: 151000, Loss: 0.15788279473781586
2018-10-02 01:33:25.023532:	Training iteration: 151200, Loss: 0.17084132134914398
2018-10-02 01:35:51.839197:	Training iteration: 151400, Loss: 0.20512433350086212
2018-10-02 01:38:18.820958:	Training iteration: 151600, Loss: 0.20703332126140594
2018-10-02 01:40:45.631307:	Training iteration: 151800, Loss: 0.12060858309268951
2018-10-02 01:43:12.762826:	Training iteration: 152000, Loss: 0.14393286406993866
2018-10-02 01:45:39.593341:	Training iteration: 152200, Loss: 0.17955425381660461
2018-10-02 01:48:06.605734:	Training iteration: 152400, Loss: 0.15531787276268005
2018-10-02 01:50:33.521090:	Training iteration: 152600, Loss: 0.14637179672718048
2018-10-02 01:53:00.903680:	Training iteration: 152800, Loss: 0.11556915938854218
2018-10-02 01:55:27.709182:	Training iteration: 153000, Loss: 0.16577021777629852
2018-10-02 01:57:54.473716:	Training iteration: 153200, Loss: 0.18215642869472504
2018-10-02 02:00:21.201335:	Training iteration: 153400, Loss: 0.16669584810733795
2018-10-02 02:02:48.275432:	Training iteration: 153600, Loss: 0.29275280237197876
2018-10-02 02:05:15.525536:	Training iteration: 153800, Loss: 0.15192268788814545
2018-10-02 02:07:42.556075:	Training iteration: 154000, Loss: 0.2510117292404175
2018-10-02 02:10:09.777357:	Training iteration: 154200, Loss: 0.19441798329353333
2018-10-02 02:12:36.831418:	Training iteration: 154400, Loss: 0.18109071254730225
2018-10-02 02:15:03.716478:	Training iteration: 154600, Loss: 0.14014655351638794
2018-10-02 02:17:31.007874:	Training iteration: 154800, Loss: 0.14590224623680115
2018-10-02 02:19:57.667672:	Training iteration: 155000, Loss: 0.21200986206531525
2018-10-02 02:22:24.527101:	Training iteration: 155200, Loss: 0.17890150845050812
2018-10-02 02:24:51.648046:	Training iteration: 155400, Loss: 0.17730514705181122
2018-10-02 02:27:18.865503:	Training iteration: 155600, Loss: 0.15370884537696838
2018-10-02 02:29:45.933012:	Training iteration: 155800, Loss: 0.19634543359279633
2018-10-02 02:32:12.951046:	Training iteration: 156000, Loss: 0.13523569703102112
2018-10-02 02:34:39.894079:	Training iteration: 156200, Loss: 0.15709173679351807
2018-10-02 02:37:06.919586:	Training iteration: 156400, Loss: 0.14176924526691437
2018-10-02 02:39:33.808105:	Training iteration: 156600, Loss: 0.12676137685775757
2018-10-02 02:42:01.130110:	Training iteration: 156800, Loss: 0.171953022480011
2018-10-02 02:44:27.937301:	Training iteration: 157000, Loss: 0.15331481397151947
2018-10-02 02:46:55.005539:	Training iteration: 157200, Loss: 0.1683504581451416
2018-10-02 02:49:21.901722:	Training iteration: 157400, Loss: 0.16464349627494812
2018-10-02 02:51:49.043783:	Training iteration: 157600, Loss: 0.20425932109355927
2018-10-02 02:54:16.066229:	Training iteration: 157800, Loss: 0.1403738558292389
2018-10-02 02:56:42.984597:	Training iteration: 158000, Loss: 0.13896819949150085
2018-10-02 02:59:09.924015:	Training iteration: 158200, Loss: 0.13998043537139893
2018-10-02 03:01:36.546100:	Training iteration: 158400, Loss: 0.17990809679031372
2018-10-02 03:04:03.147703:	Training iteration: 158600, Loss: 0.12820479273796082
2018-10-02 03:06:29.555840:	Training iteration: 158800, Loss: 0.17082668840885162
2018-10-02 03:08:56.408296:	Training iteration: 159000, Loss: 0.16172823309898376
2018-10-02 03:11:23.269393:	Training iteration: 159200, Loss: 0.17341163754463196
2018-10-02 03:13:50.537085:	Training iteration: 159400, Loss: 0.2330254763364792
2018-10-02 03:16:17.053058:	Training iteration: 159600, Loss: 0.18149475753307343
2018-10-02 03:18:44.324153:	Training iteration: 159800, Loss: 0.14494779706001282
2018-10-02 03:21:11.487527:	Training iteration: 160000, Loss: 0.20948603749275208
2018-10-02 03:23:38.383258:	Training iteration: 160200, Loss: 0.18894992768764496
2018-10-02 03:26:05.386342:	Training iteration: 160400, Loss: 0.16480140388011932
2018-10-02 03:28:32.562488:	Training iteration: 160600, Loss: 0.14037208259105682
2018-10-02 03:30:59.595890:	Training iteration: 160800, Loss: 0.1812300980091095
2018-10-02 03:33:27.061812:	Training iteration: 161000, Loss: 0.15884631872177124
2018-10-02 03:35:54.348772:	Training iteration: 161200, Loss: 0.19331221282482147
2018-10-02 03:38:21.291216:	Training iteration: 161400, Loss: 0.12440486997365952
2018-10-02 03:40:48.189788:	Training iteration: 161600, Loss: 0.13359639048576355
2018-10-02 03:43:15.059480:	Training iteration: 161800, Loss: 0.09801188856363297
2018-10-02 03:45:41.907353:	Training iteration: 162000, Loss: 0.1394188553094864
2018-10-02 03:48:08.804504:	Training iteration: 162200, Loss: 0.16656285524368286
2018-10-02 03:50:35.851044:	Training iteration: 162400, Loss: 0.14405208826065063
2018-10-02 03:53:02.530387:	Training iteration: 162600, Loss: 0.12507584691047668
2018-10-02 03:55:29.944098:	Training iteration: 162800, Loss: 0.1693587750196457
2018-10-02 03:57:56.954477:	Training iteration: 163000, Loss: 0.1683974713087082
2018-10-02 04:00:23.917468:	Training iteration: 163200, Loss: 0.16933651268482208
2018-10-02 04:02:50.785463:	Training iteration: 163400, Loss: 0.16178712248802185
2018-10-02 04:05:17.698087:	Training iteration: 163600, Loss: 0.10412979125976562
2018-10-02 04:07:44.411419:	Training iteration: 163800, Loss: 0.19077639281749725
2018-10-02 04:10:10.991922:	Training iteration: 164000, Loss: 0.18544889986515045
2018-10-02 04:12:37.831248:	Training iteration: 164200, Loss: 0.14959071576595306
2018-10-02 04:15:05.060063:	Training iteration: 164400, Loss: 0.12776078283786774
2018-10-02 04:17:32.071157:	Training iteration: 164600, Loss: 0.14838796854019165
2018-10-02 04:19:59.453012:	Training iteration: 164800, Loss: 0.15737879276275635
2018-10-02 04:22:26.750077:	Training iteration: 165000, Loss: 0.1887882649898529
2018-10-02 04:24:54.015984:	Training iteration: 165200, Loss: 0.20032140612602234
2018-10-02 04:27:21.337721:	Training iteration: 165400, Loss: 0.14948241412639618
2018-10-02 04:29:48.772521:	Training iteration: 165600, Loss: 0.15889613330364227
2018-10-02 04:32:15.732656:	Training iteration: 165800, Loss: 0.14560498297214508
2018-10-02 04:34:43.218208:	Training iteration: 166000, Loss: 0.12237343937158585
2018-10-02 04:37:09.970028:	Training iteration: 166200, Loss: 0.1407511979341507
2018-10-02 04:39:37.038878:	Training iteration: 166400, Loss: 0.14043280482292175
2018-10-02 04:42:04.042368:	Training iteration: 166600, Loss: 0.1619025021791458
2018-10-02 04:44:30.833945:	Training iteration: 166800, Loss: 0.15607546269893646
2018-10-02 04:46:57.347610:	Training iteration: 167000, Loss: 0.2076149433851242
2018-10-02 04:49:24.477764:	Training iteration: 167200, Loss: 0.14116065204143524
2018-10-02 04:51:51.650398:	Training iteration: 167400, Loss: 0.156792551279068
2018-10-02 04:54:18.785935:	Training iteration: 167600, Loss: 0.17740049958229065
2018-10-02 04:56:45.917890:	Training iteration: 167800, Loss: 0.17829257249832153
2018-10-02 04:59:13.081974:	Training iteration: 168000, Loss: 0.1742284893989563
2018-10-02 05:01:40.071925:	Training iteration: 168200, Loss: 0.1081506609916687
2018-10-02 05:04:07.070111:	Training iteration: 168400, Loss: 0.16743823885917664
2018-10-02 05:06:34.173557:	Training iteration: 168600, Loss: 0.16098779439926147
2018-10-02 05:09:01.062381:	Training iteration: 168800, Loss: 0.14800655841827393
2018-10-02 05:11:28.059058:	Training iteration: 169000, Loss: 0.19890575110912323
2018-10-02 05:13:54.849944:	Training iteration: 169200, Loss: 0.13916315138339996
2018-10-02 05:16:21.363407:	Training iteration: 169400, Loss: 0.1434694528579712
2018-10-02 05:18:48.577911:	Training iteration: 169600, Loss: 0.18752749264240265
2018-10-02 05:21:15.461810:	Training iteration: 169800, Loss: 0.1266530454158783
2018-10-02 05:23:42.694387:	Training iteration: 170000, Loss: 0.22690317034721375
2018-10-02 05:26:09.892904:	Training iteration: 170200, Loss: 0.1589183509349823
2018-10-02 05:28:37.206260:	Training iteration: 170400, Loss: 0.1839238405227661
2018-10-02 05:31:03.959893:	Training iteration: 170600, Loss: 0.13969814777374268
2018-10-02 05:33:31.189287:	Training iteration: 170800, Loss: 0.1490834802389145
2018-10-02 05:35:58.161441:	Training iteration: 171000, Loss: 0.1608392894268036
2018-10-02 05:38:25.443004:	Training iteration: 171200, Loss: 0.18207979202270508
2018-10-02 05:40:52.690191:	Training iteration: 171400, Loss: 0.20386862754821777
2018-10-02 05:43:19.884451:	Training iteration: 171600, Loss: 0.14647741615772247
2018-10-02 05:45:47.130404:	Training iteration: 171800, Loss: 0.13133572041988373
2018-10-02 05:48:14.362025:	Training iteration: 172000, Loss: 0.14251455664634705
2018-10-02 05:50:40.952786:	Training iteration: 172200, Loss: 0.13309764862060547
2018-10-02 05:53:07.807403:	Training iteration: 172400, Loss: 0.1814771443605423
2018-10-02 05:55:35.061658:	Training iteration: 172600, Loss: 0.13306865096092224
2018-10-02 05:58:01.971191:	Training iteration: 172800, Loss: 0.16557052731513977
2018-10-02 06:00:29.167658:	Training iteration: 173000, Loss: 0.19040551781654358
2018-10-02 06:02:55.937917:	Training iteration: 173200, Loss: 0.20443519949913025
2018-10-02 06:05:22.977161:	Training iteration: 173400, Loss: 0.16905906796455383
2018-10-02 06:07:49.915479:	Training iteration: 173600, Loss: 0.15925800800323486
2018-10-02 06:10:16.967777:	Training iteration: 173800, Loss: 0.15451525151729584
2018-10-02 06:12:43.971131:	Training iteration: 174000, Loss: 0.1507548689842224
2018-10-02 06:15:11.005041:	Training iteration: 174200, Loss: 0.16454097628593445
2018-10-02 06:17:37.753708:	Training iteration: 174400, Loss: 0.15406759083271027
2018-10-02 06:20:04.401472:	Training iteration: 174600, Loss: 0.15411284565925598
2018-10-02 06:22:31.114318:	Training iteration: 174800, Loss: 0.14369705319404602
2018-10-02 06:24:57.856630:	Training iteration: 175000, Loss: 0.15036916732788086
2018-10-02 06:27:25.425730:	Training iteration: 175200, Loss: 0.15547284483909607
2018-10-02 06:29:52.388913:	Training iteration: 175400, Loss: 0.19412370026111603
2018-10-02 06:32:19.814627:	Training iteration: 175600, Loss: 0.18586452305316925
2018-10-02 06:34:46.780397:	Training iteration: 175800, Loss: 0.19174042344093323
2018-10-02 06:37:14.192326:	Training iteration: 176000, Loss: 0.17359116673469543
2018-10-02 06:39:41.300001:	Training iteration: 176200, Loss: 0.14818885922431946
2018-10-02 06:42:08.485379:	Training iteration: 176400, Loss: 0.14538738131523132
2018-10-02 06:44:35.939251:	Training iteration: 176600, Loss: 0.138845294713974
2018-10-02 06:47:03.144909:	Training iteration: 176800, Loss: 0.1363934874534607
2018-10-02 06:49:30.067352:	Training iteration: 177000, Loss: 0.19492235779762268
2018-10-02 06:51:57.102595:	Training iteration: 177200, Loss: 0.1553644835948944
2018-10-02 06:54:24.449596:	Training iteration: 177400, Loss: 0.17836573719978333
2018-10-02 06:56:51.304159:	Training iteration: 177600, Loss: 0.14770855009555817
2018-10-02 06:59:18.871207:	Training iteration: 177800, Loss: 0.15084615349769592
2018-10-02 07:01:46.121675:	Training iteration: 178000, Loss: 0.12512990832328796
2018-10-02 07:04:13.474867:	Training iteration: 178200, Loss: 0.14969588816165924
2018-10-02 07:06:40.916826:	Training iteration: 178400, Loss: 0.18297788500785828
2018-10-02 07:09:07.664804:	Training iteration: 178600, Loss: 0.1704196035861969
2018-10-02 07:11:34.612190:	Training iteration: 178800, Loss: 0.1365370750427246
2018-10-02 07:14:01.701783:	Training iteration: 179000, Loss: 0.12657317519187927
2018-10-02 07:16:28.884965:	Training iteration: 179200, Loss: 0.1522933542728424
2018-10-02 07:18:55.902914:	Training iteration: 179400, Loss: 0.15823781490325928
2018-10-02 07:21:22.659463:	Training iteration: 179600, Loss: 0.18135635554790497
2018-10-02 07:23:49.216608:	Training iteration: 179800, Loss: 0.13899990916252136
2018-10-02 07:26:15.705976:	Training iteration: 180000, Loss: 0.14418579638004303
2018-10-02 07:28:42.556823:	Training iteration: 180200, Loss: 0.12966641783714294
2018-10-02 07:31:09.608533:	Training iteration: 180400, Loss: 0.12895408272743225
2018-10-02 07:33:36.440205:	Training iteration: 180600, Loss: 0.13763301074504852
2018-10-02 07:36:01.889833:	Training iteration: 180800, Loss: 0.2707552909851074
2018-10-02 07:38:27.598702:	Training iteration: 181000, Loss: 0.15433400869369507
