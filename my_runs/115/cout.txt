INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "115"
Experiment ID: 115
Preparing dataset
Dataset ready
2018-10-11 20:37:57.228970: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-10-11 20:37:57.435075: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-11 20:37:57.438016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:23:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-10-11 20:37:57.438073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:23:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Loading checkpoint
INFO:tensorflow:Restoring parameters from /home/enterprise.internal.city.ac.uk/acvn728/checkpoints/99/99-21000
INFO - tensorflow - Restoring parameters from /home/enterprise.internal.city.ac.uk/acvn728/checkpoints/99/99-21000
Starting training
2018-10-11 20:38:15.815910: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 959 of 1000
2018-10-11 20:38:16.193551: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-11 20:38:18.318848:	Training iteration: 1, Loss: 0.005692186765372753
Checkpoint
2018-10-11 20:38:19.519867:	Training iteration: 2, Loss: 0.00520977983251214
Checkpoint
2018-10-11 20:38:20.657386:	Training iteration: 3, Loss: 0.006298189051449299
Checkpoint
2018-10-11 20:38:21.794633:	Training iteration: 4, Loss: 0.005590748507529497
Checkpoint
2018-10-11 20:38:22.937137:	Training iteration: 5, Loss: 0.005579111631959677
Checkpoint
2018-10-11 20:38:24.089543:	Training iteration: 6, Loss: 0.0043845875188708305
Checkpoint
2018-10-11 20:38:25.257731:	Training iteration: 7, Loss: 0.005951399449259043
Checkpoint
2018-10-11 20:38:26.414182:	Training iteration: 8, Loss: 0.006820283830165863
Checkpoint
2018-10-11 20:38:27.574507:	Training iteration: 9, Loss: 0.0054129669442772865
Checkpoint
2018-10-11 20:38:28.737164:	Training iteration: 10, Loss: 0.006465055979788303
Checkpoint
2018-10-11 20:38:29.900525:	Training iteration: 11, Loss: 0.005184714216738939
Checkpoint
2018-10-11 20:38:31.057862:	Training iteration: 12, Loss: 0.004874586593359709
Checkpoint
2018-10-11 20:38:32.235372:	Training iteration: 13, Loss: 0.005382897797971964
Checkpoint
2018-10-11 20:38:33.406827:	Training iteration: 14, Loss: 0.005603095516562462
Checkpoint
2018-10-11 20:38:34.581932:	Training iteration: 15, Loss: 0.00685503426939249
Checkpoint
2018-10-11 20:38:35.746338:	Training iteration: 16, Loss: 0.005647567100822926
Checkpoint
2018-10-11 20:38:36.915534:	Training iteration: 17, Loss: 0.005087393335998058
Checkpoint
2018-10-11 20:38:38.088051:	Training iteration: 18, Loss: 0.005084567703306675
Checkpoint
2018-10-11 20:38:39.269763:	Training iteration: 19, Loss: 0.004600830376148224
Checkpoint
2018-10-11 20:38:40.436366:	Training iteration: 20, Loss: 0.0041345395147800446
Checkpoint
2018-10-11 20:38:41.596570:	Training iteration: 21, Loss: 0.005160479340702295
Checkpoint
2018-10-11 20:38:42.765216:	Training iteration: 22, Loss: 0.004547794349491596
Checkpoint
2018-10-11 20:38:43.919713:	Training iteration: 23, Loss: 0.005569989327341318
Checkpoint
2018-10-11 20:38:45.079265:	Training iteration: 24, Loss: 0.004704318009316921
Checkpoint
2018-10-11 20:38:46.248721:	Training iteration: 25, Loss: 0.005591381341218948
Checkpoint
2018-10-11 20:38:47.430108:	Training iteration: 26, Loss: 0.004933849908411503
Checkpoint
2018-10-11 20:38:48.597789:	Training iteration: 27, Loss: 0.005295359995216131
Checkpoint
2018-10-11 20:38:49.849673:	Training iteration: 28, Loss: 0.008618960157036781
Checkpoint
2018-10-11 20:38:51.002695:	Training iteration: 29, Loss: 0.004805179312825203
Checkpoint
2018-10-11 20:38:52.257540:	Training iteration: 30, Loss: 0.004530172795057297
Checkpoint
2018-10-11 20:38:53.410447:	Training iteration: 31, Loss: 0.006364709232002497
Checkpoint
2018-10-11 20:38:54.569656:	Training iteration: 32, Loss: 0.005921015981584787
Checkpoint
2018-10-11 20:38:55.734792:	Training iteration: 33, Loss: 0.005102019291371107
Checkpoint
2018-10-11 20:38:56.915948:	Training iteration: 34, Loss: 0.0039006825536489487
Checkpoint
2018-10-11 20:38:58.086162:	Training iteration: 35, Loss: 0.004664498846977949
Checkpoint
2018-10-11 20:38:59.271797:	Training iteration: 36, Loss: 0.004698433913290501
Checkpoint
2018-10-11 20:39:00.434887:	Training iteration: 37, Loss: 0.005909719970077276
Checkpoint
2018-10-11 20:39:01.641143:	Training iteration: 38, Loss: 0.004569660406559706
Checkpoint
2018-10-11 20:39:02.800512:	Training iteration: 39, Loss: 0.006749878171831369
Checkpoint
2018-10-11 20:39:03.961021:	Training iteration: 40, Loss: 0.005239029414951801
Checkpoint
2018-10-11 20:39:05.139322:	Training iteration: 41, Loss: 0.0061756097711622715
Checkpoint
2018-10-11 20:39:06.304368:	Training iteration: 42, Loss: 0.004496391396969557
Checkpoint
2018-10-11 20:39:07.467082:	Training iteration: 43, Loss: 0.005055519752204418
Checkpoint
2018-10-11 20:39:08.629409:	Training iteration: 44, Loss: 0.0060334657318890095
Checkpoint
2018-10-11 20:39:09.836461:	Training iteration: 45, Loss: 0.004365351051092148
Checkpoint
2018-10-11 20:39:11.000997:	Training iteration: 46, Loss: 0.005961724556982517
Checkpoint
2018-10-11 20:39:12.162954:	Training iteration: 47, Loss: 0.004399796482175589
Checkpoint
2018-10-11 20:39:13.337544:	Training iteration: 48, Loss: 0.004844902083277702
Checkpoint
2018-10-11 20:39:14.498985:	Training iteration: 49, Loss: 0.0050193872302770615
Checkpoint
2018-10-11 20:39:15.660778:	Training iteration: 50, Loss: 0.005791599862277508
Checkpoint
2018-10-11 20:39:16.831160:	Training iteration: 51, Loss: 0.005873532500118017
Checkpoint
2018-10-11 20:39:17.998748:	Training iteration: 52, Loss: 0.004590409342199564
Checkpoint
2018-10-11 20:39:19.160304:	Training iteration: 53, Loss: 0.005969717167317867
Checkpoint
2018-10-11 20:39:20.334908:	Training iteration: 54, Loss: 0.0032716647256165743
Checkpoint
2018-10-11 20:39:21.504809:	Training iteration: 55, Loss: 0.00495630968362093
Checkpoint
2018-10-11 20:39:22.670128:	Training iteration: 56, Loss: 0.004459588322788477
Checkpoint
2018-10-11 20:39:23.846902:	Training iteration: 57, Loss: 0.006993631366640329
Checkpoint
2018-10-11 20:39:25.071265:	Training iteration: 58, Loss: 0.004601793829351664
Checkpoint
2018-10-11 20:39:26.276716:	Training iteration: 59, Loss: 0.005065717734396458
Checkpoint
2018-10-11 20:39:27.719070:	Training iteration: 60, Loss: 0.006158108357340097
Checkpoint
2018-10-11 20:39:28.998602:	Training iteration: 61, Loss: 0.005784783512353897
Checkpoint
2018-10-11 20:39:30.331231:	Training iteration: 62, Loss: 0.005109464284032583
Checkpoint
2018-10-11 20:39:31.604771:	Training iteration: 63, Loss: 0.00500413728877902
Checkpoint
2018-10-11 20:39:32.958604:	Training iteration: 64, Loss: 0.006080080755054951
Checkpoint
2018-10-11 20:39:34.240362:	Training iteration: 65, Loss: 0.0061918157152831554
Checkpoint
2018-10-11 20:39:35.398947:	Training iteration: 66, Loss: 0.006581547670066357
Checkpoint
2018-10-11 20:39:36.525076:	Training iteration: 67, Loss: 0.004943777807056904
Checkpoint
2018-10-11 20:39:37.678850:	Training iteration: 68, Loss: 0.005761835724115372
Checkpoint
2018-10-11 20:39:39.020966:	Training iteration: 69, Loss: 0.005451168864965439
Checkpoint
2018-10-11 20:39:40.169744:	Training iteration: 70, Loss: 0.004895458929240704
Checkpoint
2018-10-11 20:39:41.341399:	Training iteration: 71, Loss: 0.004659181926399469
Checkpoint
2018-10-11 20:39:42.465001:	Training iteration: 72, Loss: 0.00571516202762723
Checkpoint
2018-10-11 20:39:43.813502:	Training iteration: 73, Loss: 0.004613699857145548
Checkpoint
2018-10-11 20:39:45.018505:	Training iteration: 74, Loss: 0.0048154545947909355
Checkpoint
2018-10-11 20:39:46.161514:	Training iteration: 75, Loss: 0.0056584845297038555
Checkpoint
2018-10-11 20:39:47.278022:	Training iteration: 76, Loss: 0.006075066514313221
Checkpoint
2018-10-11 20:39:48.417672:	Training iteration: 77, Loss: 0.004317303188145161
Checkpoint
2018-10-11 20:39:49.624066:	Training iteration: 78, Loss: 0.006565403193235397
Checkpoint
2018-10-11 20:39:50.851336:	Training iteration: 79, Loss: 0.004327578004449606
Checkpoint
2018-10-11 20:39:51.993424:	Training iteration: 80, Loss: 0.0065072327852249146
Checkpoint
2018-10-11 20:39:53.183842:	Training iteration: 81, Loss: 0.006253771483898163
Checkpoint
2018-10-11 20:39:54.375535:	Training iteration: 82, Loss: 0.004411404021084309
Checkpoint
2018-10-11 20:39:55.577839:	Training iteration: 83, Loss: 0.006275457795709372
Checkpoint
2018-10-11 20:39:56.756816:	Training iteration: 84, Loss: 0.004152596928179264
Checkpoint
2018-10-11 20:39:58.000146:	Training iteration: 85, Loss: 0.00572901451960206
Checkpoint
2018-10-11 20:39:59.139481:	Training iteration: 86, Loss: 0.0043959119357168674
Checkpoint
2018-10-11 20:40:00.332017:	Training iteration: 87, Loss: 0.006415071897208691
Checkpoint
2018-10-11 20:40:01.462231:	Training iteration: 88, Loss: 0.004434261471033096
Checkpoint
2018-10-11 20:40:02.603310:	Training iteration: 89, Loss: 0.004766751546412706
Checkpoint
2018-10-11 20:40:03.738237:	Training iteration: 90, Loss: 0.004375380463898182
Checkpoint
2018-10-11 20:40:04.898937:	Training iteration: 91, Loss: 0.006366252899169922
Checkpoint
2018-10-11 20:40:06.057511:	Training iteration: 92, Loss: 0.005259386263787746
Checkpoint
2018-10-11 20:40:07.229568:	Training iteration: 93, Loss: 0.005759934429079294
Checkpoint
2018-10-11 20:40:08.418104:	Training iteration: 94, Loss: 0.004526364151388407
Checkpoint
2018-10-11 20:40:09.562376:	Training iteration: 95, Loss: 0.0045723626390099525
Checkpoint
2018-10-11 20:40:10.753209:	Training iteration: 96, Loss: 0.007185150869190693
Checkpoint
2018-10-11 20:40:12.047307:	Training iteration: 97, Loss: 0.005985427647829056
Checkpoint
2018-10-11 20:40:13.207466:	Training iteration: 98, Loss: 0.004469724837690592
Checkpoint
2018-10-11 20:40:14.472517:	Training iteration: 99, Loss: 0.006545682437717915
Checkpoint
2018-10-11 20:40:15.663711:	Training iteration: 100, Loss: 0.00563944224268198
Checkpoint
2018-10-11 20:40:16.917871:	Training iteration: 101, Loss: 0.005374007858335972
Checkpoint
2018-10-11 20:40:18.080715:	Training iteration: 102, Loss: 0.005448953248560429
Checkpoint
2018-10-11 20:40:19.242249:	Training iteration: 103, Loss: 0.00575559027493
Checkpoint
2018-10-11 20:40:20.423790:	Training iteration: 104, Loss: 0.006486162543296814
Checkpoint
2018-10-11 20:40:21.586672:	Training iteration: 105, Loss: 0.00534582044929266
Checkpoint
2018-10-11 20:40:22.743680:	Training iteration: 106, Loss: 0.005399403162300587
Checkpoint
2018-10-11 20:40:23.898416:	Training iteration: 107, Loss: 0.006026295479387045
Checkpoint
2018-10-11 20:40:25.049717:	Training iteration: 108, Loss: 0.0051099639385938644
Checkpoint
2018-10-11 20:40:26.185874:	Training iteration: 109, Loss: 0.004619262181222439
Checkpoint
2018-10-11 20:40:27.331672:	Training iteration: 110, Loss: 0.0052750795148313046
Checkpoint
2018-10-11 20:40:28.488832:	Training iteration: 111, Loss: 0.006521792616695166
Checkpoint
2018-10-11 20:40:29.649569:	Training iteration: 112, Loss: 0.005712700542062521
Checkpoint
2018-10-11 20:40:30.809549:	Training iteration: 113, Loss: 0.006800966802984476
Checkpoint
2018-10-11 20:40:32.014918:	Training iteration: 114, Loss: 0.0043670558370649815
Checkpoint
2018-10-11 20:40:33.172708:	Training iteration: 115, Loss: 0.004828824196010828
Checkpoint
2018-10-11 20:40:34.329663:	Training iteration: 116, Loss: 0.00420417683199048
Checkpoint
2018-10-11 20:40:35.489113:	Training iteration: 117, Loss: 0.0054426840506494045
Checkpoint
2018-10-11 20:40:36.670048:	Training iteration: 118, Loss: 0.0056814574636518955
Checkpoint
2018-10-11 20:40:37.840614:	Training iteration: 119, Loss: 0.0056850360706448555
Checkpoint
2018-10-11 20:40:39.002552:	Training iteration: 120, Loss: 0.004136022180318832
Checkpoint
2018-10-11 20:40:40.170478:	Training iteration: 121, Loss: 0.004735435359179974
Checkpoint
2018-10-11 20:40:41.332302:	Training iteration: 122, Loss: 0.004425822291523218
Checkpoint
2018-10-11 20:40:42.493386:	Training iteration: 123, Loss: 0.004858833737671375
Checkpoint
2018-10-11 20:40:43.659178:	Training iteration: 124, Loss: 0.00473433593288064
Checkpoint
2018-10-11 20:40:44.820807:	Training iteration: 125, Loss: 0.0038161501288414
Checkpoint
2018-10-11 20:40:46.001053:	Training iteration: 126, Loss: 0.004743674304336309
Checkpoint
2018-10-11 20:40:47.171505:	Training iteration: 127, Loss: 0.0056726885959506035
Checkpoint
2018-10-11 20:40:48.350914:	Training iteration: 128, Loss: 0.005997757893055677
Checkpoint
2018-10-11 20:40:49.596839:	Training iteration: 129, Loss: 0.005890392232686281
Checkpoint
2018-10-11 20:40:50.770744:	Training iteration: 130, Loss: 0.004934304393827915
Checkpoint
2018-10-11 20:40:51.923005:	Training iteration: 131, Loss: 0.005716536194086075
Checkpoint
2018-10-11 20:40:53.226972:	Training iteration: 132, Loss: 0.007996516302227974
Checkpoint
2018-10-11 20:40:54.487935:	Training iteration: 133, Loss: 0.005071872379630804
Checkpoint
2018-10-11 20:40:55.642354:	Training iteration: 134, Loss: 0.005282450467348099
Checkpoint
2018-10-11 20:40:56.804346:	Training iteration: 135, Loss: 0.004876590333878994
Checkpoint
2018-10-11 20:40:57.976115:	Training iteration: 136, Loss: 0.005117518827319145
Checkpoint
2018-10-11 20:40:59.202250:	Training iteration: 137, Loss: 0.005636290647089481
Checkpoint
2018-10-11 20:41:00.367999:	Training iteration: 138, Loss: 0.00556660583242774
Checkpoint
2018-10-11 20:41:01.528723:	Training iteration: 139, Loss: 0.006648865528404713
Checkpoint
2018-10-11 20:41:02.684633:	Training iteration: 140, Loss: 0.005304217338562012
Checkpoint
2018-10-11 20:41:03.863382:	Training iteration: 141, Loss: 0.004491553641855717
Checkpoint
2018-10-11 20:41:05.027245:	Training iteration: 142, Loss: 0.005696299485862255
Checkpoint
2018-10-11 20:41:06.189248:	Training iteration: 143, Loss: 0.005365590564906597
Checkpoint
2018-10-11 20:41:07.367005:	Training iteration: 144, Loss: 0.004364159889519215
Checkpoint
2018-10-11 20:41:08.555463:	Training iteration: 145, Loss: 0.006574871484190226
Checkpoint
2018-10-11 20:41:09.721175:	Training iteration: 146, Loss: 0.004317058250308037
Checkpoint
2018-10-11 20:41:10.881003:	Training iteration: 147, Loss: 0.004889168776571751
Checkpoint
2018-10-11 20:41:12.043900:	Training iteration: 148, Loss: 0.006237359717488289
Checkpoint
2018-10-11 20:41:13.224431:	Training iteration: 149, Loss: 0.007234042976051569
Checkpoint
2018-10-11 20:41:14.414917:	Training iteration: 150, Loss: 0.005547641310840845
Checkpoint
2018-10-11 20:41:15.601023:	Training iteration: 151, Loss: 0.005299461539834738
Checkpoint
2018-10-11 20:41:16.771379:	Training iteration: 152, Loss: 0.005335856229066849
Checkpoint
2018-10-11 20:41:18.032112:	Training iteration: 153, Loss: 0.004678444005548954
Checkpoint
2018-10-11 20:41:19.203998:	Training iteration: 154, Loss: 0.0055455700494349
Checkpoint
2018-10-11 20:41:20.366167:	Training iteration: 155, Loss: 0.006041951011866331
Checkpoint
2018-10-11 20:41:21.537009:	Training iteration: 156, Loss: 0.00604383647441864
Checkpoint
2018-10-11 20:41:22.692102:	Training iteration: 157, Loss: 0.004560532979667187
Checkpoint
2018-10-11 20:41:23.849510:	Training iteration: 158, Loss: 0.0058766696602106094
Checkpoint
2018-10-11 20:41:25.029671:	Training iteration: 159, Loss: 0.005746851675212383
Checkpoint
2018-10-11 20:41:26.217789:	Training iteration: 160, Loss: 0.004945333115756512
Checkpoint
2018-10-11 20:41:27.385611:	Training iteration: 161, Loss: 0.004972440656274557
Checkpoint
2018-10-11 20:41:28.536933:	Training iteration: 162, Loss: 0.004583169240504503
Checkpoint
2018-10-11 20:41:29.713102:	Training iteration: 163, Loss: 0.006955370306968689
Checkpoint
2018-10-11 20:41:30.892589:	Training iteration: 164, Loss: 0.004896095488220453
Checkpoint
2018-10-11 20:41:32.053168:	Training iteration: 165, Loss: 0.006155182141810656
Checkpoint
2018-10-11 20:41:33.197201:	Training iteration: 166, Loss: 0.005322637967765331
Checkpoint
2018-10-11 20:41:34.511336:	Training iteration: 167, Loss: 0.0045487829484045506
Checkpoint
2018-10-11 20:41:35.784222:	Training iteration: 168, Loss: 0.005601308774203062
Checkpoint
2018-10-11 20:41:37.021535:	Training iteration: 169, Loss: 0.006300882901996374
Checkpoint
2018-10-11 20:41:38.427916:	Training iteration: 170, Loss: 0.005518973805010319
Checkpoint
2018-10-11 20:41:39.668755:	Training iteration: 171, Loss: 0.005761387757956982
Checkpoint
2018-10-11 20:41:40.952343:	Training iteration: 172, Loss: 0.006795804016292095
Checkpoint
2018-10-11 20:41:42.154631:	Training iteration: 173, Loss: 0.004044244531542063
Checkpoint
2018-10-11 20:41:43.330700:	Training iteration: 174, Loss: 0.00600090529769659
Checkpoint
2018-10-11 20:41:44.665031:	Training iteration: 175, Loss: 0.005039140582084656
Checkpoint
2018-10-11 20:41:45.830069:	Training iteration: 176, Loss: 0.0059934696182608604
Checkpoint
2018-10-11 20:41:46.992883:	Training iteration: 177, Loss: 0.00379417953081429
Checkpoint
2018-10-11 20:41:48.150212:	Training iteration: 178, Loss: 0.0054125250317156315
Checkpoint
2018-10-11 20:41:49.306846:	Training iteration: 179, Loss: 0.004723384510725737
Checkpoint
2018-10-11 20:41:50.667224:	Training iteration: 180, Loss: 0.006152202840894461
Checkpoint
2018-10-11 20:41:51.833615:	Training iteration: 181, Loss: 0.004767552018165588
Checkpoint
2018-10-11 20:41:53.004180:	Training iteration: 182, Loss: 0.004210437182337046
Checkpoint
2018-10-11 20:41:54.156450:	Training iteration: 183, Loss: 0.007365628145635128
Checkpoint
2018-10-11 20:41:55.388314:	Training iteration: 184, Loss: 0.00463345879688859
Checkpoint
2018-10-11 20:41:56.630000:	Training iteration: 185, Loss: 0.00600482989102602
Checkpoint
2018-10-11 20:41:57.760821:	Training iteration: 186, Loss: 0.0065574320033192635
Checkpoint
2018-10-11 20:41:58.920403:	Training iteration: 187, Loss: 0.005620331969112158
Checkpoint
2018-10-11 20:42:00.064170:	Training iteration: 188, Loss: 0.004639884922653437
Checkpoint
2018-10-11 20:42:01.188758:	Training iteration: 189, Loss: 0.0052751763723790646
Checkpoint
2018-10-11 20:42:02.330783:	Training iteration: 190, Loss: 0.005193526856601238
Checkpoint
2018-10-11 20:42:03.474550:	Training iteration: 191, Loss: 0.004906653426587582
Checkpoint
2018-10-11 20:42:04.644887:	Training iteration: 192, Loss: 0.0048161642625927925
Checkpoint
2018-10-11 20:42:05.886833:	Training iteration: 193, Loss: 0.005635970737785101
Checkpoint
2018-10-11 20:42:07.026074:	Training iteration: 194, Loss: 0.005364554934203625
Checkpoint
2018-10-11 20:42:08.251267:	Training iteration: 195, Loss: 0.003985829651355743
Checkpoint
2018-10-11 20:42:09.410030:	Training iteration: 196, Loss: 0.004099379293620586
Checkpoint
2018-10-11 20:42:10.571832:	Training iteration: 197, Loss: 0.005286167375743389
Checkpoint
2018-10-11 20:42:11.704977:	Training iteration: 198, Loss: 0.006442102137953043
Checkpoint
2018-10-11 20:42:12.852144:	Training iteration: 199, Loss: 0.004953877534717321
Checkpoint
2018-10-11 20:42:14.006539:	Training iteration: 200, Loss: 0.0056485990062355995
Checkpoint
2018-10-11 20:42:15.241408:	Training iteration: 201, Loss: 0.00510880583897233
Checkpoint
2018-10-11 20:42:16.407252:	Training iteration: 202, Loss: 0.006814186926931143
Checkpoint
2018-10-11 20:42:17.667978:	Training iteration: 203, Loss: 0.005145925097167492
Checkpoint
2018-10-11 20:42:18.839866:	Training iteration: 204, Loss: 0.0073926388286054134
Checkpoint
2018-10-11 20:42:19.976528:	Training iteration: 205, Loss: 0.00595125975087285
Checkpoint
2018-10-11 20:42:21.110078:	Training iteration: 206, Loss: 0.004546805284917355
Checkpoint
2018-10-11 20:42:22.277561:	Training iteration: 207, Loss: 0.0059152813628315926
Checkpoint
2018-10-11 20:42:23.426930:	Training iteration: 208, Loss: 0.005525749642401934
Checkpoint
2018-10-11 20:42:24.623309:	Training iteration: 209, Loss: 0.005144297145307064
Checkpoint
2018-10-11 20:42:25.812316:	Training iteration: 210, Loss: 0.005016789771616459
Checkpoint
2018-10-11 20:42:26.985638:	Training iteration: 211, Loss: 0.004970465321093798
Checkpoint
2018-10-11 20:42:28.150228:	Training iteration: 212, Loss: 0.006067304406315088
Checkpoint
2018-10-11 20:42:29.308262:	Training iteration: 213, Loss: 0.006717394106090069
Checkpoint
2018-10-11 20:42:30.480824:	Training iteration: 214, Loss: 0.004641873296350241
Checkpoint
2018-10-11 20:42:31.653480:	Training iteration: 215, Loss: 0.004779527895152569
Checkpoint
2018-10-11 20:42:32.822917:	Training iteration: 216, Loss: 0.0045948633924126625
Checkpoint
2018-10-11 20:42:34.006743:	Training iteration: 217, Loss: 0.004392261616885662
Checkpoint
2018-10-11 20:42:35.185092:	Training iteration: 218, Loss: 0.0057672662660479546
Checkpoint
2018-10-11 20:42:36.356533:	Training iteration: 219, Loss: 0.004983691964298487
Checkpoint
2018-10-11 20:42:37.613063:	Training iteration: 220, Loss: 0.005445611197501421
Checkpoint
2018-10-11 20:42:38.767620:	Training iteration: 221, Loss: 0.0075251152738928795
Checkpoint
2018-10-11 20:42:39.929292:	Training iteration: 222, Loss: 0.006034198682755232
Checkpoint
2018-10-11 20:42:41.111358:	Training iteration: 223, Loss: 0.006556454114615917
Checkpoint
2018-10-11 20:42:42.271375:	Training iteration: 224, Loss: 0.004548933822661638
Checkpoint
2018-10-11 20:42:43.436411:	Training iteration: 225, Loss: 0.006162411067634821
Checkpoint
2018-10-11 20:42:44.675141:	Training iteration: 226, Loss: 0.005339349154382944
Checkpoint
2018-10-11 20:42:45.842954:	Training iteration: 227, Loss: 0.004518297966569662
Checkpoint
2018-10-11 20:42:46.978932:	Training iteration: 228, Loss: 0.004437071271240711
Checkpoint
2018-10-11 20:42:48.140586:	Training iteration: 229, Loss: 0.004900804720818996
Checkpoint
2018-10-11 20:42:49.292692:	Training iteration: 230, Loss: 0.005193489138036966
Checkpoint
2018-10-11 20:42:50.451042:	Training iteration: 231, Loss: 0.005141747184097767
Checkpoint
2018-10-11 20:42:51.599162:	Training iteration: 232, Loss: 0.004809132777154446
Checkpoint
2018-10-11 20:42:52.752929:	Training iteration: 233, Loss: 0.0043918355368077755
Checkpoint
2018-10-11 20:42:53.911008:	Training iteration: 234, Loss: 0.005834824405610561
Checkpoint
2018-10-11 20:42:55.067037:	Training iteration: 235, Loss: 0.003731043543666601
Checkpoint
2018-10-11 20:42:56.226534:	Training iteration: 236, Loss: 0.00527800852432847
Checkpoint
2018-10-11 20:42:57.388597:	Training iteration: 237, Loss: 0.004631283227354288
Checkpoint
2018-10-11 20:42:58.548342:	Training iteration: 238, Loss: 0.0046796491369605064
Checkpoint
2018-10-11 20:42:59.703684:	Training iteration: 239, Loss: 0.006083263549953699
Checkpoint
2018-10-11 20:43:00.964354:	Training iteration: 240, Loss: 0.004846216179430485
Checkpoint
2018-10-11 20:43:02.122998:	Training iteration: 241, Loss: 0.004707736428827047
Checkpoint
2018-10-11 20:43:03.282702:	Training iteration: 242, Loss: 0.005694206804037094
Checkpoint
2018-10-11 20:43:04.446567:	Training iteration: 243, Loss: 0.005778742488473654
Checkpoint
2018-10-11 20:43:05.609643:	Training iteration: 244, Loss: 0.005489555187523365
Checkpoint
2018-10-11 20:43:06.771834:	Training iteration: 245, Loss: 0.0043530454859137535
Checkpoint
2018-10-11 20:43:07.941269:	Training iteration: 246, Loss: 0.004653695039451122
Checkpoint
2018-10-11 20:43:09.108090:	Training iteration: 247, Loss: 0.005473195109516382
Checkpoint
2018-10-11 20:43:10.306090:	Training iteration: 248, Loss: 0.005875429604202509
Checkpoint
2018-10-11 20:43:11.456871:	Training iteration: 249, Loss: 0.0049109249375760555
Checkpoint
2018-10-11 20:43:12.654204:	Training iteration: 250, Loss: 0.005496022757142782
Checkpoint
2018-10-11 20:43:13.818502:	Training iteration: 251, Loss: 0.0045500462874770164
Checkpoint
2018-10-11 20:43:15.079995:	Training iteration: 252, Loss: 0.005087865050882101
Checkpoint
2018-10-11 20:43:16.244081:	Training iteration: 253, Loss: 0.006762894336134195
Checkpoint
2018-10-11 20:43:17.503948:	Training iteration: 254, Loss: 0.005524362903088331
Checkpoint
2018-10-11 20:43:18.682786:	Training iteration: 255, Loss: 0.007197067141532898
Checkpoint
2018-10-11 20:43:19.853018:	Training iteration: 256, Loss: 0.004640880972146988
Checkpoint
2018-10-11 20:43:21.012368:	Training iteration: 257, Loss: 0.005664912983775139
Checkpoint
2018-10-11 20:43:22.190938:	Training iteration: 258, Loss: 0.004552512429654598
Checkpoint
2018-10-11 20:43:23.451260:	Training iteration: 259, Loss: 0.004497015383094549
Checkpoint
2018-10-11 20:43:24.605616:	Training iteration: 260, Loss: 0.007734821643680334
Checkpoint
2018-10-11 20:43:25.786209:	Training iteration: 261, Loss: 0.0045862942934036255
Checkpoint
2018-10-11 20:43:26.953634:	Training iteration: 262, Loss: 0.004979853518307209
Checkpoint
2018-10-11 20:43:28.117457:	Training iteration: 263, Loss: 0.004420663230121136
Checkpoint
2018-10-11 20:43:29.270631:	Training iteration: 264, Loss: 0.005300312303006649
Checkpoint
2018-10-11 20:43:30.422861:	Training iteration: 265, Loss: 0.005785622633993626
Checkpoint
2018-10-11 20:43:31.597853:	Training iteration: 266, Loss: 0.004950725939124823
Checkpoint
2018-10-11 20:43:32.754397:	Training iteration: 267, Loss: 0.004227913450449705
Checkpoint
2018-10-11 20:43:33.924227:	Training iteration: 268, Loss: 0.003657811786979437
Checkpoint
2018-10-11 20:43:35.094095:	Training iteration: 269, Loss: 0.004763406701385975
Checkpoint
2018-10-11 20:43:36.265104:	Training iteration: 270, Loss: 0.005959215108305216
Checkpoint
2018-10-11 20:43:37.426635:	Training iteration: 271, Loss: 0.006147383246570826
Checkpoint
2018-10-11 20:43:38.624780:	Training iteration: 272, Loss: 0.004982125945389271
Checkpoint
2018-10-11 20:43:39.788373:	Training iteration: 273, Loss: 0.005075584631413221
Checkpoint
2018-10-11 20:43:41.041874:	Training iteration: 274, Loss: 0.005491430405527353
Checkpoint
2018-10-11 20:43:42.190765:	Training iteration: 275, Loss: 0.004876635503023863
Checkpoint
2018-10-11 20:43:43.361267:	Training iteration: 276, Loss: 0.005508269648998976
Checkpoint
2018-10-11 20:43:44.501713:	Training iteration: 277, Loss: 0.005406975280493498
Checkpoint
2018-10-11 20:43:45.705001:	Training iteration: 278, Loss: 0.00421533640474081
Checkpoint
2018-10-11 20:43:46.877588:	Training iteration: 279, Loss: 0.008254589512944221
Checkpoint
2018-10-11 20:43:48.124559:	Training iteration: 280, Loss: 0.00523055624216795
Checkpoint
2018-10-11 20:43:49.311240:	Training iteration: 281, Loss: 0.005690012127161026
Checkpoint
2018-10-11 20:43:50.710005:	Training iteration: 282, Loss: 0.004248304292559624
Checkpoint
2018-10-11 20:43:52.071460:	Training iteration: 283, Loss: 0.0056610433384776115
Checkpoint
2018-10-11 20:43:53.228704:	Training iteration: 284, Loss: 0.006372365169227123
Checkpoint
2018-10-11 20:43:54.408756:	Training iteration: 285, Loss: 0.005726316012442112
Checkpoint
2018-10-11 20:43:55.689604:	Training iteration: 286, Loss: 0.0052190544083714485
Checkpoint
2018-10-11 20:43:56.857163:	Training iteration: 287, Loss: 0.005420614033937454
Checkpoint
2018-10-11 20:43:57.976553:	Training iteration: 288, Loss: 0.0053741540759801865
Checkpoint
2018-10-11 20:43:59.118728:	Training iteration: 289, Loss: 0.004334775730967522
Checkpoint
2018-10-11 20:44:00.281359:	Training iteration: 290, Loss: 0.0038898494094610214
Checkpoint
2018-10-11 20:44:01.458821:	Training iteration: 291, Loss: 0.006563366856426001
Checkpoint
2018-10-11 20:44:02.656331:	Training iteration: 292, Loss: 0.0054327212274074554
Checkpoint
2018-10-11 20:44:03.874033:	Training iteration: 293, Loss: 0.005700281355530024
Checkpoint
2018-10-11 20:44:05.000027:	Training iteration: 294, Loss: 0.004283747170120478
Checkpoint
2018-10-11 20:44:06.156903:	Training iteration: 295, Loss: 0.005305201280862093
Checkpoint
2018-10-11 20:44:07.499785:	Training iteration: 296, Loss: 0.0054414174519479275
Checkpoint
2018-10-11 20:44:08.637234:	Training iteration: 297, Loss: 0.0063513293862342834
Checkpoint
2018-10-11 20:44:09.791208:	Training iteration: 298, Loss: 0.005620709620416164
Checkpoint
2018-10-11 20:44:10.911087:	Training iteration: 299, Loss: 0.005408444907516241
Checkpoint
2018-10-11 20:44:12.105001:	Training iteration: 300, Loss: 0.006528934929519892
Checkpoint
2018-10-11 20:44:13.236254:	Training iteration: 301, Loss: 0.005689041223376989
Checkpoint
2018-10-11 20:44:14.386222:	Training iteration: 302, Loss: 0.0051436820067465305
Checkpoint
2018-10-11 20:44:15.540164:	Training iteration: 303, Loss: 0.005356990732252598
Checkpoint
2018-10-11 20:44:16.761400:	Training iteration: 304, Loss: 0.006598127540200949
Checkpoint
2018-10-11 20:44:17.939296:	Training iteration: 305, Loss: 0.005549449007958174
Checkpoint
2018-10-11 20:44:19.084090:	Training iteration: 306, Loss: 0.004252327140420675
Checkpoint
2018-10-11 20:44:20.357551:	Training iteration: 307, Loss: 0.0047827186062932014
Checkpoint
2018-10-11 20:44:21.506387:	Training iteration: 308, Loss: 0.006404323969036341
Checkpoint
2018-10-11 20:44:22.648733:	Training iteration: 309, Loss: 0.004788986407220364
Checkpoint
2018-10-11 20:44:23.872309:	Training iteration: 310, Loss: 0.005668602883815765
Checkpoint
2018-10-11 20:44:24.997631:	Training iteration: 311, Loss: 0.005341777112334967
Checkpoint
2018-10-11 20:44:26.120080:	Training iteration: 312, Loss: 0.003954223357141018
Checkpoint
2018-10-11 20:44:27.280015:	Training iteration: 313, Loss: 0.005187585949897766
Checkpoint
2018-10-11 20:44:28.443856:	Training iteration: 314, Loss: 0.005016586743295193
Checkpoint
2018-10-11 20:44:29.613189:	Training iteration: 315, Loss: 0.004243846051394939
Checkpoint
2018-10-11 20:44:30.786739:	Training iteration: 316, Loss: 0.004883530084043741
Checkpoint
2018-10-11 20:44:31.942770:	Training iteration: 317, Loss: 0.004656963050365448
Checkpoint
2018-10-11 20:44:33.168574:	Training iteration: 318, Loss: 0.005031535401940346
Checkpoint
2018-10-11 20:44:34.311463:	Training iteration: 319, Loss: 0.005221539177000523
Checkpoint
2018-10-11 20:44:35.454957:	Training iteration: 320, Loss: 0.004804735071957111
Checkpoint
2018-10-11 20:44:36.613992:	Training iteration: 321, Loss: 0.005784311331808567
Checkpoint
2018-10-11 20:44:37.768387:	Training iteration: 322, Loss: 0.005867136176675558
Checkpoint
2018-10-11 20:44:38.937529:	Training iteration: 323, Loss: 0.004480427596718073
Checkpoint
2018-10-11 20:44:40.244083:	Training iteration: 324, Loss: 0.005947540979832411
Checkpoint
2018-10-11 20:44:41.408739:	Training iteration: 325, Loss: 0.005286536645144224
Checkpoint
2018-10-11 20:44:42.573863:	Training iteration: 326, Loss: 0.006230512168258429
Checkpoint
2018-10-11 20:44:43.744419:	Training iteration: 327, Loss: 0.0055157700553536415
Checkpoint
2018-10-11 20:44:44.909782:	Training iteration: 328, Loss: 0.0049921004101634026
Checkpoint
2018-10-11 20:44:46.072487:	Training iteration: 329, Loss: 0.0062485989183187485
Checkpoint
2018-10-11 20:44:47.221209:	Training iteration: 330, Loss: 0.005597508512437344
Checkpoint
2018-10-11 20:44:48.382804:	Training iteration: 331, Loss: 0.00490097189322114
Checkpoint
2018-10-11 20:44:49.533367:	Training iteration: 332, Loss: 0.0038314671255648136
Checkpoint
2018-10-11 20:44:50.704287:	Training iteration: 333, Loss: 0.005648093763738871
Checkpoint
2018-10-11 20:44:51.882561:	Training iteration: 334, Loss: 0.005881261080503464
Checkpoint
2018-10-11 20:44:53.069106:	Training iteration: 335, Loss: 0.004111672751605511
Checkpoint
2018-10-11 20:44:54.267423:	Training iteration: 336, Loss: 0.004867563955485821
Checkpoint
2018-10-11 20:44:55.423684:	Training iteration: 337, Loss: 0.006736402865499258
Checkpoint
2018-10-11 20:44:56.599555:	Training iteration: 338, Loss: 0.00508534163236618
Checkpoint
2018-10-11 20:44:57.760838:	Training iteration: 339, Loss: 0.006328163202852011
Checkpoint
2018-10-11 20:44:58.937345:	Training iteration: 340, Loss: 0.005396392662078142
Checkpoint
2018-10-11 20:45:00.190810:	Training iteration: 341, Loss: 0.0046742139384150505
Checkpoint
2018-10-11 20:45:01.351277:	Training iteration: 342, Loss: 0.006850783713161945
Checkpoint
2018-10-11 20:45:02.526664:	Training iteration: 343, Loss: 0.004941795952618122
Checkpoint
2018-10-11 20:45:03.698647:	Training iteration: 344, Loss: 0.004570464137941599
Checkpoint
2018-10-11 20:45:04.846719:	Training iteration: 345, Loss: 0.004516023211181164
Checkpoint
2018-10-11 20:45:06.101923:	Training iteration: 346, Loss: 0.005272019654512405
Checkpoint
2018-10-11 20:45:07.253775:	Training iteration: 347, Loss: 0.005319362971931696
Checkpoint
2018-10-11 20:45:08.419249:	Training iteration: 348, Loss: 0.0061921305023133755
Checkpoint
2018-10-11 20:45:09.586793:	Training iteration: 349, Loss: 0.0051084221340715885
Checkpoint
2018-10-11 20:45:10.744678:	Training iteration: 350, Loss: 0.006177285686135292
Checkpoint
2018-10-11 20:45:12.001104:	Training iteration: 351, Loss: 0.0037715479265898466
Checkpoint
2018-10-11 20:45:13.198055:	Training iteration: 352, Loss: 0.005548632703721523
Checkpoint
2018-10-11 20:45:14.379845:	Training iteration: 353, Loss: 0.00470328563824296
Checkpoint
2018-10-11 20:45:15.552859:	Training iteration: 354, Loss: 0.005308350082486868
Checkpoint
2018-10-11 20:45:16.738904:	Training iteration: 355, Loss: 0.004532426595687866
Checkpoint
2018-10-11 20:45:17.915623:	Training iteration: 356, Loss: 0.004914929158985615
Checkpoint
2018-10-11 20:45:19.078622:	Training iteration: 357, Loss: 0.004621589556336403
Checkpoint
2018-10-11 20:45:20.258980:	Training iteration: 358, Loss: 0.006148400250822306
Checkpoint
2018-10-11 20:45:21.425685:	Training iteration: 359, Loss: 0.0065190731547772884
Checkpoint
2018-10-11 20:45:22.588514:	Training iteration: 360, Loss: 0.004969376139342785
Checkpoint
2018-10-11 20:45:23.771910:	Training iteration: 361, Loss: 0.004894807934761047
Checkpoint
2018-10-11 20:45:24.934977:	Training iteration: 362, Loss: 0.00354408985003829
Checkpoint
2018-10-11 20:45:26.117954:	Training iteration: 363, Loss: 0.003867328865453601
Checkpoint
2018-10-11 20:45:27.293287:	Training iteration: 364, Loss: 0.004689863417297602
Checkpoint
2018-10-11 20:45:28.461007:	Training iteration: 365, Loss: 0.007445754017680883
Checkpoint
2018-10-11 20:45:29.645133:	Training iteration: 366, Loss: 0.004526775795966387
Checkpoint
2018-10-11 20:45:30.805687:	Training iteration: 367, Loss: 0.0038617041427642107
Checkpoint
2018-10-11 20:45:31.968458:	Training iteration: 368, Loss: 0.004593387711793184
Checkpoint
2018-10-11 20:45:33.133194:	Training iteration: 369, Loss: 0.0045672692358493805
Checkpoint
2018-10-11 20:45:34.314347:	Training iteration: 370, Loss: 0.003567087696865201
Checkpoint
2018-10-11 20:45:35.518213:	Training iteration: 371, Loss: 0.00663392711430788
Checkpoint
2018-10-11 20:45:36.689170:	Training iteration: 372, Loss: 0.006310383323580027
Checkpoint
2018-10-11 20:45:37.850895:	Training iteration: 373, Loss: 0.004663498606532812
Checkpoint
2018-10-11 20:45:39.105535:	Training iteration: 374, Loss: 0.004587465897202492
Checkpoint
2018-10-11 20:45:40.278849:	Training iteration: 375, Loss: 0.005714304279536009
Checkpoint
2018-10-11 20:45:41.572983:	Training iteration: 376, Loss: 0.00469313794746995
Checkpoint
2018-10-11 20:45:42.747705:	Training iteration: 377, Loss: 0.004391002468764782
Checkpoint
2018-10-11 20:45:43.926148:	Training iteration: 378, Loss: 0.004064516164362431
Checkpoint
2018-10-11 20:45:45.102505:	Training iteration: 379, Loss: 0.004921705927699804
Checkpoint
2018-10-11 20:45:46.273292:	Training iteration: 380, Loss: 0.004461901728063822
Checkpoint
2018-10-11 20:45:47.540548:	Training iteration: 381, Loss: 0.005883792880922556
Checkpoint
2018-10-11 20:45:48.705427:	Training iteration: 382, Loss: 0.005162382964044809
Checkpoint
2018-10-11 20:45:49.867141:	Training iteration: 383, Loss: 0.0060585965402424335
Checkpoint
2018-10-11 20:45:51.122456:	Training iteration: 384, Loss: 0.006403741892427206
Checkpoint
2018-10-11 20:45:52.297803:	Training iteration: 385, Loss: 0.005226221866905689
Checkpoint
2018-10-11 20:45:53.458753:	Training iteration: 386, Loss: 0.004368206951767206
Checkpoint
2018-10-11 20:45:54.616975:	Training iteration: 387, Loss: 0.004828158766031265
Checkpoint
2018-10-11 20:45:55.767680:	Training iteration: 388, Loss: 0.005416610278189182
Checkpoint
2018-10-11 20:45:56.974694:	Training iteration: 389, Loss: 0.006562328431755304
Checkpoint
2018-10-11 20:45:58.237634:	Training iteration: 390, Loss: 0.0034709491301327944
Checkpoint
2018-10-11 20:45:59.492738:	Training iteration: 391, Loss: 0.00545057887211442
Checkpoint
2018-10-11 20:46:00.676615:	Training iteration: 392, Loss: 0.005902991164475679
Checkpoint
2018-10-11 20:46:02.113416:	Training iteration: 393, Loss: 0.005846299231052399
Checkpoint
2018-10-11 20:46:03.401369:	Training iteration: 394, Loss: 0.00458840886130929
Checkpoint
2018-10-11 20:46:04.576382:	Training iteration: 395, Loss: 0.005513752344995737
Checkpoint
2018-10-11 20:46:05.753701:	Training iteration: 396, Loss: 0.005352710373699665
Checkpoint
2018-10-11 20:46:07.019660:	Training iteration: 397, Loss: 0.005087479017674923
Checkpoint
2018-10-11 20:46:08.247708:	Training iteration: 398, Loss: 0.005306045524775982
Checkpoint
2018-10-11 20:46:09.418154:	Training iteration: 399, Loss: 0.004602921195328236
Checkpoint
2018-10-11 20:46:10.556579:	Training iteration: 400, Loss: 0.007841140031814575
Checkpoint
2018-10-11 20:46:11.708340:	Training iteration: 401, Loss: 0.005751360673457384
Checkpoint
2018-10-11 20:46:12.971367:	Training iteration: 402, Loss: 0.007874714210629463
Checkpoint
2018-10-11 20:46:14.107105:	Training iteration: 403, Loss: 0.006854464765638113
Checkpoint
2018-10-11 20:46:15.233349:	Training iteration: 404, Loss: 0.00420217402279377
Checkpoint
2018-10-11 20:46:16.422999:	Training iteration: 405, Loss: 0.004576984327286482
Checkpoint
2018-10-11 20:46:17.672645:	Training iteration: 406, Loss: 0.0054812366142869
Checkpoint
2018-10-11 20:46:19.028931:	Training iteration: 407, Loss: 0.005905237048864365
Checkpoint
2018-10-11 20:46:20.175716:	Training iteration: 408, Loss: 0.004863514099270105
Checkpoint
2018-10-11 20:46:21.311017:	Training iteration: 409, Loss: 0.004456815775483847
Checkpoint
2018-10-11 20:46:22.476159:	Training iteration: 410, Loss: 0.00451259221881628
Checkpoint
2018-10-11 20:46:23.630864:	Training iteration: 411, Loss: 0.004636117722839117
Checkpoint
2018-10-11 20:46:24.772225:	Training iteration: 412, Loss: 0.005821773782372475
Checkpoint
2018-10-11 20:46:25.921567:	Training iteration: 413, Loss: 0.005316712893545628
Checkpoint
2018-10-11 20:46:27.077273:	Training iteration: 414, Loss: 0.004855866078287363
Checkpoint
2018-10-11 20:46:28.253956:	Training iteration: 415, Loss: 0.0051314630545675755
Checkpoint
2018-10-11 20:46:29.423213:	Training iteration: 416, Loss: 0.004936140961945057
Checkpoint
2018-10-11 20:46:30.664982:	Training iteration: 417, Loss: 0.0036825700663030148
Checkpoint
2018-10-11 20:46:31.890948:	Training iteration: 418, Loss: 0.004140633158385754
Checkpoint
2018-10-11 20:46:33.037453:	Training iteration: 419, Loss: 0.006089997943490744
Checkpoint
2018-10-11 20:46:34.169350:	Training iteration: 420, Loss: 0.004342993255704641
Checkpoint
2018-10-11 20:46:35.303834:	Training iteration: 421, Loss: 0.0046019172295928
Checkpoint
2018-10-11 20:46:36.434724:	Training iteration: 422, Loss: 0.004238619469106197
Checkpoint
2018-10-11 20:46:37.554383:	Training iteration: 423, Loss: 0.006199146620929241
Checkpoint
2018-10-11 20:46:38.711340:	Training iteration: 424, Loss: 0.005328702740371227
Checkpoint
2018-10-11 20:46:39.872484:	Training iteration: 425, Loss: 0.005195389036089182
Checkpoint
2018-10-11 20:46:41.030202:	Training iteration: 426, Loss: 0.004489555489271879
Checkpoint
2018-10-11 20:46:42.203330:	Training iteration: 427, Loss: 0.00599712785333395
Checkpoint
2018-10-11 20:46:43.493763:	Training iteration: 428, Loss: 0.005880828946828842
Checkpoint
2018-10-11 20:46:44.628466:	Training iteration: 429, Loss: 0.004751173313707113
Checkpoint
2018-10-11 20:46:45.769267:	Training iteration: 430, Loss: 0.006003960035741329
Checkpoint
2018-10-11 20:46:46.929768:	Training iteration: 431, Loss: 0.004294779151678085
Checkpoint
2018-10-11 20:46:48.086101:	Training iteration: 432, Loss: 0.004838150925934315
Checkpoint
2018-10-11 20:46:49.249064:	Training iteration: 433, Loss: 0.005548340268433094
Checkpoint
2018-10-11 20:46:50.417734:	Training iteration: 434, Loss: 0.006658054888248444
Checkpoint
2018-10-11 20:46:51.589982:	Training iteration: 435, Loss: 0.004083088133484125
Checkpoint
2018-10-11 20:46:52.754103:	Training iteration: 436, Loss: 0.007342278026044369
Checkpoint
2018-10-11 20:46:53.924234:	Training iteration: 437, Loss: 0.004442664794623852
Checkpoint
2018-10-11 20:46:55.094975:	Training iteration: 438, Loss: 0.005864040460437536
Checkpoint
2018-10-11 20:46:56.311600:	Training iteration: 439, Loss: 0.0032830785494297743
Checkpoint
2018-10-11 20:46:57.476231:	Training iteration: 440, Loss: 0.005341320298612118
Checkpoint
2018-10-11 20:46:58.639078:	Training iteration: 441, Loss: 0.005025257356464863
Checkpoint
2018-10-11 20:46:59.814170:	Training iteration: 442, Loss: 0.00541716581210494
Checkpoint
2018-10-11 20:47:00.993397:	Training iteration: 443, Loss: 0.005337513983249664
Checkpoint
2018-10-11 20:47:02.176110:	Training iteration: 444, Loss: 0.0056767938658595085
Checkpoint
2018-10-11 20:47:03.362907:	Training iteration: 445, Loss: 0.004896108992397785
Checkpoint
2018-10-11 20:47:04.557555:	Training iteration: 446, Loss: 0.005106869153678417
Checkpoint
2018-10-11 20:47:05.713952:	Training iteration: 447, Loss: 0.005574820097535849
Checkpoint
2018-10-11 20:47:06.875189:	Training iteration: 448, Loss: 0.0038371633272618055
Checkpoint
2018-10-11 20:47:08.061038:	Training iteration: 449, Loss: 0.003575568553060293
Checkpoint
2018-10-11 20:47:09.224134:	Training iteration: 450, Loss: 0.005147484131157398
Checkpoint
2018-10-11 20:47:10.390849:	Training iteration: 451, Loss: 0.0049873529933393
Checkpoint
2018-10-11 20:47:11.563053:	Training iteration: 452, Loss: 0.00646687438711524
Checkpoint
2018-10-11 20:47:12.814135:	Training iteration: 453, Loss: 0.006410425994545221
Checkpoint
2018-10-11 20:47:13.964596:	Training iteration: 454, Loss: 0.0036295787431299686
Checkpoint
2018-10-11 20:47:15.113915:	Training iteration: 455, Loss: 0.005456509534269571
Checkpoint
2018-10-11 20:47:16.286711:	Training iteration: 456, Loss: 0.004105801694095135
Checkpoint
2018-10-11 20:47:17.440861:	Training iteration: 457, Loss: 0.006386970169842243
Checkpoint
2018-10-11 20:47:18.659370:	Training iteration: 458, Loss: 0.004839681088924408
Checkpoint
2018-10-11 20:47:19.809244:	Training iteration: 459, Loss: 0.006001063622534275
Checkpoint
2018-10-11 20:47:20.973701:	Training iteration: 460, Loss: 0.004006813280284405
Checkpoint
2018-10-11 20:47:22.137989:	Training iteration: 461, Loss: 0.004394629038870335
Checkpoint
2018-10-11 20:47:23.309381:	Training iteration: 462, Loss: 0.005774172488600016
Checkpoint
2018-10-11 20:47:24.610792:	Training iteration: 463, Loss: 0.004182686097919941
Checkpoint
2018-10-11 20:47:25.782349:	Training iteration: 464, Loss: 0.005255739204585552
Checkpoint
2018-10-11 20:47:26.950264:	Training iteration: 465, Loss: 0.003610658925026655
Checkpoint
2018-10-11 20:47:28.114426:	Training iteration: 466, Loss: 0.0056185731664299965
Checkpoint
2018-10-11 20:47:29.270862:	Training iteration: 467, Loss: 0.005694140680134296
Checkpoint
2018-10-11 20:47:30.431946:	Training iteration: 468, Loss: 0.006086884997785091
Checkpoint
2018-10-11 20:47:31.579206:	Training iteration: 469, Loss: 0.00517684780061245
Checkpoint
2018-10-11 20:47:32.729394:	Training iteration: 470, Loss: 0.005401872098445892
Checkpoint
2018-10-11 20:47:33.900364:	Training iteration: 471, Loss: 0.004608542658388615
Checkpoint
2018-10-11 20:47:35.163704:	Training iteration: 472, Loss: 0.005495214369148016
Checkpoint
2018-10-11 20:47:36.327709:	Training iteration: 473, Loss: 0.004865843802690506
Checkpoint
Error: cost = nan
Loading latest checkpoint
INFO:tensorflow:Restoring parameters from /home/enterprise.internal.city.ac.uk/acvn728/checkpoints/115/115-473
INFO - tensorflow - Restoring parameters from /home/enterprise.internal.city.ac.uk/acvn728/checkpoints/115/115-473
Training complete after 0 epochs.
Finished requested number of epochs.
Final validation loss: 1
This was the best validation loss achieved
Starting testing
2018-10-11 20:47:37.937709:	Entering test loop
2018-10-11 20:47:48.176817: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 892 of 1000
2018-10-11 20:47:49.256902: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-11 20:47:49.499442:	Testing iteration: 0, Loss: 0.008948002010583878
2018-10-11 20:49:12.980833:	Testing iteration: 200, Loss: 0.00841328501701355
2018-10-11 20:50:39.069452:	Testing iteration: 400, Loss: 0.008826328441500664
2018-10-11 20:52:06.246112:	Testing iteration: 600, Loss: 0.007576721720397472
2018-10-11 20:52:39.959418: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 839 of 1000
2018-10-11 20:52:42.028324: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-10-11 20:53:50.241295:	Testing iteration: 800, Loss: 0.012594416737556458
