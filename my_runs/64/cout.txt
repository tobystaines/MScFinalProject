INFO - UNet_Speech_Separation - Running command 'do_experiment'
INFO - UNet_Speech_Separation - Started run with ID "64"
Experiment ID: 64
Preparing dataset
Dataset ready
2018-09-30 16:14:34.251416: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-09-30 16:14:35.151797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-09-30 16:14:35.152512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:27:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2018-09-30 16:14:35.152530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:27:00.0, compute capability: 6.1)
Session started
Iterators created
Creating model
Starting training
2018-09-30 16:14:51.486023: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 752 of 1000
2018-09-30 16:14:54.398617: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 16:16:57.871185:	Training iteration: 200, Loss: 0.25447097420692444
2018-09-30 16:19:01.613452:	Training iteration: 400, Loss: 0.23575039207935333
2018-09-30 16:21:05.976437:	Training iteration: 600, Loss: 0.2800244688987732
2018-09-30 16:23:11.906439:	Training iteration: 800, Loss: 0.1318436712026596
2018-09-30 16:25:18.514633:	Training iteration: 1000, Loss: 0.14773280918598175
2018-09-30 16:27:25.904390:	Training iteration: 1200, Loss: 0.0898684486746788
2018-09-30 16:29:32.391814:	Training iteration: 1400, Loss: 0.12259920686483383
2018-09-30 16:31:39.255307:	Training iteration: 1600, Loss: 0.1379697024822235
2018-09-30 16:33:46.059059:	Training iteration: 1800, Loss: 0.10639365017414093
2018-09-30 16:35:53.530434:	Training iteration: 2000, Loss: 0.1032286286354065
2018-09-30 16:38:00.640140:	Training iteration: 2200, Loss: 0.10299761593341827
2018-09-30 16:40:08.575816:	Training iteration: 2400, Loss: 0.1261913925409317
2018-09-30 16:42:16.608581:	Training iteration: 2600, Loss: 0.12200997769832611
2018-09-30 16:44:24.728621:	Training iteration: 2800, Loss: 0.10813812166452408
2018-09-30 16:46:32.297362:	Training iteration: 3000, Loss: 0.11837927252054214
2018-09-30 16:48:40.139217:	Training iteration: 3200, Loss: 0.12697796523571014
2018-09-30 16:50:48.146812:	Training iteration: 3400, Loss: 0.09385620057582855
2018-09-30 16:52:56.050990:	Training iteration: 3600, Loss: 0.11698298156261444
2018-09-30 16:55:04.633950:	Training iteration: 3800, Loss: 0.12475331127643585
2018-09-30 16:57:12.674703:	Training iteration: 4000, Loss: 0.10258529335260391
2018-09-30 16:59:20.992569:	Training iteration: 4200, Loss: 0.12105800956487656
2018-09-30 17:01:29.415398:	Training iteration: 4400, Loss: 0.12109693139791489
2018-09-30 17:03:38.048130:	Training iteration: 4600, Loss: 0.13944290578365326
2018-09-30 17:05:46.265314:	Training iteration: 4800, Loss: 0.10755310207605362
2018-09-30 17:07:37.845982: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 779 of 1000
2018-09-30 17:07:40.322994: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 17:08:06.581885:	Training iteration: 5000, Loss: 0.10994412750005722
2018-09-30 17:10:13.739246:	Training iteration: 5200, Loss: 0.14245840907096863
2018-09-30 17:12:22.488959:	Training iteration: 5400, Loss: 0.14921803772449493
2018-09-30 17:14:30.473762:	Training iteration: 5600, Loss: 0.11632204055786133
2018-09-30 17:16:38.627129:	Training iteration: 5800, Loss: 0.13635610044002533
2018-09-30 17:18:47.544461:	Training iteration: 6000, Loss: 0.12345452606678009
2018-09-30 17:20:55.636175:	Training iteration: 6200, Loss: 0.11609625816345215
2018-09-30 17:23:03.719496:	Training iteration: 6400, Loss: 0.13625778257846832
2018-09-30 17:25:12.615362:	Training iteration: 6600, Loss: 0.1052791103720665
2018-09-30 17:27:20.599635:	Training iteration: 6800, Loss: 0.13182489573955536
2018-09-30 17:29:29.035535:	Training iteration: 7000, Loss: 0.12447316944599152
2018-09-30 17:31:37.661383:	Training iteration: 7200, Loss: 0.10112205892801285
2018-09-30 17:33:46.464982:	Training iteration: 7400, Loss: 0.11578736454248428
2018-09-30 17:35:55.094660:	Training iteration: 7600, Loss: 0.14841657876968384
2018-09-30 17:38:03.761125:	Training iteration: 7800, Loss: 0.10945495218038559
2018-09-30 17:40:12.499080:	Training iteration: 8000, Loss: 0.10461044311523438
2018-09-30 17:42:21.611748:	Training iteration: 8200, Loss: 0.1232539638876915
2018-09-30 17:44:29.481621:	Training iteration: 8400, Loss: 0.1319606602191925
2018-09-30 17:46:36.907541:	Training iteration: 8600, Loss: 0.10302209854125977
2018-09-30 17:48:45.045926:	Training iteration: 8800, Loss: 0.10351213067770004
2018-09-30 17:50:53.459094:	Training iteration: 9000, Loss: 0.10729680955410004
2018-09-30 17:53:01.300592:	Training iteration: 9200, Loss: 0.12210603058338165
2018-09-30 17:55:08.979858:	Training iteration: 9400, Loss: 0.11870269477367401
2018-09-30 17:57:16.717585:	Training iteration: 9600, Loss: 0.10056117922067642
2018-09-30 17:59:24.140918:	Training iteration: 9800, Loss: 0.1039675921201706
2018-09-30 18:01:07.829175: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 779 of 1000
2018-09-30 18:01:10.514554: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 18:01:43.964179:	Training iteration: 10000, Loss: 0.08578595519065857
2018-09-30 18:03:50.971147:	Training iteration: 10200, Loss: 0.10019497573375702
2018-09-30 18:05:58.658953:	Training iteration: 10400, Loss: 0.0782419890165329
2018-09-30 18:08:07.030959:	Training iteration: 10600, Loss: 0.09283420443534851
2018-09-30 18:10:15.267148:	Training iteration: 10800, Loss: 0.0847625583410263
2018-09-30 18:12:23.423473:	Training iteration: 11000, Loss: 0.10726090520620346
2018-09-30 18:14:31.129307:	Training iteration: 11200, Loss: 0.08731862157583237
2018-09-30 18:16:39.099636:	Training iteration: 11400, Loss: 0.10700353235006332
2018-09-30 18:18:47.127280:	Training iteration: 11600, Loss: 0.07754269242286682
2018-09-30 18:20:55.875630:	Training iteration: 11800, Loss: 0.12818314135074615
2018-09-30 18:23:04.150373:	Training iteration: 12000, Loss: 0.10465537011623383
2018-09-30 18:25:12.930273:	Training iteration: 12200, Loss: 0.11590220034122467
2018-09-30 18:27:21.033760:	Training iteration: 12400, Loss: 0.0761459469795227
2018-09-30 18:29:28.730084:	Training iteration: 12600, Loss: 0.10360383987426758
2018-09-30 18:31:36.193032:	Training iteration: 12800, Loss: 0.08630652725696564
2018-09-30 18:33:44.728315:	Training iteration: 13000, Loss: 0.08822034299373627
2018-09-30 18:35:52.926664:	Training iteration: 13200, Loss: 0.07613196223974228
2018-09-30 18:38:01.134994:	Training iteration: 13400, Loss: 0.11719278991222382
2018-09-30 18:40:09.129990:	Training iteration: 13600, Loss: 0.0858326107263565
2018-09-30 18:42:17.315749:	Training iteration: 13800, Loss: 0.0732157826423645
2018-09-30 18:44:25.489600:	Training iteration: 14000, Loss: 0.09684520214796066
2018-09-30 18:46:34.243455:	Training iteration: 14200, Loss: 0.09151087701320648
2018-09-30 18:48:42.663769:	Training iteration: 14400, Loss: 0.08396802842617035
2018-09-30 18:50:51.304972:	Training iteration: 14600, Loss: 0.08941756188869476
2018-09-30 18:52:59.752787:	Training iteration: 14800, Loss: 0.09430104494094849
2018-09-30 18:54:33.535090: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 798 of 1000
2018-09-30 18:54:35.898697: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 18:55:19.376771:	Training iteration: 15000, Loss: 0.10779403150081635
2018-09-30 18:57:27.723178:	Training iteration: 15200, Loss: 0.17798733711242676
2018-09-30 18:59:36.428991:	Training iteration: 15400, Loss: 0.12492351233959198
2018-09-30 19:01:45.584486:	Training iteration: 15600, Loss: 0.14122210443019867
2018-09-30 19:03:54.653621:	Training iteration: 15800, Loss: 0.18125085532665253
2018-09-30 19:06:03.911485:	Training iteration: 16000, Loss: 0.1499672681093216
2018-09-30 19:08:12.364222:	Training iteration: 16200, Loss: 0.15750421583652496
2018-09-30 19:10:21.079378:	Training iteration: 16400, Loss: 0.13817238807678223
2018-09-30 19:12:29.615474:	Training iteration: 16600, Loss: 0.1316230744123459
2018-09-30 19:14:38.119883:	Training iteration: 16800, Loss: 0.15493182837963104
2018-09-30 19:16:46.509268:	Training iteration: 17000, Loss: 0.10805930197238922
2018-09-30 19:18:55.147793:	Training iteration: 17200, Loss: 0.14511653780937195
2018-09-30 19:21:03.763799:	Training iteration: 17400, Loss: 0.14677996933460236
2018-09-30 19:23:11.867464:	Training iteration: 17600, Loss: 0.13140039145946503
2018-09-30 19:25:20.022245:	Training iteration: 17800, Loss: 0.15904943645000458
2018-09-30 19:27:28.237409:	Training iteration: 18000, Loss: 0.08930762112140656
2018-09-30 19:29:36.578798:	Training iteration: 18200, Loss: 0.13584423065185547
2018-09-30 19:31:44.645759:	Training iteration: 18400, Loss: 0.1423686444759369
2018-09-30 19:33:53.028500:	Training iteration: 18600, Loss: 0.13197341561317444
2018-09-30 19:36:01.588894:	Training iteration: 18800, Loss: 0.10911772400140762
2018-09-30 19:38:10.280277:	Training iteration: 19000, Loss: 0.11041148751974106
2018-09-30 19:40:19.073516:	Training iteration: 19200, Loss: 0.15018992125988007
2018-09-30 19:42:27.682548:	Training iteration: 19400, Loss: 0.13575294613838196
2018-09-30 19:44:36.067580:	Training iteration: 19600, Loss: 0.13120099902153015
2018-09-30 19:46:44.642585:	Training iteration: 19800, Loss: 0.1403781622648239
2018-09-30 19:48:53.039455:	Training iteration: 20000, Loss: 0.11918677389621735
2018-09-30 19:50:32.105448: I tensorflow/core/kernels/shuffle_dataset_op.cc:110] Filling up shuffle buffer (this may take a while): 847 of 1000
2018-09-30 19:50:33.838310: I tensorflow/core/kernels/shuffle_dataset_op.cc:121] Shuffle buffer filled.
2018-09-30 19:51:12.471980:	Training iteration: 20200, Loss: 0.16907501220703125
2018-09-30 19:53:20.186825:	Training iteration: 20400, Loss: 0.18268947303295135
2018-09-30 19:55:28.412340:	Training iteration: 20600, Loss: 0.179862841963768
2018-09-30 19:57:36.825052:	Training iteration: 20800, Loss: 0.17578408122062683
2018-09-30 19:59:45.226547:	Training iteration: 21000, Loss: 0.1869194209575653
2018-09-30 20:01:53.705772:	Training iteration: 21200, Loss: 0.2091081589460373
2018-09-30 20:04:01.921668:	Training iteration: 21400, Loss: 0.16088593006134033
2018-09-30 20:06:10.613366:	Training iteration: 21600, Loss: 0.14694491028785706
2018-09-30 20:08:18.746846:	Training iteration: 21800, Loss: 0.12648718059062958
2018-09-30 20:10:26.990409:	Training iteration: 22000, Loss: 0.13433769345283508
2018-09-30 20:12:35.175855:	Training iteration: 22200, Loss: 0.12404125928878784
2018-09-30 20:14:43.238349:	Training iteration: 22400, Loss: 0.11937189102172852
2018-09-30 20:16:51.288161:	Training iteration: 22600, Loss: 0.14139142632484436
2018-09-30 20:18:59.642492:	Training iteration: 22800, Loss: 0.16001102328300476
2018-09-30 20:21:08.104115:	Training iteration: 23000, Loss: 0.16418100893497467
2018-09-30 20:23:16.551219:	Training iteration: 23200, Loss: 0.13235865533351898
2018-09-30 20:25:24.925630:	Training iteration: 23400, Loss: 0.1630551815032959
2018-09-30 20:27:33.037004:	Training iteration: 23600, Loss: 0.10409144312143326
2018-09-30 20:29:41.709241:	Training iteration: 23800, Loss: 0.1350530982017517
2018-09-30 20:31:49.882794:	Training iteration: 24000, Loss: 0.15360954403877258
2018-09-30 20:33:58.219418:	Training iteration: 24200, Loss: 0.17078793048858643
2018-09-30 20:36:06.413973:	Training iteration: 24400, Loss: 0.14899753034114838
2018-09-30 20:38:14.663777:	Training iteration: 24600, Loss: 0.16430848836898804
2018-09-30 20:40:22.263970:	Training iteration: 24800, Loss: 0.20605263113975525
2018-09-30 20:42:30.668600:	Training iteration: 25000, Loss: 0.2034965455532074
2018-09-30 20:44:38.174474:	Training iteration: 25200, Loss: 0.16156814992427826
2018-09-30 20:46:45.672356:	Training iteration: 25400, Loss: 0.18198958039283752
2018-09-30 20:48:53.837338:	Training iteration: 25600, Loss: 0.32122188806533813
2018-09-30 20:51:01.205509:	Training iteration: 25800, Loss: 0.22357073426246643
2018-09-30 20:53:08.535190:	Training iteration: 26000, Loss: 0.16824465990066528
2018-09-30 20:55:15.996082:	Training iteration: 26200, Loss: 0.1537782996892929
2018-09-30 20:57:23.427377:	Training iteration: 26400, Loss: 0.1675187647342682
2018-09-30 20:59:31.206161:	Training iteration: 26600, Loss: 0.1699608713388443
2018-09-30 21:01:39.194395:	Training iteration: 26800, Loss: 0.17341852188110352
2018-09-30 21:03:47.478758:	Training iteration: 27000, Loss: 0.15533927083015442
2018-09-30 21:05:55.242272:	Training iteration: 27200, Loss: 0.1703571230173111
2018-09-30 21:08:03.629613:	Training iteration: 27400, Loss: 0.17814168334007263
2018-09-30 21:10:12.006155:	Training iteration: 27600, Loss: 0.19204634428024292
2018-09-30 21:12:20.302075:	Training iteration: 27800, Loss: 0.14443150162696838
2018-09-30 21:14:28.846125:	Training iteration: 28000, Loss: 0.19878271222114563
2018-09-30 21:16:37.435920:	Training iteration: 28200, Loss: 0.15244868397712708
2018-09-30 21:18:45.989438:	Training iteration: 28400, Loss: 0.1657925695180893
2018-09-30 21:20:54.174165:	Training iteration: 28600, Loss: 0.14336387813091278
2018-09-30 21:23:02.981664:	Training iteration: 28800, Loss: 0.1351003348827362
2018-09-30 21:25:11.323461:	Training iteration: 29000, Loss: 0.17262481153011322
2018-09-30 21:27:19.143727:	Training iteration: 29200, Loss: 0.16621717810630798
2018-09-30 21:29:27.611100:	Training iteration: 29400, Loss: 0.15187349915504456
2018-09-30 21:31:35.784515:	Training iteration: 29600, Loss: 0.20693178474903107
2018-09-30 21:33:43.941012:	Training iteration: 29800, Loss: 0.18730700016021729
2018-09-30 21:35:52.277907:	Training iteration: 30000, Loss: 0.15233179926872253
2018-09-30 21:38:00.563972:	Training iteration: 30200, Loss: 0.1390431672334671
2018-09-30 21:40:08.913555:	Training iteration: 30400, Loss: 0.1499486118555069
2018-09-30 21:42:17.633738:	Training iteration: 30600, Loss: 0.13077208399772644
2018-09-30 21:44:25.673194:	Training iteration: 30800, Loss: 0.18390165269374847
2018-09-30 21:46:34.064798:	Training iteration: 31000, Loss: 0.14145861566066742
2018-09-30 21:48:42.460774:	Training iteration: 31200, Loss: 0.17649762332439423
2018-09-30 21:50:50.695045:	Training iteration: 31400, Loss: 0.1705521047115326
2018-09-30 21:52:59.064131:	Training iteration: 31600, Loss: 0.15375185012817383
2018-09-30 21:55:07.219741:	Training iteration: 31800, Loss: 0.19901542365550995
2018-09-30 21:57:15.465904:	Training iteration: 32000, Loss: 0.18898485600948334
2018-09-30 21:59:24.383893:	Training iteration: 32200, Loss: 0.1623116433620453
2018-09-30 22:01:32.525113:	Training iteration: 32400, Loss: 0.1711571216583252
2018-09-30 22:03:40.970895:	Training iteration: 32600, Loss: 0.18702712655067444
2018-09-30 22:05:48.640876:	Training iteration: 32800, Loss: 0.1586705446243286
2018-09-30 22:07:56.321882:	Training iteration: 33000, Loss: 0.11738169193267822
2018-09-30 22:10:04.517019:	Training iteration: 33200, Loss: 0.19859477877616882
2018-09-30 22:12:12.397424:	Training iteration: 33400, Loss: 0.17444927990436554
2018-09-30 22:14:20.529851:	Training iteration: 33600, Loss: 0.18943093717098236
2018-09-30 22:16:27.942594:	Training iteration: 33800, Loss: 0.14676597714424133
2018-09-30 22:18:35.856665:	Training iteration: 34000, Loss: 0.15869002044200897
2018-09-30 22:20:43.807218:	Training iteration: 34200, Loss: 0.15677934885025024
2018-09-30 22:22:51.319481:	Training iteration: 34400, Loss: 0.2123727798461914
2018-09-30 22:24:59.444688:	Training iteration: 34600, Loss: 0.19558951258659363
2018-09-30 22:27:07.107494:	Training iteration: 34800, Loss: 0.15777328610420227
2018-09-30 22:29:15.004755:	Training iteration: 35000, Loss: 0.15411916375160217
2018-09-30 22:31:23.548852:	Training iteration: 35200, Loss: 0.14469832181930542
2018-09-30 22:33:31.662140:	Training iteration: 35400, Loss: 0.18282832205295563
2018-09-30 22:35:40.272066:	Training iteration: 35600, Loss: 0.1863919198513031
2018-09-30 22:37:48.220948:	Training iteration: 35800, Loss: 0.17418310046195984
2018-09-30 22:39:56.373409:	Training iteration: 36000, Loss: 0.1463843435049057
2018-09-30 22:42:04.648194:	Training iteration: 36200, Loss: 0.13514287769794464
2018-09-30 22:44:12.928026:	Training iteration: 36400, Loss: 0.147658109664917
2018-09-30 22:46:21.485155:	Training iteration: 36600, Loss: 0.16473278403282166
2018-09-30 22:48:29.656989:	Training iteration: 36800, Loss: 0.1505839228630066
2018-09-30 22:50:37.820196:	Training iteration: 37000, Loss: 0.16016948223114014
2018-09-30 22:52:46.330058:	Training iteration: 37200, Loss: 0.1277429610490799
2018-09-30 22:54:54.656447:	Training iteration: 37400, Loss: 0.1416655331850052
2018-09-30 22:57:02.955470:	Training iteration: 37600, Loss: 0.14504972100257874
2018-09-30 22:59:11.065776:	Training iteration: 37800, Loss: 0.14179468154907227
2018-09-30 23:01:19.713156:	Training iteration: 38000, Loss: 0.12707926332950592
2018-09-30 23:03:28.049711:	Training iteration: 38200, Loss: 0.18501369655132294
2018-09-30 23:05:36.122148:	Training iteration: 38400, Loss: 0.1770414561033249
2018-09-30 23:07:44.437202:	Training iteration: 38600, Loss: 0.14934788644313812
2018-09-30 23:09:52.933784:	Training iteration: 38800, Loss: 0.20222392678260803
2018-09-30 23:12:01.061444:	Training iteration: 39000, Loss: 0.179298534989357
2018-09-30 23:14:09.377002:	Training iteration: 39200, Loss: 0.17702069878578186
2018-09-30 23:16:17.913038:	Training iteration: 39400, Loss: 0.13496172428131104
2018-09-30 23:18:25.710187:	Training iteration: 39600, Loss: 0.15751954913139343
2018-09-30 23:20:33.850848:	Training iteration: 39800, Loss: 0.1680566370487213
2018-09-30 23:22:42.057218:	Training iteration: 40000, Loss: 0.14656424522399902
2018-09-30 23:24:50.451552:	Training iteration: 40200, Loss: 0.1996193677186966
2018-09-30 23:26:58.933499:	Training iteration: 40400, Loss: 0.20878736674785614
2018-09-30 23:29:07.643588:	Training iteration: 40600, Loss: 0.14428342878818512
2018-09-30 23:31:16.658583:	Training iteration: 40800, Loss: 0.13202720880508423
2018-09-30 23:33:24.908596:	Training iteration: 41000, Loss: 0.1613456904888153
2018-09-30 23:35:33.711325:	Training iteration: 41200, Loss: 0.1302390843629837
2018-09-30 23:37:41.268054:	Training iteration: 41400, Loss: 0.0922330766916275
2018-09-30 23:39:49.134397:	Training iteration: 41600, Loss: 0.16185936331748962
2018-09-30 23:41:57.339423:	Training iteration: 41800, Loss: 0.19203989207744598
2018-09-30 23:44:05.037544:	Training iteration: 42000, Loss: 0.18259356915950775
2018-09-30 23:46:12.647763:	Training iteration: 42200, Loss: 0.14966003596782684
2018-09-30 23:48:20.383456:	Training iteration: 42400, Loss: 0.1550721973180771
2018-09-30 23:50:28.003140:	Training iteration: 42600, Loss: 0.17916972935199738
2018-09-30 23:52:36.293460:	Training iteration: 42800, Loss: 0.12963131070137024
2018-09-30 23:54:44.217985:	Training iteration: 43000, Loss: 0.13595129549503326
2018-09-30 23:56:52.157757:	Training iteration: 43200, Loss: 0.14074398577213287
2018-09-30 23:58:59.768236:	Training iteration: 43400, Loss: 0.16656649112701416
2018-10-01 00:01:07.700869:	Training iteration: 43600, Loss: 0.17846348881721497
2018-10-01 00:03:15.999391:	Training iteration: 43800, Loss: 0.18500636518001556
2018-10-01 00:05:23.934833:	Training iteration: 44000, Loss: 0.16345040500164032
2018-10-01 00:07:32.249125:	Training iteration: 44200, Loss: 0.14573948085308075
2018-10-01 00:09:40.496171:	Training iteration: 44400, Loss: 0.14329001307487488
2018-10-01 00:11:48.776433:	Training iteration: 44600, Loss: 0.1835428774356842
2018-10-01 00:13:57.284704:	Training iteration: 44800, Loss: 0.11984596401453018
2018-10-01 00:16:05.482072:	Training iteration: 45000, Loss: 0.17056813836097717
2018-10-01 00:18:13.800648:	Training iteration: 45200, Loss: 0.1854853332042694
2018-10-01 00:20:22.004775:	Training iteration: 45400, Loss: 0.15731050074100494
2018-10-01 00:22:30.305267:	Training iteration: 45600, Loss: 0.17759856581687927
2018-10-01 00:24:38.731541:	Training iteration: 45800, Loss: 0.17643508315086365
2018-10-01 00:26:47.020569:	Training iteration: 46000, Loss: 0.188169926404953
2018-10-01 00:28:55.148370:	Training iteration: 46200, Loss: 0.21203860640525818
2018-10-01 00:31:03.163805:	Training iteration: 46400, Loss: 0.13332924246788025
2018-10-01 00:33:11.225029:	Training iteration: 46600, Loss: 0.13927388191223145
2018-10-01 00:35:19.263938:	Training iteration: 46800, Loss: 0.12925998866558075
2018-10-01 00:37:27.539274:	Training iteration: 47000, Loss: 0.20735087990760803
2018-10-01 00:39:35.672239:	Training iteration: 47200, Loss: 0.15247222781181335
2018-10-01 00:41:44.473508:	Training iteration: 47400, Loss: 0.17323748767375946
2018-10-01 00:43:52.886508:	Training iteration: 47600, Loss: 0.15122286975383759
2018-10-01 00:46:01.279588:	Training iteration: 47800, Loss: 0.18753091990947723
2018-10-01 00:48:09.743847:	Training iteration: 48000, Loss: 0.18520519137382507
2018-10-01 00:50:18.332689:	Training iteration: 48200, Loss: 0.10786034166812897
2018-10-01 00:52:26.536992:	Training iteration: 48400, Loss: 0.19511301815509796
2018-10-01 00:54:34.767417:	Training iteration: 48600, Loss: 0.1440977305173874
2018-10-01 00:56:42.994499:	Training iteration: 48800, Loss: 0.13001582026481628
2018-10-01 00:58:51.638400:	Training iteration: 49000, Loss: 0.11870460212230682
2018-10-01 01:01:00.008295:	Training iteration: 49200, Loss: 0.14530454576015472
2018-10-01 01:03:08.149083:	Training iteration: 49400, Loss: 0.1891750991344452
2018-10-01 01:05:15.923360:	Training iteration: 49600, Loss: 0.17474934458732605
2018-10-01 01:07:24.004668:	Training iteration: 49800, Loss: 0.16503497958183289
2018-10-01 01:09:31.795255:	Training iteration: 50000, Loss: 0.12987586855888367
2018-10-01 01:11:39.600657:	Training iteration: 50200, Loss: 0.17632965743541718
2018-10-01 01:13:47.414509:	Training iteration: 50400, Loss: 0.18247400224208832
2018-10-01 01:15:55.269137:	Training iteration: 50600, Loss: 0.1706092804670334
2018-10-01 01:18:02.845207:	Training iteration: 50800, Loss: 0.11430756002664566
2018-10-01 01:20:10.352476:	Training iteration: 51000, Loss: 0.1843794286251068
2018-10-01 01:22:18.072081:	Training iteration: 51200, Loss: 0.22727136313915253
2018-10-01 01:24:25.690571:	Training iteration: 51400, Loss: 0.1828741580247879
2018-10-01 01:26:33.404393:	Training iteration: 51600, Loss: 0.16363109648227692
2018-10-01 01:28:41.426916:	Training iteration: 51800, Loss: 0.1570485383272171
2018-10-01 01:30:49.535849:	Training iteration: 52000, Loss: 0.21680574119091034
2018-10-01 01:32:57.595213:	Training iteration: 52200, Loss: 0.15052595734596252
2018-10-01 01:35:05.904818:	Training iteration: 52400, Loss: 0.15512606501579285
2018-10-01 01:37:13.866676:	Training iteration: 52600, Loss: 0.1578090637922287
2018-10-01 01:39:22.033504:	Training iteration: 52800, Loss: 0.1808788776397705
2018-10-01 01:41:29.889202:	Training iteration: 53000, Loss: 0.18197093904018402
2018-10-01 01:43:38.474349:	Training iteration: 53200, Loss: 0.1890677958726883
2018-10-01 01:45:46.519839:	Training iteration: 53400, Loss: 0.1716139018535614
2018-10-01 01:47:54.695797:	Training iteration: 53600, Loss: 0.19097720086574554
2018-10-01 01:50:02.459922:	Training iteration: 53800, Loss: 0.18001477420330048
2018-10-01 01:52:11.086724:	Training iteration: 54000, Loss: 0.24537599086761475
2018-10-01 01:54:19.544604:	Training iteration: 54200, Loss: 0.16452744603157043
2018-10-01 01:56:28.174323:	Training iteration: 54400, Loss: 0.1532747447490692
2018-10-01 01:58:36.795046:	Training iteration: 54600, Loss: 0.20114007592201233
2018-10-01 02:00:45.174091:	Training iteration: 54800, Loss: 0.23847761750221252
2018-10-01 02:02:53.594104:	Training iteration: 55000, Loss: 0.1695808470249176
2018-10-01 02:05:02.168683:	Training iteration: 55200, Loss: 0.15970680117607117
2018-10-01 02:07:10.622000:	Training iteration: 55400, Loss: 0.1755761206150055
2018-10-01 02:09:19.075558:	Training iteration: 55600, Loss: 0.17327508330345154
2018-10-01 02:11:27.654258:	Training iteration: 55800, Loss: 0.16354259848594666
2018-10-01 02:13:36.086119:	Training iteration: 56000, Loss: 0.15874657034873962
2018-10-01 02:15:44.184123:	Training iteration: 56200, Loss: 0.15598943829536438
2018-10-01 02:17:52.861455:	Training iteration: 56400, Loss: 0.1660993993282318
2018-10-01 02:20:01.425390:	Training iteration: 56600, Loss: 0.1654018610715866
2018-10-01 02:22:09.599123:	Training iteration: 56800, Loss: 0.14745642244815826
2018-10-01 02:24:18.358463:	Training iteration: 57000, Loss: 0.16740408539772034
2018-10-01 02:26:26.567293:	Training iteration: 57200, Loss: 0.1917187124490738
2018-10-01 02:28:35.524928:	Training iteration: 57400, Loss: 0.17256440222263336
2018-10-01 02:30:43.922183:	Training iteration: 57600, Loss: 0.15450799465179443
2018-10-01 02:32:51.589898:	Training iteration: 57800, Loss: 0.182700976729393
2018-10-01 02:34:59.477546:	Training iteration: 58000, Loss: 0.1769697368144989
2018-10-01 02:37:07.366989:	Training iteration: 58200, Loss: 0.18076196312904358
2018-10-01 02:39:14.924981:	Training iteration: 58400, Loss: 0.1807554066181183
2018-10-01 02:41:22.576065:	Training iteration: 58600, Loss: 0.18316377699375153
2018-10-01 02:43:30.115384:	Training iteration: 58800, Loss: 0.18915341794490814
2018-10-01 02:45:38.655155:	Training iteration: 59000, Loss: 0.1605164259672165
2018-10-01 02:47:45.759285:	Training iteration: 59200, Loss: 0.16621096432209015
2018-10-01 02:49:53.153558:	Training iteration: 59400, Loss: 0.15990889072418213
2018-10-01 02:52:00.259532:	Training iteration: 59600, Loss: 0.1803921014070511
2018-10-01 02:54:08.004936:	Training iteration: 59800, Loss: 0.16165116429328918
2018-10-01 02:56:15.857534:	Training iteration: 60000, Loss: 0.1719633936882019
2018-10-01 02:58:24.392531:	Training iteration: 60200, Loss: 0.16428439319133759
2018-10-01 03:00:32.723710:	Training iteration: 60400, Loss: 0.19655120372772217
2018-10-01 03:02:40.993860:	Training iteration: 60600, Loss: 0.15099646151065826
2018-10-01 03:04:49.389750:	Training iteration: 60800, Loss: 0.2027558535337448
2018-10-01 03:06:57.839003:	Training iteration: 61000, Loss: 0.1725275218486786
2018-10-01 03:09:06.173666:	Training iteration: 61200, Loss: 0.1395888477563858
2018-10-01 03:11:14.283802:	Training iteration: 61400, Loss: 0.12652939558029175
2018-10-01 03:13:22.337764:	Training iteration: 61600, Loss: 0.16266708076000214
2018-10-01 03:15:30.884363:	Training iteration: 61800, Loss: 0.14117829501628876
2018-10-01 03:17:39.199432:	Training iteration: 62000, Loss: 0.1666105091571808
2018-10-01 03:19:47.843481:	Training iteration: 62200, Loss: 0.15543439984321594
2018-10-01 03:21:56.063688:	Training iteration: 62400, Loss: 0.146210715174675
2018-10-01 03:24:04.144934:	Training iteration: 62600, Loss: 0.15683183073997498
2018-10-01 03:26:12.178352:	Training iteration: 62800, Loss: 0.16761556267738342
2018-10-01 03:28:20.426954:	Training iteration: 63000, Loss: 0.19833476841449738
2018-10-01 03:30:28.483136:	Training iteration: 63200, Loss: 0.17454028129577637
2018-10-01 03:32:36.843472:	Training iteration: 63400, Loss: 0.19005395472049713
2018-10-01 03:34:45.076297:	Training iteration: 63600, Loss: 0.16338814795017242
2018-10-01 03:36:53.884749:	Training iteration: 63800, Loss: 0.16467125713825226
2018-10-01 03:39:01.806735:	Training iteration: 64000, Loss: 0.16576352715492249
2018-10-01 03:41:09.837289:	Training iteration: 64200, Loss: 0.1497327983379364
2018-10-01 03:43:18.161951:	Training iteration: 64400, Loss: 0.14869126677513123
2018-10-01 03:45:26.486767:	Training iteration: 64600, Loss: 0.17059312760829926
2018-10-01 03:47:34.853379:	Training iteration: 64800, Loss: 0.17576703429222107
2018-10-01 03:49:43.089309:	Training iteration: 65000, Loss: 0.16245634853839874
2018-10-01 03:51:51.443665:	Training iteration: 65200, Loss: 0.1845322549343109
2018-10-01 03:54:00.036712:	Training iteration: 65400, Loss: 0.2480284869670868
2018-10-01 03:56:08.065419:	Training iteration: 65600, Loss: 0.12701037526130676
2018-10-01 03:58:15.838936:	Training iteration: 65800, Loss: 0.19794417917728424
2018-10-01 04:00:23.725169:	Training iteration: 66000, Loss: 0.20959198474884033
2018-10-01 04:02:31.466003:	Training iteration: 66200, Loss: 0.209397554397583
2018-10-01 04:04:39.666958:	Training iteration: 66400, Loss: 0.15994378924369812
2018-10-01 04:06:47.504315:	Training iteration: 66600, Loss: 0.1933145672082901
2018-10-01 04:08:55.573032:	Training iteration: 66800, Loss: 0.1303284466266632
2018-10-01 04:11:03.212284:	Training iteration: 67000, Loss: 0.1384991705417633
2018-10-01 04:13:10.904986:	Training iteration: 67200, Loss: 0.21412257850170135
2018-10-01 04:15:18.463841:	Training iteration: 67400, Loss: 0.16224153339862823
2018-10-01 04:17:26.511499:	Training iteration: 67600, Loss: 0.19046978652477264
2018-10-01 04:19:34.317316:	Training iteration: 67800, Loss: 0.18025553226470947
2018-10-01 04:21:42.246992:	Training iteration: 68000, Loss: 0.15486884117126465
2018-10-01 04:23:50.030414:	Training iteration: 68200, Loss: 0.18769218027591705
2018-10-01 04:25:58.028909:	Training iteration: 68400, Loss: 0.14773251116275787
2018-10-01 04:28:06.292517:	Training iteration: 68600, Loss: 0.1522979587316513
2018-10-01 04:30:14.406100:	Training iteration: 68800, Loss: 0.14245985448360443
2018-10-01 04:32:22.872192:	Training iteration: 69000, Loss: 0.14375129342079163
2018-10-01 04:34:31.028431:	Training iteration: 69200, Loss: 0.19041618704795837
2018-10-01 04:36:39.223423:	Training iteration: 69400, Loss: 0.15731008350849152
2018-10-01 04:38:47.437837:	Training iteration: 69600, Loss: 0.16492879390716553
2018-10-01 04:40:55.526040:	Training iteration: 69800, Loss: 0.1264791041612625
2018-10-01 04:43:03.937036:	Training iteration: 70000, Loss: 0.15302148461341858
2018-10-01 04:45:12.248018:	Training iteration: 70200, Loss: 0.14243870973587036
2018-10-01 04:47:20.533338:	Training iteration: 70400, Loss: 0.1518627405166626
2018-10-01 04:49:28.709750:	Training iteration: 70600, Loss: 0.32816073298454285
2018-10-01 04:51:36.797132:	Training iteration: 70800, Loss: 0.18557478487491608
2018-10-01 04:53:45.055282:	Training iteration: 71000, Loss: 0.19133779406547546
2018-10-01 04:55:53.163576:	Training iteration: 71200, Loss: 0.21148379147052765
2018-10-01 04:58:01.897333:	Training iteration: 71400, Loss: 0.14560960233211517
2018-10-01 05:00:09.900561:	Training iteration: 71600, Loss: 0.1398043930530548
2018-10-01 05:02:18.238915:	Training iteration: 71800, Loss: 0.1232626661658287
2018-10-01 05:04:26.449968:	Training iteration: 72000, Loss: 0.1542181521654129
2018-10-01 05:06:34.752615:	Training iteration: 72200, Loss: 0.14568515121936798
2018-10-01 05:08:42.539421:	Training iteration: 72400, Loss: 0.17962166666984558
2018-10-01 05:10:50.653864:	Training iteration: 72600, Loss: 0.14838233590126038
2018-10-01 05:12:58.664050:	Training iteration: 72800, Loss: 0.28156930208206177
2018-10-01 05:15:07.433733:	Training iteration: 73000, Loss: 0.2607317268848419
2018-10-01 05:17:15.969652:	Training iteration: 73200, Loss: 0.19225743412971497
2018-10-01 05:19:24.097276:	Training iteration: 73400, Loss: 0.13228067755699158
2018-10-01 05:21:32.227599:	Training iteration: 73600, Loss: 0.1863262951374054
2018-10-01 05:23:40.074608:	Training iteration: 73800, Loss: 0.15785400569438934
2018-10-01 05:25:47.683718:	Training iteration: 74000, Loss: 0.12319537252187729
2018-10-01 05:27:55.698268:	Training iteration: 74200, Loss: 0.16933533549308777
2018-10-01 05:30:03.134450:	Training iteration: 74400, Loss: 0.17485935986042023
2018-10-01 05:32:10.588963:	Training iteration: 74600, Loss: 0.13433443009853363
2018-10-01 05:34:18.424020:	Training iteration: 74800, Loss: 0.15104447305202484
2018-10-01 05:36:26.112697:	Training iteration: 75000, Loss: 0.2656573951244354
2018-10-01 05:38:33.736504:	Training iteration: 75200, Loss: 0.17015643417835236
2018-10-01 05:40:41.369786:	Training iteration: 75400, Loss: 0.1507795751094818
2018-10-01 05:42:49.024136:	Training iteration: 75600, Loss: 0.14195066690444946
2018-10-01 05:44:56.602258:	Training iteration: 75800, Loss: 0.1573670208454132
2018-10-01 05:47:04.266980:	Training iteration: 76000, Loss: 0.15919749438762665
2018-10-01 05:49:11.999127:	Training iteration: 76200, Loss: 0.14783678948879242
2018-10-01 05:51:20.025548:	Training iteration: 76400, Loss: 0.18223504722118378
2018-10-01 05:53:28.243608:	Training iteration: 76600, Loss: 0.1751316487789154
2018-10-01 05:55:36.404901:	Training iteration: 76800, Loss: 0.1645783632993698
2018-10-01 05:57:44.970189:	Training iteration: 77000, Loss: 0.177569180727005
2018-10-01 05:59:53.209838:	Training iteration: 77200, Loss: 0.14324191212654114
2018-10-01 06:02:01.144503:	Training iteration: 77400, Loss: 0.1441153734922409
2018-10-01 06:04:09.637644:	Training iteration: 77600, Loss: 0.15393230319023132
2018-10-01 06:06:17.680376:	Training iteration: 77800, Loss: 0.18576852977275848
2018-10-01 06:08:25.942196:	Training iteration: 78000, Loss: 0.18911060690879822
2018-10-01 06:10:34.122011:	Training iteration: 78200, Loss: 0.13973721861839294
2018-10-01 06:12:42.405134:	Training iteration: 78400, Loss: 0.14217860996723175
2018-10-01 06:14:50.326296:	Training iteration: 78600, Loss: 0.15849097073078156
2018-10-01 06:16:58.352456:	Training iteration: 78800, Loss: 0.1696859449148178
2018-10-01 06:19:06.550541:	Training iteration: 79000, Loss: 0.1449073702096939
2018-10-01 06:21:14.513730:	Training iteration: 79200, Loss: 0.14589616656303406
2018-10-01 06:23:22.307572:	Training iteration: 79400, Loss: 0.1773620992898941
2018-10-01 06:25:30.576134:	Training iteration: 79600, Loss: 0.16784736514091492
2018-10-01 06:27:38.657985:	Training iteration: 79800, Loss: 0.15146996080875397
2018-10-01 06:29:47.196304:	Training iteration: 80000, Loss: 0.1845289021730423
2018-10-01 06:31:55.533264:	Training iteration: 80200, Loss: 0.2026095688343048
2018-10-01 06:34:04.385058:	Training iteration: 80400, Loss: 0.14638835191726685
2018-10-01 06:36:12.807752:	Training iteration: 80600, Loss: 0.2066689282655716
2018-10-01 06:38:21.040485:	Training iteration: 80800, Loss: 0.12207406759262085
2018-10-01 06:40:29.304766:	Training iteration: 81000, Loss: 0.15241694450378418
2018-10-01 06:42:37.166742:	Training iteration: 81200, Loss: 0.1695520430803299
2018-10-01 06:44:45.202893:	Training iteration: 81400, Loss: 0.17536050081253052
2018-10-01 06:46:52.785636:	Training iteration: 81600, Loss: 0.12309358268976212
2018-10-01 06:49:00.665980:	Training iteration: 81800, Loss: 0.15504713356494904
2018-10-01 06:51:08.387547:	Training iteration: 82000, Loss: 0.158732607960701
2018-10-01 06:53:16.349293:	Training iteration: 82200, Loss: 0.1594475507736206
2018-10-01 06:55:24.396151:	Training iteration: 82400, Loss: 0.16125452518463135
2018-10-01 06:57:31.826065:	Training iteration: 82600, Loss: 0.14782121777534485
2018-10-01 06:59:39.441174:	Training iteration: 82800, Loss: 0.13232174515724182
2018-10-01 07:01:46.924182:	Training iteration: 83000, Loss: 0.15066763758659363
2018-10-01 07:03:54.968438:	Training iteration: 83200, Loss: 0.1719285249710083
2018-10-01 07:06:02.570899:	Training iteration: 83400, Loss: 0.16723208129405975
2018-10-01 07:08:10.695449:	Training iteration: 83600, Loss: 0.19929733872413635
2018-10-01 07:10:18.543110:	Training iteration: 83800, Loss: 0.18222877383232117
2018-10-01 07:12:26.649167:	Training iteration: 84000, Loss: 0.13641764223575592
2018-10-01 07:14:34.847783:	Training iteration: 84200, Loss: 0.1604081094264984
2018-10-01 07:16:42.963368:	Training iteration: 84400, Loss: 0.19943548738956451
2018-10-01 07:18:51.197713:	Training iteration: 84600, Loss: 0.17816562950611115
2018-10-01 07:21:00.046131:	Training iteration: 84800, Loss: 0.1262294054031372
2018-10-01 07:23:08.303540:	Training iteration: 85000, Loss: 0.21633489429950714
2018-10-01 07:25:16.583661:	Training iteration: 85200, Loss: 0.15935660898685455
2018-10-01 07:27:24.974298:	Training iteration: 85400, Loss: 0.14431598782539368
2018-10-01 07:29:33.269740:	Training iteration: 85600, Loss: 0.18994122743606567
2018-10-01 07:31:41.493240:	Training iteration: 85800, Loss: 0.19942785799503326
2018-10-01 07:33:49.678664:	Training iteration: 86000, Loss: 0.13949646055698395
2018-10-01 07:35:57.748955:	Training iteration: 86200, Loss: 0.16783499717712402
2018-10-01 07:38:05.721379:	Training iteration: 86400, Loss: 0.15346892178058624
2018-10-01 07:40:13.877740:	Training iteration: 86600, Loss: 0.16474834084510803
2018-10-01 07:42:22.073040:	Training iteration: 86800, Loss: 0.1540054827928543
2018-10-01 07:44:30.401382:	Training iteration: 87000, Loss: 0.15677393972873688
2018-10-01 07:46:38.775072:	Training iteration: 87200, Loss: 0.1500801146030426
2018-10-01 07:48:47.191450:	Training iteration: 87400, Loss: 0.16022247076034546
2018-10-01 07:50:55.439770:	Training iteration: 87600, Loss: 0.1860055774450302
2018-10-01 07:53:03.777976:	Training iteration: 87800, Loss: 0.16874048113822937
2018-10-01 07:55:12.263612:	Training iteration: 88000, Loss: 0.15429382026195526
2018-10-01 07:57:20.594717:	Training iteration: 88200, Loss: 0.19821055233478546
2018-10-01 07:59:28.660739:	Training iteration: 88400, Loss: 0.13463327288627625
2018-10-01 08:01:36.719592:	Training iteration: 88600, Loss: 0.18065795302391052
2018-10-01 08:03:45.183056:	Training iteration: 88800, Loss: 0.16565237939357758
2018-10-01 08:05:53.158143:	Training iteration: 89000, Loss: 0.13595648109912872
2018-10-01 08:08:01.326370:	Training iteration: 89200, Loss: 0.24591776728630066
2018-10-01 08:10:09.249349:	Training iteration: 89400, Loss: 0.20677292346954346
2018-10-01 08:12:16.883077:	Training iteration: 89600, Loss: 0.18178565800189972
2018-10-01 08:14:25.037036:	Training iteration: 89800, Loss: 0.16604827344417572
2018-10-01 08:16:32.712670:	Training iteration: 90000, Loss: 0.17102651298046112
2018-10-01 08:18:39.963163:	Training iteration: 90200, Loss: 0.1580759882926941
2018-10-01 08:20:47.687144:	Training iteration: 90400, Loss: 0.15874959528446198
2018-10-01 08:22:55.509281:	Training iteration: 90600, Loss: 0.15251319110393524
2018-10-01 08:25:03.707199:	Training iteration: 90800, Loss: 0.15331266820430756
2018-10-01 08:27:11.685114:	Training iteration: 91000, Loss: 0.13115254044532776
2018-10-01 08:29:19.846659:	Training iteration: 91200, Loss: 0.16273489594459534
2018-10-01 08:31:27.785656:	Training iteration: 91400, Loss: 0.1712171584367752
2018-10-01 08:33:35.645513:	Training iteration: 91600, Loss: 0.12819699943065643
2018-10-01 08:35:44.018743:	Training iteration: 91800, Loss: 0.1435602903366089
2018-10-01 08:37:51.795265:	Training iteration: 92000, Loss: 0.16498523950576782
2018-10-01 08:39:59.885450:	Training iteration: 92200, Loss: 0.17909395694732666
2018-10-01 08:42:08.127729:	Training iteration: 92400, Loss: 0.12794974446296692
2018-10-01 08:44:16.071146:	Training iteration: 92600, Loss: 0.20102417469024658
2018-10-01 08:46:24.614959:	Training iteration: 92800, Loss: 0.19900265336036682
2018-10-01 08:48:32.794559:	Training iteration: 93000, Loss: 0.18549668788909912
2018-10-01 08:50:40.755281:	Training iteration: 93200, Loss: 0.1742270290851593
2018-10-01 08:52:48.900487:	Training iteration: 93400, Loss: 0.16831840574741364
2018-10-01 08:54:57.134998:	Training iteration: 93600, Loss: 0.13856062293052673
2018-10-01 08:57:05.163612:	Training iteration: 93800, Loss: 0.17471322417259216
2018-10-01 08:59:13.121275:	Training iteration: 94000, Loss: 0.1774916648864746
2018-10-01 09:01:21.070221:	Training iteration: 94200, Loss: 0.16039568185806274
