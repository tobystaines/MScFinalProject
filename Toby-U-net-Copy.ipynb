{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Self-Contained Source Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "First let's import libraries and create some convenience functions for our neural network operations with sane default values.\n",
    "\n",
    "Unfortunately there isn't an easy way to hide code blocks in jupyter, so just skip over this :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pycochleagram.cochleagram as cgram\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "plt.rcParams['image.cmap'] = 'hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def concat(x, y):\n",
    "    return tf.concat([x, y], axis=3)\n",
    "\n",
    "\n",
    "def conv(inputs, filters, kernel_size, stride):\n",
    "    out = tf.layers.conv2d(\n",
    "        inputs, filters=filters, kernel_size=kernel_size,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        strides=stride, padding='SAME')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def deconv(inputs, filters, kernel_size, stride):\n",
    "    out = tf.layers.conv2d_transpose(\n",
    "        inputs, filters=filters, kernel_size=kernel_size,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        strides=stride, padding='SAME')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def batch_norm(inputs, is_training, reuse):\n",
    "    return tf.contrib.layers.batch_norm(\n",
    "        inputs,\n",
    "        decay=0.9,\n",
    "        updates_collections=None,\n",
    "        epsilon=1e-5,\n",
    "        scale=True,\n",
    "        is_training=is_training,\n",
    "        reuse=reuse)\n",
    "\n",
    "\n",
    "def dropout(inputs, rate):\n",
    "    return tf.nn.dropout(inputs, keep_prob=1 - rate)\n",
    "\n",
    "\n",
    "def relu(inputs):\n",
    "    return tf.nn.relu(inputs)\n",
    "\n",
    "\n",
    "def tanh(inputs):\n",
    "    return tf.nn.tanh(inputs)\n",
    "\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    with tf.variable_scope('lrelu'):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "\n",
    "def read_audio(path, sample_rate, n_channels):\n",
    "\n",
    "    def read_audio_py(py_path):\n",
    "        #if n_channels == 1:\n",
    "            mono, _ = librosa.load(py_path, sr=sample_rate, mono=True)\n",
    "            return np.expand_dims(mono, 1)\n",
    "        #elif n_channels == 2:\n",
    "            #stereo, _ = librosa.load(py_path, sr=sample_rate, mono=False)\n",
    "            #return stereo.T\n",
    "        #else:\n",
    "            #raise ValueError('Invalid channels: %d' % n_channels)\n",
    "\n",
    "    return tf.py_func(read_audio_py, [path], tf.float32, stateful=False)\n",
    "\n",
    "\n",
    "def fake_stereo(audio):\n",
    "\n",
    "    def fake_stereo(x):\n",
    "        return tf.stack([x, x], 1)\n",
    "    \n",
    "    voice = audio[:, 0]\n",
    "    mixed = voice * 2\n",
    "    return fake_stereo(mixed), fake_stereo(voice)\n",
    "\n",
    "\n",
    "def compute_spectrogram(audio, n_fft, fft_hop, n_channels):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio : single to dual channel audio shaped (n_samples, n_channels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n_frames, 1 + n_fft / 2, n_channels * 2), where the\n",
    "        last dimension is (left_mag, right_mag, left_phase, right_phase)\n",
    "    '''\n",
    "\n",
    "    def stft(x):\n",
    "        spec = librosa.stft(\n",
    "            x, n_fft=n_fft, hop_length=fft_hop, window='hann')\n",
    "        # TODO: normalize?\n",
    "        #mag = np.abs(spec)\n",
    "        #temp = mag - mag.min()\n",
    "        #mag_norm = temp / temp.max()\n",
    "        return np.abs(spec), np.angle(spec)\n",
    "\n",
    "    #def stereo_func(py_audio):\n",
    "    #    left_mag, left_phase = stft(py_audio[:, 0])\n",
    "    #    right_mag, right_phase = stft(py_audio[:, 1])\n",
    "    #    ret = np.array([left_mag, right_mag, left_phase, right_phase]).T\n",
    "    #    return ret.astype(np.float32)\n",
    "\n",
    "    def mono_func(py_audio):\n",
    "        mag, phase = stft(py_audio[:, 0])\n",
    "        ret = np.array([mag, phase]).T\n",
    "        return ret.astype(np.float32)\n",
    "\n",
    "    #if n_channels == 2:\n",
    "    #    func = stereo_func\n",
    "    #elif n_channels == 1:\n",
    "    #    func = mono_func\n",
    "    #else:\n",
    "    #    raise ValueError('Invalid channels: %d' % n_channels)\n",
    "\n",
    "    with tf.name_scope('read_spectrogram'):\n",
    "        ret = tf.py_func(mono_func, [audio], tf.float32, stateful=False)\n",
    "        ret.set_shape([None, 1 + n_fft / 2, 2])   # n_channels * 2])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def extract_spectrogram_patches(\n",
    "        spec, n_fft, n_channels, patch_window, patch_hop):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    spec : Spectrogram of shape (n_frames, 1 + n_fft / 2, n_channels * 2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n_patches, patch_window, 1 + n_fft / 2, n_channels * 2)\n",
    "        containing patches from spec.\n",
    "    '''\n",
    "    with tf.name_scope('extract_spectrogram_patches'):\n",
    "        spec4d = tf.expand_dims(spec, 0)\n",
    "\n",
    "        patches = tf.extract_image_patches(\n",
    "            spec4d, ksizes=[1, patch_window, 1 + n_fft / 2, 1],\n",
    "            strides=[1, patch_hop, 1 + n_fft / 2, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "\n",
    "        num_patches = tf.shape(patches)[1]\n",
    "\n",
    "        return tf.reshape(patches, [num_patches, patch_window,\n",
    "                                    int(1 + n_fft / 2), 2])   # int(n_channels * 2)]) #here was '1 + n_fft / 2, n_channels * 2', \n",
    "                                                                              #it was causing an error in 6th code block\n",
    "\n",
    "def replace_spectrogram_patches(patches):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Tensor of shape (n_patches, patch_window, 1 + n_fft / 2, n_channels * 2)\n",
    "    containing patches from spec.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spec : Spectrogram of shape (n_frames, 1 + n_fft / 2, n_channels * 2)\n",
    "    '''\n",
    "    # Need to account for patch overlap\n",
    "    # Is it possible to account for no set num patches per whole spectrogram?\n",
    "    pass\n",
    "\n",
    "def invert_spectrogram(mag, phase, n_fft, fft_hop):\n",
    "\n",
    "    # Could be a problem if the spec has been normalised\n",
    "    spec = np.array([mag.T, phase.T]).T\n",
    "    wave = librosa.istft(spec, win_length=n_fft, hop_length=fft_hop, window='hann')\n",
    "\n",
    "    return wave\n",
    "\n",
    "def compute_cochleagram(audio, SAMPLE_RATE, low_lim, hi_lim):\n",
    "    coch = cgram.human_cochleagram(audio, SAMPLE_RATE, low_lim, hi_lim, strict=False)\n",
    "    \n",
    "        \n",
    "def hwr_tf(x):\n",
    "    return x * tf.cast(x > 0.0, tf.float32)\n",
    "\n",
    "\n",
    "def compute_acapella_diff(mixed, noise):\n",
    "    mixed_mag = mixed[:, :, 0:, :2]\n",
    "    mixed_phase = mixed[:, :, 0:, 2:]\n",
    "    noise_mag = noise[:, :, 0:, :2]\n",
    "    voice_mag = hwr_tf(mixed_mag - noise_mag) # TODO: normalize?\n",
    "    voice_phase = mixed_phase\n",
    "    return mixed, noise, tf.concat((voice_mag, voice_phase), axis=3)\n",
    "\n",
    "\n",
    "def partial_argv(func, *args, **kwargs):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    func    : A function that takes scalar argument and returns scalar value\n",
    "    *args   : Args to partially apply to func\n",
    "    *kwargs : Keyword args to partially apply to func\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    func(*args) : A function that maps func over *args and returns a tuple\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    func = partial_argv(abs)\n",
    "    func(-1, 2, -3, 4)\n",
    "\n",
    "    # returns (1, 2, 3, 4)\n",
    "    '''\n",
    "    return lambda *other_args: tuple(map(partial(func, *args, **kwargs), other_args))\n",
    "\n",
    "\n",
    "def zip_tensor_slices(*args):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    *args : list of _n_ _k_-dimensional tensors, where _k_ >= 2\n",
    "        The first dimension has _m_ elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : Dataset of _m_ examples, where each example has _n_\n",
    "        records of _k_ - 1 dimensions.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ds = (\n",
    "        tf.data.Dataset.zip((\n",
    "            tf.data.Dataset.from_tensors([[1,2], [3,4], [5, 6]]),\n",
    "            tf.data.Dataset.from_tensors([[10, 20], [30, 40], [50, 60]])\n",
    "        ))\n",
    "        .flat_map(zip_tensor_slices)  # <--- *HERE*\n",
    "    )\n",
    "    el = ds.make_one_shot_iterator().get_next()\n",
    "    print sess.run(el)\n",
    "    print sess.run(el)\n",
    "\n",
    "    # Output:\n",
    "    # (array([1, 2], dtype=int32), array([10, 20], dtype=int32))\n",
    "    # (array([3, 4], dtype=int32), array([30, 40], dtype=int32))\n",
    "    '''\n",
    "    return tf.data.Dataset.zip(tuple([\n",
    "        tf.data.Dataset.from_tensor_slices(arg)\n",
    "        for arg in args\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's define a few constants we'll use throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100\n",
    "N_FFT = 1024\n",
    "FFT_HOP = 256\n",
    "N_CHANNELS = 1   #was N_CHANNELS = 2\n",
    "N_PARALLEL_READERS = 4\n",
    "PATCH_WINDOW = 256\n",
    "PATCH_HOP = 128\n",
    "BATCH_SIZE = 8\n",
    "N_SHUFFLE = 20\n",
    "#low_lim = 50\n",
    "#hi_lim = 5000\n",
    "\n",
    "# With default params, each spectrum represents 0.02s, and hop length is 0.005s (so ~200frames/s)\n",
    "# Each patch is roughly 1.25s long - this is a lot shorter than those used in the music paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## U-Net Model\n",
    "\n",
    "Next, we create our U-Nets, one for noise and one for voice. Each `UNetModel` has a `UNetEncoder` and a `UNetDecoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class UNetModel(object):\n",
    "\n",
    "    def __init__(self, mixed, voice, mixed_phase, is_training):\n",
    "        self.mixed = mixed\n",
    "        self.voice = voice\n",
    "        self.mixed_phase = mixed_phase\n",
    "        self.is_training = is_training\n",
    "\n",
    "        self.voice_mask_unet = UNet(mixed, is_training=is_training, reuse=False, name='voice-mask-unet')\n",
    "\n",
    "        self.voice_mask = self.voice_mask_unet.output\n",
    "\n",
    "        self.gen_voice = self.voice_mask * mixed\n",
    "\n",
    "        self.cost = l1_loss(self.gen_voice, voice)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=0.0002,\n",
    "            beta1=0.5,\n",
    "        )\n",
    "        self.train_op = self.optimizer.minimize(self.cost)\n",
    "\n",
    "\n",
    "class UNet(object):\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse, name):\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "\n",
    "            self.encoder = UNetEncoder(input_tensor, is_training, reuse)\n",
    "            self.decoder = UNetDecoder(self.encoder.output, self.encoder, is_training, reuse)\n",
    "\n",
    "            self.output = tanh(self.decoder.output) / 2 + .5\n",
    "\n",
    "\n",
    "class UNetEncoder(object):\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse):\n",
    "        net = input_tensor\n",
    "        with tf.variable_scope('encoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = conv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                self.l1 = net\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l2 = net\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l3 = net\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l4 = net\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l5 = net\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=512, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n",
    "\n",
    "\n",
    "class UNetDecoder(object):\n",
    "\n",
    "    def __init__(self, input_tensor, encoder, is_training, reuse):\n",
    "        net = input_tensor\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = relu(net)\n",
    "                net = deconv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = relu(concat(net, encoder.l5))\n",
    "                net = deconv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = relu(concat(net, encoder.l4))\n",
    "                net = deconv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = relu(concat(net, encoder.l3))\n",
    "                net = deconv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = relu(concat(net, encoder.l2))\n",
    "                net = deconv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = relu(concat(net, encoder.l1))\n",
    "                net = deconv(net, filters=1, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Data Pipelines\n",
    "\n",
    "Next we define our data reading functions. We make use of [the Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def training_dataset(\n",
    "        data_folder,\n",
    "        sample_rate,\n",
    "        n_fft,\n",
    "        fft_hop,\n",
    "        n_channels,\n",
    "        patch_window,\n",
    "        patch_hop,\n",
    "        #batch_size,\n",
    "        #n_shuffle,\n",
    "        n_parallel_readers\n",
    "):\n",
    "    \"\"\"Still need to fix this to stop it producing a tuple\"\"\"\n",
    "    return (\n",
    "        tf.data.Dataset.list_files(data_folder + '/*.wav')\n",
    "        .map(partial(\n",
    "            read_audio,\n",
    "            sample_rate=sample_rate,\n",
    "            n_channels=n_channels\n",
    "        ), num_parallel_calls=n_parallel_readers)\n",
    "        #.map(fake_stereo, num_parallel_calls=n_parallel_readers)\n",
    "        .map(partial_argv(\n",
    "            compute_spectrogram,\n",
    "            n_fft=n_fft,\n",
    "            fft_hop=fft_hop,\n",
    "            n_channels=n_channels,\n",
    "        ), num_parallel_calls=n_parallel_readers)\n",
    "        .map(partial_argv(\n",
    "            extract_spectrogram_patches,\n",
    "            n_fft=n_fft,\n",
    "            n_channels=n_channels,\n",
    "            patch_window=patch_window,\n",
    "            patch_hop=patch_hop,\n",
    "        ))\n",
    "        .flat_map(zip_tensor_slices)\n",
    "        #.batch(batch_size)\n",
    "        #.shuffle(n_shuffle)\n",
    "        #.repeat()\n",
    "    )\n",
    "\n",
    "def zip_datasets(dataset_a, dataset_b, n_shuffle, batch_size):\n",
    "    return tf.data.Dataset.zip((dataset_a, dataset_b)).batch(batch_size).shuffle(n_shuffle).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next let's create the Tensorflow [Session](https://www.tensorflow.org/programmers_guide/graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can look at some examples from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_folder = 'C:/Users/Toby/Jupyter Notebooks/My Work/MSc Project/Test Audio/Noisy'\n",
    "clean_folder = 'C:/Users/Toby/Jupyter Notebooks/My Work/MSc Project/Test Audio/Clean'\n",
    "mds = training_dataset(mixed_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP,  N_PARALLEL_READERS)\n",
    "cds = training_dataset(clean_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP, N_PARALLEL_READERS)\n",
    "ds = zip_datasets(mds, cds, BATCH_SIZE, N_SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed, voice = ds.make_one_shot_iterator().get_next()\n",
    "(m,), (v,) = sess.run([mixed, voice]) # Bit of a hack here to get around my tuple problem in training_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Here `mixed`, `noise`, and `voise` are [tensors](https://www.tensorflow.org/programmers_guide/tensors), whereas `m`, `n`, and `v` are regular python variables containing the training examples.\n",
    "\n",
    "`m`, `n`, and `v` are all the same shape: (batch\\_size, patch\\_window, n\\_fft + 1, 4)\n",
    "\n",
    "where the last dimension is composed of [left mag, right mag, left phase, right phase]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(20,20))\n",
    "ax1.imshow(m[0, :, :, 0])\n",
    "ax2.imshow(m[0, :, :, 1])\n",
    "ax3.imshow(v[0, :, :, 0])\n",
    "ax4.imshow(v[0, :, :, 1])\n",
    "ax5.imshow(m[2, :, :, 0])\n",
    "ax6.imshow(m[2, :, :, 1])\n",
    "ax7.imshow(v[2, :, :, 0])\n",
    "ax8.imshow(v[2, :, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have both the model and the data pipeline we can actually train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_mag = mixed[0][:, :, 1:, :2] # Yet more hacking to get around this tuple problem\n",
    "mixed_phase = mixed[0][:, :, 1:, 2:]\n",
    "voice_mag = voice[0][:, :, 1:, :2]\n",
    "\n",
    "model = UNetModel(\n",
    "    mixed_mag,\n",
    "    voice_mag,\n",
    "    mixed_phase,\n",
    "    is_training\n",
    ")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Predictions before training on a random batch from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "single_mixed_batch, single_voice_batch = sess.run([mixed_mag, voice_mag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gen_voice, voice_mask = sess.run([model.gen_voice, model.voice_mask], {is_training: True, model.mixed: single_mixed_batch})\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,20))\n",
    "\n",
    "ax1.imshow(single_mixed_batch[0, :200, :100, 0])\n",
    "ax1.set_title('mixed')\n",
    "ax2.imshow(single_voice_batch[0, :200, :100, 0])\n",
    "ax2.set_title('real voice')\n",
    "ax3.imshow(voice_mask[0, :200, :100, 0])\n",
    "ax3.set_title('voice mask before training')\n",
    "ax4.imshow(gen_voice[0, :200, :100, 0])\n",
    "ax4.set_title('generated voice  before training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's train a small number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    _, cost = sess.run([model.train_op, model.cost], {model.is_training: True})\n",
    "    print(\"            , {0}, {1}\".format(i, cost)) #here we had code 'print \"            \", i, cost', it was causing an error\n",
    "                                                    #most likely related to python syntax changes in newer version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gen_voice, voice_mask = sess.run([model.gen_voice, model.voice_mask], {is_training: True, model.mixed: single_mixed_batch})\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,20))\n",
    "\n",
    "ax1.imshow(single_mixed_batch[0, :200, :100, 0])\n",
    "ax1.set_title('mixed')\n",
    "ax2.imshow(single_voice_batch[0, :200, :100, 0])\n",
    "ax2.set_title('real voice')\n",
    "ax3.imshow(voice_mask[0, :200, :100, 0])\n",
    "ax3.set_title('voice mask after 10 train iterations')\n",
    "ax4.imshow(gen_voice[0, :200, :100, 0])\n",
    "ax4.set_title('generated voice after 10 train iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's train a little longer this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    _, cost = sess.run([model.train_op, model.cost], {model.is_training: True})\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(\"            , {0}, {1}\".format(i, cost)) #same code from above 'print \"            \", i, cost', was causing an error\n",
    "print('Finished training {n} batches. Final cost: {c}'.format(n=i+1, c=cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gen_voice, voice_mask = sess.run([model.gen_voice, model.voice_mask], {is_training: False, model.mixed: single_mixed_batch})\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,20))\n",
    "\n",
    "ax1.imshow(single_mixed_batch[0, :200, :100, 0])\n",
    "ax1.set_title('mixed')\n",
    "ax2.imshow(single_voice_batch[0, :200, :100, 0])\n",
    "ax2.set_title('real voice')\n",
    "ax3.imshow(voice_mask[0, :200, :100, 0])\n",
    "ax3.set_title('voice mask after 30 train iterations')\n",
    "ax4.imshow(gen_voice[0, :200, :100, 0])\n",
    "ax4.set_title('generated voice after 30 train iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio_t = read_audio('C:/Users/Toby/Jupyter Notebooks/My Work/MSc Project/LDC2017S24.iso.wav', sample_rate=SAMPLE_RATE, n_channels=N_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spec_t = compute_spectrogram(audio_t, N_FFT, FFT_HOP, N_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch_t = extract_spectrogram_patches(spec_t, N_FFT, N_CHANNELS, PATCH_WINDOW, PATCH_HOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio, spec, patch = sess.run([audio_t, spec_t, patch_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spec.shape)\n",
    "print(patch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_2D_audio(t, f, x, title, vmax=0.2):\n",
    "    amp = 2 * np.sqrt(2)\n",
    "    plt.pcolormesh(t, f, x, vmax=vmax)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Time')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(spec.shape[0])\n",
    "f = np.arange(spec.shape[1])\n",
    "plot_2D_audio(t, f, spec[:,:,0].T, 'Spectrogram', vmax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.arange(patch.shape[1])\n",
    "f = np.arange(patch.shape[2])\n",
    "plot_2D_audio(t, f,patch[2,:,:,0].T, 'Spectrogram Patch', vmax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spectrogramToAudioFile(magnitude, fftWindowSize, hopSize, phaseIterations=10, phase=None, length=None):\n",
    "    '''\n",
    "    Computes an audio signal from the given magnitude spectrogram, and optionally an initial phase.\n",
    "    Griffin-Lim is executed to recover/refine the given the phase from the magnitude spectrogram.\n",
    "    :param magnitude: Magnitudes to be converted to audio\n",
    "    :param fftWindowSize: Size of FFT window used to create magnitudes\n",
    "    :param hopSize: Hop size in frames used to create magnitudes\n",
    "    :param phaseIterations: Number of Griffin-Lim iterations to recover phase\n",
    "    :param phase: If given, starts ISTFT with this particular phase matrix\n",
    "    :param length: If given, audio signal is clipped/padded to this number of frames\n",
    "    :return: \n",
    "    '''\n",
    "    if phase is not None:\n",
    "        if phaseIterations > 0:\n",
    "            # Refine audio given initial phase with a number of iterations\n",
    "            return reconPhase(magnitude, fftWindowSize, hopSize, phaseIterations, phase, length)\n",
    "        # reconstructing the new complex matrix\n",
    "        stftMatrix = magnitude * np.exp(phase * 1j) # magnitude * e^(j*phase)\n",
    "        audio = librosa.istft(stftMatrix, hop_length=hopSize, length=length)\n",
    "    else:\n",
    "        audio = reconPhase(magnitude, fftWindowSize, hopSize, phaseIterations)\n",
    "    return audio\n",
    "\n",
    "def reconPhase(magnitude, fftWindowSize, hopSize, phaseIterations=10, initPhase=None, length=None):\n",
    "    '''\n",
    "    Griffin-Lim algorithm for reconstructing the phase for a given magnitude spectrogram, optionally with a given\n",
    "    intial phase.\n",
    "    :param magnitude: Magnitudes to be converted to audio\n",
    "    :param fftWindowSize: Size of FFT window used to create magnitudes\n",
    "    :param hopSize: Hop size in frames used to create magnitudes\n",
    "    :param phaseIterations: Number of Griffin-Lim iterations to recover phase\n",
    "    :param initPhase: If given, starts reconstruction with this particular phase matrix\n",
    "    :param length: If given, audio signal is clipped/padded to this number of frames\n",
    "    :return: \n",
    "    '''\n",
    "    for i in range(phaseIterations):\n",
    "        if i == 0:\n",
    "            if initPhase is None:\n",
    "                reconstruction = np.random.random_sample(magnitude.shape) + 1j * (2 * np.pi * np.random.random_sample(magnitude.shape) - np.pi)\n",
    "            else:\n",
    "                reconstruction = np.exp(initPhase * 1j) # e^(j*phase), so that angle => phase\n",
    "        else:\n",
    "            reconstruction = librosa.stft(audio, fftWindowSize, hopSize)\n",
    "        spectrum = magnitude * np.exp(1j * np.angle(reconstruction))\n",
    "        if i == phaseIterations - 1:\n",
    "            audio = librosa.istft(spectrum, hopSize, length=length)\n",
    "        else:\n",
    "            audio = librosa.istft(spectrum, hopSize)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "path = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Mixed/F01_22GC010A_BUS.CH1.wav'\n",
    "\n",
    "audio_tensor = read_audio(path, 44100, 1)\n",
    "spec_tensor = compute_spectrogram(audio_tensor, N_FFT, FFT_HOP, N_CHANNELS)\n",
    "spec, audio = sess.run([spec_tensor, audio_tensor])\n",
    "print('Original audio shape: ', audio.shape)\n",
    "print('Spectrogram shape: ', spec.shape)\n",
    "\n",
    "\n",
    "# Split out the magnitude and phase\n",
    "mag = spec[:,:,0]\n",
    "phase = spec[:,:,1]\n",
    "print('Magnitude shape: ', mag.shape)\n",
    "print('Phase shape: ', phase.shape)\n",
    "\n",
    "# Now reconstruct the audio from magnitude and phase\n",
    "new_audio = spectrogramToAudioFile(mag.T, N_FFT, FFT_HOP, phaseIterations=0, phase=phase.T)\n",
    "print('Reconstructed audio shape: ', new_audio.shape)\n",
    "\n",
    "# plot the original wave form with the reconstructed one\n",
    "x0 = np.arange(audio.shape[0])\n",
    "x1 = np.arange(new_audio.shape[0])\n",
    "fig, ax = plt.subplots(2,1,sharex=True)\n",
    "ax[0].plot(x0,audio)\n",
    "ax[0].set_title('Original Waveform')\n",
    "ax[1].plot(x1,new_audio)\n",
    "ax[1].set_title('Reconstructed Waveform')\n",
    "ax[1].set_xlabel('Time [samples]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.expand_dims(new_audio,1).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'F01_22GC010A_BTH.CH1.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FALSE!\n"
     ]
    }
   ],
   "source": [
    "if re.search('CH0',filename):\n",
    "    print('TRUE!')\n",
    "else:\n",
    "    print('FALSE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE!\n"
     ]
    }
   ],
   "source": [
    "if re.search('^((?!CH0).)*$',filename):\n",
    "    print('TRUE!')\n",
    "else:\n",
    "    print('FALSE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = ['CH1', 'CH3', 'CH0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CH1', 'CH3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: re.search('^((?!CH0).)*$',x), test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = [0,1,2,3,4]\n",
    "res = list(filter(lambda x: x>2, nums))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_list = range(-5, 5)\n",
    "less_than_zero = list(filter(lambda x: x < 0, number_list))\n",
    "print(less_than_zero)\n",
    "\n",
    "# Output: [-5, -4, -3, -2, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Voice'\n",
    "dataset = tf.data.Dataset.list_files(data_folder + '/*.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "self_contained_ikala.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
