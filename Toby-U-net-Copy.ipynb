{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Self-Contained Source Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "First let's import libraries and create some convenience functions for our neural network operations with sane default values.\n",
    "\n",
    "Unfortunately there isn't an easy way to hide code blocks in jupyter, so just skip over this :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n",
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\pycochleagram-0.1-py3.6.egg\\pycochleagram\\erbfilter.py:8: RuntimeWarning: pycochleagram using non-interactive Agg matplotlib backend\n",
      "  from pycochleagram import utils\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pycochleagram.cochleagram as cgram\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "plt.rcParams['image.cmap'] = 'hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def concat(x, y):\n",
    "    return tf.concat([x, y], axis=3)\n",
    "\n",
    "\n",
    "def conv(inputs, filters, kernel_size, stride):\n",
    "    out = tf.layers.conv2d(\n",
    "        inputs, filters=filters, kernel_size=kernel_size,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        strides=stride, padding='SAME')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def deconv(inputs, filters, kernel_size, stride):\n",
    "    out = tf.layers.conv2d_transpose(\n",
    "        inputs, filters=filters, kernel_size=kernel_size,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        strides=stride, padding='SAME')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def batch_norm(inputs, is_training, reuse):\n",
    "    return tf.contrib.layers.batch_norm(\n",
    "        inputs,\n",
    "        decay=0.9,\n",
    "        updates_collections=None,\n",
    "        epsilon=1e-5,\n",
    "        scale=True,\n",
    "        is_training=is_training,\n",
    "        reuse=reuse)\n",
    "\n",
    "\n",
    "def dropout(inputs, rate):\n",
    "    return tf.nn.dropout(inputs, keep_prob=1 - rate)\n",
    "\n",
    "\n",
    "def relu(inputs):\n",
    "    return tf.nn.relu(inputs)\n",
    "\n",
    "\n",
    "def tanh(inputs):\n",
    "    return tf.nn.tanh(inputs)\n",
    "\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    with tf.variable_scope('lrelu'):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "\n",
    "def read_audio(path, sample_rate, n_channels):\n",
    "\n",
    "    def read_audio_py(py_path):\n",
    "        #if n_channels == 1:\n",
    "            mono, _ = librosa.load(py_path, sr=sample_rate, mono=True)\n",
    "            return np.expand_dims(mono, 1)\n",
    "        #elif n_channels == 2:\n",
    "            #stereo, _ = librosa.load(py_path, sr=sample_rate, mono=False)\n",
    "            #return stereo.T\n",
    "        #else:\n",
    "            #raise ValueError('Invalid channels: %d' % n_channels)\n",
    "\n",
    "    return tf.py_func(read_audio_py, [path], tf.float32, stateful=False)\n",
    "\n",
    "\n",
    "def fake_stereo(audio):\n",
    "\n",
    "    def fake_stereo(x):\n",
    "        return tf.stack([x, x], 1)\n",
    "    \n",
    "    voice = audio[:, 0]\n",
    "    mixed = voice * 2\n",
    "    return fake_stereo(mixed), fake_stereo(voice)\n",
    "\n",
    "\n",
    "def compute_spectrogram(audio, n_fft, fft_hop, n_channels):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio : single to dual channel audio shaped (n_samples, n_channels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n_frames, 1 + n_fft / 2, n_channels * 2), where the\n",
    "        last dimension is (left_mag, right_mag, left_phase, right_phase)\n",
    "    '''\n",
    "\n",
    "    def stft(x):\n",
    "        spec = librosa.stft(\n",
    "            x, n_fft=n_fft, hop_length=fft_hop, window='hann')\n",
    "        # TODO: normalize?\n",
    "        #mag = np.abs(spec)\n",
    "        #temp = mag - mag.min()\n",
    "        #mag_norm = temp / temp.max()\n",
    "        return np.abs(spec), np.angle(spec)\n",
    "\n",
    "    #def stereo_func(py_audio):\n",
    "    #    left_mag, left_phase = stft(py_audio[:, 0])\n",
    "    #    right_mag, right_phase = stft(py_audio[:, 1])\n",
    "    #    ret = np.array([left_mag, right_mag, left_phase, right_phase]).T\n",
    "    #    return ret.astype(np.float32)\n",
    "\n",
    "    def mono_func(py_audio):\n",
    "        mag, phase = stft(py_audio[:, 0])\n",
    "        ret = np.array([mag, phase]).T\n",
    "        return ret.astype(np.float32)\n",
    "\n",
    "    #if n_channels == 2:\n",
    "    #    func = stereo_func\n",
    "    #elif n_channels == 1:\n",
    "    #    func = mono_func\n",
    "    #else:\n",
    "    #    raise ValueError('Invalid channels: %d' % n_channels)\n",
    "\n",
    "    with tf.name_scope('read_spectrogram'):\n",
    "        ret = tf.py_func(mono_func, [audio], tf.float32, stateful=False)\n",
    "        ret.set_shape([None, 1 + n_fft / 2, 2])   # n_channels * 2])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def extract_spectrogram_patches(\n",
    "        spec, n_fft, n_channels, patch_window, patch_hop):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    spec : Spectrogram of shape (n_frames, 1 + n_fft / 2, n_channels * 2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n_patches, patch_window, 1 + n_fft / 2, n_channels * 2)\n",
    "        containing patches from spec.\n",
    "    '''\n",
    "    with tf.name_scope('extract_spectrogram_patches'):\n",
    "        spec4d = tf.expand_dims(spec, 0)\n",
    "\n",
    "        patches = tf.extract_image_patches(\n",
    "            spec4d, ksizes=[1, patch_window, 1 + n_fft / 2, 1],\n",
    "            strides=[1, patch_hop, 1 + n_fft / 2, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "\n",
    "        num_patches = tf.shape(patches)[1]\n",
    "\n",
    "        return tf.reshape(patches, [num_patches, patch_window,\n",
    "                                    int(1 + n_fft / 2), 2])   # int(n_channels * 2)]) #here was '1 + n_fft / 2, n_channels * 2', \n",
    "                                                                              #it was causing an error in 6th code block\n",
    "\n",
    "def replace_spectrogram_patches(patches):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Tensor of shape (n_patches, patch_window, 1 + n_fft / 2, n_channels * 2)\n",
    "    containing patches from spec.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spec : Spectrogram of shape (n_frames, 1 + n_fft / 2, n_channels * 2)\n",
    "    '''\n",
    "    # Need to account for patch overlap\n",
    "    # Is it possible to account for no set num patches per whole spectrogram?\n",
    "    pass\n",
    "\n",
    "def invert_spectrogram(mag, phase, n_fft, fft_hop):\n",
    "\n",
    "    # Could be a problem if the spec has been normalised\n",
    "    spec = np.array([mag.T, phase.T]).T\n",
    "    wave = librosa.istft(spec, win_length=n_fft, hop_length=fft_hop, window='hann')\n",
    "\n",
    "    return wave\n",
    "\n",
    "def compute_cochleagram(audio, SAMPLE_RATE, low_lim, hi_lim):\n",
    "    coch = cgram.human_cochleagram(audio, SAMPLE_RATE, low_lim, hi_lim, strict=False)\n",
    "    \n",
    "        \n",
    "def hwr_tf(x):\n",
    "    return x * tf.cast(x > 0.0, tf.float32)\n",
    "\n",
    "\n",
    "def compute_acapella_diff(mixed, noise):\n",
    "    mixed_mag = mixed[:, :, 0:, :2]\n",
    "    mixed_phase = mixed[:, :, 0:, 2:]\n",
    "    noise_mag = noise[:, :, 0:, :2]\n",
    "    voice_mag = hwr_tf(mixed_mag - noise_mag) # TODO: normalize?\n",
    "    voice_phase = mixed_phase\n",
    "    return mixed, noise, tf.concat((voice_mag, voice_phase), axis=3)\n",
    "\n",
    "\n",
    "def partial_argv(func, *args, **kwargs):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    func    : A function that takes scalar argument and returns scalar value\n",
    "    *args   : Args to partially apply to func\n",
    "    *kwargs : Keyword args to partially apply to func\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    func(*args) : A function that maps func over *args and returns a tuple\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    func = partial_argv(abs)\n",
    "    func(-1, 2, -3, 4)\n",
    "\n",
    "    # returns (1, 2, 3, 4)\n",
    "    '''\n",
    "    return lambda *other_args: tuple(map(partial(func, *args, **kwargs), other_args))\n",
    "\n",
    "\n",
    "def zip_tensor_slices(*args):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    *args : list of _n_ _k_-dimensional tensors, where _k_ >= 2\n",
    "        The first dimension has _m_ elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : Dataset of _m_ examples, where each example has _n_\n",
    "        records of _k_ - 1 dimensions.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ds = (\n",
    "        tf.data.Dataset.zip((\n",
    "            tf.data.Dataset.from_tensors([[1,2], [3,4], [5, 6]]),\n",
    "            tf.data.Dataset.from_tensors([[10, 20], [30, 40], [50, 60]])\n",
    "        ))\n",
    "        .flat_map(zip_tensor_slices)  # <--- *HERE*\n",
    "    )\n",
    "    el = ds.make_one_shot_iterator().get_next()\n",
    "    print sess.run(el)\n",
    "    print sess.run(el)\n",
    "\n",
    "    # Output:\n",
    "    # (array([1, 2], dtype=int32), array([10, 20], dtype=int32))\n",
    "    # (array([3, 4], dtype=int32), array([30, 40], dtype=int32))\n",
    "    '''\n",
    "    return tf.data.Dataset.zip(tuple([\n",
    "        tf.data.Dataset.from_tensor_slices(arg)\n",
    "        for arg in args\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's define a few constants we'll use throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100\n",
    "N_FFT = 1024\n",
    "FFT_HOP = 256\n",
    "N_CHANNELS = 1   #was N_CHANNELS = 2\n",
    "N_PARALLEL_READERS = 4\n",
    "PATCH_WINDOW = 256\n",
    "PATCH_HOP = 128\n",
    "BATCH_SIZE = 8\n",
    "N_SHUFFLE = 20\n",
    "#low_lim = 50\n",
    "#hi_lim = 5000\n",
    "\n",
    "# With default params, each spectrum represents 0.02s, and hop length is 0.005s (so ~200frames/s)\n",
    "# Each patch is roughly 1.25s long - this is a lot shorter than those used in the music paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## U-Net Model\n",
    "\n",
    "Next, we create our U-Nets, one for noise and one for voice. Each `UNetModel` has a `UNetEncoder` and a `UNetDecoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class UNetModel(object):\n",
    "\n",
    "    def __init__(self, mixed, voice, mixed_phase, is_training):\n",
    "        self.mixed = mixed\n",
    "        self.voice = voice\n",
    "        self.mixed_phase = mixed_phase\n",
    "        self.is_training = is_training\n",
    "\n",
    "        self.voice_mask_unet = UNet(mixed, is_training=is_training, reuse=False, name='voice-mask-unet')\n",
    "\n",
    "        self.voice_mask = self.voice_mask_unet.output\n",
    "\n",
    "        self.gen_voice = self.voice_mask * mixed\n",
    "\n",
    "        self.cost = l1_loss(self.gen_voice, voice)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=0.0002,\n",
    "            beta1=0.5,\n",
    "        )\n",
    "        self.train_op = self.optimizer.minimize(self.cost)\n",
    "\n",
    "\n",
    "class UNet(object):\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse, name):\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "\n",
    "            self.encoder = UNetEncoder(input_tensor, is_training, reuse)\n",
    "            self.decoder = UNetDecoder(self.encoder.output, self.encoder, is_training, reuse)\n",
    "\n",
    "            self.output = tanh(self.decoder.output) / 2 + .5\n",
    "\n",
    "\n",
    "class UNetEncoder(object):\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse):\n",
    "        net = input_tensor\n",
    "        with tf.variable_scope('encoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = conv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                self.l1 = net\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l2 = net\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l3 = net\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l4 = net\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l5 = net\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=512, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n",
    "\n",
    "\n",
    "class UNetDecoder(object):\n",
    "\n",
    "    def __init__(self, input_tensor, encoder, is_training, reuse):\n",
    "        net = input_tensor\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = relu(net)\n",
    "                net = deconv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = relu(concat(net, encoder.l5))\n",
    "                net = deconv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = relu(concat(net, encoder.l4))\n",
    "                net = deconv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = relu(concat(net, encoder.l3))\n",
    "                net = deconv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = relu(concat(net, encoder.l2))\n",
    "                net = deconv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = relu(concat(net, encoder.l1))\n",
    "                net = deconv(net, filters=1, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Data Pipelines\n",
    "\n",
    "Next we define our data reading functions. We make use of [the Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def training_dataset(\n",
    "        data_folder,\n",
    "        sample_rate,\n",
    "        n_fft,\n",
    "        fft_hop,\n",
    "        n_channels,\n",
    "        patch_window,\n",
    "        patch_hop,\n",
    "        #batch_size,\n",
    "        #n_shuffle,\n",
    "        n_parallel_readers\n",
    "):\n",
    "    \"\"\"Still need to fix this to stop it producing a tuple\"\"\"\n",
    "    return (\n",
    "        tf.data.Dataset.list_files(data_folder + '/*.wav')\n",
    "        .map(partial(\n",
    "            read_audio,\n",
    "            sample_rate=sample_rate,\n",
    "            n_channels=n_channels\n",
    "        ), num_parallel_calls=n_parallel_readers)\n",
    "        #.map(fake_stereo, num_parallel_calls=n_parallel_readers)\n",
    "        .map(partial_argv(\n",
    "            compute_spectrogram,\n",
    "            n_fft=n_fft,\n",
    "            fft_hop=fft_hop,\n",
    "            n_channels=n_channels,\n",
    "        ), num_parallel_calls=n_parallel_readers)\n",
    "        .map(partial_argv(\n",
    "            extract_spectrogram_patches,\n",
    "            n_fft=n_fft,\n",
    "            n_channels=n_channels,\n",
    "            patch_window=patch_window,\n",
    "            patch_hop=patch_hop,\n",
    "        ))\n",
    "        .flat_map(zip_tensor_slices)\n",
    "        #.batch(batch_size)\n",
    "        #.shuffle(n_shuffle)\n",
    "        #.repeat()\n",
    "    )\n",
    "\n",
    "def zip_datasets(dataset_a, dataset_b, n_shuffle, batch_size):\n",
    "    return tf.data.Dataset.zip((dataset_a, dataset_b)).batch(batch_size).shuffle(n_shuffle).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next let's create the Tensorflow [Session](https://www.tensorflow.org/programmers_guide/graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can look at some examples from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_folder = 'C:/Users/Toby/Jupyter Notebooks/My Work/MSc Project/Test Audio/Noisy'\n",
    "clean_folder = 'C:/Users/Toby/Jupyter Notebooks/My Work/MSc Project/Test Audio/Clean'\n",
    "mds = training_dataset(mixed_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP,  N_PARALLEL_READERS)\n",
    "cds = training_dataset(clean_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP, N_PARALLEL_READERS)\n",
    "ds = zip_datasets(mds, cds, BATCH_SIZE, N_SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed, voice = ds.make_one_shot_iterator().get_next()\n",
    "(m,), (v,) = sess.run([mixed, voice]) # Bit of a hack here to get around my tuple problem in training_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Here `mixed`, `noise`, and `voise` are [tensors](https://www.tensorflow.org/programmers_guide/tensors), whereas `m`, `n`, and `v` are regular python variables containing the training examples.\n",
    "\n",
    "`m`, `n`, and `v` are all the same shape: (batch\\_size, patch\\_window, n\\_fft + 1, 4)\n",
    "\n",
    "where the last dimension is composed of [left mag, right mag, left phase, right phase]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(4, 2, figsize=(20,20))\n",
    "ax1.imshow(m[0, :, :, 0])\n",
    "ax2.imshow(m[0, :, :, 1])\n",
    "ax3.imshow(v[0, :, :, 0])\n",
    "ax4.imshow(v[0, :, :, 1])\n",
    "ax5.imshow(m[2, :, :, 0])\n",
    "ax6.imshow(m[2, :, :, 1])\n",
    "ax7.imshow(v[2, :, :, 0])\n",
    "ax8.imshow(v[2, :, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have both the model and the data pipeline we can actually train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_mag = mixed[0][:, :, 1:, :2] # Yet more hacking to get around this tuple problem\n",
    "mixed_phase = mixed[0][:, :, 1:, 2:]\n",
    "voice_mag = voice[0][:, :, 1:, :2]\n",
    "\n",
    "model = UNetModel(\n",
    "    mixed_mag,\n",
    "    voice_mag,\n",
    "    mixed_phase,\n",
    "    is_training\n",
    ")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Predictions before training on a random batch from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "single_mixed_batch, single_voice_batch = sess.run([mixed_mag, voice_mag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gen_voice, voice_mask = sess.run([model.gen_voice, model.voice_mask], {is_training: True, model.mixed: single_mixed_batch})\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,20))\n",
    "\n",
    "ax1.imshow(single_mixed_batch[0, :200, :100, 0])\n",
    "ax1.set_title('mixed')\n",
    "ax2.imshow(single_voice_batch[0, :200, :100, 0])\n",
    "ax2.set_title('real voice')\n",
    "ax3.imshow(voice_mask[0, :200, :100, 0])\n",
    "ax3.set_title('voice mask before training')\n",
    "ax4.imshow(gen_voice[0, :200, :100, 0])\n",
    "ax4.set_title('generated voice  before training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's train a small number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    _, cost = sess.run([model.train_op, model.cost], {model.is_training: True})\n",
    "    print(\"            , {0}, {1}\".format(i, cost)) #here we had code 'print \"            \", i, cost', it was causing an error\n",
    "                                                    #most likely related to python syntax changes in newer version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gen_voice, voice_mask = sess.run([model.gen_voice, model.voice_mask], {is_training: True, model.mixed: single_mixed_batch})\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,20))\n",
    "\n",
    "ax1.imshow(single_mixed_batch[0, :200, :100, 0])\n",
    "ax1.set_title('mixed')\n",
    "ax2.imshow(single_voice_batch[0, :200, :100, 0])\n",
    "ax2.set_title('real voice')\n",
    "ax3.imshow(voice_mask[0, :200, :100, 0])\n",
    "ax3.set_title('voice mask after 10 train iterations')\n",
    "ax4.imshow(gen_voice[0, :200, :100, 0])\n",
    "ax4.set_title('generated voice after 10 train iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's train a little longer this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    _, cost = sess.run([model.train_op, model.cost], {model.is_training: True})\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(\"            , {0}, {1}\".format(i, cost)) #same code from above 'print \"            \", i, cost', was causing an error\n",
    "print('Finished training {n} batches. Final cost: {c}'.format(n=i+1, c=cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gen_voice, voice_mask = sess.run([model.gen_voice, model.voice_mask], {is_training: False, model.mixed: single_mixed_batch})\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(20,20))\n",
    "\n",
    "ax1.imshow(single_mixed_batch[0, :200, :100, 0])\n",
    "ax1.set_title('mixed')\n",
    "ax2.imshow(single_voice_batch[0, :200, :100, 0])\n",
    "ax2.set_title('real voice')\n",
    "ax3.imshow(voice_mask[0, :200, :100, 0])\n",
    "ax3.set_title('voice mask after 30 train iterations')\n",
    "ax4.imshow(gen_voice[0, :200, :100, 0])\n",
    "ax4.set_title('generated voice after 30 train iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio_t = read_audio('C:/Users/Toby/Jupyter Notebooks/My Work/MSc Project/LDC2017S24.iso.wav', sample_rate=SAMPLE_RATE, n_channels=N_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spec_t = compute_spectrogram(audio_t, N_FFT, FFT_HOP, N_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patch_t = extract_spectrogram_patches(spec_t, N_FFT, N_CHANNELS, PATCH_WINDOW, PATCH_HOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "audio, spec, patch = sess.run([audio_t, spec_t, patch_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(spec.shape)\n",
    "print(patch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_2D_audio(t, f, x, title, vmax=0.2):\n",
    "    amp = 2 * np.sqrt(2)\n",
    "    plt.pcolormesh(t, f, x, vmax=vmax)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlabel('Time')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.arange(spec.shape[0])\n",
    "f = np.arange(spec.shape[1])\n",
    "plot_2D_audio(t, f, spec[:,:,0].T, 'Spectrogram', vmax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.arange(patch.shape[1])\n",
    "f = np.arange(patch.shape[2])\n",
    "plot_2D_audio(t, f,patch[2,:,:,0].T, 'Spectrogram Patch', vmax=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spectrogramToAudioFile(magnitude, fftWindowSize, hopSize, phaseIterations=10, phase=None, length=None):\n",
    "    '''\n",
    "    Computes an audio signal from the given magnitude spectrogram, and optionally an initial phase.\n",
    "    Griffin-Lim is executed to recover/refine the given the phase from the magnitude spectrogram.\n",
    "    :param magnitude: Magnitudes to be converted to audio\n",
    "    :param fftWindowSize: Size of FFT window used to create magnitudes\n",
    "    :param hopSize: Hop size in frames used to create magnitudes\n",
    "    :param phaseIterations: Number of Griffin-Lim iterations to recover phase\n",
    "    :param phase: If given, starts ISTFT with this particular phase matrix\n",
    "    :param length: If given, audio signal is clipped/padded to this number of frames\n",
    "    :return: \n",
    "    '''\n",
    "    if phase is not None:\n",
    "        if phaseIterations > 0:\n",
    "            # Refine audio given initial phase with a number of iterations\n",
    "            return reconPhase(magnitude, fftWindowSize, hopSize, phaseIterations, phase, length)\n",
    "        # reconstructing the new complex matrix\n",
    "        stftMatrix = magnitude * np.exp(phase * 1j) # magnitude * e^(j*phase)\n",
    "        audio = librosa.istft(stftMatrix, hop_length=hopSize, length=length)\n",
    "    else:\n",
    "        audio = reconPhase(magnitude, fftWindowSize, hopSize, phaseIterations)\n",
    "    return audio\n",
    "\n",
    "def reconPhase(magnitude, fftWindowSize, hopSize, phaseIterations=10, initPhase=None, length=None):\n",
    "    '''\n",
    "    Griffin-Lim algorithm for reconstructing the phase for a given magnitude spectrogram, optionally with a given\n",
    "    intial phase.\n",
    "    :param magnitude: Magnitudes to be converted to audio\n",
    "    :param fftWindowSize: Size of FFT window used to create magnitudes\n",
    "    :param hopSize: Hop size in frames used to create magnitudes\n",
    "    :param phaseIterations: Number of Griffin-Lim iterations to recover phase\n",
    "    :param initPhase: If given, starts reconstruction with this particular phase matrix\n",
    "    :param length: If given, audio signal is clipped/padded to this number of frames\n",
    "    :return: \n",
    "    '''\n",
    "    for i in range(phaseIterations):\n",
    "        if i == 0:\n",
    "            if initPhase is None:\n",
    "                reconstruction = np.random.random_sample(magnitude.shape) + 1j * (2 * np.pi * np.random.random_sample(magnitude.shape) - np.pi)\n",
    "            else:\n",
    "                reconstruction = np.exp(initPhase * 1j) # e^(j*phase), so that angle => phase\n",
    "        else:\n",
    "            reconstruction = librosa.stft(audio, fftWindowSize, hopSize)\n",
    "        spectrum = magnitude * np.exp(1j * np.angle(reconstruction))\n",
    "        if i == phaseIterations - 1:\n",
    "            audio = librosa.istft(spectrum, hopSize, length=length)\n",
    "        else:\n",
    "            audio = librosa.istft(spectrum, hopSize)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "path = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Mixed/F01_22GC010A_BUS.CH1.wav'\n",
    "\n",
    "audio_tensor = read_audio(path, 44100, 1)\n",
    "spec_tensor = compute_spectrogram(audio_tensor, N_FFT, FFT_HOP, N_CHANNELS)\n",
    "spec, audio = sess.run([spec_tensor, audio_tensor])\n",
    "print('Original audio shape: ', audio.shape)\n",
    "print('Spectrogram shape: ', spec.shape)\n",
    "\n",
    "\n",
    "# Split out the magnitude and phase\n",
    "mag = spec[:,:,0]\n",
    "phase = spec[:,:,1]\n",
    "print('Magnitude shape: ', mag.shape)\n",
    "print('Phase shape: ', phase.shape)\n",
    "\n",
    "# Now reconstruct the audio from magnitude and phase\n",
    "new_audio = spectrogramToAudioFile(mag.T, N_FFT, FFT_HOP, phaseIterations=0, phase=phase.T)\n",
    "print('Reconstructed audio shape: ', new_audio.shape)\n",
    "\n",
    "# plot the original wave form with the reconstructed one\n",
    "x0 = np.arange(audio.shape[0])\n",
    "x1 = np.arange(new_audio.shape[0])\n",
    "fig, ax = plt.subplots(2,1,sharex=True)\n",
    "ax[0].plot(x0,audio)\n",
    "ax[0].set_title('Original Waveform')\n",
    "ax[1].plot(x1,new_audio)\n",
    "ax[1].set_title('Reconstructed Waveform')\n",
    "ax[1].set_xlabel('Time [samples]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.expand_dims(new_audio,1).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'F01_22GC010A_BTH.CH1.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FALSE!\n"
     ]
    }
   ],
   "source": [
    "if re.search('CH0',filename):\n",
    "    print('TRUE!')\n",
    "else:\n",
    "    print('FALSE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE!\n"
     ]
    }
   ],
   "source": [
    "if re.search('^((?!CH0).)*$',filename):\n",
    "    print('TRUE!')\n",
    "else:\n",
    "    print('FALSE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = ['CH1', 'CH3', 'CH0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CH1', 'CH3']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: re.search('^((?!CH0).)*$',x), test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = [0,1,2,3,4]\n",
    "res = list(filter(lambda x: x>2, nums))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_list = range(-5, 5)\n",
    "less_than_zero = list(filter(lambda x: x < 0, number_list))\n",
    "print(less_than_zero)\n",
    "\n",
    "# Output: [-5, -4, -3, -2, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Voice'\n",
    "dataset = tf.data.Dataset.list_files(data_folder + '/*.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from Main import ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - UNet_Speech_Separation - Running command 'do_experiment'\n",
      "INFO - UNet_Speech_Separation - Started run with ID \"8\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset\n",
      "Creating model\n",
      "Running initialisation test\n",
      "Starting testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\scipy\\fftpack\\basic.py:159: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  z[index] = x\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing complete. Mean results over test set:\n",
      "Loss: 0.28623372316360474\n",
      "SDR:  -5.765735672649822\n",
      "SIR:  inf\n",
      "SAR:  -5.765735672649822\n",
      "Starting training\n",
      "Epoch 1 finished.\n",
      "Validating\n",
      "Validation check mean loss: 0.23105376958847046\n",
      "Validation loss has improved!\n",
      "Finished requested number of epochs. Training complete.\n",
      "Best validation loss: 0.23105376958847046\n",
      "Checkpoint\n",
      "Starting testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - UNet_Speech_Separation - Completed after 0:01:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing complete. Mean results over test set:\n",
      "Loss: 0.2952589988708496\n",
      "SDR:  -6.633871603900573\n",
      "SIR:  inf\n",
      "SAR:  -6.633871603900573\n",
      "All done!\n",
      "Initial test loss: 0.28623372316360474\n",
      "Final test loss: 0.2952589988708496\n"
     ]
    }
   ],
   "source": [
    "r = ex.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/84569/84569-16\n"
     ]
    }
   ],
   "source": [
    "checkpoint = 'C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/84569/84569-16'\n",
    "restorer = tf.train.import_meta_graph(checkpoint+'.meta')\n",
    "restorer.restore(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Fetch argument 'U_Net_Model/' cannot be interpreted as a Tensor. (\"The name 'U_Net_Model/' refers to an Operation not in the graph.\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    281\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[1;32m--> 282\u001b[1;33m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[0;32m    283\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3589\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3590\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[1;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[0;32m   3649\u001b[0m           raise KeyError(\"The name %s refers to an Operation not in the \"\n\u001b[1;32m-> 3650\u001b[1;33m                          \"graph.\" % repr(name))\n\u001b[0m\u001b[0;32m   3651\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes_by_name\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The name 'U_Net_Model/' refers to an Operation not in the graph.\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9b12d6cbbf31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'U_Net_Model/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[1;32m-> 1120\u001b[1;33m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[0;32m   1121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \"\"\"\n\u001b[0;32m    426\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;31m# Did not find anything.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[0;32m    290\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n\u001b[1;32m--> 292\u001b[1;33m                          'Tensor. (%s)' % (fetch, str(e)))\n\u001b[0m\u001b[0;32m    293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_contraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Fetch argument 'U_Net_Model/' cannot be interpreted as a Tensor. (\"The name 'U_Net_Model/' refers to an Operation not in the graph.\")"
     ]
    }
   ],
   "source": [
    "model = sess.run('U_Net_Model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-1/BatchNorm/beta/Adam (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-1/BatchNorm/beta/Adam_1 (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-1/BatchNorm/gamma/Adam (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-1/BatchNorm/gamma/Adam_1 (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-1/conv2d_transpose/bias/Adam (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-1/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,256,512]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,256,512]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-2/BatchNorm/beta/Adam (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-2/BatchNorm/beta/Adam_1 (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-2/BatchNorm/gamma/Adam (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-2/BatchNorm/gamma/Adam_1 (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-2/conv2d_transpose/bias/Adam (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-2/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,128,512]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,128,512]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-3/BatchNorm/beta/Adam (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-3/BatchNorm/beta/Adam_1 (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-3/BatchNorm/gamma/Adam (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-3/BatchNorm/gamma/Adam_1 (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-3/conv2d_transpose/bias/Adam (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-3/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,64,256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,64,256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-4/BatchNorm/beta/Adam (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-4/BatchNorm/beta/Adam_1 (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-4/BatchNorm/gamma/Adam (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-4/BatchNorm/gamma/Adam_1 (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-4/conv2d_transpose/bias/Adam (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-4/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,32,128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,32,128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-5/BatchNorm/beta/Adam (DT_FLOAT) [16]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-5/BatchNorm/beta/Adam_1 (DT_FLOAT) [16]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-5/BatchNorm/gamma/Adam (DT_FLOAT) [16]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-5/BatchNorm/gamma/Adam_1 (DT_FLOAT) [16]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-5/conv2d_transpose/bias/Adam (DT_FLOAT) [16]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-5/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [16]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,16,64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,16,64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-6/conv2d_transpose/bias/Adam (DT_FLOAT) [1]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-6/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [1]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,1,32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,1,32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-1/conv2d/bias/Adam (DT_FLOAT) [16]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-1/conv2d/bias/Adam_1 (DT_FLOAT) [16]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-1/conv2d/kernel/Adam (DT_FLOAT) [5,5,1,16]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-1/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,1,16]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-2/BatchNorm/beta/Adam (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-2/BatchNorm/beta/Adam_1 (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-2/BatchNorm/gamma/Adam (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-2/BatchNorm/gamma/Adam_1 (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-2/conv2d/bias/Adam (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-2/conv2d/bias/Adam_1 (DT_FLOAT) [32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-2/conv2d/kernel/Adam (DT_FLOAT) [5,5,16,32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-2/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,16,32]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-3/BatchNorm/beta/Adam (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-3/BatchNorm/beta/Adam_1 (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-3/BatchNorm/gamma/Adam (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-3/BatchNorm/gamma/Adam_1 (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-3/conv2d/bias/Adam (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-3/conv2d/bias/Adam_1 (DT_FLOAT) [64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-3/conv2d/kernel/Adam (DT_FLOAT) [5,5,32,64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-3/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,32,64]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-4/BatchNorm/beta/Adam (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-4/BatchNorm/beta/Adam_1 (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-4/BatchNorm/gamma/Adam (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-4/BatchNorm/gamma/Adam_1 (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-4/conv2d/bias/Adam (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-4/conv2d/bias/Adam_1 (DT_FLOAT) [128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-4/conv2d/kernel/Adam (DT_FLOAT) [5,5,64,128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-4/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,64,128]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-5/BatchNorm/beta/Adam (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-5/BatchNorm/beta/Adam_1 (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-5/BatchNorm/gamma/Adam (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-5/BatchNorm/gamma/Adam_1 (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-5/conv2d/bias/Adam (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-5/conv2d/bias/Adam_1 (DT_FLOAT) [256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-5/conv2d/kernel/Adam (DT_FLOAT) [5,5,128,256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-5/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,128,256]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-6/conv2d/bias/Adam (DT_FLOAT) [512]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-6/conv2d/bias/Adam_1 (DT_FLOAT) [512]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-6/conv2d/kernel/Adam (DT_FLOAT) [5,5,256,512]\n",
      "U_Net_Model/U_Net_Model/voice-mask-unet/encoder/layer-6/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,256,512]\n",
      "U_Net_Model/beta1_power (DT_FLOAT) []\n",
      "U_Net_Model/beta2_power (DT_FLOAT) []\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-1/BatchNorm/beta (DT_FLOAT) [256]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-1/BatchNorm/gamma (DT_FLOAT) [256]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-1/BatchNorm/moving_mean (DT_FLOAT) [256]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-1/BatchNorm/moving_variance (DT_FLOAT) [256]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-1/conv2d_transpose/bias (DT_FLOAT) [256]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel (DT_FLOAT) [5,5,256,512]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-2/BatchNorm/beta (DT_FLOAT) [128]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-2/BatchNorm/gamma (DT_FLOAT) [128]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-2/BatchNorm/moving_mean (DT_FLOAT) [128]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-2/BatchNorm/moving_variance (DT_FLOAT) [128]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-2/conv2d_transpose/bias (DT_FLOAT) [128]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel (DT_FLOAT) [5,5,128,512]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-3/BatchNorm/beta (DT_FLOAT) [64]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-3/BatchNorm/gamma (DT_FLOAT) [64]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-3/BatchNorm/moving_mean (DT_FLOAT) [64]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-3/BatchNorm/moving_variance (DT_FLOAT) [64]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-3/conv2d_transpose/bias (DT_FLOAT) [64]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel (DT_FLOAT) [5,5,64,256]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-4/BatchNorm/beta (DT_FLOAT) [32]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-4/BatchNorm/gamma (DT_FLOAT) [32]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-4/BatchNorm/moving_mean (DT_FLOAT) [32]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-4/BatchNorm/moving_variance (DT_FLOAT) [32]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-4/conv2d_transpose/bias (DT_FLOAT) [32]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel (DT_FLOAT) [5,5,32,128]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-5/BatchNorm/beta (DT_FLOAT) [16]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-5/BatchNorm/gamma (DT_FLOAT) [16]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-5/BatchNorm/moving_mean (DT_FLOAT) [16]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-5/BatchNorm/moving_variance (DT_FLOAT) [16]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-5/conv2d_transpose/bias (DT_FLOAT) [16]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel (DT_FLOAT) [5,5,16,64]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-6/conv2d_transpose/bias (DT_FLOAT) [1]\n",
      "U_Net_Model/voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel (DT_FLOAT) [5,5,1,32]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-1/conv2d/bias (DT_FLOAT) [16]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-1/conv2d/kernel (DT_FLOAT) [5,5,1,16]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-2/BatchNorm/beta (DT_FLOAT) [32]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-2/BatchNorm/gamma (DT_FLOAT) [32]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-2/BatchNorm/moving_mean (DT_FLOAT) [32]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-2/BatchNorm/moving_variance (DT_FLOAT) [32]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-2/conv2d/bias (DT_FLOAT) [32]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-2/conv2d/kernel (DT_FLOAT) [5,5,16,32]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-3/BatchNorm/beta (DT_FLOAT) [64]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-3/BatchNorm/gamma (DT_FLOAT) [64]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-3/BatchNorm/moving_mean (DT_FLOAT) [64]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-3/BatchNorm/moving_variance (DT_FLOAT) [64]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-3/conv2d/bias (DT_FLOAT) [64]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-3/conv2d/kernel (DT_FLOAT) [5,5,32,64]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-4/BatchNorm/beta (DT_FLOAT) [128]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-4/BatchNorm/gamma (DT_FLOAT) [128]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-4/BatchNorm/moving_mean (DT_FLOAT) [128]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-4/BatchNorm/moving_variance (DT_FLOAT) [128]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-4/conv2d/bias (DT_FLOAT) [128]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-4/conv2d/kernel (DT_FLOAT) [5,5,64,128]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-5/BatchNorm/beta (DT_FLOAT) [256]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-5/BatchNorm/gamma (DT_FLOAT) [256]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-5/BatchNorm/moving_mean (DT_FLOAT) [256]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-5/BatchNorm/moving_variance (DT_FLOAT) [256]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-5/conv2d/bias (DT_FLOAT) [256]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-5/conv2d/kernel (DT_FLOAT) [5,5,128,256]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-6/conv2d/bias (DT_FLOAT) [512]\n",
      "U_Net_Model/voice-mask-unet/encoder/layer-6/conv2d/kernel (DT_FLOAT) [5,5,256,512]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "# List ALL tensors.\n",
    "print_tensors_in_checkpoint_file(file_name='C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/314463/314463-9', tensor_name='', all_tensors='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Toby\\\\MSc_Project\\\\MScFinalProject\\\\..'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd(),'..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "self_contained_ikala.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
