{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Self-Contained Source Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "First let's import libraries and create some convenience functions for our neural network operations with sane default values.\n",
    "\n",
    "Unfortunately there isn't an easy way to hide code blocks in jupyter, so just skip over this :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "plt.rcParams['image.cmap'] = 'hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def concat(x, y):\n",
    "    return tf.concat([x, y], axis=3)\n",
    "\n",
    "\n",
    "def conv(inputs, filters, kernel_size, stride):\n",
    "    out = tf.layers.conv2d(\n",
    "        inputs, filters=filters, kernel_size=kernel_size,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        strides=stride, padding='SAME')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def deconv(inputs, filters, kernel_size, stride):\n",
    "    out = tf.layers.conv2d_transpose(\n",
    "        inputs, filters=filters, kernel_size=kernel_size,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        strides=stride, padding='SAME')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def batch_norm(inputs, is_training, reuse):\n",
    "    return tf.contrib.layers.batch_norm(\n",
    "        inputs,\n",
    "        decay=0.9,\n",
    "        updates_collections=None,\n",
    "        epsilon=1e-5,\n",
    "        scale=True,\n",
    "        is_training=is_training,\n",
    "        reuse=reuse)\n",
    "\n",
    "\n",
    "def dropout(inputs, rate):\n",
    "    return tf.nn.dropout(inputs, keep_prob=1 - rate)\n",
    "\n",
    "\n",
    "def relu(inputs):\n",
    "    return tf.nn.relu(inputs)\n",
    "\n",
    "\n",
    "def tanh(inputs):\n",
    "    return tf.nn.tanh(inputs)\n",
    "\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    with tf.variable_scope('lrelu'):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "\n",
    "def read_audio(path, sample_rate, n_channels):\n",
    "\n",
    "    def read_audio_py(py_path):\n",
    "        #if n_channels == 1:\n",
    "            mono, _ = librosa.load(py_path, sr=sample_rate, mono=True)\n",
    "            return np.expand_dims(mono, 1)\n",
    "        #elif n_channels == 2:\n",
    "            #stereo, _ = librosa.load(py_path, sr=sample_rate, mono=False)\n",
    "            #return stereo.T\n",
    "        #else:\n",
    "            #raise ValueError('Invalid channels: %d' % n_channels)\n",
    "\n",
    "    return tf.py_func(read_audio_py, [path], tf.float32, stateful=False)\n",
    "\n",
    "\n",
    "def fake_stereo(audio):\n",
    "\n",
    "    def fake_stereo(x):\n",
    "        return tf.stack([x, x], 1)\n",
    "    \n",
    "    voice = audio[:, 0]\n",
    "    mixed = voice * 2\n",
    "    return fake_stereo(mixed), fake_stereo(voice)\n",
    "\n",
    "\n",
    "def compute_spectrogram(audio, n_fft, fft_hop, n_channels):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio : single to dual channel audio shaped (n_samples, n_channels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n_frames, 1 + n_fft / 2, n_channels * 2), where the\n",
    "        last dimension is (left_mag, right_mag, left_phase, right_phase)\n",
    "    '''\n",
    "\n",
    "    def stft(x):\n",
    "        spec = librosa.stft(\n",
    "            x, n_fft=n_fft, hop_length=fft_hop, window='hann')\n",
    "        # TODO: normalize?\n",
    "        #mag = np.abs(spec)\n",
    "        #temp = mag - mag.min()\n",
    "        #mag_norm = temp / temp.max()\n",
    "        return np.abs(spec), np.angle(spec)\n",
    "\n",
    "    #def stereo_func(py_audio):\n",
    "    #    left_mag, left_phase = stft(py_audio[:, 0])\n",
    "    #    right_mag, right_phase = stft(py_audio[:, 1])\n",
    "    #    ret = np.array([left_mag, right_mag, left_phase, right_phase]).T\n",
    "    #    return ret.astype(np.float32)\n",
    "\n",
    "    def mono_func(py_audio):\n",
    "        mag, phase = stft(py_audio[:, 0])\n",
    "        ret = np.array([mag, phase]).T\n",
    "        return ret.astype(np.float32)\n",
    "\n",
    "    #if n_channels == 2:\n",
    "    #    func = stereo_func\n",
    "    #elif n_channels == 1:\n",
    "    #    func = mono_func\n",
    "    #else:\n",
    "    #    raise ValueError('Invalid channels: %d' % n_channels)\n",
    "\n",
    "    with tf.name_scope('read_spectrogram'):\n",
    "        ret = tf.py_func(mono_func, [audio], tf.float32, stateful=False)\n",
    "        ret.set_shape([None, 1 + n_fft / 2, 2])   # n_channels * 2])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def extract_spectrogram_patches(\n",
    "        spec, n_fft, n_channels, patch_window, patch_hop):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    spec : Spectrogram of shape (n_frames, 1 + n_fft / 2, n_channels * 2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n_patches, patch_window, 1 + n_fft / 2, n_channels * 2)\n",
    "        containing patches from spec.\n",
    "    '''\n",
    "    with tf.name_scope('extract_spectrogram_patches'):\n",
    "        spec4d = tf.expand_dims(spec, 0)\n",
    "\n",
    "        patches = tf.extract_image_patches(\n",
    "            spec4d, ksizes=[1, patch_window, 1 + n_fft / 2, 1],\n",
    "            strides=[1, patch_hop, 1 + n_fft / 2, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "\n",
    "        num_patches = tf.shape(patches)[1]\n",
    "\n",
    "        return tf.reshape(patches, [num_patches, patch_window,\n",
    "                                    int(1 + n_fft / 2), 2])   # int(n_channels * 2)]) #here was '1 + n_fft / 2, n_channels * 2', \n",
    "                                                                              #it was causing an error in 6th code block\n",
    "\n",
    "def replace_spectrogram_patches(patches):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Tensor of shape (n_patches, patch_window, 1 + n_fft / 2, n_channels * 2)\n",
    "    containing patches from spec.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spec : Spectrogram of shape (n_frames, 1 + n_fft / 2, n_channels * 2)\n",
    "    '''\n",
    "    # Need to account for patch overlap\n",
    "    # Is it possible to account for no set num patches per whole spectrogram?\n",
    "    pass\n",
    "\n",
    "def invert_spectrogram(mag, phase, n_fft, fft_hop):\n",
    "\n",
    "    # Could be a problem if the spec has been normalised\n",
    "    spec = np.array([mag.T, phase.T]).T\n",
    "    wave = librosa.istft(spec, win_length=n_fft, hop_length=fft_hop, window='hann')\n",
    "\n",
    "    return wave\n",
    "\n",
    "def compute_cochleagram(audio, SAMPLE_RATE, low_lim, hi_lim):\n",
    "    coch = cgram.human_cochleagram(audio, SAMPLE_RATE, low_lim, hi_lim, strict=False)\n",
    "    \n",
    "        \n",
    "def hwr_tf(x):\n",
    "    return x * tf.cast(x > 0.0, tf.float32)\n",
    "\n",
    "\n",
    "def compute_acapella_diff(mixed, noise):\n",
    "    mixed_mag = mixed[:, :, 0:, :2]\n",
    "    mixed_phase = mixed[:, :, 0:, 2:]\n",
    "    noise_mag = noise[:, :, 0:, :2]\n",
    "    voice_mag = hwr_tf(mixed_mag - noise_mag) # TODO: normalize?\n",
    "    voice_phase = mixed_phase\n",
    "    return mixed, noise, tf.concat((voice_mag, voice_phase), axis=3)\n",
    "\n",
    "\n",
    "def partial_argv(func, *args, **kwargs):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    func    : A function that takes scalar argument and returns scalar value\n",
    "    *args   : Args to partially apply to func\n",
    "    *kwargs : Keyword args to partially apply to func\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    func(*args) : A function that maps func over *args and returns a tuple\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    func = partial_argv(abs)\n",
    "    func(-1, 2, -3, 4)\n",
    "\n",
    "    # returns (1, 2, 3, 4)\n",
    "    '''\n",
    "    return lambda *other_args: tuple(map(partial(func, *args, **kwargs), other_args))\n",
    "\n",
    "\n",
    "def zip_tensor_slices(*args):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    *args : list of _n_ _k_-dimensional tensors, where _k_ >= 2\n",
    "        The first dimension has _m_ elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : Dataset of _m_ examples, where each example has _n_\n",
    "        records of _k_ - 1 dimensions.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ds = (\n",
    "        tf.data.Dataset.zip((\n",
    "            tf.data.Dataset.from_tensors([[1,2], [3,4], [5, 6]]),\n",
    "            tf.data.Dataset.from_tensors([[10, 20], [30, 40], [50, 60]])\n",
    "        ))\n",
    "        .flat_map(zip_tensor_slices)  # <--- *HERE*\n",
    "    )\n",
    "    el = ds.make_one_shot_iterator().get_next()\n",
    "    print sess.run(el)\n",
    "    print sess.run(el)\n",
    "\n",
    "    # Output:\n",
    "    # (array([1, 2], dtype=int32), array([10, 20], dtype=int32))\n",
    "    # (array([3, 4], dtype=int32), array([30, 40], dtype=int32))\n",
    "    '''\n",
    "    return tf.data.Dataset.zip(tuple([\n",
    "        tf.data.Dataset.from_tensor_slices(arg)\n",
    "        for arg in args\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's define a few constants we'll use throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100\n",
    "N_FFT = 1024\n",
    "FFT_HOP = 256\n",
    "N_CHANNELS = 1   #was N_CHANNELS = 2\n",
    "N_PARALLEL_READERS = 4\n",
    "PATCH_WINDOW = 256\n",
    "PATCH_HOP = 128\n",
    "BATCH_SIZE = 8\n",
    "N_SHUFFLE = 20\n",
    "#low_lim = 50\n",
    "#hi_lim = 5000\n",
    "\n",
    "# With default params, each spectrum represents 0.02s, and hop length is 0.005s (so ~200frames/s)\n",
    "# Each patch is roughly 1.25s long - this is a lot shorter than those used in the music paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## U-Net Model\n",
    "\n",
    "Next, we create our U-Nets, one for noise and one for voice. Each `UNetModel` has a `UNetEncoder` and a `UNetDecoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class UNetModel(object):\n",
    "\n",
    "    def __init__(self, mixed, voice, mixed_phase, is_training):\n",
    "        self.mixed = mixed\n",
    "        self.voice = voice\n",
    "        self.mixed_phase = mixed_phase\n",
    "        self.is_training = is_training\n",
    "\n",
    "        self.voice_mask_unet = UNet(mixed, is_training=is_training, reuse=False, name='voice-mask-unet')\n",
    "\n",
    "        self.voice_mask = self.voice_mask_unet.output\n",
    "\n",
    "        self.gen_voice = self.voice_mask * mixed\n",
    "\n",
    "        self.cost = l1_loss(self.gen_voice, voice)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=0.0002,\n",
    "            beta1=0.5,\n",
    "        )\n",
    "        self.train_op = self.optimizer.minimize(self.cost)\n",
    "\n",
    "\n",
    "class UNet(object):\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse, name):\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "\n",
    "            self.encoder = UNetEncoder(input_tensor, is_training, reuse)\n",
    "            self.decoder = UNetDecoder(self.encoder.output, self.encoder, is_training, reuse)\n",
    "\n",
    "            self.output = tanh(self.decoder.output) / 2 + .5\n",
    "\n",
    "\n",
    "class UNetEncoder(object):\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse):\n",
    "        net = input_tensor\n",
    "        with tf.variable_scope('encoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = conv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                self.l1 = net\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l2 = net\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l3 = net\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l4 = net\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l5 = net\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=512, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n",
    "\n",
    "\n",
    "class UNetDecoder(object):\n",
    "\n",
    "    def __init__(self, input_tensor, encoder, is_training, reuse):\n",
    "        net = input_tensor\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = relu(net)\n",
    "                net = deconv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = relu(concat(net, encoder.l5))\n",
    "                net = deconv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = relu(concat(net, encoder.l4))\n",
    "                net = deconv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = relu(concat(net, encoder.l3))\n",
    "                net = deconv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = relu(concat(net, encoder.l2))\n",
    "                net = deconv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = relu(concat(net, encoder.l1))\n",
    "                net = deconv(net, filters=1, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Data Pipelines\n",
    "\n",
    "Next we define our data reading functions. We make use of [the Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def training_dataset(\n",
    "        data_folder,\n",
    "        sample_rate,\n",
    "        n_fft,\n",
    "        fft_hop,\n",
    "        n_channels,\n",
    "        patch_window,\n",
    "        patch_hop,\n",
    "        #batch_size,\n",
    "        #n_shuffle,\n",
    "        n_parallel_readers\n",
    "):\n",
    "    \"\"\"Still need to fix this to stop it producing a tuple\"\"\"\n",
    "    return (\n",
    "        tf.data.Dataset.list_files(data_folder + '/*.wav', shuffle=False)\n",
    "        .map(partial(\n",
    "            read_audio,\n",
    "            sample_rate=sample_rate,\n",
    "            n_channels=n_channels\n",
    "        ), num_parallel_calls=n_parallel_readers)\n",
    "        #.map(fake_stereo, num_parallel_calls=n_parallel_readers)\n",
    "        .map(partial_argv(\n",
    "            compute_spectrogram,\n",
    "            n_fft=n_fft,\n",
    "            fft_hop=fft_hop,\n",
    "            n_channels=n_channels,\n",
    "        ), num_parallel_calls=n_parallel_readers)\n",
    "        .map(partial_argv(\n",
    "            extract_spectrogram_patches,\n",
    "            n_fft=n_fft,\n",
    "            n_channels=n_channels,\n",
    "            patch_window=patch_window,\n",
    "            patch_hop=patch_hop,\n",
    "        ))\n",
    "        .flat_map(zip_tensor_slices)\n",
    "        #.batch(batch_size)\n",
    "        #.shuffle(n_shuffle)\n",
    "        #.repeat()\n",
    "    )\n",
    "\n",
    "def zip_datasets(dataset_a, dataset_b, n_shuffle, batch_size):\n",
    "    return tf.data.Dataset.zip((dataset_a, dataset_b)).batch(batch_size).shuffle(n_shuffle).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next let's create the Tensorflow [Session](https://www.tensorflow.org/programmers_guide/graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_folder = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Mixed'\n",
    "clean_folder = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Voice'\n",
    "mds = training_dataset(mixed_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP,  N_PARALLEL_READERS)\n",
    "cds = training_dataset(clean_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP, N_PARALLEL_READERS)\n",
    "ds = zip_datasets(mds, cds, BATCH_SIZE, N_SHUFFLE)\n",
    "mixed, voice = ds.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have both the model and the data pipeline we can actually train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_mag = mixed[0][:, :, 1:, :2] # Yet more hacking to get around this tuple problem\n",
    "mixed_phase = mixed[0][:, :, 1:, 2:]\n",
    "voice_mag = voice[0][:, :, 1:, :2]\n",
    "\n",
    "model = UNetModel(\n",
    "    mixed_mag,\n",
    "    voice_mag,\n",
    "    mixed_phase,\n",
    "    is_training\n",
    ")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's train a small number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            , 0, 0.9017301797866821\n",
      "            , 1, 0.8937149047851562\n",
      "            , 2, 0.9163195490837097\n",
      "            , 3, 0.8991764187812805\n",
      "            , 4, 0.8907873034477234\n",
      "            , 5, 0.8830124139785767\n",
      "            , 6, 0.904699444770813\n",
      "            , 7, 0.8945050239562988\n",
      "            , 8, 0.9005669355392456\n",
      "            , 9, 0.9013681411743164\n",
      "            , 10, 0.8971115350723267\n",
      "            , 11, 0.8839073181152344\n",
      "            , 12, 0.8955399394035339\n",
      "            , 13, 0.886378288269043\n",
      "            , 14, 0.8712484240531921\n",
      "            , 15, 0.8710516691207886\n",
      "            , 16, 0.8755843043327332\n",
      "            , 17, 0.8774926066398621\n",
      "            , 18, 0.876166045665741\n",
      "            , 19, 0.8949783444404602\n",
      "            , 20, 0.8879263997077942\n",
      "            , 21, 0.8847169876098633\n",
      "            , 22, 0.8744929432868958\n",
      "            , 23, 0.8806953430175781\n",
      "            , 24, 0.86963951587677\n",
      "            , 25, 0.8711778521537781\n",
      "            , 26, 0.8654783368110657\n",
      "            , 27, 0.8804380297660828\n",
      "            , 28, 0.875217080116272\n",
      "            , 29, 0.8570481538772583\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    _, cost = sess.run([model.train_op, model.cost], {model.is_training: True})\n",
    "    print(\"            , {0}, {1}\".format(i, cost)) #here we had code 'print \"            \", i, cost', it was causing an error\n",
    "                                                    #most likely related to python syntax changes in newer version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's try to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/Checkpoint_test/Checkpoint_test-29'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2)\n",
    "checkpoint_folder = 'C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/Checkpoint_test/Checkpoint_test'\n",
    "saver.save(sess, checkpoint_folder, global_step=int(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the contenst of the saved checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta1_power (DT_FLOAT) []\n",
      "beta2_power (DT_FLOAT) []\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/beta (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/beta/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/beta/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/gamma (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/gamma/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/gamma/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/moving_mean (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/moving_variance (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/bias (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/bias/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/beta (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/beta/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/beta/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/gamma (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/gamma/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/gamma/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/moving_mean (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/moving_variance (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/bias (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/bias/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel (DT_FLOAT) [5,5,128,512]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,128,512]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,128,512]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/beta (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/beta/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/beta/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/gamma (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/gamma/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/gamma/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/moving_mean (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/moving_variance (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/bias (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/bias/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel (DT_FLOAT) [5,5,64,256]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,64,256]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,64,256]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/beta (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/beta/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/beta/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/gamma (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/gamma/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/gamma/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/moving_mean (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/moving_variance (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/bias (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/bias/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel (DT_FLOAT) [5,5,32,128]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,32,128]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,32,128]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/beta (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/beta/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/beta/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/gamma (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/gamma/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/gamma/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/moving_mean (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/moving_variance (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/bias (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/bias/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel (DT_FLOAT) [5,5,16,64]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,16,64]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,16,64]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/bias (DT_FLOAT) [1]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/bias/Adam (DT_FLOAT) [1]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [1]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel (DT_FLOAT) [5,5,1,32]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,1,32]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,1,32]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/bias (DT_FLOAT) [16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/bias/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/bias/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/kernel (DT_FLOAT) [5,5,2,16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/kernel/Adam (DT_FLOAT) [5,5,2,16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,2,16]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/beta (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/beta/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/beta/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/gamma (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/gamma/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/gamma/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/moving_mean (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/moving_variance (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/bias (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/bias/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/bias/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/kernel (DT_FLOAT) [5,5,16,32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/kernel/Adam (DT_FLOAT) [5,5,16,32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,16,32]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/beta (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/beta/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/beta/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/gamma (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/gamma/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/gamma/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/moving_mean (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/moving_variance (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/bias (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/bias/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/bias/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/kernel (DT_FLOAT) [5,5,32,64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/kernel/Adam (DT_FLOAT) [5,5,32,64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,32,64]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/beta (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/beta/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/beta/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/gamma (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/gamma/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/gamma/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/moving_mean (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/moving_variance (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/bias (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/bias/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/bias/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/kernel (DT_FLOAT) [5,5,64,128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/kernel/Adam (DT_FLOAT) [5,5,64,128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,64,128]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/beta (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/beta/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/beta/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/gamma (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/gamma/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/gamma/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/moving_mean (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/moving_variance (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/bias (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/bias/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/bias/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/kernel (DT_FLOAT) [5,5,128,256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/kernel/Adam (DT_FLOAT) [5,5,128,256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,128,256]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/bias (DT_FLOAT) [512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/bias/Adam (DT_FLOAT) [512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/bias/Adam_1 (DT_FLOAT) [512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/kernel (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/kernel/Adam (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,256,512]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "checkpoint = checkpoint_folder + '-' + str(i)\n",
    "# List ALL tensors.\n",
    "print_tensors_in_checkpoint_file(file_name=checkpoint, tensor_name='', all_tensors='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now delete the variable 'model', close the session and reset the graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and restore the checkpoint in a new session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/Checkpoint_test/Checkpoint_test-29\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "restorer = tf.train.import_meta_graph(checkpoint+'.meta')\n",
    "restorer.restore(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'voice_mask_unet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-bd669e1cb951>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoice_mask_unet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'voice_mask_unet'"
     ]
    }
   ],
   "source": [
    "model.voice_mask_unet.decoder.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "self_contained_ikala.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
