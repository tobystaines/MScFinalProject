{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Self-Contained Source Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "First let's import libraries and create some convenience functions for our neural network operations with sane default values.\n",
    "\n",
    "Unfortunately there isn't an easy way to hide code blocks in jupyter, so just skip over this :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 8]\n",
    "plt.rcParams['image.cmap'] = 'hot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def concat(x, y):\n",
    "    return tf.concat([x, y], axis=3)\n",
    "\n",
    "\n",
    "def conv(inputs, filters, kernel_size, stride):\n",
    "    out = tf.layers.conv2d(\n",
    "        inputs, filters=filters, kernel_size=kernel_size,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        strides=stride, padding='SAME')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def deconv(inputs, filters, kernel_size, stride):\n",
    "    out = tf.layers.conv2d_transpose(\n",
    "        inputs, filters=filters, kernel_size=kernel_size,\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        strides=stride, padding='SAME')\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def batch_norm(inputs, is_training, reuse):\n",
    "    return tf.contrib.layers.batch_norm(\n",
    "        inputs,\n",
    "        decay=0.9,\n",
    "        updates_collections=None,\n",
    "        epsilon=1e-5,\n",
    "        scale=True,\n",
    "        is_training=is_training,\n",
    "        reuse=reuse)\n",
    "\n",
    "\n",
    "def dropout(inputs, rate):\n",
    "    return tf.nn.dropout(inputs, keep_prob=1 - rate)\n",
    "\n",
    "\n",
    "def relu(inputs):\n",
    "    return tf.nn.relu(inputs)\n",
    "\n",
    "\n",
    "def tanh(inputs):\n",
    "    return tf.nn.tanh(inputs)\n",
    "\n",
    "\n",
    "def lrelu(x, leak=0.2):\n",
    "    with tf.variable_scope('lrelu'):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "\n",
    "def l1_loss(x, y):\n",
    "    return tf.reduce_mean(tf.abs(x - y))\n",
    "\n",
    "\n",
    "def read_audio(path, sample_rate, n_channels):\n",
    "\n",
    "    def read_audio_py(py_path):\n",
    "        #if n_channels == 1:\n",
    "            mono, _ = librosa.load(py_path, sr=sample_rate, mono=True)\n",
    "            return np.expand_dims(mono, 1)\n",
    "        #elif n_channels == 2:\n",
    "            #stereo, _ = librosa.load(py_path, sr=sample_rate, mono=False)\n",
    "            #return stereo.T\n",
    "        #else:\n",
    "            #raise ValueError('Invalid channels: %d' % n_channels)\n",
    "\n",
    "    return tf.py_func(read_audio_py, [path], tf.float32, stateful=False)\n",
    "\n",
    "\n",
    "def fake_stereo(audio):\n",
    "\n",
    "    def fake_stereo(x):\n",
    "        return tf.stack([x, x], 1)\n",
    "    \n",
    "    voice = audio[:, 0]\n",
    "    mixed = voice * 2\n",
    "    return fake_stereo(mixed), fake_stereo(voice)\n",
    "\n",
    "\n",
    "def compute_spectrogram(audio, n_fft, fft_hop, n_channels):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio : single to dual channel audio shaped (n_samples, n_channels)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n_frames, 1 + n_fft / 2, n_channels * 2), where the\n",
    "        last dimension is (left_mag, right_mag, left_phase, right_phase)\n",
    "    '''\n",
    "\n",
    "    def stft(x):\n",
    "        spec = librosa.stft(\n",
    "            x, n_fft=n_fft, hop_length=fft_hop, window='hann')\n",
    "        # TODO: normalize?\n",
    "        #mag = np.abs(spec)\n",
    "        #temp = mag - mag.min()\n",
    "        #mag_norm = temp / temp.max()\n",
    "        return np.abs(spec), np.angle(spec)\n",
    "\n",
    "    #def stereo_func(py_audio):\n",
    "    #    left_mag, left_phase = stft(py_audio[:, 0])\n",
    "    #    right_mag, right_phase = stft(py_audio[:, 1])\n",
    "    #    ret = np.array([left_mag, right_mag, left_phase, right_phase]).T\n",
    "    #    return ret.astype(np.float32)\n",
    "\n",
    "    def mono_func(py_audio):\n",
    "        mag, phase = stft(py_audio[:, 0])\n",
    "        ret = np.array([mag, phase]).T\n",
    "        return ret.astype(np.float32)\n",
    "\n",
    "    #if n_channels == 2:\n",
    "    #    func = stereo_func\n",
    "    #elif n_channels == 1:\n",
    "    #    func = mono_func\n",
    "    #else:\n",
    "    #    raise ValueError('Invalid channels: %d' % n_channels)\n",
    "\n",
    "    with tf.name_scope('read_spectrogram'):\n",
    "        ret = tf.py_func(mono_func, [audio], tf.float32, stateful=False)\n",
    "        ret.set_shape([None, 1 + n_fft / 2, 2])   # n_channels * 2])\n",
    "    return ret\n",
    "\n",
    "\n",
    "def extract_spectrogram_patches(\n",
    "        spec, n_fft, n_channels, patch_window, patch_hop):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    spec : Spectrogram of shape (n_frames, 1 + n_fft / 2, n_channels * 2)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tensor of shape (n_patches, patch_window, 1 + n_fft / 2, n_channels * 2)\n",
    "        containing patches from spec.\n",
    "    '''\n",
    "    with tf.name_scope('extract_spectrogram_patches'):\n",
    "        spec4d = tf.expand_dims(spec, 0)\n",
    "\n",
    "        patches = tf.extract_image_patches(\n",
    "            spec4d, ksizes=[1, patch_window, 1 + n_fft / 2, 1],\n",
    "            strides=[1, patch_hop, 1 + n_fft / 2, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding='VALID'\n",
    "        )\n",
    "\n",
    "        num_patches = tf.shape(patches)[1]\n",
    "\n",
    "        return tf.reshape(patches, [num_patches, patch_window,\n",
    "                                    int(1 + n_fft / 2), 2])   # int(n_channels * 2)]) #here was '1 + n_fft / 2, n_channels * 2', \n",
    "                                                                              #it was causing an error in 6th code block\n",
    "\n",
    "def replace_spectrogram_patches(patches):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    Tensor of shape (n_patches, patch_window, 1 + n_fft / 2, n_channels * 2)\n",
    "    containing patches from spec.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    spec : Spectrogram of shape (n_frames, 1 + n_fft / 2, n_channels * 2)\n",
    "    '''\n",
    "    # Need to account for patch overlap\n",
    "    # Is it possible to account for no set num patches per whole spectrogram?\n",
    "    pass\n",
    "\n",
    "def invert_spectrogram(mag, phase, n_fft, fft_hop):\n",
    "\n",
    "    # Could be a problem if the spec has been normalised\n",
    "    spec = np.array([mag.T, phase.T]).T\n",
    "    wave = librosa.istft(spec, win_length=n_fft, hop_length=fft_hop, window='hann')\n",
    "\n",
    "    return wave\n",
    "\n",
    "def compute_cochleagram(audio, SAMPLE_RATE, low_lim, hi_lim):\n",
    "    coch = cgram.human_cochleagram(audio, SAMPLE_RATE, low_lim, hi_lim, strict=False)\n",
    "    \n",
    "        \n",
    "def hwr_tf(x):\n",
    "    return x * tf.cast(x > 0.0, tf.float32)\n",
    "\n",
    "\n",
    "def compute_acapella_diff(mixed, noise):\n",
    "    mixed_mag = mixed[:, :, 0:, :2]\n",
    "    mixed_phase = mixed[:, :, 0:, 2:]\n",
    "    noise_mag = noise[:, :, 0:, :2]\n",
    "    voice_mag = hwr_tf(mixed_mag - noise_mag) # TODO: normalize?\n",
    "    voice_phase = mixed_phase\n",
    "    return mixed, noise, tf.concat((voice_mag, voice_phase), axis=3)\n",
    "\n",
    "\n",
    "def partial_argv(func, *args, **kwargs):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    func    : A function that takes scalar argument and returns scalar value\n",
    "    *args   : Args to partially apply to func\n",
    "    *kwargs : Keyword args to partially apply to func\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    func(*args) : A function that maps func over *args and returns a tuple\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    func = partial_argv(abs)\n",
    "    func(-1, 2, -3, 4)\n",
    "\n",
    "    # returns (1, 2, 3, 4)\n",
    "    '''\n",
    "    return lambda *other_args: tuple(map(partial(func, *args, **kwargs), other_args))\n",
    "\n",
    "\n",
    "def zip_tensor_slices(*args):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    *args : list of _n_ _k_-dimensional tensors, where _k_ >= 2\n",
    "        The first dimension has _m_ elements.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : Dataset of _m_ examples, where each example has _n_\n",
    "        records of _k_ - 1 dimensions.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    ds = (\n",
    "        tf.data.Dataset.zip((\n",
    "            tf.data.Dataset.from_tensors([[1,2], [3,4], [5, 6]]),\n",
    "            tf.data.Dataset.from_tensors([[10, 20], [30, 40], [50, 60]])\n",
    "        ))\n",
    "        .flat_map(zip_tensor_slices)  # <--- *HERE*\n",
    "    )\n",
    "    el = ds.make_one_shot_iterator().get_next()\n",
    "    print sess.run(el)\n",
    "    print sess.run(el)\n",
    "\n",
    "    # Output:\n",
    "    # (array([1, 2], dtype=int32), array([10, 20], dtype=int32))\n",
    "    # (array([3, 4], dtype=int32), array([30, 40], dtype=int32))\n",
    "    '''\n",
    "    return tf.data.Dataset.zip(tuple([\n",
    "        tf.data.Dataset.from_tensor_slices(arg)\n",
    "        for arg in args\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's define a few constants we'll use throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 44100\n",
    "N_FFT = 1024\n",
    "FFT_HOP = 256\n",
    "N_CHANNELS = 1   #was N_CHANNELS = 2\n",
    "N_PARALLEL_READERS = 4\n",
    "PATCH_WINDOW = 256\n",
    "PATCH_HOP = 128\n",
    "BATCH_SIZE = 8\n",
    "N_SHUFFLE = 20\n",
    "#low_lim = 50\n",
    "#hi_lim = 5000\n",
    "\n",
    "# With default params, each spectrum represents 0.02s, and hop length is 0.005s (so ~200frames/s)\n",
    "# Each patch is roughly 1.25s long - this is a lot shorter than those used in the music paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## U-Net Model\n",
    "\n",
    "Next, we create our U-Nets, one for noise and one for voice. Each `UNetModel` has a `UNetEncoder` and a `UNetDecoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class UNetModel(object):\n",
    "\n",
    "    def __init__(self, mixed, voice, mixed_phase, is_training):\n",
    "        self.mixed = mixed\n",
    "        self.voice = voice\n",
    "        self.mixed_phase = mixed_phase\n",
    "        self.is_training = is_training\n",
    "\n",
    "        self.voice_mask_unet = UNet(mixed, is_training=is_training, reuse=False, name='voice-mask-unet')\n",
    "\n",
    "        self.voice_mask = self.voice_mask_unet.output\n",
    "\n",
    "        self.gen_voice = self.voice_mask * mixed\n",
    "\n",
    "        self.cost = l1_loss(self.gen_voice, voice)\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=0.0002,\n",
    "            beta1=0.5,\n",
    "        )\n",
    "        self.train_op = self.optimizer.minimize(self.cost)\n",
    "\n",
    "\n",
    "class UNet(object):\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse, name):\n",
    "        with tf.variable_scope(name, reuse=reuse):\n",
    "\n",
    "            self.encoder = UNetEncoder(input_tensor, is_training, reuse)\n",
    "            self.decoder = UNetDecoder(self.encoder.output, self.encoder, is_training, reuse)\n",
    "\n",
    "            self.output = tanh(self.decoder.output) / 2 + .5\n",
    "\n",
    "\n",
    "class UNetEncoder(object):\n",
    "\n",
    "    def __init__(self, input_tensor, is_training, reuse):\n",
    "        net = input_tensor\n",
    "        with tf.variable_scope('encoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = conv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                self.l1 = net\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l2 = net\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l3 = net\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l4 = net\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                self.l5 = net\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = lrelu(net)\n",
    "                net = conv(net, filters=512, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n",
    "\n",
    "\n",
    "class UNetDecoder(object):\n",
    "\n",
    "    def __init__(self, input_tensor, encoder, is_training, reuse):\n",
    "        net = input_tensor\n",
    "\n",
    "        with tf.variable_scope('decoder'):\n",
    "            with tf.variable_scope('layer-1'):\n",
    "                net = relu(net)\n",
    "                net = deconv(net, filters=256, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-2'):\n",
    "                net = relu(concat(net, encoder.l5))\n",
    "                net = deconv(net, filters=128, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-3'):\n",
    "                net = relu(concat(net, encoder.l4))\n",
    "                net = deconv(net, filters=64, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "                net = dropout(net, .5)\n",
    "\n",
    "            with tf.variable_scope('layer-4'):\n",
    "                net = relu(concat(net, encoder.l3))\n",
    "                net = deconv(net, filters=32, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "\n",
    "            with tf.variable_scope('layer-5'):\n",
    "                net = relu(concat(net, encoder.l2))\n",
    "                net = deconv(net, filters=16, kernel_size=5, stride=(2, 2))\n",
    "                net = batch_norm(net, is_training=is_training, reuse=reuse)\n",
    "\n",
    "            with tf.variable_scope('layer-6'):\n",
    "                net = relu(concat(net, encoder.l1))\n",
    "                net = deconv(net, filters=1, kernel_size=5, stride=(2, 2))\n",
    "\n",
    "            self.output = net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Data Pipelines\n",
    "\n",
    "Next we define our data reading functions. We make use of [the Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def training_dataset(\n",
    "        data_folder,\n",
    "        sample_rate,\n",
    "        n_fft,\n",
    "        fft_hop,\n",
    "        n_channels,\n",
    "        patch_window,\n",
    "        patch_hop,\n",
    "        #batch_size,\n",
    "        #n_shuffle,\n",
    "        n_parallel_readers\n",
    "):\n",
    "    \"\"\"Still need to fix this to stop it producing a tuple\"\"\"\n",
    "    return (\n",
    "        tf.data.Dataset.list_files(data_folder + '/*.wav', shuffle=False)\n",
    "        .map(partial(\n",
    "            read_audio,\n",
    "            sample_rate=sample_rate,\n",
    "            n_channels=n_channels\n",
    "        ), num_parallel_calls=n_parallel_readers)\n",
    "        #.map(fake_stereo, num_parallel_calls=n_parallel_readers)\n",
    "        .map(partial_argv(\n",
    "            compute_spectrogram,\n",
    "            n_fft=n_fft,\n",
    "            fft_hop=fft_hop,\n",
    "            n_channels=n_channels,\n",
    "        ), num_parallel_calls=n_parallel_readers)\n",
    "        .map(partial_argv(\n",
    "            extract_spectrogram_patches,\n",
    "            n_fft=n_fft,\n",
    "            n_channels=n_channels,\n",
    "            patch_window=patch_window,\n",
    "            patch_hop=patch_hop,\n",
    "        ))\n",
    "        .flat_map(zip_tensor_slices)\n",
    "        #.batch(batch_size)\n",
    "        #.shuffle(n_shuffle)\n",
    "        #.repeat()\n",
    "    )\n",
    "\n",
    "def zip_datasets(dataset_a, dataset_b, n_shuffle, batch_size):\n",
    "    return tf.data.Dataset.zip((dataset_a, dataset_b)).batch(batch_size).shuffle(n_shuffle).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next let's create the Tensorflow [Session](https://www.tensorflow.org/programmers_guide/graphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_folder = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Mixed'\n",
    "clean_folder = 'C:/Users/Toby/MSc_Project/Test_Audio/GANdatasetsMini/train_sup/Voice'\n",
    "mds = training_dataset(mixed_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP,  N_PARALLEL_READERS)\n",
    "cds = training_dataset(clean_folder, SAMPLE_RATE, N_FFT, FFT_HOP, N_CHANNELS, PATCH_WINDOW, PATCH_HOP, N_PARALLEL_READERS)\n",
    "ds = zip_datasets(mds, cds, BATCH_SIZE, N_SHUFFLE)\n",
    "mixed, voice = ds.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "Now that we have both the model and the data pipeline we can actually train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "collapsed": true,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "is_training = tf.placeholder(shape=(), dtype=bool)\n",
    "mixed_mag = mixed[0][:, :, 1:, :2] # Yet more hacking to get around this tuple problem\n",
    "mixed_phase = mixed[0][:, :, 1:, 2:]\n",
    "voice_mag = voice[0][:, :, 1:, :2]\n",
    "\n",
    "model = UNetModel(\n",
    "    mixed_mag,\n",
    "    voice_mag,\n",
    "    mixed_phase,\n",
    "    is_training\n",
    ")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's train a small number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=() dtype=bool>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            , 0, 0.9014049768447876\n",
      "            , 1, 0.9061511754989624\n",
      "            , 2, 0.8849630355834961\n",
      "            , 3, 0.8870550394058228\n",
      "            , 4, 0.908058762550354\n",
      "            , 5, 0.8874322772026062\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    _, cost = sess.run([model.train_op, model.cost], {model.is_training: True})\n",
    "    print(\"            , {0}, {1}\".format(i, cost)) #here we had code 'print \"            \", i, cost', it was causing an error\n",
    "                                                    #most likely related to python syntax changes in newer version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's try to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/Checkpoint_test/Checkpoint_test-5'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), write_version=tf.train.SaverDef.V2)\n",
    "checkpoint_folder = 'C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/Checkpoint_test/Checkpoint_test'\n",
    "saver.save(sess, checkpoint_folder, global_step=int(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the contenst of the saved checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta1_power (DT_FLOAT) []\n",
      "beta2_power (DT_FLOAT) []\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/beta (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/beta/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/beta/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/gamma (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/gamma/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/gamma/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/moving_mean (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/BatchNorm/moving_variance (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/bias (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/bias/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/decoder/layer-1/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/beta (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/beta/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/beta/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/gamma (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/gamma/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/gamma/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/moving_mean (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/BatchNorm/moving_variance (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/bias (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/bias/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel (DT_FLOAT) [5,5,128,512]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,128,512]\n",
      "voice-mask-unet/decoder/layer-2/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,128,512]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/beta (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/beta/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/beta/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/gamma (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/gamma/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/gamma/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/moving_mean (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/BatchNorm/moving_variance (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/bias (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/bias/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel (DT_FLOAT) [5,5,64,256]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,64,256]\n",
      "voice-mask-unet/decoder/layer-3/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,64,256]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/beta (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/beta/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/beta/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/gamma (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/gamma/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/gamma/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/moving_mean (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/BatchNorm/moving_variance (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/bias (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/bias/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel (DT_FLOAT) [5,5,32,128]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,32,128]\n",
      "voice-mask-unet/decoder/layer-4/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,32,128]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/beta (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/beta/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/beta/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/gamma (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/gamma/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/gamma/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/moving_mean (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/BatchNorm/moving_variance (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/bias (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/bias/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel (DT_FLOAT) [5,5,16,64]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,16,64]\n",
      "voice-mask-unet/decoder/layer-5/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,16,64]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/bias (DT_FLOAT) [1]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/bias/Adam (DT_FLOAT) [1]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/bias/Adam_1 (DT_FLOAT) [1]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel (DT_FLOAT) [5,5,1,32]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel/Adam (DT_FLOAT) [5,5,1,32]\n",
      "voice-mask-unet/decoder/layer-6/conv2d_transpose/kernel/Adam_1 (DT_FLOAT) [5,5,1,32]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/bias (DT_FLOAT) [16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/bias/Adam (DT_FLOAT) [16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/bias/Adam_1 (DT_FLOAT) [16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/kernel (DT_FLOAT) [5,5,2,16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/kernel/Adam (DT_FLOAT) [5,5,2,16]\n",
      "voice-mask-unet/encoder/layer-1/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,2,16]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/beta (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/beta/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/beta/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/gamma (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/gamma/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/gamma/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/moving_mean (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/BatchNorm/moving_variance (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/bias (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/bias/Adam (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/bias/Adam_1 (DT_FLOAT) [32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/kernel (DT_FLOAT) [5,5,16,32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/kernel/Adam (DT_FLOAT) [5,5,16,32]\n",
      "voice-mask-unet/encoder/layer-2/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,16,32]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/beta (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/beta/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/beta/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/gamma (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/gamma/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/gamma/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/moving_mean (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/BatchNorm/moving_variance (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/bias (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/bias/Adam (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/bias/Adam_1 (DT_FLOAT) [64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/kernel (DT_FLOAT) [5,5,32,64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/kernel/Adam (DT_FLOAT) [5,5,32,64]\n",
      "voice-mask-unet/encoder/layer-3/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,32,64]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/beta (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/beta/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/beta/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/gamma (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/gamma/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/gamma/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/moving_mean (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/BatchNorm/moving_variance (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/bias (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/bias/Adam (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/bias/Adam_1 (DT_FLOAT) [128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/kernel (DT_FLOAT) [5,5,64,128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/kernel/Adam (DT_FLOAT) [5,5,64,128]\n",
      "voice-mask-unet/encoder/layer-4/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,64,128]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/beta (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/beta/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/beta/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/gamma (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/gamma/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/gamma/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/moving_mean (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/BatchNorm/moving_variance (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/bias (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/bias/Adam (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/bias/Adam_1 (DT_FLOAT) [256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/kernel (DT_FLOAT) [5,5,128,256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/kernel/Adam (DT_FLOAT) [5,5,128,256]\n",
      "voice-mask-unet/encoder/layer-5/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,128,256]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/bias (DT_FLOAT) [512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/bias/Adam (DT_FLOAT) [512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/bias/Adam_1 (DT_FLOAT) [512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/kernel (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/kernel/Adam (DT_FLOAT) [5,5,256,512]\n",
      "voice-mask-unet/encoder/layer-6/conv2d/kernel/Adam_1 (DT_FLOAT) [5,5,256,512]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file\n",
    "\n",
    "checkpoint = checkpoint_folder + '-' + str(i)\n",
    "# List ALL tensors.\n",
    "print_tensors_in_checkpoint_file(file_name=checkpoint, tensor_name='', all_tensors='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now delete the variable 'model', close the session and reset the graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "sess.close()\n",
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and restore the checkpoint in a new session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:/Users/Toby/MSc_Project/MScFinalProjectCheckpoints/Checkpoint_test/Checkpoint_test-5\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "#restorer = tf.train.import_meta_graph(checkpoint+'.meta')\n",
    "restorer.restore(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.72174802e-04, -5.28769684e-04,  4.09609405e-04, -3.05528374e-04,\n",
       "       -1.40330696e-04, -3.98728094e-04,  9.28305264e-04, -4.39357595e-04,\n",
       "       -3.80593759e-04, -2.81086192e-04, -6.32346608e-04, -1.36023853e-04,\n",
       "        1.93302025e-04,  6.68708526e-04,  5.52889425e-04, -1.40083415e-04,\n",
       "        7.00607081e-04,  2.71814351e-04, -4.34743357e-04, -1.92980879e-05,\n",
       "        5.28124627e-04,  1.33599096e-05, -2.72065168e-04, -1.94654145e-04,\n",
       "       -2.81259534e-04, -5.89492702e-05, -3.60130158e-04, -8.89658913e-05,\n",
       "       -2.53801059e-04, -1.02392631e-04,  1.87109399e-05, -5.01169532e-04,\n",
       "        7.09555679e-05,  1.20294462e-04,  3.34152137e-04, -3.48195201e-04,\n",
       "        2.57933279e-04,  6.15494937e-05, -2.85425165e-04, -7.93815590e-04,\n",
       "        3.39562277e-04, -2.36640990e-05,  6.93324429e-04,  6.47780951e-04,\n",
       "        1.44117279e-04, -4.34764166e-04, -3.43337480e-04,  3.85374471e-04,\n",
       "        2.26988210e-04, -9.88403626e-05, -3.54940974e-04,  7.57644477e-04,\n",
       "        6.32583629e-04,  2.03679811e-04, -2.70984783e-05,  5.69607015e-04,\n",
       "        3.40217550e-04,  2.91814038e-04,  2.44458584e-04, -5.48434909e-04,\n",
       "        6.39060163e-05,  5.27019030e-04, -1.02476508e-04,  7.47059676e-06,\n",
       "        8.05171323e-04, -2.03618561e-04, -1.38897027e-04, -9.33335163e-04,\n",
       "       -8.76189879e-05,  5.17837179e-04, -1.51121174e-04, -3.84971761e-04,\n",
       "       -3.20924795e-04,  7.47315469e-04, -1.59093121e-04,  5.61046254e-05,\n",
       "       -9.82044658e-05,  3.71989823e-04, -1.81097639e-04,  8.51613004e-06,\n",
       "       -4.84438700e-04, -4.88112273e-05,  3.34340235e-04,  2.74738180e-04,\n",
       "        1.85032783e-04,  5.06734767e-04, -7.08807842e-04,  6.70120877e-04,\n",
       "        1.77863345e-04,  2.70639546e-04, -5.17113716e-04, -2.60336907e-04,\n",
       "       -4.07167943e-04, -2.06474360e-04,  4.87935758e-05,  2.25190917e-04,\n",
       "        8.00278503e-04,  1.84082732e-04,  2.69242591e-05,  6.89043954e-04,\n",
       "        4.01122699e-04, -3.58914956e-04, -2.45088217e-04,  2.67872441e-04,\n",
       "       -1.49711152e-04,  2.05082295e-04,  8.72145683e-05, -1.02426551e-04,\n",
       "       -1.60908094e-05, -7.90149934e-05, -5.95834208e-05, -3.71077767e-04,\n",
       "        7.49506944e-05, -2.83340225e-04, -4.60584590e-04, -3.97158728e-04,\n",
       "       -3.57332639e-04, -1.75865789e-04, -3.11021984e-04,  2.58829037e-04,\n",
       "        1.29755586e-04,  3.70535912e-04,  3.45313048e-04, -2.36885317e-05,\n",
       "       -3.02841945e-04,  5.60128305e-04, -4.54376968e-05,  1.45752601e-05,\n",
       "       -7.56182417e-05,  2.57028063e-04,  7.19985343e-04,  1.91610481e-04,\n",
       "        3.99858691e-04,  7.64538476e-04, -5.98261831e-04,  3.39845661e-04,\n",
       "        2.88887270e-04, -3.39909129e-05, -2.77033512e-04, -1.33702240e-04,\n",
       "       -5.09756865e-05,  3.61287908e-04,  2.25971395e-04,  1.52564637e-04,\n",
       "        1.91649655e-04, -6.34738826e-04,  1.06441941e-04,  1.61575037e-04,\n",
       "        1.81404903e-04, -9.73826900e-05,  6.63351617e-04,  4.88482649e-04,\n",
       "       -1.04587580e-05,  2.13129650e-04, -5.39676577e-04, -4.23196238e-04,\n",
       "       -5.12658851e-04,  9.01336534e-05, -5.54078782e-04, -7.41776021e-04,\n",
       "       -2.90260767e-04,  1.33782989e-04, -6.25713263e-04, -3.51530092e-04,\n",
       "        1.77255366e-04, -5.01172268e-04, -2.88119365e-04, -2.98004277e-04,\n",
       "       -1.55695452e-04,  2.75284547e-04, -9.45661886e-05,  3.15553916e-04,\n",
       "       -3.10474366e-04,  5.11982304e-04, -7.37193914e-04,  6.44474407e-04,\n",
       "        4.77273250e-04,  8.34239123e-04,  2.61460111e-04, -4.72534011e-04,\n",
       "       -2.95757287e-04, -3.25005851e-04, -3.45080858e-04, -5.88024210e-04,\n",
       "        6.57168566e-04,  5.26902499e-04, -3.06560163e-04,  7.80805247e-04,\n",
       "       -3.88843997e-04,  6.08779083e-04,  4.88158767e-05,  3.50877701e-04,\n",
       "        1.81103824e-05,  7.63818316e-05, -7.44099088e-06,  2.04021286e-04,\n",
       "        7.70276238e-05,  1.07111126e-04, -3.74368916e-04, -6.58879115e-04,\n",
       "        5.54547703e-04,  1.81517869e-04,  5.62313944e-04,  4.76373709e-04,\n",
       "       -3.21337197e-04,  7.25241116e-05, -4.86248930e-04,  7.36540824e-04,\n",
       "        4.72551910e-04, -8.57315143e-04, -2.92374782e-04, -2.89401185e-04,\n",
       "        2.86023424e-04,  3.48963775e-04, -3.02545697e-04, -3.10741802e-04,\n",
       "       -5.82091976e-04,  8.88453869e-05, -2.68551434e-04,  1.58705036e-04,\n",
       "       -1.91387313e-04,  4.56973430e-05, -9.49108740e-04,  3.53861571e-04,\n",
       "       -2.16164917e-04, -1.41188852e-04,  5.84095033e-05,  3.56897566e-04,\n",
       "       -9.09512164e-05,  2.67833093e-04, -2.26213568e-04, -2.60079163e-04,\n",
       "       -8.73280587e-05, -1.13894130e-04,  3.00568383e-04,  1.34735295e-04,\n",
       "        1.63985154e-04,  4.26549814e-04, -4.95676104e-05,  2.86199152e-04,\n",
       "        7.50155828e-04, -3.77812248e-05, -5.38105494e-04, -6.39515114e-04,\n",
       "       -2.29242767e-04,  3.25900764e-05, -3.33615986e-04,  4.21895762e-04,\n",
       "        3.95556926e-05, -3.34660115e-04,  2.78536390e-05, -4.18501004e-04,\n",
       "       -2.26404838e-04,  5.04311582e-04,  4.13278758e-04,  4.48411884e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run('voice-mask-unet/decoder/layer-1/BatchNorm/beta:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'train_op'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-8e700babc5d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'train_op'"
     ]
    }
   ],
   "source": [
    "model.train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "name": "self_contained_ikala.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
